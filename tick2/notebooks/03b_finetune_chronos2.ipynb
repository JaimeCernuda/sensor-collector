{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChronoTick 2: Chronos-2 Fine-Tuning\n",
    "\n",
    "Fine-tune Amazon Chronos-2 Small (28M) using LoRA and full fine-tuning.\n",
    "Supports covariates via past_covariates and future_covariates dicts.\n",
    "\n",
    "## Experiments\n",
    "- E1: LoRA FT on univariate drift only\n",
    "- E2: LoRA FT with sensor covariates\n",
    "- E3: Full FT vs LoRA comparison\n",
    "\n",
    "## Training Mode\n",
    "Set `TRAINING_MODE` to \"combined\" (all 4 machines) or \"per_machine\"."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    REPO_DIR = \"/content/sensor-collector\"\n",
    "    REPO_URL = \"https://github.com/JaimeCernuda/sensor-collector.git\"\n",
    "    GITHUB_TOKEN = None\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        GITHUB_TOKEN = userdata.get(\"GITHUB_TOKEN\")\n",
    "    except Exception:\n",
    "        print(\"WARNING: GITHUB_TOKEN not available\")\n",
    "\n",
    "    auth_url = (\n",
    "        f\"https://{GITHUB_TOKEN}@github.com/JaimeCernuda/sensor-collector.git\"\n",
    "        if GITHUB_TOKEN\n",
    "        else REPO_URL\n",
    "    )\n",
    "\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        subprocess.run(\n",
    "            [\"git\", \"-C\", REPO_DIR, \"remote\", \"set-url\", \"origin\", auth_url],\n",
    "            check=True,\n",
    "        )\n",
    "        subprocess.run(\n",
    "            [\"git\", \"-C\", REPO_DIR, \"fetch\", \"-q\", \"origin\"], check=True\n",
    "        )\n",
    "        subprocess.run(\n",
    "            [\"git\", \"-C\", REPO_DIR, \"reset\", \"--hard\", \"origin/main\"],\n",
    "            check=True,\n",
    "        )\n",
    "    else:\n",
    "        subprocess.run(\n",
    "            [\"git\", \"clone\", \"-q\", auth_url, REPO_DIR], check=True\n",
    "        )\n",
    "\n",
    "    subprocess.run(\n",
    "        [\"git\", \"-C\", REPO_DIR, \"config\", \"user.name\", \"Colab Runner\"],\n",
    "        check=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"git\", \"-C\", REPO_DIR, \"config\", \"user.email\", \"colab@chronotick.dev\"],\n",
    "        check=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"pip\", \"install\", \"-q\", \"-e\", f\"{REPO_DIR}/tick2/\"], check=True\n",
    "    )\n",
    "\n",
    "    tick2_src = f\"{REPO_DIR}/tick2/src\"\n",
    "    if tick2_src not in sys.path:\n",
    "        sys.path.insert(0, tick2_src)\n",
    "\n",
    "    # Always mount Drive — needed for checkpoint persistence (models too large for git)\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    # Data: prefer repo copy, fall back to Drive\n",
    "    DATA_DIR = f\"{REPO_DIR}/sensors/data\"\n",
    "    if not os.path.isdir(f\"{DATA_DIR}/24h_snapshot\"):\n",
    "        DATA_DIR = \"/content/drive/MyDrive/chronotick2/data\"\n",
    "\n",
    "    RESULTS_DIR = f\"{REPO_DIR}/tick2/notebooks/output/03\"\n",
    "else:\n",
    "    GITHUB_TOKEN = None\n",
    "    DATA_DIR = None\n",
    "    RESULTS_DIR = os.path.join(\n",
    "        os.path.dirname(\"__file__\") if \"__file__\" in dir() else \".\",\n",
    "        \"output\",\n",
    "        \"03\",\n",
    "    )\n",
    "\n",
    "DEVICE_DIR_MAP = {\"cuda\": \"gpu\", \"cpu\": \"cpu\"}\n",
    "\n",
    "\n",
    "def checkpoint_push(label):\n",
    "    \"\"\"Git add, commit, and push results after a step completes.\"\"\"\n",
    "    if not IN_COLAB:\n",
    "        return\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"git\", \"-C\", REPO_DIR, \"add\", \"tick2/notebooks/output/03/\"],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        status = subprocess.run(\n",
    "            [\n",
    "                \"git\", \"-C\", REPO_DIR, \"status\", \"--porcelain\",\n",
    "                \"tick2/notebooks/output/03/\",\n",
    "            ],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "        if not status.stdout.strip():\n",
    "            return\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"git\", \"-C\", REPO_DIR, \"commit\", \"-m\",\n",
    "                f\"results: notebook 03b chronos2 {label} ({device_label})\",\n",
    "            ],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        if GITHUB_TOKEN:\n",
    "            subprocess.run([\"git\", \"-C\", REPO_DIR, \"fetch\", \"-q\", \"origin\"], capture_output=True, timeout=30)\n",
    "            subprocess.run([\"git\", \"-C\", REPO_DIR, \"rebase\", \"origin/main\"], capture_output=True, timeout=30)\n",
    "            subprocess.run(\n",
    "                [\"git\", \"-C\", REPO_DIR, \"push\"],\n",
    "                check=True,\n",
    "                capture_output=True,\n",
    "                timeout=60,\n",
    "            )\n",
    "            print(f\"  [CHECKPOINT] Pushed {label}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [CHECKPOINT WARNING] {e}\")\n",
    "\n",
    "\n",
    "print(f\"Environment: {'Colab' if IN_COLAB else 'Local'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Install Chronos-2 dependencies ===\nif IN_COLAB:\n    subprocess.run([\"pip\", \"install\", \"-q\", \"chronos-forecasting[extras]>=2.2\"], check=True)\n\nfrom chronos import BaseChronosPipeline\n\nprint(\"chronos-forecasting ready\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === Imports, Config & Training Mode ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from tick2.data.preprocessing import TARGET_COL, load_all\n",
    "from tick2.finetuning.base import FineTuneConfig\n",
    "from tick2.finetuning.data_prep import prepare_datasets\n",
    "from tick2.finetuning.chronos2_ft import finetune_chronos2, load_finetuned_chronos2\n",
    "from tick2.finetuning.evaluate import (\n",
    "    evaluate_finetuned,\n",
    "    load_zero_shot_baselines,\n",
    "    compare_ft_vs_zero_shot,\n",
    ")\n",
    "from tick2.utils.gpu import clear_gpu_memory\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# --- Training configuration ---\n",
    "# TRAINING_MODE: \"combined\" = train on all 4 machines, \"per_machine\" = separate\n",
    "TRAINING_MODE = \"combined\"\n",
    "# DEVICE_OVERRIDE: None = auto-detect, \"cuda\" = force GPU, \"cpu\" = force CPU\n",
    "DEVICE_OVERRIDE = None\n",
    "# FORCE_RETRAIN: re-run fine-tuning even if cached results exist\n",
    "FORCE_RETRAIN = False\n",
    "# FINETUNE_MODE: \"lora\" = LoRA adapter, \"full\" = full parameter fine-tuning\n",
    "FINETUNE_MODE = \"lora\"  # \"lora\" or \"full\"\n",
    "\n",
    "device = DEVICE_OVERRIDE or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_label = DEVICE_DIR_MAP.get(device, device)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    vram = getattr(props, \"total_memory\", getattr(props, \"total_mem\", 0))\n",
    "    print(f\"GPU:  {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {vram / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "config = FineTuneConfig(\n",
    "    context_length=2048,\n",
    "    prediction_length=96,\n",
    "    max_covariates=30,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Device: {device}, Mode: {TRAINING_MODE}, FT: {FINETUNE_MODE}\")\n",
    "print(f\"Context: {config.context_length}, Prediction: {config.prediction_length}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === Load and Prepare Data ===\n",
    "data_dir = Path(DATA_DIR) if DATA_DIR else None\n",
    "prepared = prepare_datasets(config, data_dir=data_dir)\n",
    "\n",
    "for name, p in prepared.items():\n",
    "    print(\n",
    "        f\"  {name:16s}: train={len(p.split.train)}, \"\n",
    "        f\"val={len(p.split.val)}, test={len(p.split.test)}, \"\n",
    "        f\"features={len(p.feature_cols)}\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Fine-Tune Chronos-2 ===\n# NOTE: Chronos-2's pipeline.fit() is opaque — no epoch-level checkpoint saves\n# are exposed. If the Colab runtime disconnects mid-training, progress is lost.\n# Consider using Colab Pro for persistent runtimes when training Chronos-2, or\n# reduce num_steps to complete training within the free-tier timeout.\n\nfrom tick2.utils.colab import save_checkpoint_to_drive, load_checkpoint_from_drive, setup_training_log\n\noutput_base = Path(RESULTS_DIR)\nft_output_dir = output_base / \"chronos2_ft\" / TRAINING_MODE\ndevice_results_dir = output_base / device_label\n\n# Persist training logs to disk (timing, errors, warnings)\nlog_path = setup_training_log(ft_output_dir)\nprint(f\"Training log: {log_path}\")\n\ncached_path = device_results_dir / f\"chronos2-ft-{FINETUNE_MODE}_{TRAINING_MODE}.csv\"\n\n# Check for existing Drive checkpoint (resume after disconnect)\ndrive_model_name = f\"chronos2_ft/{TRAINING_MODE}/combined_best\"\nckpt_local = ft_output_dir / \"combined\" / \"combined_best\"\nif not cached_path.exists() and not FORCE_RETRAIN and not ckpt_local.exists():\n    resumed = load_checkpoint_from_drive(\n        model_name=drive_model_name,\n        local_path=str(ckpt_local),\n    )\n    if resumed:\n        print(f\"[RESUMED] Loaded checkpoint from Drive: {resumed}\")\n\nif cached_path.exists() and not FORCE_RETRAIN:\n    print(f\"[CACHED] {cached_path}\")\nelif ckpt_local.exists() and not FORCE_RETRAIN:\n    print(f\"[CACHED] Checkpoint exists at {ckpt_local}, skipping training\")\nelse:\n    clear_gpu_memory()\n    ft_results = finetune_chronos2(\n        prepared=prepared,\n        config=config,\n        output_dir=str(ft_output_dir),\n        training_mode=TRAINING_MODE,\n        finetune_mode=FINETUNE_MODE,\n        with_covariates=True,\n        learning_rate=1e-5 if FINETUNE_MODE == \"lora\" else 1e-6,\n        num_steps=1000,\n        batch_size=256,\n        device_map=device,\n    )\n    for r in ft_results:\n        print(\n            f\"  {r.machine}: {r.training_time_s:.1f}s, \"\n            f\"ckpt={r.checkpoint_path}\"\n        )\n\n    # Save checkpoint to Drive for persistence\n    save_checkpoint_to_drive(\n        local_path=ft_output_dir / \"combined\",\n        model_name=f\"chronos2_ft/{TRAINING_MODE}/combined_best\",\n    )\n\n    checkpoint_push(\"finetuning\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# === Evaluate Fine-Tuned Model ===\nfrom tick2.models.chronos2 import Chronos2Wrapper\n\nif cached_path.exists() and not FORCE_RETRAIN:\n    ft_eval_df = pd.read_csv(cached_path)\n    print(f\"Loaded cached evaluation: {len(ft_eval_df)} rows\")\nelse:\n    # Determine checkpoint path based on training mode\n    if TRAINING_MODE == \"combined\":\n        ckpt_path = ft_output_dir / \"combined\" / \"combined_best\"\n        if not ckpt_path.exists():\n            # Fall back to parent directory if Chronos saved differently\n            ckpt_path = ft_output_dir / \"combined\"\n    else:\n        ckpt_path = ft_output_dir\n\n    ft_pipeline = load_finetuned_chronos2(str(ckpt_path), device_map=device)\n\n    # Create a wrapper that uses the fine-tuned pipeline\n    ft_wrapper = Chronos2Wrapper(\n        model_id=str(ckpt_path), model_name=f\"chronos2-ft-{FINETUNE_MODE}\"\n    )\n    ft_wrapper._pipeline = ft_pipeline\n    ft_wrapper._device = device\n\n    ft_eval_df = evaluate_finetuned(\n        model=ft_wrapper,\n        prepared=prepared,\n        config=config,\n        training_mode=f\"ft_{TRAINING_MODE}\",\n    )\n\n    device_results_dir.mkdir(parents=True, exist_ok=True)\n    ft_eval_df.to_csv(cached_path, index=False)\n    checkpoint_push(\"evaluation\")\n\nprint(f\"Evaluation rows: {len(ft_eval_df)}\")\nprint(f\"Mean MAE:  {ft_eval_df['mae'].mean():.4f}\")\nprint(f\"Mean RMSE: {ft_eval_df['rmse'].mean():.4f}\")\nif ft_eval_df[\"coverage\"].notna().any():\n    print(f\"Mean Coverage: {ft_eval_df['coverage'].mean():.1%}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === Load Zero-Shot Baselines ===\n",
    "zs_dir = output_base.parent / \"output\" / \"02\"\n",
    "zs_results = load_zero_shot_baselines(zs_dir, model_name=\"chronos2-small\")\n",
    "print(f\"Zero-shot baselines: {len(zs_results)} rows\")\n",
    "if not zs_results.empty:\n",
    "    print(f\"  Mean MAE: {zs_results['mae'].mean():.4f}\")\n",
    "    print(f\"  Machines: {zs_results['machine'].unique().tolist()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === Comparison: Fine-Tuned vs Zero-Shot ===\n",
    "combined = compare_ft_vs_zero_shot(ft_eval_df, zs_results)\n",
    "\n",
    "# Print per-machine improvement\n",
    "print(\"Per-machine improvement (FT vs zero-shot):\")\n",
    "print(\"-\" * 60)\n",
    "for machine in sorted(combined[\"machine\"].unique()):\n",
    "    m_df = combined[combined[\"machine\"] == machine]\n",
    "    group_cols = [\"context_length\", \"horizon\"]\n",
    "    available_cols = [c for c in group_cols if c in m_df.columns]\n",
    "    if not available_cols:\n",
    "        continue\n",
    "    for _, group in m_df.groupby(available_cols):\n",
    "        zs = group[group[\"training_mode\"] == \"zero_shot\"][\"mae\"].values\n",
    "        ft = group[group[\"training_mode\"] != \"zero_shot\"][\"mae\"].values\n",
    "        if len(zs) > 0 and len(ft) > 0:\n",
    "            improvement = (zs[0] - ft[0]) / zs[0] * 100\n",
    "            ctx = group[\"context_length\"].iloc[0]\n",
    "            hz = group[\"horizon\"].iloc[0]\n",
    "            print(\n",
    "                f\"  {machine:16s} ctx={ctx:5d} hz={hz:4d}: \"\n",
    "                f\"{zs[0]:.4f} -> {ft[0]:.4f} ({improvement:+.1f}%)\"\n",
    "            )\n",
    "\n",
    "print(f\"\\nCombined results: {len(combined)} rows\")\n",
    "if IN_COLAB:\n",
    "    display(combined)\n",
    "else:\n",
    "    print(combined.to_string())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === Visualizations ===\n",
    "results_dir = Path(RESULTS_DIR)\n",
    "fig_dir = results_dir / \"figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not combined.empty:\n",
    "    # --- 1. MAE Comparison: FT vs Zero-Shot by Machine ---\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    comparison_data = combined.groupby([\"machine\", \"training_mode\"])[\"mae\"].mean().unstack()\n",
    "    comparison_data.plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_ylabel(\"MAE (ppm)\")\n",
    "    ax.set_title(f\"Chronos-2 Fine-Tuned ({FINETUNE_MODE}) vs Zero-Shot\")\n",
    "    ax.legend(title=\"Training Mode\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\n",
    "        fig_dir / f\"mae_ft_vs_zs_{FINETUNE_MODE}.png\", dpi=150, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # --- 2. MAE by Context Length: FT vs Zero-Shot ---\n",
    "    if len(combined[\"context_length\"].unique()) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        sns.lineplot(\n",
    "            data=combined,\n",
    "            x=\"context_length\",\n",
    "            y=\"mae\",\n",
    "            hue=\"training_mode\",\n",
    "            style=\"training_mode\",\n",
    "            markers=True,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_xlabel(\"Context Length (timesteps)\")\n",
    "        ax.set_ylabel(\"MAE (ppm)\")\n",
    "        ax.set_title(\"MAE vs Context Length (FT vs Zero-Shot)\")\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(\n",
    "            fig_dir / f\"ctx_sensitivity_{FINETUNE_MODE}.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    # --- 3. Coverage Comparison (if available) ---\n",
    "    if combined[\"coverage\"].notna().any():\n",
    "        cov_data = combined[combined[\"coverage\"].notna()]\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        cov_pivot = cov_data.groupby(\n",
    "            [\"machine\", \"training_mode\"]\n",
    "        )[\"coverage\"].mean().unstack()\n",
    "        cov_pivot.plot(kind=\"bar\", ax=ax)\n",
    "        ax.set_ylabel(\"Coverage (proportion)\")\n",
    "        ax.set_title(\"Prediction Interval Coverage (FT vs Zero-Shot)\")\n",
    "        ax.axhline(y=0.8, color=\"r\", linestyle=\"--\", alpha=0.5, label=\"Target (80%)\")\n",
    "        ax.legend(title=\"Training Mode\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(\n",
    "            fig_dir / f\"coverage_{FINETUNE_MODE}.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    # --- 4. Inference Latency Comparison ---\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    latency_data = combined.groupby(\"training_mode\")[\"inference_ms\"].mean().sort_values()\n",
    "    latency_data.plot(kind=\"barh\", ax=ax, color=\"steelblue\")\n",
    "    ax.set_xlabel(\"Mean Inference Time (ms)\")\n",
    "    ax.set_title(\"Inference Latency: FT vs Zero-Shot\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\n",
    "        fig_dir / f\"latency_{FINETUNE_MODE}.png\", dpi=150, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Saved figures to: {fig_dir}\")\n",
    "else:\n",
    "    print(\"No combined results to visualize.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === Export Results ===\n",
    "from tick2.benchmark.reporting import results_to_latex, save_results\n",
    "\n",
    "if not combined.empty:\n",
    "    # Save combined CSV and LaTeX table\n",
    "    csv_path, latex_path = save_results(\n",
    "        combined,\n",
    "        results_dir,\n",
    "        prefix=f\"chronos2_ft_{FINETUNE_MODE}_{TRAINING_MODE}\",\n",
    "    )\n",
    "    print(f\"CSV:   {csv_path}\")\n",
    "    print(f\"LaTeX: {latex_path}\")\n",
    "    print()\n",
    "    print(\n",
    "        results_to_latex(\n",
    "            combined,\n",
    "            caption=(\n",
    "                f\"Chronos-2 fine-tuning ({FINETUNE_MODE}) vs zero-shot \"\n",
    "                f\"({TRAINING_MODE} training)\"\n",
    "            ),\n",
    "            label=f\"tab:chronos2-ft-{FINETUNE_MODE}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Also save per-FT-mode evaluation separately\n",
    "    ft_only = combined[combined[\"training_mode\"] != \"zero_shot\"]\n",
    "    if not ft_only.empty:\n",
    "        ft_csv = device_results_dir / f\"chronos2-ft-{FINETUNE_MODE}_eval.csv\"\n",
    "        ft_only.to_csv(ft_csv, index=False)\n",
    "        print(f\"\\nFT-only CSV: {ft_csv}\")\n",
    "else:\n",
    "    print(\"No results to export.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === Final Push ===\n",
    "if IN_COLAB:\n",
    "    os.chdir(REPO_DIR)\n",
    "\n",
    "    # Stage all outputs: CSVs, figures, LaTeX tables\n",
    "    subprocess.run(\n",
    "        [\"git\", \"add\", \"tick2/notebooks/output/03/\"], check=True\n",
    "    )\n",
    "\n",
    "    # Check if there is anything new to commit\n",
    "    status = subprocess.run(\n",
    "        [\"git\", \"status\", \"--porcelain\", \"tick2/notebooks/output/03/\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "    )\n",
    "    if status.stdout.strip():\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"git\", \"commit\", \"-m\",\n",
    "                f\"results: notebook 03b chronos2 figures and combined \"\n",
    "                f\"({FINETUNE_MODE}, {TRAINING_MODE}, {device_label})\",\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "        if GITHUB_TOKEN:\n",
    "            subprocess.run([\"git\", \"fetch\", \"-q\", \"origin\"], capture_output=True, timeout=30)\n",
    "            subprocess.run([\"git\", \"rebase\", \"origin/main\"], capture_output=True, timeout=30)\n",
    "            subprocess.run([\"git\", \"push\"], check=True)\n",
    "            print(\n",
    "                \"Pushed final outputs (figures + combined CSV) to GitHub.\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"Committed locally but GITHUB_TOKEN not set -- skipping push.\\n\"\n",
    "                \"Set the secret in Colab sidebar > Secrets > GITHUB_TOKEN\"\n",
    "            )\n",
    "    else:\n",
    "        print(\n",
    "            \"No new outputs to commit \"\n",
    "            \"(per-step checkpoints already pushed).\"\n",
    "        )\n",
    "else:\n",
    "    print(f\"Local run. Outputs saved to: {results_dir}\")\n",
    "    print(\n",
    "        \"Run 'git add tick2/notebooks/output/03/ && git commit && git push' \"\n",
    "        \"to share.\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
