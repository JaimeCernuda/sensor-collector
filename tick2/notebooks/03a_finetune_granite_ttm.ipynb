{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChronoTick 2: Granite TTM Fine-Tuning\n",
    "\n",
    "Fine-tune IBM Granite TTM (1-5M params) with channel-mix decoder for\n",
    "multivariate covariate utilization. Uses HF Trainer with frozen backbone\n",
    "and early stopping.\n",
    "\n",
    "## Experiments\n",
    "- E1: Univariate FT (channel-independent, baseline)\n",
    "- E2: Channel-mix FT with top-10 SHAP features\n",
    "- E3: Channel-mix FT with top-30 SHAP features\n",
    "\n",
    "## Training Mode\n",
    "Set `TRAINING_MODE` in Cell 3 to control:\n",
    "- `\"combined\"`: train on all 4 machines (default)\n",
    "- `\"per_machine\"`: train separately per machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Environment Setup ===\nimport os\nimport subprocess\nimport sys\n\nIN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n\nif IN_COLAB:\n    REPO_DIR = \"/content/sensor-collector\"\n    REPO_URL = \"https://github.com/JaimeCernuda/sensor-collector.git\"\n\n    # Read GitHub token from Colab secrets (set via sidebar key icon).\n    # Required for git push; without it the notebook runs but cannot push.\n    GITHUB_TOKEN = None\n    try:\n        from google.colab import userdata\n        GITHUB_TOKEN = userdata.get(\"GITHUB_TOKEN\")\n    except Exception:\n        print(\"WARNING: GITHUB_TOKEN secret not available. Git push will be skipped.\")\n        print(\"  To enable: run from Colab UI with Secrets > GITHUB_TOKEN set.\")\n\n    # Build authenticated URL if token available\n    if GITHUB_TOKEN:\n        auth_url = f\"https://{GITHUB_TOKEN}@github.com/JaimeCernuda/sensor-collector.git\"\n    else:\n        auth_url = REPO_URL\n\n    # Clone or pull latest tick2 code from GitHub\n    if os.path.exists(REPO_DIR):\n        # Update remote URL in case token was added after initial clone\n        subprocess.run([\"git\", \"-C\", REPO_DIR, \"remote\", \"set-url\", \"origin\", auth_url], check=True)\n        # Reset to remote HEAD to avoid divergence from previous Colab commits.\n        # Colab outputs are regenerated each run, so local commits are disposable.\n        subprocess.run([\"git\", \"-C\", REPO_DIR, \"fetch\", \"-q\", \"origin\"], check=True)\n        subprocess.run([\"git\", \"-C\", REPO_DIR, \"reset\", \"--hard\", \"origin/main\"], check=True)\n    else:\n        subprocess.run([\"git\", \"clone\", \"-q\", auth_url, REPO_DIR], check=True)\n\n    # Configure git identity (Colab has no global config)\n    subprocess.run([\"git\", \"-C\", REPO_DIR, \"config\", \"user.name\", \"Colab Runner\"], check=True)\n    subprocess.run([\"git\", \"-C\", REPO_DIR, \"config\", \"user.email\", \"colab@chronotick.dev\"], check=True)\n\n    # Install tick2 package in editable mode\n    subprocess.run([\"pip\", \"install\", \"-q\", \"-e\", f\"{REPO_DIR}/tick2/\"], check=True)\n\n    # Ensure tick2 is importable (pip install via subprocess doesn't always\n    # update sys.path in the running kernel)\n    tick2_src = f\"{REPO_DIR}/tick2/src\"\n    if tick2_src not in sys.path:\n        sys.path.insert(0, tick2_src)\n\n    # Always mount Drive â€” needed for checkpoint persistence (models too large for git)\n    from google.colab import drive\n    drive.mount(\"/content/drive\")\n\n    # Data: prefer repo copy, fall back to Drive\n    DATA_DIR = f\"{REPO_DIR}/sensors/data\"\n    if not os.path.isdir(f\"{DATA_DIR}/24h_snapshot\"):\n        DATA_DIR = \"/content/drive/MyDrive/chronotick2/data\"\n\n    # Output directory inside the repo (will be git-pushed)\n    RESULTS_DIR = f\"{REPO_DIR}/tick2/notebooks/output/03\"\nelse:\n    GITHUB_TOKEN = None\n    DATA_DIR = None\n    RESULTS_DIR = os.path.join(os.path.dirname(__file__) if \"__file__\" in dir() else \".\", \"output\", \"03\")\n\nprint(f\"Environment: {'Colab' if IN_COLAB else 'Local'}\")\nprint(f\"Data dir:    {DATA_DIR or '(default)'}\")\nprint(f\"Results dir: {RESULTS_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Granite TTM Dependencies ===\n# granite-tsfm pins torch<2.9; install with --no-deps to keep CUDA torch\nimport subprocess\nif IN_COLAB:\n    subprocess.run([\"pip\", \"install\", \"-q\", \"granite-tsfm>=0.3.3\", \"--no-deps\"], check=True)\n    subprocess.run([\"pip\", \"install\", \"-q\", \"transformers>=4.56,<5\", \"datasets\", \"deprecated\"], check=True)\n\n# Deep verify\nfrom tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\nfrom tsfm_public import TimeSeriesPreprocessor, get_datasets\nfrom tsfm_public.toolkit.get_model import get_model\nprint(\"granite-tsfm ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports, Config & TRAINING_MODE ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from tick2.data.preprocessing import TARGET_COL, load_all, get_feature_cols\n",
    "from tick2.data.splits import temporal_split, extract_samples\n",
    "from tick2.finetuning.base import FineTuneConfig\n",
    "from tick2.finetuning.data_prep import prepare_datasets, select_top_features\n",
    "from tick2.finetuning.granite_ft import finetune_granite\n",
    "from tick2.finetuning.evaluate import evaluate_finetuned, load_zero_shot_baselines, compare_ft_vs_zero_shot\n",
    "from tick2.utils.gpu import clear_gpu_memory\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "TRAINING_MODE = \"combined\"  # \"combined\" or \"per_machine\"\n",
    "DEVICE_OVERRIDE = None\n",
    "FORCE_RETRAIN = False\n",
    "\n",
    "device = DEVICE_OVERRIDE or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE_DIR_MAP = {\"cuda\": \"gpu\", \"cpu\": \"cpu\"}\n",
    "device_label = DEVICE_DIR_MAP.get(device, device)\n",
    "\n",
    "config = FineTuneConfig(\n",
    "    context_length=512,\n",
    "    prediction_length=96,\n",
    "    max_covariates=30,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Training mode: {TRAINING_MODE}\")\n",
    "print(f\"Context: {config.context_length}, Horizon: {config.prediction_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Data + Temporal Split ===\n",
    "data_dir = Path(DATA_DIR) if DATA_DIR else None\n",
    "prepared = prepare_datasets(config, data_dir=data_dir)\n",
    "for name, p in prepared.items():\n",
    "    print(f\"  {name:16s}: train={len(p.split.train)}, val={len(p.split.val)}, test={len(p.split.test)}, features={len(p.feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Fine-Tune Granite TTM ===\nimport os\nimport subprocess\n\nfrom tick2.utils.colab import save_checkpoint_to_drive, load_checkpoint_from_drive, setup_training_log\n\noutput_base = Path(RESULTS_DIR)\nft_output_dir = output_base / \"granite_ttm_ft\" / TRAINING_MODE\ndevice_results_dir = output_base / device_label\ndevice_results_dir.mkdir(parents=True, exist_ok=True)\n\n# Persist training logs to disk (epoch losses, early stopping, errors)\nlog_path = setup_training_log(ft_output_dir)\nprint(f\"Training log: {log_path}\")\n\n\ndef checkpoint_push(label: str) -> None:\n    \"\"\"Git add, commit, and push results after a step completes.\n\n    Called after training/evaluation so progress survives Colab disconnects.\n    On restart (git clone + reset --hard), pushed results are preserved and\n    the resume logic skips completed steps.\n    \"\"\"\n    if not IN_COLAB:\n        return\n    try:\n        subprocess.run(\n            [\"git\", \"-C\", REPO_DIR, \"add\", \"tick2/notebooks/output/03/\"],\n            check=True, capture_output=True,\n        )\n        status = subprocess.run(\n            [\"git\", \"-C\", REPO_DIR, \"status\", \"--porcelain\",\n             \"tick2/notebooks/output/03/\"],\n            capture_output=True, text=True,\n        )\n        if not status.stdout.strip():\n            return  # nothing new to commit\n        subprocess.run(\n            [\"git\", \"-C\", REPO_DIR, \"commit\", \"-m\",\n             f\"results: notebook 03 granite-ttm-ft {label} ({device_label})\"],\n            check=True, capture_output=True,\n        )\n        if GITHUB_TOKEN:\n            subprocess.run(\n                [\"git\", \"-C\", REPO_DIR, \"push\"],\n                check=True, capture_output=True, timeout=60,\n            )\n            print(f\"  [CHECKPOINT] Pushed {label} results\")\n        else:\n            print(f\"  [CHECKPOINT] Committed (no token for push)\")\n    except Exception as e:\n        print(f\"  [CHECKPOINT WARNING] {e}\")\n\n\n# --- Experiment definitions ---\n# E1: Univariate FT (channel-independent decoder, no covariates)\n# E2: Channel-mix FT with top-10 SHAP features\n# E3: Channel-mix FT with top-30 SHAP features (default max_covariates)\nEXPERIMENTS = [\n    {\n        \"name\": \"E1_univariate\",\n        \"decoder_mode\": \"common_channel\",\n        \"max_covariates\": 0,\n    },\n    {\n        \"name\": \"E2_mix10\",\n        \"decoder_mode\": \"mix_channel\",\n        \"max_covariates\": 10,\n    },\n    {\n        \"name\": \"E3_mix30\",\n        \"decoder_mode\": \"mix_channel\",\n        \"max_covariates\": 30,\n    },\n]\n\nall_ft_results = []  # FineTuneResult objects across experiments\nexperiment_labels = {}  # experiment name -> list of FineTuneResult\n\nfor exp in EXPERIMENTS:\n    exp_name = exp[\"name\"]\n    print(f\"\\n{'='*60}\")\n    print(f\"  {exp_name}  (decoder={exp['decoder_mode']}, max_cov={exp['max_covariates']})\")\n    print(f\"{'='*60}\")\n\n    # Check for cached checkpoint (local first, then Drive)\n    exp_checkpoint_dir = ft_output_dir / exp_name\n    cached_flag = exp_checkpoint_dir / \"best\" / \"config.json\"\n\n    if not cached_flag.exists() and not FORCE_RETRAIN:\n        # Try restoring from Drive\n        drive_model_name = f\"granite_ttm_ft/{TRAINING_MODE}/{exp_name}\"\n        resumed = load_checkpoint_from_drive(\n            model_name=drive_model_name,\n            local_path=str(exp_checkpoint_dir),\n        )\n        if resumed:\n            print(f\"  [RESUMED] Loaded checkpoint from Drive: {resumed}\")\n\n    if cached_flag.exists() and not FORCE_RETRAIN:\n        print(f\"  [CACHED] Checkpoint exists at {exp_checkpoint_dir / 'best'}\")\n        # Create a stub result for tracking\n        from tick2.finetuning.base import FineTuneResult\n        stub = FineTuneResult(\n            model_name=f\"granite-ttm-ft-{exp_name}\",\n            machine=TRAINING_MODE,\n            checkpoint_path=str(exp_checkpoint_dir / \"best\"),\n            config=exp,\n        )\n        all_ft_results.append(stub)\n        experiment_labels[exp_name] = [stub]\n        continue\n\n    # Prepare data with experiment-specific max_covariates\n    exp_config = FineTuneConfig(\n        context_length=config.context_length,\n        prediction_length=config.prediction_length,\n        max_covariates=exp[\"max_covariates\"] if exp[\"max_covariates\"] > 0 else config.max_covariates,\n        seed=config.seed,\n    )\n\n    clear_gpu_memory()\n\n    try:\n        ft_results = finetune_granite(\n            prepared=prepared,\n            config=exp_config,\n            output_dir=str(exp_checkpoint_dir),\n            training_mode=TRAINING_MODE,\n            decoder_mode=exp[\"decoder_mode\"],\n            freeze_backbone=True,\n            learning_rate=0.001,\n            num_epochs=50,\n            batch_size=64,\n            early_stopping_patience=10,\n        )\n\n        for r in ft_results:\n            r.model_name = f\"granite-ttm-ft-{exp_name}\"\n            print(f\"  {r.machine}: {r.training_time_s:.1f}s, best_epoch={r.best_epoch}\")\n            if r.val_loss:\n                print(f\"    val_loss: {r.val_loss[r.best_epoch]:.6f}\")\n\n        all_ft_results.extend(ft_results)\n        experiment_labels[exp_name] = ft_results\n\n        # Save checkpoint to Drive for persistence\n        save_checkpoint_to_drive(\n            local_path=exp_checkpoint_dir,\n            model_name=f\"granite_ttm_ft/{TRAINING_MODE}/{exp_name}\",\n        )\n\n        # Checkpoint push after each experiment\n        checkpoint_push(exp_name)\n\n    except Exception as e:\n        print(f\"  [FAIL] {exp_name}: {e}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n        clear_gpu_memory()\n\nprint(f\"\\n{'='*60}\")\nprint(f\"  Completed experiments: {list(experiment_labels.keys())}\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate FT Models on Test Set ===\n",
    "from tick2.finetuning.granite_ft import load_finetuned_granite\n",
    "from tick2.models.granite import GraniteTTMWrapper\n",
    "\n",
    "eval_dfs = []\n",
    "\n",
    "for exp in EXPERIMENTS:\n",
    "    exp_name = exp[\"name\"]\n",
    "    print(f\"\\n--- Evaluating {exp_name} ---\")\n",
    "\n",
    "    # Check for cached evaluation results\n",
    "    cached_eval_path = device_results_dir / f\"granite-ttm-ft-{exp_name}_{TRAINING_MODE}.csv\"\n",
    "    if cached_eval_path.exists() and not FORCE_RETRAIN:\n",
    "        print(f\"  [CACHED] Loading from {cached_eval_path}\")\n",
    "        eval_dfs.append(pd.read_csv(cached_eval_path))\n",
    "        continue\n",
    "\n",
    "    # Find the checkpoint\n",
    "    results_for_exp = experiment_labels.get(exp_name, [])\n",
    "    if not results_for_exp:\n",
    "        print(f\"  [SKIP] No training results for {exp_name}\")\n",
    "        continue\n",
    "\n",
    "    checkpoint_path = results_for_exp[0].checkpoint_path\n",
    "    if not checkpoint_path or not Path(checkpoint_path).exists():\n",
    "        # Try default path\n",
    "        checkpoint_path = str(ft_output_dir / exp_name / TRAINING_MODE / \"best\")\n",
    "        if not Path(checkpoint_path).exists():\n",
    "            checkpoint_path = str(ft_output_dir / exp_name / \"best\")\n",
    "\n",
    "    if not Path(checkpoint_path).exists():\n",
    "        print(f\"  [SKIP] Checkpoint not found for {exp_name}\")\n",
    "        continue\n",
    "\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    try:\n",
    "        # Load fine-tuned model and wrap it in a GraniteTTMWrapper-like interface\n",
    "        ft_model_raw = load_finetuned_granite(\n",
    "            checkpoint_path,\n",
    "            context_length=config.context_length,\n",
    "            prediction_length=config.prediction_length,\n",
    "        )\n",
    "\n",
    "        # Create a wrapper that conforms to ModelWrapper protocol\n",
    "        wrapper = GraniteTTMWrapper(\n",
    "            model_name=f\"granite-ttm-ft-{exp_name}\",\n",
    "            context_length=config.context_length,\n",
    "            prediction_length=config.prediction_length,\n",
    "        )\n",
    "        # Replace the internal model with our fine-tuned one\n",
    "        wrapper._model = ft_model_raw\n",
    "        wrapper._device = device\n",
    "        wrapper._model.to(device)\n",
    "\n",
    "        # Extract training metadata\n",
    "        ft_epochs = results_for_exp[0].best_epoch if results_for_exp else None\n",
    "        ft_time = results_for_exp[0].training_time_s if results_for_exp else None\n",
    "        ft_machines = results_for_exp[0].machine if results_for_exp else \"\"\n",
    "\n",
    "        eval_df = evaluate_finetuned(\n",
    "            model=wrapper,\n",
    "            prepared=prepared,\n",
    "            config=config,\n",
    "            training_mode=f\"ft_{TRAINING_MODE}\",\n",
    "            ft_epochs=ft_epochs,\n",
    "            ft_time_s=ft_time,\n",
    "            ft_train_machines=ft_machines,\n",
    "            context_lengths=[config.context_length],\n",
    "            horizons=[60, 96],\n",
    "            n_samples=25,\n",
    "            progress=True,\n",
    "        )\n",
    "\n",
    "        if not eval_df.empty:\n",
    "            # Tag with experiment name\n",
    "            eval_df[\"experiment\"] = exp_name\n",
    "            eval_df.to_csv(cached_eval_path, index=False)\n",
    "            eval_dfs.append(eval_df)\n",
    "            print(f\"  Mean MAE: {eval_df['mae'].mean():.4f}\")\n",
    "            print(f\"  Saved: {cached_eval_path}\")\n",
    "        else:\n",
    "            print(f\"  [WARN] No evaluation results for {exp_name}\")\n",
    "\n",
    "        checkpoint_push(f\"eval-{exp_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] Evaluation {exp_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        del wrapper, ft_model_raw\n",
    "        clear_gpu_memory()\n",
    "\n",
    "# Combine all evaluation results\n",
    "if eval_dfs:\n",
    "    ft_results_df = pd.concat(eval_dfs, ignore_index=True)\n",
    "    print(f\"\\nTotal FT evaluation rows: {len(ft_results_df)}\")\n",
    "    display(ft_results_df)\n",
    "else:\n",
    "    ft_results_df = pd.DataFrame()\n",
    "    print(\"No evaluation results collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Zero-Shot Baselines from Notebook 02 ===\n",
    "zs_dir = Path(RESULTS_DIR).parent / \"02\"\n",
    "zs_results = load_zero_shot_baselines(zs_dir, model_name=\"granite-ttm\")\n",
    "\n",
    "if not zs_results.empty:\n",
    "    print(f\"Zero-shot baselines: {len(zs_results)} rows\")\n",
    "    print(f\"  Machines: {zs_results['machine'].unique().tolist()}\")\n",
    "    print(f\"  Mean MAE: {zs_results['mae'].mean():.4f}\")\n",
    "else:\n",
    "    print(\"No zero-shot baselines found in output/02/. Run notebook 02 first.\")\n",
    "    print(f\"  Searched: {zs_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Comparison Table: FT vs Zero-Shot ===\n",
    "if not ft_results_df.empty and not zs_results.empty:\n",
    "    comparison = compare_ft_vs_zero_shot(ft_results_df, zs_results)\n",
    "\n",
    "    # Compute improvement percentage per machine\n",
    "    summary_rows = []\n",
    "    for machine in comparison[\"machine\"].unique():\n",
    "        m_data = comparison[comparison[\"machine\"] == machine]\n",
    "        zs_mae = m_data[m_data[\"training_mode\"] == \"zero_shot\"][\"mae\"].mean()\n",
    "\n",
    "        for exp_name in [e[\"name\"] for e in EXPERIMENTS]:\n",
    "            ft_mask = m_data[\"model\"].str.contains(exp_name, na=False)\n",
    "            ft_mae = m_data[ft_mask][\"mae\"].mean() if ft_mask.any() else None\n",
    "\n",
    "            if ft_mae is not None and zs_mae > 0:\n",
    "                improvement = (zs_mae - ft_mae) / zs_mae * 100\n",
    "                summary_rows.append({\n",
    "                    \"machine\": machine,\n",
    "                    \"experiment\": exp_name,\n",
    "                    \"zs_mae\": zs_mae,\n",
    "                    \"ft_mae\": ft_mae,\n",
    "                    \"improvement_pct\": improvement,\n",
    "                })\n",
    "\n",
    "    if summary_rows:\n",
    "        summary_df = pd.DataFrame(summary_rows)\n",
    "        print(\"\\n=== FT vs Zero-Shot Improvement ===\")\n",
    "        display(summary_df.round(4))\n",
    "\n",
    "        print(\"\\n=== Overall ===\")\n",
    "        for exp_name in [e[\"name\"] for e in EXPERIMENTS]:\n",
    "            exp_data = summary_df[summary_df[\"experiment\"] == exp_name]\n",
    "            if not exp_data.empty:\n",
    "                mean_imp = exp_data[\"improvement_pct\"].mean()\n",
    "                mean_ft_mae = exp_data[\"ft_mae\"].mean()\n",
    "                print(f\"  {exp_name}: mean MAE={mean_ft_mae:.4f}, mean improvement={mean_imp:+.1f}%\")\n",
    "    else:\n",
    "        print(\"Could not compute improvement (check data alignment).\")\n",
    "\n",
    "    print(\"\\n=== Full Comparison ===\")\n",
    "    display(comparison)\n",
    "elif ft_results_df.empty:\n",
    "    print(\"No FT results to compare.\")\n",
    "else:\n",
    "    print(\"No zero-shot baselines to compare against.\")\n",
    "    if not ft_results_df.empty:\n",
    "        print(\"\\n=== FT Results (standalone) ===\")\n",
    "        display(ft_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualizations ===\n",
    "results_dir = Path(RESULTS_DIR)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 1. MAE Comparison Bar Chart: FT vs Zero-Shot per Machine ---\n",
    "if not ft_results_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    # Build comparison data\n",
    "    plot_rows = []\n",
    "\n",
    "    # Add zero-shot baseline\n",
    "    if not zs_results.empty:\n",
    "        for machine in zs_results[\"machine\"].unique():\n",
    "            m_zs = zs_results[zs_results[\"machine\"] == machine]\n",
    "            plot_rows.append({\n",
    "                \"machine\": machine,\n",
    "                \"variant\": \"Zero-Shot\",\n",
    "                \"mae\": m_zs[\"mae\"].mean(),\n",
    "            })\n",
    "\n",
    "    # Add FT experiments\n",
    "    for exp_name in [e[\"name\"] for e in EXPERIMENTS]:\n",
    "        exp_data = ft_results_df[ft_results_df[\"model\"].str.contains(exp_name, na=False)]\n",
    "        for machine in exp_data[\"machine\"].unique():\n",
    "            m_ft = exp_data[exp_data[\"machine\"] == machine]\n",
    "            plot_rows.append({\n",
    "                \"machine\": machine,\n",
    "                \"variant\": exp_name,\n",
    "                \"mae\": m_ft[\"mae\"].mean(),\n",
    "            })\n",
    "\n",
    "    if plot_rows:\n",
    "        plot_df = pd.DataFrame(plot_rows)\n",
    "        sns.barplot(data=plot_df, x=\"machine\", y=\"mae\", hue=\"variant\", ax=ax)\n",
    "        ax.set_ylabel(\"MAE (ppm)\")\n",
    "        ax.set_title(\"Granite TTM: Fine-Tuned vs Zero-Shot MAE by Machine\")\n",
    "        ax.legend(title=\"Variant\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(results_dir / \"ft_vs_zs_mae_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "        print(\"No data for MAE comparison plot.\")\n",
    "\n",
    "# --- 2. Training Loss Curves ---\n",
    "if all_ft_results:\n",
    "    fig, axes = plt.subplots(1, len(EXPERIMENTS), figsize=(5 * len(EXPERIMENTS), 4), squeeze=False)\n",
    "\n",
    "    for idx, exp in enumerate(EXPERIMENTS):\n",
    "        ax = axes[0, idx]\n",
    "        exp_name = exp[\"name\"]\n",
    "        results_for_exp = experiment_labels.get(exp_name, [])\n",
    "\n",
    "        has_data = False\n",
    "        for r in results_for_exp:\n",
    "            if r.train_loss:\n",
    "                ax.plot(r.train_loss, label=\"Train\", alpha=0.7)\n",
    "                has_data = True\n",
    "            if r.val_loss:\n",
    "                ax.plot(r.val_loss, label=\"Validation\", alpha=0.7)\n",
    "                # Mark best epoch\n",
    "                ax.axvline(r.best_epoch, color=\"red\", linestyle=\"--\", alpha=0.5, label=f\"Best (epoch {r.best_epoch})\")\n",
    "                has_data = True\n",
    "\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_title(exp_name)\n",
    "        if has_data:\n",
    "            ax.legend(fontsize=8)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"(cached, no loss history)\", transform=ax.transAxes, ha=\"center\")\n",
    "\n",
    "    plt.suptitle(\"Granite TTM Fine-Tuning Loss Curves\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(results_dir / \"ft_training_loss_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# --- 3. Coverage Comparison (if applicable) ---\n",
    "if not ft_results_df.empty and \"coverage\" in ft_results_df.columns:\n",
    "    cov_data = ft_results_df[ft_results_df[\"coverage\"].notna()]\n",
    "    if not cov_data.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        sns.barplot(data=cov_data, x=\"machine\", y=\"coverage\", hue=\"model\", ax=ax)\n",
    "        ax.axhline(0.8, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"80% target\")\n",
    "        ax.set_ylabel(\"Coverage\")\n",
    "        ax.set_title(\"Prediction Interval Coverage by Machine\")\n",
    "        ax.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(results_dir / \"ft_coverage_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "print(f\"Saved figures to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Export Results CSV + LaTeX ===\n",
    "from tick2.benchmark.reporting import save_results, results_to_latex\n",
    "\n",
    "results_dir = Path(RESULTS_DIR)\n",
    "device_results_dir = results_dir / device_label\n",
    "device_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save FT results\n",
    "if not ft_results_df.empty:\n",
    "    ft_csv_path = device_results_dir / f\"granite-ttm-ft_{TRAINING_MODE}.csv\"\n",
    "    ft_results_df.to_csv(ft_csv_path, index=False)\n",
    "    print(f\"FT results CSV: {ft_csv_path}\")\n",
    "\n",
    "# Save combined comparison\n",
    "if not ft_results_df.empty and not zs_results.empty:\n",
    "    comparison = compare_ft_vs_zero_shot(ft_results_df, zs_results)\n",
    "    csv_path, latex_path = save_results(\n",
    "        comparison, results_dir, prefix=\"granite_ttm_ft_comparison\"\n",
    "    )\n",
    "    print(f\"\\nComparison CSV:   {csv_path}\")\n",
    "    print(f\"Comparison LaTeX: {latex_path}\")\n",
    "    print(f\"\\n{results_to_latex(comparison, caption='Granite TTM fine-tuning vs zero-shot', label='tab:granite-ft')}\")\n",
    "elif not ft_results_df.empty:\n",
    "    csv_path, latex_path = save_results(\n",
    "        ft_results_df, results_dir, prefix=\"granite_ttm_ft\"\n",
    "    )\n",
    "    print(f\"FT-only CSV:   {csv_path}\")\n",
    "    print(f\"FT-only LaTeX: {latex_path}\")\n",
    "    print(f\"\\n{results_to_latex(ft_results_df, caption='Granite TTM fine-tuning results', label='tab:granite-ft')}\")\n",
    "else:\n",
    "    print(\"No results to export.\")\n",
    "\n",
    "# Save training metadata\n",
    "if all_ft_results:\n",
    "    meta_rows = []\n",
    "    for r in all_ft_results:\n",
    "        meta_rows.append({\n",
    "            \"model\": r.model_name,\n",
    "            \"machine\": r.machine,\n",
    "            \"training_time_s\": r.training_time_s,\n",
    "            \"best_epoch\": r.best_epoch,\n",
    "            \"checkpoint_path\": r.checkpoint_path,\n",
    "            **r.config,\n",
    "        })\n",
    "    meta_df = pd.DataFrame(meta_rows)\n",
    "    meta_path = results_dir / f\"granite_ttm_ft_training_meta_{TRAINING_MODE}.csv\"\n",
    "    meta_df.to_csv(meta_path, index=False)\n",
    "    print(f\"\\nTraining metadata: {meta_path}\")\n",
    "    display(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save & Push Results ===\n",
    "# Final push: figures, combined CSVs, and LaTeX tables.\n",
    "# Per-experiment results were already pushed incrementally by checkpoint_push().\n",
    "if IN_COLAB:\n",
    "    os.chdir(REPO_DIR)\n",
    "\n",
    "    # Stage all fine-tuning outputs (figures, combined files, any stragglers)\n",
    "    subprocess.run([\"git\", \"add\", \"tick2/notebooks/output/03/\"], check=True)\n",
    "\n",
    "    # Check if there's anything new to commit\n",
    "    status = subprocess.run(\n",
    "        [\"git\", \"status\", \"--porcelain\", \"tick2/notebooks/output/03/\"],\n",
    "        capture_output=True, text=True,\n",
    "    )\n",
    "    if status.stdout.strip():\n",
    "        subprocess.run(\n",
    "            [\"git\", \"commit\", \"-m\",\n",
    "             f\"results: notebook 03 granite-ttm-ft figures and combined results ({device_label})\"],\n",
    "            check=True,\n",
    "        )\n",
    "        if GITHUB_TOKEN:\n",
    "            subprocess.run([\"git\", \"push\"], check=True)\n",
    "            print(\"Pushed final outputs (figures + combined CSV) to GitHub.\")\n",
    "        else:\n",
    "            print(\"Committed locally but GITHUB_TOKEN not set -- skipping push.\")\n",
    "            print(\"Set the secret in Colab sidebar > Secrets > GITHUB_TOKEN\")\n",
    "    else:\n",
    "        print(\"No new outputs to commit (per-experiment checkpoints already pushed).\")\n",
    "else:\n",
    "    print(f\"Local run. Outputs saved to: {results_dir}\")\n",
    "    print(\"Run 'git add tick2/notebooks/output/03/ && git commit && git push' to share.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 0,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}