{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ChronoTick 2: Granite TTM Fine-Tuning — Larger Context (ctx=1024)\n",
    "\n",
    "Fine-tune IBM Granite TTM at larger context windows to test whether more\n",
    "context improves fine-tuned predictions (especially for ares-comp-10).\n",
    "\n",
    "## Motivation\n",
    "- Notebook 03a trained at (512, 96), but ZS results show ctx=1024+ is dramatically better\n",
    "- ZS@512 is broken for TTM (no zero-shot branch), so 03a's comparison was misleading\n",
    "- ares-comp-10 FT@512 MAE=0.171 vs ZS@1800 MAE=0.054 — needs more context\n",
    "\n",
    "## Experiments\n",
    "**Group A — (1024, 96): Larger context, same horizon**\n",
    "- E4: Univariate FT (common_channel, baseline)\n",
    "- E5: Channel-mix FT with top-10 SHAP features\n",
    "\n",
    "**Group B — (1024, 192): Larger context + longer horizon**\n",
    "- E6: Univariate FT (common_channel, baseline)\n",
    "- E7: Channel-mix FT with top-10 SHAP features\n",
    "\n",
    "## Training Mode\n",
    "Set `TRAINING_MODE` in Cell 3 to control:\n",
    "- `\"combined\"`: train on all 4 machines (default)\n",
    "- `\"per_machine\"`: train separately per machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    REPO_DIR = \"/content/sensor-collector\"\n",
    "    REPO_URL = \"https://github.com/JaimeCernuda/sensor-collector.git\"\n",
    "\n",
    "    # Read GitHub token from Colab secrets (set via sidebar key icon).\n",
    "    # Required for git push; without it the notebook runs but cannot push.\n",
    "    GITHUB_TOKEN = None\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "\n",
    "        GITHUB_TOKEN = userdata.get(\"GITHUB_TOKEN\")\n",
    "    except Exception:\n",
    "        print(\"WARNING: GITHUB_TOKEN secret not available. Git push will be skipped.\")\n",
    "        print(\"  To enable: run from Colab UI with Secrets > GITHUB_TOKEN set.\")\n",
    "\n",
    "    # Build authenticated URL if token available\n",
    "    if GITHUB_TOKEN:\n",
    "        auth_url = (\n",
    "            f\"https://{GITHUB_TOKEN}@github.com/JaimeCernuda/sensor-collector.git\"\n",
    "        )\n",
    "    else:\n",
    "        auth_url = REPO_URL\n",
    "\n",
    "    # Clone or pull latest tick2 code from GitHub\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        # Update remote URL in case token was added after initial clone\n",
    "        subprocess.run(\n",
    "            [\"git\", \"-C\", REPO_DIR, \"remote\", \"set-url\", \"origin\", auth_url], check=True\n",
    "        )\n",
    "        # Reset to remote HEAD to avoid divergence from previous Colab commits.\n",
    "        # Colab outputs are regenerated each run, so local commits are disposable.\n",
    "        subprocess.run([\"git\", \"-C\", REPO_DIR, \"fetch\", \"-q\", \"origin\"], check=True)\n",
    "        subprocess.run(\n",
    "            [\"git\", \"-C\", REPO_DIR, \"reset\", \"--hard\", \"origin/main\"], check=True\n",
    "        )\n",
    "    else:\n",
    "        subprocess.run([\"git\", \"clone\", \"-q\", auth_url, REPO_DIR], check=True)\n",
    "\n",
    "    # Configure git identity (Colab has no global config)\n",
    "    subprocess.run(\n",
    "        [\"git\", \"-C\", REPO_DIR, \"config\", \"user.name\", \"Colab Runner\"], check=True\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"git\", \"-C\", REPO_DIR, \"config\", \"user.email\", \"colab@chronotick.dev\"],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    # Install tick2 package in editable mode\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"-e\", f\"{REPO_DIR}/tick2/\"], check=True)\n",
    "\n",
    "    # Ensure tick2 is importable (pip install via subprocess doesn't always\n",
    "    # update sys.path in the running kernel)\n",
    "    tick2_src = f\"{REPO_DIR}/tick2/src\"\n",
    "    if tick2_src not in sys.path:\n",
    "        sys.path.insert(0, tick2_src)\n",
    "\n",
    "    # Always mount Drive — needed for checkpoint persistence (models too large for git)\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    # Data: prefer repo copy, fall back to Drive\n",
    "    DATA_DIR = f\"{REPO_DIR}/sensors/data\"\n",
    "    if not os.path.isdir(f\"{DATA_DIR}/24h_snapshot\"):\n",
    "        DATA_DIR = \"/content/drive/MyDrive/chronotick2/data\"\n",
    "\n",
    "    # Output directory inside the repo (will be git-pushed)\n",
    "    RESULTS_DIR = f\"{REPO_DIR}/tick2/notebooks/output/03\"\n",
    "else:\n",
    "    GITHUB_TOKEN = None\n",
    "    DATA_DIR = None\n",
    "    RESULTS_DIR = os.path.join(\n",
    "        os.path.dirname(__file__) if \"__file__\" in dir() else \".\", \"output\", \"03\"\n",
    "    )\n",
    "\n",
    "print(f\"Environment: {'Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"Data dir:    {DATA_DIR or '(default)'}\")\n",
    "print(f\"Results dir: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Granite TTM Dependencies ===\n",
    "# granite-tsfm pins torch<2.9; install with --no-deps to keep CUDA torch\n",
    "import subprocess\n",
    "\n",
    "if IN_COLAB:\n",
    "    subprocess.run(\n",
    "        [\"pip\", \"install\", \"-q\", \"granite-tsfm>=0.3.3\", \"--no-deps\"], check=True\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"pip\", \"install\", \"-q\", \"transformers>=4.56,<5\", \"datasets\", \"deprecated\"],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "# Deep verify\n",
    "print(\"granite-tsfm ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports, Config & TRAINING_MODE ===\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from tick2.finetuning.base import FineTuneConfig\n",
    "from tick2.finetuning.data_prep import prepare_datasets\n",
    "from tick2.finetuning.evaluate import (\n",
    "    compare_ft_vs_zero_shot,\n",
    "    evaluate_finetuned,\n",
    "    load_zero_shot_baselines,\n",
    ")\n",
    "from tick2.finetuning.granite_ft import finetune_granite\n",
    "from tick2.utils.gpu import clear_gpu_memory\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "TRAINING_MODE = \"combined\"  # \"combined\" or \"per_machine\"\n",
    "DEVICE_OVERRIDE = None\n",
    "FORCE_RETRAIN = False\n",
    "\n",
    "device = DEVICE_OVERRIDE or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE_DIR_MAP = {\"cuda\": \"gpu\", \"cpu\": \"cpu\"}\n",
    "device_label = DEVICE_DIR_MAP.get(device, device)\n",
    "\n",
    "# Data preparation uses max_covariates=30 to load all features; experiments\n",
    "# select subsets.  Context/prediction lengths are set per experiment group.\n",
    "config_base = FineTuneConfig(\n",
    "    context_length=1024,\n",
    "    prediction_length=96,\n",
    "    max_covariates=30,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Training mode: {TRAINING_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Data + Temporal Split ===\n",
    "data_dir = Path(DATA_DIR) if DATA_DIR else None\n",
    "prepared = prepare_datasets(config_base, data_dir=data_dir)\n",
    "for name, p in prepared.items():\n",
    "    n_train = len(p.split.train)\n",
    "    n_val = len(p.split.val)\n",
    "    n_test = len(p.split.test)\n",
    "    n_feat = len(p.feature_cols)\n",
    "    print(\n",
    "        f\"  {name:16s}: train={n_train}, val={n_val}, test={n_test}, features={n_feat}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fine-Tune Granite TTM — ctx=1024 Experiments ===\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from tick2.utils.colab import (\n",
    "    load_checkpoint_from_drive,\n",
    "    save_checkpoint_to_drive,\n",
    "    setup_training_log,\n",
    ")\n",
    "\n",
    "output_base = Path(RESULTS_DIR)\n",
    "ft_output_dir = output_base / \"granite_ttm_ft\" / TRAINING_MODE\n",
    "device_results_dir = output_base / device_label\n",
    "device_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Persist training logs to disk (epoch losses, early stopping, errors)\n",
    "log_path = setup_training_log(ft_output_dir)\n",
    "print(f\"Training log: {log_path}\")\n",
    "\n",
    "\n",
    "def checkpoint_push(label: str) -> None:\n",
    "    \"\"\"Git add, commit, and push results after a step completes.\"\"\"\n",
    "    if not IN_COLAB:\n",
    "        return\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"git\", \"-C\", REPO_DIR, \"add\", \"tick2/notebooks/output/03/\"],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        status = subprocess.run(\n",
    "            [\n",
    "                \"git\",\n",
    "                \"-C\",\n",
    "                REPO_DIR,\n",
    "                \"status\",\n",
    "                \"--porcelain\",\n",
    "                \"tick2/notebooks/output/03/\",\n",
    "            ],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "        if not status.stdout.strip():\n",
    "            return  # nothing new to commit\n",
    "        msg = f\"results: notebook 03a2 granite-ttm-ft-ctx1024 {label} ({device_label})\"\n",
    "        subprocess.run(\n",
    "            [\"git\", \"-C\", REPO_DIR, \"commit\", \"-m\", msg],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        if GITHUB_TOKEN:\n",
    "            subprocess.run(\n",
    "                [\"git\", \"-C\", REPO_DIR, \"fetch\", \"-q\", \"origin\"],\n",
    "                capture_output=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "            subprocess.run(\n",
    "                [\"git\", \"-C\", REPO_DIR, \"rebase\", \"origin/main\"],\n",
    "                capture_output=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "            subprocess.run(\n",
    "                [\"git\", \"-C\", REPO_DIR, \"push\"],\n",
    "                check=True,\n",
    "                capture_output=True,\n",
    "                timeout=60,\n",
    "            )\n",
    "            print(f\"  [CHECKPOINT] Pushed {label} results\")\n",
    "        else:\n",
    "            print(\"  [CHECKPOINT] Committed (no token for push)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [CHECKPOINT WARNING] {e}\")\n",
    "\n",
    "\n",
    "# --- Experiment definitions ---\n",
    "# Group A: (1024, 96) — larger context, same horizon as 03a\n",
    "# Group B: (1024, 192) — larger context + longer horizon (unlocks hz=120)\n",
    "EXPERIMENTS = [\n",
    "    # Group A: (1024, 96)\n",
    "    {\n",
    "        \"name\": \"E4_ctx1024_uni\",\n",
    "        \"context_length\": 1024,\n",
    "        \"prediction_length\": 96,\n",
    "        \"decoder_mode\": \"common_channel\",\n",
    "        \"max_covariates\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"E5_ctx1024_mix10\",\n",
    "        \"context_length\": 1024,\n",
    "        \"prediction_length\": 96,\n",
    "        \"decoder_mode\": \"mix_channel\",\n",
    "        \"max_covariates\": 10,\n",
    "    },\n",
    "    # Group B: (1024, 192)\n",
    "    {\n",
    "        \"name\": \"E6_ctx1024_h192_uni\",\n",
    "        \"context_length\": 1024,\n",
    "        \"prediction_length\": 192,\n",
    "        \"decoder_mode\": \"common_channel\",\n",
    "        \"max_covariates\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"E7_ctx1024_h192_mix10\",\n",
    "        \"context_length\": 1024,\n",
    "        \"prediction_length\": 192,\n",
    "        \"decoder_mode\": \"mix_channel\",\n",
    "        \"max_covariates\": 10,\n",
    "    },\n",
    "]\n",
    "\n",
    "all_ft_results = []  # FineTuneResult objects across experiments\n",
    "experiment_labels = {}  # experiment name -> list of FineTuneResult\n",
    "\n",
    "for exp in EXPERIMENTS:\n",
    "    exp_name = exp[\"name\"]\n",
    "    ctx_len = exp[\"context_length\"]\n",
    "    pred_len = exp[\"prediction_length\"]\n",
    "    dec = exp[\"decoder_mode\"]\n",
    "    mcov = exp[\"max_covariates\"]\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\n",
    "        f\"  {exp_name}  (ctx={ctx_len}, pred={pred_len}, decoder={dec}, max_cov={mcov})\"\n",
    "    )\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Check for cached checkpoint (local first, then Drive)\n",
    "    exp_checkpoint_dir = ft_output_dir / exp_name\n",
    "    cached_flags = list(exp_checkpoint_dir.glob(\"*/best/config.json\"))\n",
    "\n",
    "    if not cached_flags and not FORCE_RETRAIN:\n",
    "        # Try restoring from Drive\n",
    "        drive_model_name = f\"granite_ttm_ft/{TRAINING_MODE}/{exp_name}\"\n",
    "        resumed = load_checkpoint_from_drive(\n",
    "            model_name=drive_model_name,\n",
    "            local_path=str(exp_checkpoint_dir),\n",
    "        )\n",
    "        if resumed:\n",
    "            print(f\"  [RESUMED] Loaded from Drive: {resumed}\")\n",
    "            cached_flags = list(exp_checkpoint_dir.glob(\"*/best/config.json\"))\n",
    "\n",
    "    if cached_flags and not FORCE_RETRAIN:\n",
    "        best_path = cached_flags[0].parent\n",
    "        print(f\"  [CACHED] Checkpoint exists at {best_path}\")\n",
    "        from tick2.finetuning.base import FineTuneResult\n",
    "\n",
    "        stub = FineTuneResult(\n",
    "            model_name=f\"granite-ttm-ft-{exp_name}\",\n",
    "            machine=TRAINING_MODE,\n",
    "            checkpoint_path=str(best_path),\n",
    "            config=exp,\n",
    "        )\n",
    "        all_ft_results.append(stub)\n",
    "        experiment_labels[exp_name] = [stub]\n",
    "        continue\n",
    "\n",
    "    # Prepare data with experiment-specific config\n",
    "    max_cov = (\n",
    "        exp[\"max_covariates\"]\n",
    "        if exp[\"max_covariates\"] > 0\n",
    "        else config_base.max_covariates\n",
    "    )\n",
    "    exp_config = FineTuneConfig(\n",
    "        context_length=ctx_len,\n",
    "        prediction_length=pred_len,\n",
    "        max_covariates=max_cov,\n",
    "        seed=config_base.seed,\n",
    "    )\n",
    "\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    try:\n",
    "        ft_results = finetune_granite(\n",
    "            prepared=prepared,\n",
    "            config=exp_config,\n",
    "            output_dir=str(exp_checkpoint_dir),\n",
    "            training_mode=TRAINING_MODE,\n",
    "            decoder_mode=exp[\"decoder_mode\"],\n",
    "            freeze_backbone=True,\n",
    "            learning_rate=0.001,\n",
    "            num_epochs=150,\n",
    "            batch_size=64,\n",
    "            early_stopping_patience=10,\n",
    "        )\n",
    "\n",
    "        for r in ft_results:\n",
    "            r.model_name = f\"granite-ttm-ft-{exp_name}\"\n",
    "            print(f\"  {r.machine}: {r.training_time_s:.1f}s, best_epoch={r.best_epoch}\")\n",
    "            if r.val_loss:\n",
    "                print(f\"    val_loss: {r.val_loss[r.best_epoch]:.6f}\")\n",
    "\n",
    "        all_ft_results.extend(ft_results)\n",
    "        experiment_labels[exp_name] = ft_results\n",
    "\n",
    "        # Save checkpoint to Drive for persistence\n",
    "        save_checkpoint_to_drive(\n",
    "            local_path=exp_checkpoint_dir,\n",
    "            model_name=(f\"granite_ttm_ft/{TRAINING_MODE}/{exp_name}\"),\n",
    "        )\n",
    "\n",
    "        # Checkpoint push after each experiment\n",
    "        checkpoint_push(exp_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] {exp_name}: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        clear_gpu_memory()\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"  Completed: {list(experiment_labels.keys())}\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate FT Models on Test Set ===\n",
    "from tick2.finetuning.data_prep import combine_training_data\n",
    "from tick2.finetuning.granite_ft import load_finetuned_granite\n",
    "from tick2.models.granite import GraniteTTMWrapper\n",
    "\n",
    "# Compute shared feature intersection once (same as training used)\n",
    "_, shared_features_all = combine_training_data(prepared)\n",
    "print(f\"Shared features across all machines: {len(shared_features_all)}\")\n",
    "\n",
    "# Evaluation grid per experiment group:\n",
    "# Group A (pred_len=96):  ctx=[1024], hz=[60, 96]\n",
    "# Group B (pred_len=192): ctx=[1024], hz=[60, 96, 120, 192]\n",
    "EVAL_GRIDS = {\n",
    "    96: {\"context_lengths\": [1024], \"horizons\": [60, 96]},\n",
    "    192: {\"context_lengths\": [1024], \"horizons\": [60, 96, 120, 192]},\n",
    "}\n",
    "\n",
    "eval_dfs = []\n",
    "\n",
    "for exp in EXPERIMENTS:\n",
    "    exp_name = exp[\"name\"]\n",
    "    ctx_len = exp[\"context_length\"]\n",
    "    pred_len = exp[\"prediction_length\"]\n",
    "    print(f\"\\n--- Evaluating {exp_name} (ctx={ctx_len}, pred={pred_len}) ---\")\n",
    "\n",
    "    # Check for cached evaluation results\n",
    "    cached_eval_path = (\n",
    "        device_results_dir / f\"granite-ttm-ft-{exp_name}_{TRAINING_MODE}.csv\"\n",
    "    )\n",
    "    if cached_eval_path.exists() and not FORCE_RETRAIN:\n",
    "        print(f\"  [CACHED] Loading from {cached_eval_path}\")\n",
    "        eval_dfs.append(pd.read_csv(cached_eval_path))\n",
    "        continue\n",
    "\n",
    "    # Find the checkpoint\n",
    "    results_for_exp = experiment_labels.get(exp_name, [])\n",
    "    if not results_for_exp:\n",
    "        print(f\"  [SKIP] No training results for {exp_name}\")\n",
    "        continue\n",
    "\n",
    "    checkpoint_path = results_for_exp[0].checkpoint_path\n",
    "    if not checkpoint_path or not Path(checkpoint_path).exists():\n",
    "        checkpoint_path = str(ft_output_dir / exp_name / TRAINING_MODE / \"best\")\n",
    "        if not Path(checkpoint_path).exists():\n",
    "            checkpoint_path = str(ft_output_dir / exp_name / \"best\")\n",
    "\n",
    "    if not Path(checkpoint_path).exists():\n",
    "        print(f\"  [SKIP] Checkpoint not found for {exp_name}\")\n",
    "        continue\n",
    "\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    try:\n",
    "        # Load fine-tuned model\n",
    "        ft_model_raw = load_finetuned_granite(\n",
    "            checkpoint_path,\n",
    "            context_length=ctx_len,\n",
    "            prediction_length=pred_len,\n",
    "        )\n",
    "\n",
    "        # Create wrapper conforming to ModelWrapper protocol\n",
    "        wrapper = GraniteTTMWrapper(\n",
    "            model_name=f\"granite-ttm-ft-{exp_name}\",\n",
    "            context_length=ctx_len,\n",
    "            prediction_length=pred_len,\n",
    "        )\n",
    "        # Replace internal model with our fine-tuned one\n",
    "        wrapper._model = ft_model_raw\n",
    "        wrapper._device = device\n",
    "        wrapper._model.to(device)\n",
    "\n",
    "        # Auto-detect channel count for mix_channel models\n",
    "        n_channels = getattr(ft_model_raw.config, \"num_input_channels\", 1)\n",
    "        wrapper._n_input_channels = n_channels\n",
    "\n",
    "        # For mix_channel models, use the shared feature\n",
    "        # intersection (same columns and order as training)\n",
    "        if n_channels > 1:\n",
    "            n_cov = n_channels - 1\n",
    "            eval_features = shared_features_all[:n_cov]\n",
    "            print(\n",
    "                f\"  Mix-channel: {n_channels} channels (1 target + {n_cov} covariates)\"\n",
    "            )\n",
    "        else:\n",
    "            eval_features = None\n",
    "            print(f\"  Univariate: {n_channels} channel\")\n",
    "\n",
    "        # Extract training metadata\n",
    "        ft_epochs = results_for_exp[0].best_epoch\n",
    "        ft_time = results_for_exp[0].training_time_s\n",
    "        ft_machines = results_for_exp[0].machine\n",
    "\n",
    "        # Use the correct eval grid for prediction_length\n",
    "        eval_grid = EVAL_GRIDS[pred_len]\n",
    "\n",
    "        # Config matching this experiment's ctx/pred lengths\n",
    "        eval_config = FineTuneConfig(\n",
    "            context_length=ctx_len,\n",
    "            prediction_length=pred_len,\n",
    "            max_covariates=config_base.max_covariates,\n",
    "            seed=config_base.seed,\n",
    "        )\n",
    "\n",
    "        eval_df = evaluate_finetuned(\n",
    "            model=wrapper,\n",
    "            prepared=prepared,\n",
    "            config=eval_config,\n",
    "            training_mode=f\"ft_{TRAINING_MODE}\",\n",
    "            ft_epochs=ft_epochs,\n",
    "            ft_time_s=ft_time,\n",
    "            ft_train_machines=ft_machines,\n",
    "            context_lengths=eval_grid[\"context_lengths\"],\n",
    "            horizons=eval_grid[\"horizons\"],\n",
    "            n_samples=25,\n",
    "            progress=True,\n",
    "            shared_feature_cols=eval_features,\n",
    "        )\n",
    "\n",
    "        if not eval_df.empty:\n",
    "            eval_df[\"experiment\"] = exp_name\n",
    "            eval_df.to_csv(cached_eval_path, index=False)\n",
    "            eval_dfs.append(eval_df)\n",
    "            print(f\"  Mean MAE: {eval_df['mae'].mean():.4f}\")\n",
    "            print(f\"  Saved: {cached_eval_path}\")\n",
    "        else:\n",
    "            print(f\"  [WARN] No eval results for {exp_name}\")\n",
    "\n",
    "        checkpoint_push(f\"eval-{exp_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] Evaluation {exp_name}: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        del wrapper, ft_model_raw\n",
    "        clear_gpu_memory()\n",
    "\n",
    "# Combine all evaluation results\n",
    "if eval_dfs:\n",
    "    ft_results_df = pd.concat(eval_dfs, ignore_index=True)\n",
    "    print(f\"\\nTotal FT eval rows: {len(ft_results_df)}\")\n",
    "    display(ft_results_df)\n",
    "else:\n",
    "    ft_results_df = pd.DataFrame()\n",
    "    print(\"No evaluation results collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Baselines: ZS from Notebook 02 + FT@512 from Notebook 03a ===\n",
    "zs_dir = Path(RESULTS_DIR).parent / \"02\"\n",
    "zs_results_raw = load_zero_shot_baselines(zs_dir, model_name=\"granite-ttm\")\n",
    "\n",
    "# Filter to valid ZS contexts (no ZS branch at ctx=512 for 96-step horizon)\n",
    "if not zs_results_raw.empty:\n",
    "    zs_results = zs_results_raw[zs_results_raw[\"context_length\"] >= 1024].copy()\n",
    "    print(f\"Zero-shot baselines: {len(zs_results)} rows (ctx>=1024)\")\n",
    "    print(f\"  Contexts: {sorted(zs_results['context_length'].unique().tolist())}\")\n",
    "else:\n",
    "    zs_results = pd.DataFrame()\n",
    "    print(\"No zero-shot baselines found.\")\n",
    "\n",
    "# Load FT@512 results from notebook 03a for cross-context comparison\n",
    "ft512_dfs = []\n",
    "ft512_dir = Path(RESULTS_DIR) / device_label\n",
    "for exp_name in [\"E1_univariate\", \"E2_mix10\", \"E3_mix30\"]:\n",
    "    csv_path = ft512_dir / f\"granite-ttm-ft-{exp_name}_{TRAINING_MODE}.csv\"\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df[\"experiment\"] = exp_name\n",
    "        ft512_dfs.append(df)\n",
    "\n",
    "if ft512_dfs:\n",
    "    ft512_results = pd.concat(ft512_dfs, ignore_index=True)\n",
    "    print(f\"\\nFT@512 baselines (from 03a): {len(ft512_results)} rows\")\n",
    "    print(f\"  Experiments: {ft512_results['experiment'].unique().tolist()}\")\n",
    "else:\n",
    "    ft512_results = pd.DataFrame()\n",
    "    print(\"\\nNo FT@512 baselines found (run notebook 03a first).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Comparison Tables ===\n",
    "\n",
    "# --- Table 1: FT@1024 vs ZS@1024 (apples-to-apples) ---\n",
    "if not ft_results_df.empty and not zs_results.empty:\n",
    "    zs_at_1024 = zs_results[zs_results[\"context_length\"] == 1024]\n",
    "\n",
    "    if not zs_at_1024.empty:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Table 1: FT@1024 vs ZS@1024 (apples-to-apples)\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        summary_rows = []\n",
    "        for machine in ft_results_df[\"machine\"].unique():\n",
    "            zs_m = zs_at_1024[zs_at_1024[\"machine\"] == machine]\n",
    "            if zs_m.empty:\n",
    "                continue\n",
    "            zs_mae = zs_m[\"mae\"].mean()\n",
    "\n",
    "            for exp in EXPERIMENTS:\n",
    "                exp_name = exp[\"name\"]\n",
    "                ft_mask = (\n",
    "                    ft_results_df[\"model\"].str.contains(exp_name, na=False)\n",
    "                    & (ft_results_df[\"machine\"] == machine)\n",
    "                    & (ft_results_df[\"context_length\"] == 1024)\n",
    "                )\n",
    "                if not ft_mask.any():\n",
    "                    continue\n",
    "                ft_mae = ft_results_df.loc[ft_mask, \"mae\"].mean()\n",
    "\n",
    "                if zs_mae > 0:\n",
    "                    imp = (zs_mae - ft_mae) / zs_mae * 100\n",
    "                    summary_rows.append(\n",
    "                        {\n",
    "                            \"machine\": machine,\n",
    "                            \"experiment\": exp_name,\n",
    "                            \"ft_mae\": ft_mae,\n",
    "                            \"zs_1024_mae\": zs_mae,\n",
    "                            \"vs_zs1024_pct\": imp,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if summary_rows:\n",
    "            display(pd.DataFrame(summary_rows).round(4))\n",
    "        else:\n",
    "            print(\"  No overlapping machines.\")\n",
    "    else:\n",
    "        print(\"No ZS@1024 baselines available.\")\n",
    "\n",
    "# --- Table 2: FT@1024 vs Best ZS (context gap analysis) ---\n",
    "if not ft_results_df.empty and not zs_results.empty:\n",
    "    best_zs_per_machine = (\n",
    "        zs_results.groupby(\"machine\")[\"mae\"]\n",
    "        .agg([\"min\", \"idxmin\"])\n",
    "        .rename(columns={\"min\": \"best_zs_mae\"})\n",
    "    )\n",
    "    best_zs_per_machine[\"best_zs_ctx\"] = zs_results.loc[\n",
    "        best_zs_per_machine[\"idxmin\"], \"context_length\"\n",
    "    ].values\n",
    "    best_zs_per_machine = best_zs_per_machine.drop(columns=[\"idxmin\"])\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"Table 2: FT@1024 vs Best ZS (context gap)\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    summary_rows = []\n",
    "    for machine in ft_results_df[\"machine\"].unique():\n",
    "        if machine not in best_zs_per_machine.index:\n",
    "            continue\n",
    "        bzs_mae = best_zs_per_machine.loc[machine, \"best_zs_mae\"]\n",
    "        bzs_ctx = int(best_zs_per_machine.loc[machine, \"best_zs_ctx\"])\n",
    "\n",
    "        for exp in EXPERIMENTS:\n",
    "            exp_name = exp[\"name\"]\n",
    "            ft_mask = ft_results_df[\"model\"].str.contains(exp_name, na=False) & (\n",
    "                ft_results_df[\"machine\"] == machine\n",
    "            )\n",
    "            if not ft_mask.any():\n",
    "                continue\n",
    "            ft_mae = ft_results_df.loc[ft_mask, \"mae\"].mean()\n",
    "\n",
    "            if bzs_mae > 0:\n",
    "                imp = (bzs_mae - ft_mae) / bzs_mae * 100\n",
    "                summary_rows.append(\n",
    "                    {\n",
    "                        \"machine\": machine,\n",
    "                        \"experiment\": exp_name,\n",
    "                        \"ft_ctx\": 1024,\n",
    "                        \"ft_mae\": ft_mae,\n",
    "                        \"best_zs_ctx\": bzs_ctx,\n",
    "                        \"best_zs_mae\": bzs_mae,\n",
    "                        \"vs_best_zs_pct\": imp,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if summary_rows:\n",
    "        display(pd.DataFrame(summary_rows).round(4))\n",
    "    else:\n",
    "        print(\"  No overlapping machines.\")\n",
    "\n",
    "# --- Table 3: FT@512 vs FT@1024 — more context? ---\n",
    "if not ft_results_df.empty and not ft512_results.empty:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"Table 3: FT@512 vs FT@1024 — more context?\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    ft512_to_ft1024 = {\n",
    "        \"E1_univariate\": \"E4_ctx1024_uni\",\n",
    "        \"E2_mix10\": \"E5_ctx1024_mix10\",\n",
    "    }\n",
    "\n",
    "    summary_rows = []\n",
    "    for machine in ft_results_df[\"machine\"].unique():\n",
    "        for ft512_exp, ft1024_exp in ft512_to_ft1024.items():\n",
    "            ft512_mask = ft512_results[\"model\"].str.contains(ft512_exp, na=False) & (\n",
    "                ft512_results[\"machine\"] == machine\n",
    "            )\n",
    "            ft1024_mask = (\n",
    "                ft_results_df[\"model\"].str.contains(ft1024_exp, na=False)\n",
    "                & (ft_results_df[\"machine\"] == machine)\n",
    "                & (ft_results_df[\"context_length\"] == 1024)\n",
    "            )\n",
    "\n",
    "            if not ft512_mask.any() or not ft1024_mask.any():\n",
    "                continue\n",
    "            ft512_mae = ft512_results.loc[ft512_mask, \"mae\"].mean()\n",
    "            ft1024_mae = ft_results_df.loc[ft1024_mask, \"mae\"].mean()\n",
    "\n",
    "            if ft512_mae > 0:\n",
    "                imp = (ft512_mae - ft1024_mae) / ft512_mae * 100\n",
    "                dec = \"uni\" if \"uni\" in ft1024_exp else \"mix10\"\n",
    "                summary_rows.append(\n",
    "                    {\n",
    "                        \"machine\": machine,\n",
    "                        \"decoder\": dec,\n",
    "                        \"ft512_exp\": ft512_exp,\n",
    "                        \"ft512_mae\": ft512_mae,\n",
    "                        \"ft1024_exp\": ft1024_exp,\n",
    "                        \"ft1024_mae\": ft1024_mae,\n",
    "                        \"ctx_improvement_pct\": imp,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if summary_rows:\n",
    "        ctx_df = pd.DataFrame(summary_rows)\n",
    "        display(ctx_df.round(4))\n",
    "\n",
    "        print(\"\\nPer-decoder summary:\")\n",
    "        for dec in ctx_df[\"decoder\"].unique():\n",
    "            dec_data = ctx_df[ctx_df[\"decoder\"] == dec]\n",
    "            mean_imp = dec_data[\"ctx_improvement_pct\"].mean()\n",
    "            print(f\"  {dec}: mean improvement = {mean_imp:+.1f}%\")\n",
    "    else:\n",
    "        print(\"  No comparable experiments.\")\n",
    "\n",
    "if ft_results_df.empty:\n",
    "    print(\"No FT@1024 results to compare.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualizations ===\n",
    "results_dir = Path(RESULTS_DIR)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 1. MAE Comparison Bar Chart ---\n",
    "if not ft_results_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "    plot_rows = []\n",
    "\n",
    "    # Add ZS@1024 baseline\n",
    "    if not zs_results.empty:\n",
    "        zs_1024 = zs_results[zs_results[\"context_length\"] == 1024]\n",
    "        for machine in zs_1024[\"machine\"].unique():\n",
    "            m_zs = zs_1024[zs_1024[\"machine\"] == machine]\n",
    "            plot_rows.append(\n",
    "                {\n",
    "                    \"machine\": machine,\n",
    "                    \"variant\": \"ZS@1024\",\n",
    "                    \"mae\": m_zs[\"mae\"].mean(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Add FT@512 baselines (from 03a)\n",
    "    if not ft512_results.empty:\n",
    "        for exp_name in [\"E2_mix10\"]:\n",
    "            exp_data = ft512_results[ft512_results[\"experiment\"] == exp_name]\n",
    "            for machine in exp_data[\"machine\"].unique():\n",
    "                m_ft = exp_data[exp_data[\"machine\"] == machine]\n",
    "                plot_rows.append(\n",
    "                    {\n",
    "                        \"machine\": machine,\n",
    "                        \"variant\": \"FT@512 (E2)\",\n",
    "                        \"mae\": m_ft[\"mae\"].mean(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Add FT@1024 experiments\n",
    "    for exp in EXPERIMENTS:\n",
    "        exp_name = exp[\"name\"]\n",
    "        exp_data = ft_results_df[\n",
    "            ft_results_df[\"model\"].str.contains(exp_name, na=False)\n",
    "            & (ft_results_df[\"context_length\"] == 1024)\n",
    "        ]\n",
    "        for machine in exp_data[\"machine\"].unique():\n",
    "            m_ft = exp_data[exp_data[\"machine\"] == machine]\n",
    "            plot_rows.append(\n",
    "                {\n",
    "                    \"machine\": machine,\n",
    "                    \"variant\": exp_name,\n",
    "                    \"mae\": m_ft[\"mae\"].mean(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if plot_rows:\n",
    "        plot_df = pd.DataFrame(plot_rows)\n",
    "        sns.barplot(\n",
    "            data=plot_df,\n",
    "            x=\"machine\",\n",
    "            y=\"mae\",\n",
    "            hue=\"variant\",\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_ylabel(\"MAE (ppm)\")\n",
    "        ax.set_title(\"Granite TTM ctx=1024: FT vs ZS MAE by Machine\")\n",
    "        ax.legend(\n",
    "            title=\"Variant\",\n",
    "            bbox_to_anchor=(1.05, 1),\n",
    "            loc=\"upper left\",\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(\n",
    "            results_dir / \"ft_ctx1024_vs_zs_mae_comparison.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "        print(\"No data for MAE comparison plot.\")\n",
    "\n",
    "# --- 2. Training Loss Curves ---\n",
    "if all_ft_results:\n",
    "    n_exp = len(EXPERIMENTS)\n",
    "    fig, axes = plt.subplots(\n",
    "        1,\n",
    "        n_exp,\n",
    "        figsize=(5 * n_exp, 4),\n",
    "        squeeze=False,\n",
    "    )\n",
    "\n",
    "    for idx, exp in enumerate(EXPERIMENTS):\n",
    "        ax = axes[0, idx]\n",
    "        exp_name = exp[\"name\"]\n",
    "        results_for_exp = experiment_labels.get(exp_name, [])\n",
    "\n",
    "        has_data = False\n",
    "        for r in results_for_exp:\n",
    "            if r.train_loss:\n",
    "                ax.plot(r.train_loss, label=\"Train\", alpha=0.7)\n",
    "                has_data = True\n",
    "            if r.val_loss:\n",
    "                ax.plot(\n",
    "                    r.val_loss,\n",
    "                    label=\"Validation\",\n",
    "                    alpha=0.7,\n",
    "                )\n",
    "                ax.axvline(\n",
    "                    r.best_epoch,\n",
    "                    color=\"red\",\n",
    "                    linestyle=\"--\",\n",
    "                    alpha=0.5,\n",
    "                    label=f\"Best (epoch {r.best_epoch})\",\n",
    "                )\n",
    "                has_data = True\n",
    "\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.set_title(exp_name)\n",
    "        if has_data:\n",
    "            ax.legend(fontsize=8)\n",
    "        else:\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                \"(cached, no loss history)\",\n",
    "                transform=ax.transAxes,\n",
    "                ha=\"center\",\n",
    "            )\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Granite TTM ctx=1024 FT Loss Curves\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\n",
    "        results_dir / \"ft_ctx1024_training_loss_curves.png\",\n",
    "        dpi=150,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Saved figures to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Export Results CSV + LaTeX ===\n",
    "from tick2.benchmark.reporting import results_to_latex, save_results\n",
    "\n",
    "results_dir = Path(RESULTS_DIR)\n",
    "device_results_dir = results_dir / device_label\n",
    "device_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save FT@1024 results\n",
    "if not ft_results_df.empty:\n",
    "    ft_csv = device_results_dir / f\"granite-ttm-ft-ctx1024_{TRAINING_MODE}.csv\"\n",
    "    ft_results_df.to_csv(ft_csv, index=False)\n",
    "    print(f\"FT@1024 results CSV: {ft_csv}\")\n",
    "\n",
    "# Save combined comparison with ZS\n",
    "if not ft_results_df.empty and not zs_results.empty:\n",
    "    comparison = compare_ft_vs_zero_shot(\n",
    "        ft_results_df,\n",
    "        zs_results,\n",
    "    )\n",
    "    csv_path, latex_path = save_results(\n",
    "        comparison,\n",
    "        results_dir,\n",
    "        prefix=\"granite_ttm_ft_ctx1024_comparison\",\n",
    "    )\n",
    "    print(f\"\\nComparison CSV:   {csv_path}\")\n",
    "    print(f\"Comparison LaTeX: {latex_path}\")\n",
    "    latex = results_to_latex(\n",
    "        comparison,\n",
    "        caption=\"Granite TTM FT (ctx=1024) vs zero-shot\",\n",
    "        label=\"tab:granite-ft-ctx1024\",\n",
    "    )\n",
    "    print(f\"\\n{latex}\")\n",
    "elif not ft_results_df.empty:\n",
    "    csv_path, latex_path = save_results(\n",
    "        ft_results_df,\n",
    "        results_dir,\n",
    "        prefix=\"granite_ttm_ft_ctx1024\",\n",
    "    )\n",
    "    print(f\"FT-only CSV:   {csv_path}\")\n",
    "    print(f\"FT-only LaTeX: {latex_path}\")\n",
    "    latex = results_to_latex(\n",
    "        ft_results_df,\n",
    "        caption=\"Granite TTM FT (ctx=1024) results\",\n",
    "        label=\"tab:granite-ft-ctx1024\",\n",
    "    )\n",
    "    print(f\"\\n{latex}\")\n",
    "else:\n",
    "    print(\"No results to export.\")\n",
    "\n",
    "# Save training metadata\n",
    "if all_ft_results:\n",
    "    meta_rows = []\n",
    "    for r in all_ft_results:\n",
    "        meta_rows.append(\n",
    "            {\n",
    "                \"model\": r.model_name,\n",
    "                \"machine\": r.machine,\n",
    "                \"training_time_s\": r.training_time_s,\n",
    "                \"best_epoch\": r.best_epoch,\n",
    "                \"checkpoint_path\": r.checkpoint_path,\n",
    "                **r.config,\n",
    "            }\n",
    "        )\n",
    "    meta_df = pd.DataFrame(meta_rows)\n",
    "    meta_path = results_dir / f\"granite_ttm_ft_ctx1024_meta_{TRAINING_MODE}.csv\"\n",
    "    meta_df.to_csv(meta_path, index=False)\n",
    "    print(f\"\\nTraining metadata: {meta_path}\")\n",
    "    display(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save & Push Results ===\n",
    "if IN_COLAB:\n",
    "    os.chdir(REPO_DIR)\n",
    "\n",
    "    subprocess.run(\n",
    "        [\"git\", \"add\", \"tick2/notebooks/output/03/\"],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    status = subprocess.run(\n",
    "        [\"git\", \"status\", \"--porcelain\", \"tick2/notebooks/output/03/\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "    )\n",
    "    if status.stdout.strip():\n",
    "        msg = (\n",
    "            \"results: notebook 03a2 granite-ttm-ft-ctx1024\"\n",
    "            f\" figures and combined results ({device_label})\"\n",
    "        )\n",
    "        subprocess.run(\n",
    "            [\"git\", \"commit\", \"-m\", msg],\n",
    "            check=True,\n",
    "        )\n",
    "        if GITHUB_TOKEN:\n",
    "            subprocess.run(\n",
    "                [\"git\", \"fetch\", \"-q\", \"origin\"],\n",
    "                capture_output=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "            subprocess.run(\n",
    "                [\"git\", \"rebase\", \"origin/main\"],\n",
    "                capture_output=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "            subprocess.run([\"git\", \"push\"], check=True)\n",
    "            print(\"Pushed final outputs to GitHub.\")\n",
    "        else:\n",
    "            print(\"Committed locally (no token for push).\")\n",
    "            print(\"Set Colab sidebar > Secrets > GITHUB_TOKEN\")\n",
    "    else:\n",
    "        print(\"No new outputs to commit.\")\n",
    "else:\n",
    "    print(f\"Local run. Outputs saved to: {results_dir}\")\n",
    "    print(\n",
    "        \"Run 'git add tick2/notebooks/output/03/ && git commit && git push' to share.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}