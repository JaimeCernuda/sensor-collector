{
  "best_global_step": 139080,
  "best_metric": 0.009853038005530834,
  "best_model_checkpoint": "/content/sensor-collector/tick2/notebooks/output/03/granite_ttm_ft/combined/E3_mix30/combined/checkpoints/checkpoint-139080",
  "epoch": 49.0,
  "eval_steps": 500,
  "global_step": 170373,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014380212827149842,
      "grad_norm": 0.0007249508053064346,
      "learning_rate": 0.0009997181478285878,
      "loss": 0.0051,
      "step": 50
    },
    {
      "epoch": 0.028760425654299683,
      "grad_norm": 0.00027690245769917965,
      "learning_rate": 0.000999430543572045,
      "loss": 0.0049,
      "step": 100
    },
    {
      "epoch": 0.04314063848144953,
      "grad_norm": 0.007957613095641136,
      "learning_rate": 0.0009991429393155019,
      "loss": 0.0019,
      "step": 150
    },
    {
      "epoch": 0.057520851308599366,
      "grad_norm": 0.00012427315232343972,
      "learning_rate": 0.000998855335058959,
      "loss": 0.0026,
      "step": 200
    },
    {
      "epoch": 0.0719010641357492,
      "grad_norm": 0.00013154521002434194,
      "learning_rate": 0.0009985677308024159,
      "loss": 0.0036,
      "step": 250
    },
    {
      "epoch": 0.08628127696289906,
      "grad_norm": 0.00018302668468095362,
      "learning_rate": 0.0009982801265458728,
      "loss": 0.0028,
      "step": 300
    },
    {
      "epoch": 0.1006614897900489,
      "grad_norm": 0.00013436973677016795,
      "learning_rate": 0.00099799252228933,
      "loss": 0.0035,
      "step": 350
    },
    {
      "epoch": 0.11504170261719873,
      "grad_norm": 0.0001265051687369123,
      "learning_rate": 0.000997704918032787,
      "loss": 0.0051,
      "step": 400
    },
    {
      "epoch": 0.12942191544434858,
      "grad_norm": 3.585782178561203e-05,
      "learning_rate": 0.000997417313776244,
      "loss": 0.0015,
      "step": 450
    },
    {
      "epoch": 0.1438021282714984,
      "grad_norm": 0.0001215280790347606,
      "learning_rate": 0.000997129709519701,
      "loss": 0.0024,
      "step": 500
    },
    {
      "epoch": 0.15818234109864826,
      "grad_norm": 0.00032931260648183525,
      "learning_rate": 0.000996842105263158,
      "loss": 0.0038,
      "step": 550
    },
    {
      "epoch": 0.1725625539257981,
      "grad_norm": 0.006777677219361067,
      "learning_rate": 0.0009965545010066149,
      "loss": 0.0025,
      "step": 600
    },
    {
      "epoch": 0.18694276675294794,
      "grad_norm": 0.046615246683359146,
      "learning_rate": 0.000996266896750072,
      "loss": 0.0015,
      "step": 650
    },
    {
      "epoch": 0.2013229795800978,
      "grad_norm": 7.922297663753852e-05,
      "learning_rate": 0.0009959792924935289,
      "loss": 0.002,
      "step": 700
    },
    {
      "epoch": 0.21570319240724764,
      "grad_norm": 0.00026861715014092624,
      "learning_rate": 0.000995691688236986,
      "loss": 0.0022,
      "step": 750
    },
    {
      "epoch": 0.23008340523439746,
      "grad_norm": 0.0001378882152494043,
      "learning_rate": 0.000995404083980443,
      "loss": 0.003,
      "step": 800
    },
    {
      "epoch": 0.24446361806154732,
      "grad_norm": 0.007783736102283001,
      "learning_rate": 0.0009951164797239,
      "loss": 0.0019,
      "step": 850
    },
    {
      "epoch": 0.25884383088869717,
      "grad_norm": 0.00015929335495457053,
      "learning_rate": 0.000994828875467357,
      "loss": 0.0029,
      "step": 900
    },
    {
      "epoch": 0.273224043715847,
      "grad_norm": 0.00013439776375889778,
      "learning_rate": 0.000994541271210814,
      "loss": 0.0049,
      "step": 950
    },
    {
      "epoch": 0.2876042565429968,
      "grad_norm": 0.0002461958792991936,
      "learning_rate": 0.000994253666954271,
      "loss": 0.0042,
      "step": 1000
    },
    {
      "epoch": 0.30198446937014667,
      "grad_norm": 0.00022669386817142367,
      "learning_rate": 0.0009939660626977279,
      "loss": 0.0028,
      "step": 1050
    },
    {
      "epoch": 0.3163646821972965,
      "grad_norm": 0.002101097023114562,
      "learning_rate": 0.000993678458441185,
      "loss": 0.0069,
      "step": 1100
    },
    {
      "epoch": 0.3307448950244464,
      "grad_norm": 5.6795448472257704e-05,
      "learning_rate": 0.0009933908541846419,
      "loss": 0.0035,
      "step": 1150
    },
    {
      "epoch": 0.3451251078515962,
      "grad_norm": 0.16546878218650818,
      "learning_rate": 0.000993103249928099,
      "loss": 0.008,
      "step": 1200
    },
    {
      "epoch": 0.359505320678746,
      "grad_norm": 0.0026290984824299812,
      "learning_rate": 0.000992815645671556,
      "loss": 0.0035,
      "step": 1250
    },
    {
      "epoch": 0.3738855335058959,
      "grad_norm": 0.00019147142302244902,
      "learning_rate": 0.000992528041415013,
      "loss": 0.0046,
      "step": 1300
    },
    {
      "epoch": 0.3882657463330457,
      "grad_norm": 0.005189558025449514,
      "learning_rate": 0.00099224043715847,
      "loss": 0.0036,
      "step": 1350
    },
    {
      "epoch": 0.4026459591601956,
      "grad_norm": 0.00018229398119729012,
      "learning_rate": 0.000991952832901927,
      "loss": 0.0042,
      "step": 1400
    },
    {
      "epoch": 0.41702617198734543,
      "grad_norm": 0.003051755717024207,
      "learning_rate": 0.000991665228645384,
      "loss": 0.0073,
      "step": 1450
    },
    {
      "epoch": 0.4314063848144953,
      "grad_norm": 0.005789210554212332,
      "learning_rate": 0.0009913776243888409,
      "loss": 0.003,
      "step": 1500
    },
    {
      "epoch": 0.4457865976416451,
      "grad_norm": 8.739270560909063e-05,
      "learning_rate": 0.000991090020132298,
      "loss": 0.0052,
      "step": 1550
    },
    {
      "epoch": 0.46016681046879493,
      "grad_norm": 0.26876741647720337,
      "learning_rate": 0.000990802415875755,
      "loss": 0.0031,
      "step": 1600
    },
    {
      "epoch": 0.4745470232959448,
      "grad_norm": 0.0001008793551591225,
      "learning_rate": 0.000990514811619212,
      "loss": 0.0043,
      "step": 1650
    },
    {
      "epoch": 0.48892723612309463,
      "grad_norm": 0.0002746466198004782,
      "learning_rate": 0.0009902272073626691,
      "loss": 0.0016,
      "step": 1700
    },
    {
      "epoch": 0.5033074489502445,
      "grad_norm": 0.00013959116768091917,
      "learning_rate": 0.000989939603106126,
      "loss": 0.0049,
      "step": 1750
    },
    {
      "epoch": 0.5176876617773943,
      "grad_norm": 0.00011754687875509262,
      "learning_rate": 0.000989651998849583,
      "loss": 0.0022,
      "step": 1800
    },
    {
      "epoch": 0.5320678746045442,
      "grad_norm": 0.018085921183228493,
      "learning_rate": 0.00098936439459304,
      "loss": 0.0035,
      "step": 1850
    },
    {
      "epoch": 0.546448087431694,
      "grad_norm": 0.0005703771021217108,
      "learning_rate": 0.000989076790336497,
      "loss": 0.0028,
      "step": 1900
    },
    {
      "epoch": 0.5608283002588438,
      "grad_norm": 3.5283530451124534e-05,
      "learning_rate": 0.000988789186079954,
      "loss": 0.0021,
      "step": 1950
    },
    {
      "epoch": 0.5752085130859936,
      "grad_norm": 7.86307718954049e-05,
      "learning_rate": 0.000988501581823411,
      "loss": 0.0044,
      "step": 2000
    },
    {
      "epoch": 0.5895887259131435,
      "grad_norm": 0.00015990088286343962,
      "learning_rate": 0.000988213977566868,
      "loss": 0.0007,
      "step": 2050
    },
    {
      "epoch": 0.6039689387402933,
      "grad_norm": 0.05878976732492447,
      "learning_rate": 0.000987926373310325,
      "loss": 0.0018,
      "step": 2100
    },
    {
      "epoch": 0.6183491515674432,
      "grad_norm": 0.0005224064807407558,
      "learning_rate": 0.0009876387690537821,
      "loss": 0.0044,
      "step": 2150
    },
    {
      "epoch": 0.632729364394593,
      "grad_norm": 0.039768584072589874,
      "learning_rate": 0.000987351164797239,
      "loss": 0.0024,
      "step": 2200
    },
    {
      "epoch": 0.6471095772217429,
      "grad_norm": 0.0052498141303658485,
      "learning_rate": 0.000987063560540696,
      "loss": 0.0047,
      "step": 2250
    },
    {
      "epoch": 0.6614897900488927,
      "grad_norm": 0.0011406405828893185,
      "learning_rate": 0.000986775956284153,
      "loss": 0.0034,
      "step": 2300
    },
    {
      "epoch": 0.6758700028760426,
      "grad_norm": 0.020373672246932983,
      "learning_rate": 0.00098648835202761,
      "loss": 0.0042,
      "step": 2350
    },
    {
      "epoch": 0.6902502157031924,
      "grad_norm": 0.010980927385389805,
      "learning_rate": 0.000986200747771067,
      "loss": 0.0023,
      "step": 2400
    },
    {
      "epoch": 0.7046304285303423,
      "grad_norm": 0.00024849362671375275,
      "learning_rate": 0.000985913143514524,
      "loss": 0.0048,
      "step": 2450
    },
    {
      "epoch": 0.719010641357492,
      "grad_norm": 0.023436563089489937,
      "learning_rate": 0.000985625539257981,
      "loss": 0.0033,
      "step": 2500
    },
    {
      "epoch": 0.7333908541846419,
      "grad_norm": 0.06787385791540146,
      "learning_rate": 0.000985337935001438,
      "loss": 0.0012,
      "step": 2550
    },
    {
      "epoch": 0.7477710670117917,
      "grad_norm": 0.0005073189968243241,
      "learning_rate": 0.0009850503307448951,
      "loss": 0.0027,
      "step": 2600
    },
    {
      "epoch": 0.7621512798389416,
      "grad_norm": 0.05077870190143585,
      "learning_rate": 0.000984762726488352,
      "loss": 0.0023,
      "step": 2650
    },
    {
      "epoch": 0.7765314926660914,
      "grad_norm": 0.00716278376057744,
      "learning_rate": 0.000984475122231809,
      "loss": 0.0045,
      "step": 2700
    },
    {
      "epoch": 0.7909117054932413,
      "grad_norm": 0.011484568938612938,
      "learning_rate": 0.000984187517975266,
      "loss": 0.002,
      "step": 2750
    },
    {
      "epoch": 0.8052919183203912,
      "grad_norm": 0.00457044318318367,
      "learning_rate": 0.000983899913718723,
      "loss": 0.0011,
      "step": 2800
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 5.131031866767444e-05,
      "learning_rate": 0.00098361230946218,
      "loss": 0.0018,
      "step": 2850
    },
    {
      "epoch": 0.8340523439746909,
      "grad_norm": 0.00025973204174079,
      "learning_rate": 0.0009833247052056372,
      "loss": 0.0052,
      "step": 2900
    },
    {
      "epoch": 0.8484325568018407,
      "grad_norm": 0.00019429581880103797,
      "learning_rate": 0.0009830371009490941,
      "loss": 0.0015,
      "step": 2950
    },
    {
      "epoch": 0.8628127696289906,
      "grad_norm": 0.030744412913918495,
      "learning_rate": 0.000982749496692551,
      "loss": 0.003,
      "step": 3000
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 0.005423381458967924,
      "learning_rate": 0.0009824618924360081,
      "loss": 0.001,
      "step": 3050
    },
    {
      "epoch": 0.8915731952832902,
      "grad_norm": 0.000133428635308519,
      "learning_rate": 0.000982174288179465,
      "loss": 0.0018,
      "step": 3100
    },
    {
      "epoch": 0.90595340811044,
      "grad_norm": 0.00020465312991291285,
      "learning_rate": 0.0009818866839229222,
      "loss": 0.001,
      "step": 3150
    },
    {
      "epoch": 0.9203336209375899,
      "grad_norm": 0.016606565564870834,
      "learning_rate": 0.000981599079666379,
      "loss": 0.0034,
      "step": 3200
    },
    {
      "epoch": 0.9347138337647397,
      "grad_norm": 4.4482792873168364e-05,
      "learning_rate": 0.0009813114754098362,
      "loss": 0.0027,
      "step": 3250
    },
    {
      "epoch": 0.9490940465918896,
      "grad_norm": 0.012614646926522255,
      "learning_rate": 0.000981023871153293,
      "loss": 0.0023,
      "step": 3300
    },
    {
      "epoch": 0.9634742594190394,
      "grad_norm": 8.581902511650696e-05,
      "learning_rate": 0.0009807362668967502,
      "loss": 0.0006,
      "step": 3350
    },
    {
      "epoch": 0.9778544722461893,
      "grad_norm": 0.0010926349787041545,
      "learning_rate": 0.0009804486626402071,
      "loss": 0.0047,
      "step": 3400
    },
    {
      "epoch": 0.9922346850733391,
      "grad_norm": 0.011867048218846321,
      "learning_rate": 0.000980161058383664,
      "loss": 0.0027,
      "step": 3450
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.010526402853429317,
      "eval_runtime": 17.0205,
      "eval_samples_per_second": 2803.611,
      "eval_steps_per_second": 43.829,
      "step": 3477
    },
    {
      "epoch": 1.006614897900489,
      "grad_norm": 0.0028138074558228254,
      "learning_rate": 0.0009798734541271211,
      "loss": 0.0029,
      "step": 3500
    },
    {
      "epoch": 1.0209951107276387,
      "grad_norm": 0.006975580006837845,
      "learning_rate": 0.000979585849870578,
      "loss": 0.0021,
      "step": 3550
    },
    {
      "epoch": 1.0353753235547887,
      "grad_norm": 0.0044575841166079044,
      "learning_rate": 0.0009792982456140352,
      "loss": 0.0065,
      "step": 3600
    },
    {
      "epoch": 1.0497555363819384,
      "grad_norm": 0.00010369835945311934,
      "learning_rate": 0.000979010641357492,
      "loss": 0.0025,
      "step": 3650
    },
    {
      "epoch": 1.0641357492090884,
      "grad_norm": 0.022190719842910767,
      "learning_rate": 0.0009787230371009492,
      "loss": 0.0019,
      "step": 3700
    },
    {
      "epoch": 1.0785159620362381,
      "grad_norm": 0.0007390800747089088,
      "learning_rate": 0.000978435432844406,
      "loss": 0.0032,
      "step": 3750
    },
    {
      "epoch": 1.092896174863388,
      "grad_norm": 0.00015379072283394635,
      "learning_rate": 0.0009781478285878632,
      "loss": 0.0047,
      "step": 3800
    },
    {
      "epoch": 1.1072763876905378,
      "grad_norm": 0.00016345611948054284,
      "learning_rate": 0.0009778602243313201,
      "loss": 0.0016,
      "step": 3850
    },
    {
      "epoch": 1.1216566005176876,
      "grad_norm": 0.013787403702735901,
      "learning_rate": 0.000977572620074777,
      "loss": 0.004,
      "step": 3900
    },
    {
      "epoch": 1.1360368133448375,
      "grad_norm": 0.013450069352984428,
      "learning_rate": 0.0009772850158182341,
      "loss": 0.0034,
      "step": 3950
    },
    {
      "epoch": 1.1504170261719873,
      "grad_norm": 8.799476199783385e-05,
      "learning_rate": 0.000976997411561691,
      "loss": 0.0047,
      "step": 4000
    },
    {
      "epoch": 1.1647972389991372,
      "grad_norm": 6.106002547312528e-05,
      "learning_rate": 0.0009767098073051482,
      "loss": 0.0032,
      "step": 4050
    },
    {
      "epoch": 1.179177451826287,
      "grad_norm": 0.02133622020483017,
      "learning_rate": 0.0009764222030486052,
      "loss": 0.0035,
      "step": 4100
    },
    {
      "epoch": 1.193557664653437,
      "grad_norm": 0.00014061832916922867,
      "learning_rate": 0.0009761345987920621,
      "loss": 0.0032,
      "step": 4150
    },
    {
      "epoch": 1.2079378774805867,
      "grad_norm": 0.013985767960548401,
      "learning_rate": 0.0009758469945355192,
      "loss": 0.0034,
      "step": 4200
    },
    {
      "epoch": 1.2223180903077366,
      "grad_norm": 0.00013648711319547147,
      "learning_rate": 0.0009755593902789761,
      "loss": 0.0036,
      "step": 4250
    },
    {
      "epoch": 1.2366983031348864,
      "grad_norm": 0.01754017546772957,
      "learning_rate": 0.0009752717860224331,
      "loss": 0.0041,
      "step": 4300
    },
    {
      "epoch": 1.2510785159620363,
      "grad_norm": 0.23807808756828308,
      "learning_rate": 0.0009749841817658902,
      "loss": 0.001,
      "step": 4350
    },
    {
      "epoch": 1.265458728789186,
      "grad_norm": 0.07255807518959045,
      "learning_rate": 0.0009746965775093471,
      "loss": 0.0033,
      "step": 4400
    },
    {
      "epoch": 1.2798389416163358,
      "grad_norm": 0.04416953772306442,
      "learning_rate": 0.0009744089732528042,
      "loss": 0.0056,
      "step": 4450
    },
    {
      "epoch": 1.2942191544434858,
      "grad_norm": 0.08848638832569122,
      "learning_rate": 0.0009741213689962612,
      "loss": 0.0063,
      "step": 4500
    },
    {
      "epoch": 1.3085993672706355,
      "grad_norm": 6.468814535764977e-05,
      "learning_rate": 0.0009738337647397182,
      "loss": 0.0057,
      "step": 4550
    },
    {
      "epoch": 1.3229795800977855,
      "grad_norm": 0.001540954108349979,
      "learning_rate": 0.0009735461604831751,
      "loss": 0.0062,
      "step": 4600
    },
    {
      "epoch": 1.3373597929249352,
      "grad_norm": 0.13519582152366638,
      "learning_rate": 0.0009732585562266322,
      "loss": 0.0029,
      "step": 4650
    },
    {
      "epoch": 1.3517400057520852,
      "grad_norm": 0.01091248169541359,
      "learning_rate": 0.0009729709519700892,
      "loss": 0.0027,
      "step": 4700
    },
    {
      "epoch": 1.366120218579235,
      "grad_norm": 0.006889140699058771,
      "learning_rate": 0.0009726833477135461,
      "loss": 0.0035,
      "step": 4750
    },
    {
      "epoch": 1.380500431406385,
      "grad_norm": 0.028092553839087486,
      "learning_rate": 0.0009723957434570032,
      "loss": 0.0045,
      "step": 4800
    },
    {
      "epoch": 1.3948806442335346,
      "grad_norm": 0.13116206228733063,
      "learning_rate": 0.0009721081392004601,
      "loss": 0.0057,
      "step": 4850
    },
    {
      "epoch": 1.4092608570606844,
      "grad_norm": 0.0001926108234329149,
      "learning_rate": 0.0009718205349439172,
      "loss": 0.0055,
      "step": 4900
    },
    {
      "epoch": 1.4236410698878343,
      "grad_norm": 0.02009659633040428,
      "learning_rate": 0.0009715329306873742,
      "loss": 0.0045,
      "step": 4950
    },
    {
      "epoch": 1.4380212827149843,
      "grad_norm": 0.06007599085569382,
      "learning_rate": 0.0009712453264308312,
      "loss": 0.0016,
      "step": 5000
    },
    {
      "epoch": 1.452401495542134,
      "grad_norm": 0.0001896229077829048,
      "learning_rate": 0.0009709577221742882,
      "loss": 0.0046,
      "step": 5050
    },
    {
      "epoch": 1.4667817083692838,
      "grad_norm": 0.0005971050122752786,
      "learning_rate": 0.0009706701179177452,
      "loss": 0.006,
      "step": 5100
    },
    {
      "epoch": 1.4811619211964338,
      "grad_norm": 0.0009857098339125514,
      "learning_rate": 0.0009703825136612022,
      "loss": 0.0014,
      "step": 5150
    },
    {
      "epoch": 1.4955421340235835,
      "grad_norm": 0.0006004741881042719,
      "learning_rate": 0.0009700949094046591,
      "loss": 0.0042,
      "step": 5200
    },
    {
      "epoch": 1.5099223468507335,
      "grad_norm": 0.0001518544158898294,
      "learning_rate": 0.0009698073051481162,
      "loss": 0.0046,
      "step": 5250
    },
    {
      "epoch": 1.5243025596778832,
      "grad_norm": 0.002851051976904273,
      "learning_rate": 0.0009695197008915733,
      "loss": 0.0037,
      "step": 5300
    },
    {
      "epoch": 1.538682772505033,
      "grad_norm": 0.00024935760302469134,
      "learning_rate": 0.0009692320966350302,
      "loss": 0.0014,
      "step": 5350
    },
    {
      "epoch": 1.553062985332183,
      "grad_norm": 0.00018088435172103345,
      "learning_rate": 0.0009689444923784873,
      "loss": 0.0034,
      "step": 5400
    },
    {
      "epoch": 1.5674431981593329,
      "grad_norm": 0.014405342750251293,
      "learning_rate": 0.0009686568881219442,
      "loss": 0.0036,
      "step": 5450
    },
    {
      "epoch": 1.5818234109864826,
      "grad_norm": 0.003865449223667383,
      "learning_rate": 0.0009683692838654012,
      "loss": 0.0063,
      "step": 5500
    },
    {
      "epoch": 1.5962036238136323,
      "grad_norm": 0.0007725924951955676,
      "learning_rate": 0.0009680816796088582,
      "loss": 0.0041,
      "step": 5550
    },
    {
      "epoch": 1.6105838366407823,
      "grad_norm": 0.021678194403648376,
      "learning_rate": 0.0009677940753523152,
      "loss": 0.0057,
      "step": 5600
    },
    {
      "epoch": 1.6249640494679323,
      "grad_norm": 0.00011379682109691203,
      "learning_rate": 0.0009675064710957722,
      "loss": 0.0015,
      "step": 5650
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 0.07793557643890381,
      "learning_rate": 0.0009672188668392292,
      "loss": 0.0037,
      "step": 5700
    },
    {
      "epoch": 1.6537244751222318,
      "grad_norm": 0.0002003927220357582,
      "learning_rate": 0.0009669312625826863,
      "loss": 0.0039,
      "step": 5750
    },
    {
      "epoch": 1.6681046879493815,
      "grad_norm": 0.006638043560087681,
      "learning_rate": 0.0009666436583261432,
      "loss": 0.0014,
      "step": 5800
    },
    {
      "epoch": 1.6824849007765315,
      "grad_norm": 0.0001647080498514697,
      "learning_rate": 0.0009663560540696003,
      "loss": 0.0033,
      "step": 5850
    },
    {
      "epoch": 1.6968651136036814,
      "grad_norm": 0.01579662226140499,
      "learning_rate": 0.0009660684498130573,
      "loss": 0.0014,
      "step": 5900
    },
    {
      "epoch": 1.7112453264308312,
      "grad_norm": 0.00046954548452049494,
      "learning_rate": 0.0009657808455565142,
      "loss": 0.0015,
      "step": 5950
    },
    {
      "epoch": 1.725625539257981,
      "grad_norm": 0.001647424534894526,
      "learning_rate": 0.0009654932412999713,
      "loss": 0.0045,
      "step": 6000
    },
    {
      "epoch": 1.7400057520851309,
      "grad_norm": 0.00012586235243361443,
      "learning_rate": 0.0009652056370434282,
      "loss": 0.0014,
      "step": 6050
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 9.470925579080358e-05,
      "learning_rate": 0.0009649180327868852,
      "loss": 0.0044,
      "step": 6100
    },
    {
      "epoch": 1.7687661777394306,
      "grad_norm": 0.0028427825309336185,
      "learning_rate": 0.0009646304285303422,
      "loss": 0.0023,
      "step": 6150
    },
    {
      "epoch": 1.7831463905665803,
      "grad_norm": 0.0003613557491917163,
      "learning_rate": 0.0009643428242737993,
      "loss": 0.0019,
      "step": 6200
    },
    {
      "epoch": 1.7975266033937303,
      "grad_norm": 0.10504703968763351,
      "learning_rate": 0.0009640552200172563,
      "loss": 0.0029,
      "step": 6250
    },
    {
      "epoch": 1.8119068162208802,
      "grad_norm": 0.0012521123280748725,
      "learning_rate": 0.0009637676157607133,
      "loss": 0.0035,
      "step": 6300
    },
    {
      "epoch": 1.82628702904803,
      "grad_norm": 0.00012881599832326174,
      "learning_rate": 0.0009634800115041703,
      "loss": 0.0023,
      "step": 6350
    },
    {
      "epoch": 1.8406672418751797,
      "grad_norm": 0.006565559655427933,
      "learning_rate": 0.0009631924072476272,
      "loss": 0.0034,
      "step": 6400
    },
    {
      "epoch": 1.8550474547023295,
      "grad_norm": 0.007715684361755848,
      "learning_rate": 0.0009629048029910843,
      "loss": 0.0026,
      "step": 6450
    },
    {
      "epoch": 1.8694276675294794,
      "grad_norm": 0.10777425765991211,
      "learning_rate": 0.0009626171987345413,
      "loss": 0.0014,
      "step": 6500
    },
    {
      "epoch": 1.8838078803566294,
      "grad_norm": 0.04109620302915573,
      "learning_rate": 0.0009623295944779982,
      "loss": 0.0007,
      "step": 6550
    },
    {
      "epoch": 1.8981880931837791,
      "grad_norm": 0.0002595782862044871,
      "learning_rate": 0.0009620419902214554,
      "loss": 0.0006,
      "step": 6600
    },
    {
      "epoch": 1.9125683060109289,
      "grad_norm": 0.00563693605363369,
      "learning_rate": 0.0009617543859649123,
      "loss": 0.004,
      "step": 6650
    },
    {
      "epoch": 1.9269485188380788,
      "grad_norm": 0.0011222816538065672,
      "learning_rate": 0.0009614667817083693,
      "loss": 0.0027,
      "step": 6700
    },
    {
      "epoch": 1.9413287316652288,
      "grad_norm": 0.03373021259903908,
      "learning_rate": 0.0009611791774518263,
      "loss": 0.0027,
      "step": 6750
    },
    {
      "epoch": 1.9557089444923785,
      "grad_norm": 0.00017049552116077393,
      "learning_rate": 0.0009608915731952833,
      "loss": 0.0028,
      "step": 6800
    },
    {
      "epoch": 1.9700891573195283,
      "grad_norm": 0.003357719164341688,
      "learning_rate": 0.0009606039689387403,
      "loss": 0.0036,
      "step": 6850
    },
    {
      "epoch": 1.984469370146678,
      "grad_norm": 0.0009131688857451081,
      "learning_rate": 0.0009603163646821973,
      "loss": 0.001,
      "step": 6900
    },
    {
      "epoch": 1.998849582973828,
      "grad_norm": 0.10870469361543655,
      "learning_rate": 0.0009600287604256543,
      "loss": 0.0025,
      "step": 6950
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.010953507386147976,
      "eval_runtime": 19.1223,
      "eval_samples_per_second": 2495.469,
      "eval_steps_per_second": 39.012,
      "step": 6954
    },
    {
      "epoch": 2.013229795800978,
      "grad_norm": 0.017226815223693848,
      "learning_rate": 0.0009597411561691112,
      "loss": 0.003,
      "step": 7000
    },
    {
      "epoch": 2.0276100086281277,
      "grad_norm": 0.026358533650636673,
      "learning_rate": 0.0009594535519125684,
      "loss": 0.0026,
      "step": 7050
    },
    {
      "epoch": 2.0419902214552774,
      "grad_norm": 5.610207881545648e-05,
      "learning_rate": 0.0009591659476560254,
      "loss": 0.0022,
      "step": 7100
    },
    {
      "epoch": 2.056370434282427,
      "grad_norm": 0.04790158569812775,
      "learning_rate": 0.0009588783433994823,
      "loss": 0.0019,
      "step": 7150
    },
    {
      "epoch": 2.0707506471095773,
      "grad_norm": 0.0009793753270059824,
      "learning_rate": 0.0009585907391429394,
      "loss": 0.003,
      "step": 7200
    },
    {
      "epoch": 2.085130859936727,
      "grad_norm": 0.04886336252093315,
      "learning_rate": 0.0009583031348863963,
      "loss": 0.0006,
      "step": 7250
    },
    {
      "epoch": 2.099511072763877,
      "grad_norm": 8.163329039234668e-05,
      "learning_rate": 0.0009580155306298533,
      "loss": 0.0013,
      "step": 7300
    },
    {
      "epoch": 2.1138912855910266,
      "grad_norm": 8.493756467942148e-05,
      "learning_rate": 0.0009577279263733103,
      "loss": 0.0042,
      "step": 7350
    },
    {
      "epoch": 2.1282714984181768,
      "grad_norm": 0.008857173845171928,
      "learning_rate": 0.0009574403221167673,
      "loss": 0.006,
      "step": 7400
    },
    {
      "epoch": 2.1426517112453265,
      "grad_norm": 0.2102125585079193,
      "learning_rate": 0.0009571527178602243,
      "loss": 0.0043,
      "step": 7450
    },
    {
      "epoch": 2.1570319240724762,
      "grad_norm": 0.0002267297386424616,
      "learning_rate": 0.0009568651136036814,
      "loss": 0.0047,
      "step": 7500
    },
    {
      "epoch": 2.171412136899626,
      "grad_norm": 0.00015099091979209334,
      "learning_rate": 0.0009565775093471384,
      "loss": 0.0015,
      "step": 7550
    },
    {
      "epoch": 2.185792349726776,
      "grad_norm": 0.004134874325245619,
      "learning_rate": 0.0009562899050905953,
      "loss": 0.0027,
      "step": 7600
    },
    {
      "epoch": 2.200172562553926,
      "grad_norm": 0.02182961441576481,
      "learning_rate": 0.0009560023008340524,
      "loss": 0.0025,
      "step": 7650
    },
    {
      "epoch": 2.2145527753810756,
      "grad_norm": 0.03324694186449051,
      "learning_rate": 0.0009557146965775093,
      "loss": 0.002,
      "step": 7700
    },
    {
      "epoch": 2.2289329882082254,
      "grad_norm": 0.005017496179789305,
      "learning_rate": 0.0009554270923209663,
      "loss": 0.0037,
      "step": 7750
    },
    {
      "epoch": 2.243313201035375,
      "grad_norm": 0.0004502972587943077,
      "learning_rate": 0.0009551394880644234,
      "loss": 0.0036,
      "step": 7800
    },
    {
      "epoch": 2.2576934138625253,
      "grad_norm": 0.0034192868042737246,
      "learning_rate": 0.0009548518838078803,
      "loss": 0.0016,
      "step": 7850
    },
    {
      "epoch": 2.272073626689675,
      "grad_norm": 0.0011966318124905229,
      "learning_rate": 0.0009545642795513374,
      "loss": 0.0017,
      "step": 7900
    },
    {
      "epoch": 2.286453839516825,
      "grad_norm": 0.030740350484848022,
      "learning_rate": 0.0009542766752947944,
      "loss": 0.0016,
      "step": 7950
    },
    {
      "epoch": 2.3008340523439745,
      "grad_norm": 7.414869469357654e-05,
      "learning_rate": 0.0009539890710382514,
      "loss": 0.0027,
      "step": 8000
    },
    {
      "epoch": 2.3152142651711247,
      "grad_norm": 0.01967107318341732,
      "learning_rate": 0.0009537014667817084,
      "loss": 0.0028,
      "step": 8050
    },
    {
      "epoch": 2.3295944779982745,
      "grad_norm": 0.00010516938345972449,
      "learning_rate": 0.0009534138625251654,
      "loss": 0.0017,
      "step": 8100
    },
    {
      "epoch": 2.343974690825424,
      "grad_norm": 0.0005889614694751799,
      "learning_rate": 0.0009531262582686224,
      "loss": 0.0063,
      "step": 8150
    },
    {
      "epoch": 2.358354903652574,
      "grad_norm": 0.07598575204610825,
      "learning_rate": 0.0009528386540120793,
      "loss": 0.0019,
      "step": 8200
    },
    {
      "epoch": 2.372735116479724,
      "grad_norm": 0.0005735246231779456,
      "learning_rate": 0.0009525510497555364,
      "loss": 0.002,
      "step": 8250
    },
    {
      "epoch": 2.387115329306874,
      "grad_norm": 0.0007472290308214724,
      "learning_rate": 0.0009522634454989933,
      "loss": 0.0026,
      "step": 8300
    },
    {
      "epoch": 2.4014955421340236,
      "grad_norm": 0.0006792037165723741,
      "learning_rate": 0.0009519758412424504,
      "loss": 0.0016,
      "step": 8350
    },
    {
      "epoch": 2.4158757549611733,
      "grad_norm": 0.0007011480629444122,
      "learning_rate": 0.0009516882369859075,
      "loss": 0.0011,
      "step": 8400
    },
    {
      "epoch": 2.430255967788323,
      "grad_norm": 0.0005489727482199669,
      "learning_rate": 0.0009514006327293644,
      "loss": 0.0025,
      "step": 8450
    },
    {
      "epoch": 2.4446361806154733,
      "grad_norm": 0.0001522094098618254,
      "learning_rate": 0.0009511130284728214,
      "loss": 0.0018,
      "step": 8500
    },
    {
      "epoch": 2.459016393442623,
      "grad_norm": 0.06795293837785721,
      "learning_rate": 0.0009508254242162784,
      "loss": 0.0013,
      "step": 8550
    },
    {
      "epoch": 2.4733966062697728,
      "grad_norm": 0.03810068592429161,
      "learning_rate": 0.0009505378199597354,
      "loss": 0.0011,
      "step": 8600
    },
    {
      "epoch": 2.4877768190969225,
      "grad_norm": 0.0077244192361831665,
      "learning_rate": 0.0009502502157031924,
      "loss": 0.0052,
      "step": 8650
    },
    {
      "epoch": 2.5021570319240727,
      "grad_norm": 0.002095903269946575,
      "learning_rate": 0.0009499626114466494,
      "loss": 0.0025,
      "step": 8700
    },
    {
      "epoch": 2.5165372447512224,
      "grad_norm": 0.0025366642512381077,
      "learning_rate": 0.0009496750071901065,
      "loss": 0.004,
      "step": 8750
    },
    {
      "epoch": 2.530917457578372,
      "grad_norm": 0.02479581907391548,
      "learning_rate": 0.0009493874029335634,
      "loss": 0.0015,
      "step": 8800
    },
    {
      "epoch": 2.545297670405522,
      "grad_norm": 0.004827072378247976,
      "learning_rate": 0.0009490997986770205,
      "loss": 0.0029,
      "step": 8850
    },
    {
      "epoch": 2.5596778832326716,
      "grad_norm": 0.0365603044629097,
      "learning_rate": 0.0009488121944204774,
      "loss": 0.0062,
      "step": 8900
    },
    {
      "epoch": 2.574058096059822,
      "grad_norm": 0.0001484023523516953,
      "learning_rate": 0.0009485245901639344,
      "loss": 0.0028,
      "step": 8950
    },
    {
      "epoch": 2.5884383088869716,
      "grad_norm": 0.00010989970905939117,
      "learning_rate": 0.0009482369859073915,
      "loss": 0.0044,
      "step": 9000
    },
    {
      "epoch": 2.6028185217141213,
      "grad_norm": 0.00021487179037649184,
      "learning_rate": 0.0009479493816508484,
      "loss": 0.0029,
      "step": 9050
    },
    {
      "epoch": 2.617198734541271,
      "grad_norm": 0.00040783945587463677,
      "learning_rate": 0.0009476617773943054,
      "loss": 0.0058,
      "step": 9100
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 0.000944676750805229,
      "learning_rate": 0.0009473741731377624,
      "loss": 0.0028,
      "step": 9150
    },
    {
      "epoch": 2.645959160195571,
      "grad_norm": 0.02337396703660488,
      "learning_rate": 0.0009470865688812195,
      "loss": 0.0033,
      "step": 9200
    },
    {
      "epoch": 2.6603393730227207,
      "grad_norm": 0.00023588766634929925,
      "learning_rate": 0.0009467989646246765,
      "loss": 0.0022,
      "step": 9250
    },
    {
      "epoch": 2.6747195858498705,
      "grad_norm": 0.0005507753230631351,
      "learning_rate": 0.0009465113603681335,
      "loss": 0.005,
      "step": 9300
    },
    {
      "epoch": 2.68909979867702,
      "grad_norm": 0.00021175282017793506,
      "learning_rate": 0.0009462237561115905,
      "loss": 0.0017,
      "step": 9350
    },
    {
      "epoch": 2.7034800115041704,
      "grad_norm": 0.001393888727761805,
      "learning_rate": 0.0009459361518550474,
      "loss": 0.0079,
      "step": 9400
    },
    {
      "epoch": 2.71786022433132,
      "grad_norm": 0.0004672286449931562,
      "learning_rate": 0.0009456485475985045,
      "loss": 0.0013,
      "step": 9450
    },
    {
      "epoch": 2.73224043715847,
      "grad_norm": 0.003274873597547412,
      "learning_rate": 0.0009453609433419614,
      "loss": 0.004,
      "step": 9500
    },
    {
      "epoch": 2.74662064998562,
      "grad_norm": 0.00018760349485091865,
      "learning_rate": 0.0009450733390854184,
      "loss": 0.0032,
      "step": 9550
    },
    {
      "epoch": 2.76100086281277,
      "grad_norm": 0.1130153015255928,
      "learning_rate": 0.0009447857348288756,
      "loss": 0.0067,
      "step": 9600
    },
    {
      "epoch": 2.7753810756399195,
      "grad_norm": 0.0050518508069217205,
      "learning_rate": 0.0009444981305723325,
      "loss": 0.004,
      "step": 9650
    },
    {
      "epoch": 2.7897612884670693,
      "grad_norm": 0.0007746358751319349,
      "learning_rate": 0.0009442105263157895,
      "loss": 0.0037,
      "step": 9700
    },
    {
      "epoch": 2.804141501294219,
      "grad_norm": 0.00019135148613713682,
      "learning_rate": 0.0009439229220592465,
      "loss": 0.0016,
      "step": 9750
    },
    {
      "epoch": 2.8185217141213688,
      "grad_norm": 0.07309357821941376,
      "learning_rate": 0.0009436353178027035,
      "loss": 0.0059,
      "step": 9800
    },
    {
      "epoch": 2.832901926948519,
      "grad_norm": 0.14664575457572937,
      "learning_rate": 0.0009433477135461605,
      "loss": 0.0012,
      "step": 9850
    },
    {
      "epoch": 2.8472821397756687,
      "grad_norm": 0.00043900718446820974,
      "learning_rate": 0.0009430601092896175,
      "loss": 0.0057,
      "step": 9900
    },
    {
      "epoch": 2.8616623526028184,
      "grad_norm": 0.0001627492456464097,
      "learning_rate": 0.0009427725050330745,
      "loss": 0.0029,
      "step": 9950
    },
    {
      "epoch": 2.8760425654299686,
      "grad_norm": 0.0004414579307194799,
      "learning_rate": 0.0009424849007765314,
      "loss": 0.0036,
      "step": 10000
    },
    {
      "epoch": 2.8904227782571184,
      "grad_norm": 0.000694042188115418,
      "learning_rate": 0.0009421972965199886,
      "loss": 0.004,
      "step": 10050
    },
    {
      "epoch": 2.904802991084268,
      "grad_norm": 0.06309361010789871,
      "learning_rate": 0.0009419096922634455,
      "loss": 0.0022,
      "step": 10100
    },
    {
      "epoch": 2.919183203911418,
      "grad_norm": 0.019856765866279602,
      "learning_rate": 0.0009416220880069025,
      "loss": 0.0009,
      "step": 10150
    },
    {
      "epoch": 2.9335634167385676,
      "grad_norm": 8.136955875670537e-05,
      "learning_rate": 0.0009413344837503596,
      "loss": 0.0029,
      "step": 10200
    },
    {
      "epoch": 2.9479436295657173,
      "grad_norm": 0.10819751024246216,
      "learning_rate": 0.0009410468794938165,
      "loss": 0.0057,
      "step": 10250
    },
    {
      "epoch": 2.9623238423928675,
      "grad_norm": 0.054982416331768036,
      "learning_rate": 0.0009407592752372735,
      "loss": 0.0041,
      "step": 10300
    },
    {
      "epoch": 2.9767040552200172,
      "grad_norm": 0.0070265112444758415,
      "learning_rate": 0.0009404716709807305,
      "loss": 0.002,
      "step": 10350
    },
    {
      "epoch": 2.991084268047167,
      "grad_norm": 0.07188228517770767,
      "learning_rate": 0.0009401840667241875,
      "loss": 0.0013,
      "step": 10400
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.010180546902120113,
      "eval_runtime": 16.6269,
      "eval_samples_per_second": 2869.985,
      "eval_steps_per_second": 44.867,
      "step": 10431
    },
    {
      "epoch": 3.0054644808743167,
      "grad_norm": 0.0017000290099531412,
      "learning_rate": 0.0009398964624676444,
      "loss": 0.0051,
      "step": 10450
    },
    {
      "epoch": 3.019844693701467,
      "grad_norm": 0.0006991426926106215,
      "learning_rate": 0.0009396088582111016,
      "loss": 0.003,
      "step": 10500
    },
    {
      "epoch": 3.0342249065286166,
      "grad_norm": 0.00050606724107638,
      "learning_rate": 0.0009393212539545586,
      "loss": 0.0032,
      "step": 10550
    },
    {
      "epoch": 3.0486051193557664,
      "grad_norm": 0.001587900216691196,
      "learning_rate": 0.0009390336496980155,
      "loss": 0.0053,
      "step": 10600
    },
    {
      "epoch": 3.062985332182916,
      "grad_norm": 0.0002467128215357661,
      "learning_rate": 0.0009387460454414726,
      "loss": 0.0043,
      "step": 10650
    },
    {
      "epoch": 3.0773655450100663,
      "grad_norm": 0.008596235886216164,
      "learning_rate": 0.0009384584411849295,
      "loss": 0.0022,
      "step": 10700
    },
    {
      "epoch": 3.091745757837216,
      "grad_norm": 0.006316909566521645,
      "learning_rate": 0.0009381708369283865,
      "loss": 0.0043,
      "step": 10750
    },
    {
      "epoch": 3.106125970664366,
      "grad_norm": 0.00011094332148786634,
      "learning_rate": 0.0009378832326718436,
      "loss": 0.0027,
      "step": 10800
    },
    {
      "epoch": 3.1205061834915155,
      "grad_norm": 0.0003161021159030497,
      "learning_rate": 0.0009375956284153005,
      "loss": 0.0036,
      "step": 10850
    },
    {
      "epoch": 3.1348863963186657,
      "grad_norm": 0.0002669449313543737,
      "learning_rate": 0.0009373080241587575,
      "loss": 0.0014,
      "step": 10900
    },
    {
      "epoch": 3.1492666091458155,
      "grad_norm": 0.012097095139324665,
      "learning_rate": 0.0009370204199022146,
      "loss": 0.0039,
      "step": 10950
    },
    {
      "epoch": 3.163646821972965,
      "grad_norm": 0.0002685905492398888,
      "learning_rate": 0.0009367328156456716,
      "loss": 0.0042,
      "step": 11000
    },
    {
      "epoch": 3.178027034800115,
      "grad_norm": 0.00019234654610045254,
      "learning_rate": 0.0009364452113891285,
      "loss": 0.0037,
      "step": 11050
    },
    {
      "epoch": 3.1924072476272647,
      "grad_norm": 0.0005915600340813398,
      "learning_rate": 0.0009361576071325856,
      "loss": 0.0016,
      "step": 11100
    },
    {
      "epoch": 3.206787460454415,
      "grad_norm": 0.03012719936668873,
      "learning_rate": 0.0009358700028760426,
      "loss": 0.0062,
      "step": 11150
    },
    {
      "epoch": 3.2211676732815646,
      "grad_norm": 0.04669320583343506,
      "learning_rate": 0.0009355823986194995,
      "loss": 0.0049,
      "step": 11200
    },
    {
      "epoch": 3.2355478861087144,
      "grad_norm": 0.0016327338526025414,
      "learning_rate": 0.0009352947943629566,
      "loss": 0.0006,
      "step": 11250
    },
    {
      "epoch": 3.249928098935864,
      "grad_norm": 0.032900191843509674,
      "learning_rate": 0.0009350071901064135,
      "loss": 0.0034,
      "step": 11300
    },
    {
      "epoch": 3.2643083117630143,
      "grad_norm": 4.6906072384445e-05,
      "learning_rate": 0.0009347195858498705,
      "loss": 0.0024,
      "step": 11350
    },
    {
      "epoch": 3.278688524590164,
      "grad_norm": 0.007359127048403025,
      "learning_rate": 0.0009344319815933277,
      "loss": 0.0013,
      "step": 11400
    },
    {
      "epoch": 3.2930687374173138,
      "grad_norm": 0.2700754702091217,
      "learning_rate": 0.0009341443773367846,
      "loss": 0.0033,
      "step": 11450
    },
    {
      "epoch": 3.3074489502444635,
      "grad_norm": 0.015016899444162846,
      "learning_rate": 0.0009338567730802416,
      "loss": 0.0035,
      "step": 11500
    },
    {
      "epoch": 3.3218291630716132,
      "grad_norm": 0.0019407430663704872,
      "learning_rate": 0.0009335691688236986,
      "loss": 0.0016,
      "step": 11550
    },
    {
      "epoch": 3.3362093758987634,
      "grad_norm": 0.0007303602178581059,
      "learning_rate": 0.0009332815645671556,
      "loss": 0.0018,
      "step": 11600
    },
    {
      "epoch": 3.350589588725913,
      "grad_norm": 3.785606531891972e-05,
      "learning_rate": 0.0009329939603106125,
      "loss": 0.0024,
      "step": 11650
    },
    {
      "epoch": 3.364969801553063,
      "grad_norm": 0.0006308754673227668,
      "learning_rate": 0.0009327063560540696,
      "loss": 0.0008,
      "step": 11700
    },
    {
      "epoch": 3.3793500143802127,
      "grad_norm": 0.004224003758281469,
      "learning_rate": 0.0009324187517975266,
      "loss": 0.0038,
      "step": 11750
    },
    {
      "epoch": 3.393730227207363,
      "grad_norm": 0.0016804004553705454,
      "learning_rate": 0.0009321311475409836,
      "loss": 0.0032,
      "step": 11800
    },
    {
      "epoch": 3.4081104400345126,
      "grad_norm": 0.0011080971453338861,
      "learning_rate": 0.0009318435432844407,
      "loss": 0.0022,
      "step": 11850
    },
    {
      "epoch": 3.4224906528616623,
      "grad_norm": 0.0013747097691521049,
      "learning_rate": 0.0009315559390278976,
      "loss": 0.0028,
      "step": 11900
    },
    {
      "epoch": 3.436870865688812,
      "grad_norm": 0.00022203760454431176,
      "learning_rate": 0.0009312683347713546,
      "loss": 0.0023,
      "step": 11950
    },
    {
      "epoch": 3.451251078515962,
      "grad_norm": 0.0256979800760746,
      "learning_rate": 0.0009309807305148117,
      "loss": 0.0013,
      "step": 12000
    },
    {
      "epoch": 3.465631291343112,
      "grad_norm": 0.00043890217784792185,
      "learning_rate": 0.0009306931262582686,
      "loss": 0.0024,
      "step": 12050
    },
    {
      "epoch": 3.4800115041702617,
      "grad_norm": 0.08258029818534851,
      "learning_rate": 0.0009304055220017257,
      "loss": 0.0024,
      "step": 12100
    },
    {
      "epoch": 3.4943917169974115,
      "grad_norm": 0.001182793639600277,
      "learning_rate": 0.0009301179177451826,
      "loss": 0.0044,
      "step": 12150
    },
    {
      "epoch": 3.5087719298245617,
      "grad_norm": 0.00010800011659739539,
      "learning_rate": 0.0009298303134886396,
      "loss": 0.0029,
      "step": 12200
    },
    {
      "epoch": 3.5231521426517114,
      "grad_norm": 0.16374525427818298,
      "learning_rate": 0.0009295427092320966,
      "loss": 0.0058,
      "step": 12250
    },
    {
      "epoch": 3.537532355478861,
      "grad_norm": 0.00011891114991158247,
      "learning_rate": 0.0009292551049755537,
      "loss": 0.0021,
      "step": 12300
    },
    {
      "epoch": 3.551912568306011,
      "grad_norm": 0.00023914838675409555,
      "learning_rate": 0.0009289675007190107,
      "loss": 0.0022,
      "step": 12350
    },
    {
      "epoch": 3.5662927811331606,
      "grad_norm": 0.001688192249275744,
      "learning_rate": 0.0009286798964624676,
      "loss": 0.005,
      "step": 12400
    },
    {
      "epoch": 3.5806729939603104,
      "grad_norm": 0.06307843327522278,
      "learning_rate": 0.0009283922922059247,
      "loss": 0.0035,
      "step": 12450
    },
    {
      "epoch": 3.5950532067874605,
      "grad_norm": 0.0009530865354463458,
      "learning_rate": 0.0009281046879493816,
      "loss": 0.0041,
      "step": 12500
    },
    {
      "epoch": 3.6094334196146103,
      "grad_norm": 0.00020795752061530948,
      "learning_rate": 0.0009278170836928387,
      "loss": 0.0043,
      "step": 12550
    },
    {
      "epoch": 3.62381363244176,
      "grad_norm": 0.04212401807308197,
      "learning_rate": 0.0009275294794362957,
      "loss": 0.0019,
      "step": 12600
    },
    {
      "epoch": 3.63819384526891,
      "grad_norm": 0.00018903282762039453,
      "learning_rate": 0.0009272418751797527,
      "loss": 0.0026,
      "step": 12650
    },
    {
      "epoch": 3.65257405809606,
      "grad_norm": 0.0003575083683244884,
      "learning_rate": 0.0009269542709232098,
      "loss": 0.0035,
      "step": 12700
    },
    {
      "epoch": 3.6669542709232097,
      "grad_norm": 8.231074752984568e-05,
      "learning_rate": 0.0009266666666666667,
      "loss": 0.0027,
      "step": 12750
    },
    {
      "epoch": 3.6813344837503594,
      "grad_norm": 0.022913150489330292,
      "learning_rate": 0.0009263790624101237,
      "loss": 0.004,
      "step": 12800
    },
    {
      "epoch": 3.695714696577509,
      "grad_norm": 0.028911365196108818,
      "learning_rate": 0.0009260914581535806,
      "loss": 0.0018,
      "step": 12850
    },
    {
      "epoch": 3.710094909404659,
      "grad_norm": 0.0081450454890728,
      "learning_rate": 0.0009258038538970377,
      "loss": 0.0027,
      "step": 12900
    },
    {
      "epoch": 3.724475122231809,
      "grad_norm": 0.0005563368322327733,
      "learning_rate": 0.0009255162496404947,
      "loss": 0.0011,
      "step": 12950
    },
    {
      "epoch": 3.738855335058959,
      "grad_norm": 0.021634792909026146,
      "learning_rate": 0.0009252286453839517,
      "loss": 0.0046,
      "step": 13000
    },
    {
      "epoch": 3.7532355478861086,
      "grad_norm": 0.0006277926149778068,
      "learning_rate": 0.0009249410411274087,
      "loss": 0.0064,
      "step": 13050
    },
    {
      "epoch": 3.7676157607132588,
      "grad_norm": 0.00023247233184520155,
      "learning_rate": 0.0009246534368708657,
      "loss": 0.0023,
      "step": 13100
    },
    {
      "epoch": 3.7819959735404085,
      "grad_norm": 0.00045640591997653246,
      "learning_rate": 0.0009243658326143228,
      "loss": 0.0058,
      "step": 13150
    },
    {
      "epoch": 3.7963761863675582,
      "grad_norm": 6.528451922349632e-05,
      "learning_rate": 0.0009240782283577797,
      "loss": 0.0035,
      "step": 13200
    },
    {
      "epoch": 3.810756399194708,
      "grad_norm": 3.193200973328203e-05,
      "learning_rate": 0.0009237906241012367,
      "loss": 0.0037,
      "step": 13250
    },
    {
      "epoch": 3.8251366120218577,
      "grad_norm": 0.0019390026573091745,
      "learning_rate": 0.0009235030198446938,
      "loss": 0.0008,
      "step": 13300
    },
    {
      "epoch": 3.839516824849008,
      "grad_norm": 0.0014205968473106623,
      "learning_rate": 0.0009232154155881507,
      "loss": 0.0026,
      "step": 13350
    },
    {
      "epoch": 3.8538970376761577,
      "grad_norm": 0.05335541069507599,
      "learning_rate": 0.0009229278113316077,
      "loss": 0.001,
      "step": 13400
    },
    {
      "epoch": 3.8682772505033074,
      "grad_norm": 0.05264423042535782,
      "learning_rate": 0.0009226402070750647,
      "loss": 0.0035,
      "step": 13450
    },
    {
      "epoch": 3.882657463330457,
      "grad_norm": 6.116063013905659e-05,
      "learning_rate": 0.0009223526028185218,
      "loss": 0.0066,
      "step": 13500
    },
    {
      "epoch": 3.8970376761576073,
      "grad_norm": 7.465619273716584e-05,
      "learning_rate": 0.0009220649985619788,
      "loss": 0.0038,
      "step": 13550
    },
    {
      "epoch": 3.911417888984757,
      "grad_norm": 5.911625339649618e-05,
      "learning_rate": 0.0009217773943054358,
      "loss": 0.0023,
      "step": 13600
    },
    {
      "epoch": 3.925798101811907,
      "grad_norm": 0.0024094032123684883,
      "learning_rate": 0.0009214897900488928,
      "loss": 0.0026,
      "step": 13650
    },
    {
      "epoch": 3.9401783146390565,
      "grad_norm": 6.926412606844679e-05,
      "learning_rate": 0.0009212021857923497,
      "loss": 0.0043,
      "step": 13700
    },
    {
      "epoch": 3.9545585274662063,
      "grad_norm": 0.03651101514697075,
      "learning_rate": 0.0009209145815358068,
      "loss": 0.0027,
      "step": 13750
    },
    {
      "epoch": 3.9689387402933565,
      "grad_norm": 0.0009108211961574852,
      "learning_rate": 0.0009206269772792637,
      "loss": 0.0012,
      "step": 13800
    },
    {
      "epoch": 3.983318953120506,
      "grad_norm": 0.03611027076840401,
      "learning_rate": 0.0009203393730227207,
      "loss": 0.0042,
      "step": 13850
    },
    {
      "epoch": 3.997699165947656,
      "grad_norm": 0.0003915058623533696,
      "learning_rate": 0.0009200517687661779,
      "loss": 0.002,
      "step": 13900
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.010776981711387634,
      "eval_runtime": 17.4105,
      "eval_samples_per_second": 2740.816,
      "eval_steps_per_second": 42.848,
      "step": 13908
    },
    {
      "epoch": 4.012079378774806,
      "grad_norm": 0.07578763365745544,
      "learning_rate": 0.0009197641645096348,
      "loss": 0.0019,
      "step": 13950
    },
    {
      "epoch": 4.026459591601956,
      "grad_norm": 0.0006943584303371608,
      "learning_rate": 0.0009194765602530918,
      "loss": 0.0063,
      "step": 14000
    },
    {
      "epoch": 4.040839804429106,
      "grad_norm": 0.0007912368746474385,
      "learning_rate": 0.0009191889559965488,
      "loss": 0.0053,
      "step": 14050
    },
    {
      "epoch": 4.055220017256255,
      "grad_norm": 0.0030468080658465624,
      "learning_rate": 0.0009189013517400058,
      "loss": 0.0046,
      "step": 14100
    },
    {
      "epoch": 4.069600230083405,
      "grad_norm": 0.0002660219033714384,
      "learning_rate": 0.0009186137474834628,
      "loss": 0.0052,
      "step": 14150
    },
    {
      "epoch": 4.083980442910555,
      "grad_norm": 0.020164506509900093,
      "learning_rate": 0.0009183261432269198,
      "loss": 0.003,
      "step": 14200
    },
    {
      "epoch": 4.098360655737705,
      "grad_norm": 0.0006130823749117553,
      "learning_rate": 0.0009180385389703768,
      "loss": 0.0014,
      "step": 14250
    },
    {
      "epoch": 4.112740868564854,
      "grad_norm": 0.00037577361217699945,
      "learning_rate": 0.0009177509347138337,
      "loss": 0.0047,
      "step": 14300
    },
    {
      "epoch": 4.127121081392005,
      "grad_norm": 0.03761625662446022,
      "learning_rate": 0.0009174633304572909,
      "loss": 0.0052,
      "step": 14350
    },
    {
      "epoch": 4.141501294219155,
      "grad_norm": 8.896917279344052e-05,
      "learning_rate": 0.0009171757262007478,
      "loss": 0.0031,
      "step": 14400
    },
    {
      "epoch": 4.155881507046304,
      "grad_norm": 0.0003806121530942619,
      "learning_rate": 0.0009168881219442048,
      "loss": 0.0028,
      "step": 14450
    },
    {
      "epoch": 4.170261719873454,
      "grad_norm": 0.00011310071568004787,
      "learning_rate": 0.0009166005176876619,
      "loss": 0.0015,
      "step": 14500
    },
    {
      "epoch": 4.184641932700604,
      "grad_norm": 0.005730039905756712,
      "learning_rate": 0.0009163129134311188,
      "loss": 0.0021,
      "step": 14550
    },
    {
      "epoch": 4.199022145527754,
      "grad_norm": 0.0006279484950937331,
      "learning_rate": 0.0009160253091745758,
      "loss": 0.0025,
      "step": 14600
    },
    {
      "epoch": 4.213402358354903,
      "grad_norm": 0.0006608570693060756,
      "learning_rate": 0.0009157377049180328,
      "loss": 0.0027,
      "step": 14650
    },
    {
      "epoch": 4.227782571182053,
      "grad_norm": 0.014179178513586521,
      "learning_rate": 0.0009154501006614898,
      "loss": 0.003,
      "step": 14700
    },
    {
      "epoch": 4.242162784009203,
      "grad_norm": 0.014833774417638779,
      "learning_rate": 0.0009151624964049468,
      "loss": 0.003,
      "step": 14750
    },
    {
      "epoch": 4.2565429968363535,
      "grad_norm": 0.0003257201751694083,
      "learning_rate": 0.0009148748921484039,
      "loss": 0.0032,
      "step": 14800
    },
    {
      "epoch": 4.270923209663503,
      "grad_norm": 0.006452851463109255,
      "learning_rate": 0.0009145872878918609,
      "loss": 0.0016,
      "step": 14850
    },
    {
      "epoch": 4.285303422490653,
      "grad_norm": 0.022091150283813477,
      "learning_rate": 0.0009142996836353178,
      "loss": 0.0041,
      "step": 14900
    },
    {
      "epoch": 4.299683635317803,
      "grad_norm": 0.000317933241603896,
      "learning_rate": 0.0009140120793787749,
      "loss": 0.0016,
      "step": 14950
    },
    {
      "epoch": 4.3140638481449525,
      "grad_norm": 0.0002468244347255677,
      "learning_rate": 0.0009137244751222318,
      "loss": 0.002,
      "step": 15000
    },
    {
      "epoch": 4.328444060972102,
      "grad_norm": 0.00033494451781734824,
      "learning_rate": 0.0009134368708656888,
      "loss": 0.0027,
      "step": 15050
    },
    {
      "epoch": 4.342824273799252,
      "grad_norm": 0.0001104229741031304,
      "learning_rate": 0.0009131492666091459,
      "loss": 0.0057,
      "step": 15100
    },
    {
      "epoch": 4.357204486626402,
      "grad_norm": 0.00014851738524157554,
      "learning_rate": 0.0009128616623526028,
      "loss": 0.0035,
      "step": 15150
    },
    {
      "epoch": 4.371584699453552,
      "grad_norm": 0.001284414087422192,
      "learning_rate": 0.0009125740580960598,
      "loss": 0.0029,
      "step": 15200
    },
    {
      "epoch": 4.385964912280702,
      "grad_norm": 0.00676223449409008,
      "learning_rate": 0.0009122864538395169,
      "loss": 0.0024,
      "step": 15250
    },
    {
      "epoch": 4.400345125107852,
      "grad_norm": 0.0006650393479503691,
      "learning_rate": 0.0009119988495829739,
      "loss": 0.0021,
      "step": 15300
    },
    {
      "epoch": 4.4147253379350015,
      "grad_norm": 0.0003326642618048936,
      "learning_rate": 0.0009117112453264309,
      "loss": 0.004,
      "step": 15350
    },
    {
      "epoch": 4.429105550762151,
      "grad_norm": 0.0015065865591168404,
      "learning_rate": 0.0009114236410698879,
      "loss": 0.0083,
      "step": 15400
    },
    {
      "epoch": 4.443485763589301,
      "grad_norm": 0.0023219576105475426,
      "learning_rate": 0.0009111360368133449,
      "loss": 0.0027,
      "step": 15450
    },
    {
      "epoch": 4.457865976416451,
      "grad_norm": 0.030753118917346,
      "learning_rate": 0.0009108484325568018,
      "loss": 0.0078,
      "step": 15500
    },
    {
      "epoch": 4.4722461892436005,
      "grad_norm": 0.016677387058734894,
      "learning_rate": 0.0009105608283002589,
      "loss": 0.0025,
      "step": 15550
    },
    {
      "epoch": 4.48662640207075,
      "grad_norm": 0.0019616938661783934,
      "learning_rate": 0.0009102732240437158,
      "loss": 0.0034,
      "step": 15600
    },
    {
      "epoch": 4.5010066148979,
      "grad_norm": 0.00014433581964112818,
      "learning_rate": 0.0009099856197871728,
      "loss": 0.0029,
      "step": 15650
    },
    {
      "epoch": 4.515386827725051,
      "grad_norm": 4.540706140687689e-05,
      "learning_rate": 0.00090969801553063,
      "loss": 0.0014,
      "step": 15700
    },
    {
      "epoch": 4.5297670405522,
      "grad_norm": 0.008139514364302158,
      "learning_rate": 0.0009094104112740869,
      "loss": 0.005,
      "step": 15750
    },
    {
      "epoch": 4.54414725337935,
      "grad_norm": 0.0017459472874179482,
      "learning_rate": 0.0009091228070175439,
      "loss": 0.0014,
      "step": 15800
    },
    {
      "epoch": 4.5585274662065,
      "grad_norm": 6.004059105180204e-05,
      "learning_rate": 0.0009088352027610009,
      "loss": 0.0015,
      "step": 15850
    },
    {
      "epoch": 4.57290767903365,
      "grad_norm": 0.04775194451212883,
      "learning_rate": 0.0009085475985044579,
      "loss": 0.0042,
      "step": 15900
    },
    {
      "epoch": 4.587287891860799,
      "grad_norm": 0.004454067442566156,
      "learning_rate": 0.0009082599942479148,
      "loss": 0.0051,
      "step": 15950
    },
    {
      "epoch": 4.601668104687949,
      "grad_norm": 0.0027630352415144444,
      "learning_rate": 0.0009079723899913719,
      "loss": 0.0049,
      "step": 16000
    },
    {
      "epoch": 4.6160483175151,
      "grad_norm": 0.00017860055959317833,
      "learning_rate": 0.0009076847857348289,
      "loss": 0.0022,
      "step": 16050
    },
    {
      "epoch": 4.630428530342249,
      "grad_norm": 0.000905957946088165,
      "learning_rate": 0.0009073971814782858,
      "loss": 0.0044,
      "step": 16100
    },
    {
      "epoch": 4.644808743169399,
      "grad_norm": 0.0026815910823643208,
      "learning_rate": 0.000907109577221743,
      "loss": 0.0029,
      "step": 16150
    },
    {
      "epoch": 4.659188955996549,
      "grad_norm": 0.0121180210262537,
      "learning_rate": 0.0009068219729651999,
      "loss": 0.0015,
      "step": 16200
    },
    {
      "epoch": 4.673569168823699,
      "grad_norm": 0.00016207840235438198,
      "learning_rate": 0.0009065343687086569,
      "loss": 0.0016,
      "step": 16250
    },
    {
      "epoch": 4.687949381650848,
      "grad_norm": 0.009057643823325634,
      "learning_rate": 0.000906246764452114,
      "loss": 0.0024,
      "step": 16300
    },
    {
      "epoch": 4.702329594477998,
      "grad_norm": 0.00024228417896665633,
      "learning_rate": 0.0009059591601955709,
      "loss": 0.0017,
      "step": 16350
    },
    {
      "epoch": 4.716709807305148,
      "grad_norm": 0.055817149579524994,
      "learning_rate": 0.0009056715559390279,
      "loss": 0.0025,
      "step": 16400
    },
    {
      "epoch": 4.731090020132298,
      "grad_norm": 9.468725329497829e-05,
      "learning_rate": 0.0009053839516824849,
      "loss": 0.0044,
      "step": 16450
    },
    {
      "epoch": 4.745470232959448,
      "grad_norm": 0.0013266634196043015,
      "learning_rate": 0.000905096347425942,
      "loss": 0.001,
      "step": 16500
    },
    {
      "epoch": 4.759850445786598,
      "grad_norm": 0.1688023805618286,
      "learning_rate": 0.0009048087431693989,
      "loss": 0.0047,
      "step": 16550
    },
    {
      "epoch": 4.774230658613748,
      "grad_norm": 0.0012350963661447167,
      "learning_rate": 0.000904521138912856,
      "loss": 0.0055,
      "step": 16600
    },
    {
      "epoch": 4.7886108714408975,
      "grad_norm": 0.06874196231365204,
      "learning_rate": 0.000904233534656313,
      "loss": 0.0011,
      "step": 16650
    },
    {
      "epoch": 4.802991084268047,
      "grad_norm": 8.588893251726404e-05,
      "learning_rate": 0.0009039459303997699,
      "loss": 0.0029,
      "step": 16700
    },
    {
      "epoch": 4.817371297095197,
      "grad_norm": 0.00014466923312284052,
      "learning_rate": 0.000903658326143227,
      "loss": 0.0024,
      "step": 16750
    },
    {
      "epoch": 4.831751509922347,
      "grad_norm": 0.1145133450627327,
      "learning_rate": 0.0009033707218866839,
      "loss": 0.0018,
      "step": 16800
    },
    {
      "epoch": 4.846131722749496,
      "grad_norm": 0.0001758899015840143,
      "learning_rate": 0.0009030831176301409,
      "loss": 0.0021,
      "step": 16850
    },
    {
      "epoch": 4.860511935576646,
      "grad_norm": 0.011381590738892555,
      "learning_rate": 0.000902795513373598,
      "loss": 0.0047,
      "step": 16900
    },
    {
      "epoch": 4.874892148403797,
      "grad_norm": 0.08404083549976349,
      "learning_rate": 0.000902507909117055,
      "loss": 0.0036,
      "step": 16950
    },
    {
      "epoch": 4.8892723612309466,
      "grad_norm": 0.07979408651590347,
      "learning_rate": 0.000902220304860512,
      "loss": 0.0022,
      "step": 17000
    },
    {
      "epoch": 4.903652574058096,
      "grad_norm": 0.0011322665959596634,
      "learning_rate": 0.000901932700603969,
      "loss": 0.0029,
      "step": 17050
    },
    {
      "epoch": 4.918032786885246,
      "grad_norm": 0.002792698796838522,
      "learning_rate": 0.000901645096347426,
      "loss": 0.0015,
      "step": 17100
    },
    {
      "epoch": 4.932412999712396,
      "grad_norm": 0.0002756283793132752,
      "learning_rate": 0.0009013574920908829,
      "loss": 0.0013,
      "step": 17150
    },
    {
      "epoch": 4.9467932125395455,
      "grad_norm": 0.00018916797125712037,
      "learning_rate": 0.00090106988783434,
      "loss": 0.0023,
      "step": 17200
    },
    {
      "epoch": 4.961173425366695,
      "grad_norm": 0.00021441999706439674,
      "learning_rate": 0.000900782283577797,
      "loss": 0.0024,
      "step": 17250
    },
    {
      "epoch": 4.975553638193845,
      "grad_norm": 0.03423567861318588,
      "learning_rate": 0.0009004946793212539,
      "loss": 0.0036,
      "step": 17300
    },
    {
      "epoch": 4.989933851020995,
      "grad_norm": 0.00016499118646606803,
      "learning_rate": 0.000900207075064711,
      "loss": 0.0032,
      "step": 17350
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.012427354231476784,
      "eval_runtime": 17.5091,
      "eval_samples_per_second": 2725.38,
      "eval_steps_per_second": 42.606,
      "step": 17385
    },
    {
      "epoch": 5.004314063848145,
      "grad_norm": 0.000513944891281426,
      "learning_rate": 0.000899919470808168,
      "loss": 0.002,
      "step": 17400
    },
    {
      "epoch": 5.018694276675295,
      "grad_norm": 0.008828443475067616,
      "learning_rate": 0.000899631866551625,
      "loss": 0.0039,
      "step": 17450
    },
    {
      "epoch": 5.033074489502445,
      "grad_norm": 0.0004309829673729837,
      "learning_rate": 0.0008993442622950821,
      "loss": 0.0022,
      "step": 17500
    },
    {
      "epoch": 5.047454702329595,
      "grad_norm": 0.0005690238322131336,
      "learning_rate": 0.000899056658038539,
      "loss": 0.0046,
      "step": 17550
    },
    {
      "epoch": 5.061834915156744,
      "grad_norm": 0.028617236763238907,
      "learning_rate": 0.000898769053781996,
      "loss": 0.0029,
      "step": 17600
    },
    {
      "epoch": 5.076215127983894,
      "grad_norm": 0.05099090188741684,
      "learning_rate": 0.000898481449525453,
      "loss": 0.0013,
      "step": 17650
    },
    {
      "epoch": 5.090595340811044,
      "grad_norm": 0.0006919340812601149,
      "learning_rate": 0.00089819384526891,
      "loss": 0.0022,
      "step": 17700
    },
    {
      "epoch": 5.1049755536381936,
      "grad_norm": 0.00014832703163847327,
      "learning_rate": 0.0008979062410123669,
      "loss": 0.0005,
      "step": 17750
    },
    {
      "epoch": 5.119355766465343,
      "grad_norm": 0.01568674109876156,
      "learning_rate": 0.000897618636755824,
      "loss": 0.0018,
      "step": 17800
    },
    {
      "epoch": 5.133735979292494,
      "grad_norm": 0.0003039515868294984,
      "learning_rate": 0.0008973310324992811,
      "loss": 0.0017,
      "step": 17850
    },
    {
      "epoch": 5.148116192119644,
      "grad_norm": 0.0007353696855716407,
      "learning_rate": 0.000897043428242738,
      "loss": 0.003,
      "step": 17900
    },
    {
      "epoch": 5.162496404946793,
      "grad_norm": 0.0002944084699265659,
      "learning_rate": 0.0008967558239861951,
      "loss": 0.0044,
      "step": 17950
    },
    {
      "epoch": 5.176876617773943,
      "grad_norm": 0.18079470098018646,
      "learning_rate": 0.000896468219729652,
      "loss": 0.0027,
      "step": 18000
    },
    {
      "epoch": 5.191256830601093,
      "grad_norm": 0.0018710476579144597,
      "learning_rate": 0.000896180615473109,
      "loss": 0.004,
      "step": 18050
    },
    {
      "epoch": 5.205637043428243,
      "grad_norm": 0.00013929438136983663,
      "learning_rate": 0.0008958930112165661,
      "loss": 0.0042,
      "step": 18100
    },
    {
      "epoch": 5.220017256255392,
      "grad_norm": 0.12375964969396591,
      "learning_rate": 0.000895605406960023,
      "loss": 0.0013,
      "step": 18150
    },
    {
      "epoch": 5.234397469082542,
      "grad_norm": 0.00011553170043043792,
      "learning_rate": 0.00089531780270348,
      "loss": 0.0047,
      "step": 18200
    },
    {
      "epoch": 5.248777681909692,
      "grad_norm": 0.030433768406510353,
      "learning_rate": 0.000895030198446937,
      "loss": 0.0028,
      "step": 18250
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 0.05830792710185051,
      "learning_rate": 0.0008947425941903941,
      "loss": 0.0039,
      "step": 18300
    },
    {
      "epoch": 5.277538107563992,
      "grad_norm": 0.004631573334336281,
      "learning_rate": 0.000894454989933851,
      "loss": 0.0034,
      "step": 18350
    },
    {
      "epoch": 5.291918320391142,
      "grad_norm": 0.19239898025989532,
      "learning_rate": 0.0008941673856773081,
      "loss": 0.0039,
      "step": 18400
    },
    {
      "epoch": 5.306298533218292,
      "grad_norm": 0.0003589122206903994,
      "learning_rate": 0.0008938797814207651,
      "loss": 0.0041,
      "step": 18450
    },
    {
      "epoch": 5.320678746045441,
      "grad_norm": 0.014331522397696972,
      "learning_rate": 0.000893592177164222,
      "loss": 0.0018,
      "step": 18500
    },
    {
      "epoch": 5.335058958872591,
      "grad_norm": 0.0003667481942102313,
      "learning_rate": 0.0008933045729076791,
      "loss": 0.0036,
      "step": 18550
    },
    {
      "epoch": 5.349439171699741,
      "grad_norm": 0.002962737577036023,
      "learning_rate": 0.000893016968651136,
      "loss": 0.0025,
      "step": 18600
    },
    {
      "epoch": 5.363819384526891,
      "grad_norm": 0.14002829790115356,
      "learning_rate": 0.000892729364394593,
      "loss": 0.0028,
      "step": 18650
    },
    {
      "epoch": 5.37819959735404,
      "grad_norm": 0.0005830164882354438,
      "learning_rate": 0.0008924417601380501,
      "loss": 0.0039,
      "step": 18700
    },
    {
      "epoch": 5.392579810181191,
      "grad_norm": 0.0010565478587523103,
      "learning_rate": 0.0008921541558815071,
      "loss": 0.0039,
      "step": 18750
    },
    {
      "epoch": 5.406960023008341,
      "grad_norm": 0.00015447483747266233,
      "learning_rate": 0.0008918665516249641,
      "loss": 0.0054,
      "step": 18800
    },
    {
      "epoch": 5.4213402358354905,
      "grad_norm": 0.0008798511698842049,
      "learning_rate": 0.0008915789473684211,
      "loss": 0.0039,
      "step": 18850
    },
    {
      "epoch": 5.43572044866264,
      "grad_norm": 0.01760677807033062,
      "learning_rate": 0.0008912913431118781,
      "loss": 0.0046,
      "step": 18900
    },
    {
      "epoch": 5.45010066148979,
      "grad_norm": 0.0534023754298687,
      "learning_rate": 0.000891003738855335,
      "loss": 0.0023,
      "step": 18950
    },
    {
      "epoch": 5.46448087431694,
      "grad_norm": 0.0005911309272050858,
      "learning_rate": 0.0008907161345987921,
      "loss": 0.002,
      "step": 19000
    },
    {
      "epoch": 5.4788610871440895,
      "grad_norm": 0.0005378825007937849,
      "learning_rate": 0.0008904285303422491,
      "loss": 0.0023,
      "step": 19050
    },
    {
      "epoch": 5.493241299971239,
      "grad_norm": 0.000705221900716424,
      "learning_rate": 0.000890140926085706,
      "loss": 0.0032,
      "step": 19100
    },
    {
      "epoch": 5.507621512798389,
      "grad_norm": 0.010076300241053104,
      "learning_rate": 0.0008898533218291632,
      "loss": 0.0018,
      "step": 19150
    },
    {
      "epoch": 5.52200172562554,
      "grad_norm": 0.0030856465455144644,
      "learning_rate": 0.0008895657175726201,
      "loss": 0.0028,
      "step": 19200
    },
    {
      "epoch": 5.536381938452689,
      "grad_norm": 7.571720198029652e-05,
      "learning_rate": 0.0008892781133160771,
      "loss": 0.005,
      "step": 19250
    },
    {
      "epoch": 5.550762151279839,
      "grad_norm": 3.6836361687164754e-05,
      "learning_rate": 0.0008889905090595341,
      "loss": 0.0019,
      "step": 19300
    },
    {
      "epoch": 5.565142364106989,
      "grad_norm": 0.0005369231221266091,
      "learning_rate": 0.0008887029048029911,
      "loss": 0.0038,
      "step": 19350
    },
    {
      "epoch": 5.5795225769341386,
      "grad_norm": 0.02999238856136799,
      "learning_rate": 0.0008884153005464481,
      "loss": 0.0027,
      "step": 19400
    },
    {
      "epoch": 5.593902789761288,
      "grad_norm": 0.06737782806158066,
      "learning_rate": 0.0008881276962899051,
      "loss": 0.0044,
      "step": 19450
    },
    {
      "epoch": 5.608283002588438,
      "grad_norm": 0.0022307217586785555,
      "learning_rate": 0.0008878400920333621,
      "loss": 0.0038,
      "step": 19500
    },
    {
      "epoch": 5.622663215415588,
      "grad_norm": 0.02433500438928604,
      "learning_rate": 0.000887552487776819,
      "loss": 0.0038,
      "step": 19550
    },
    {
      "epoch": 5.6370434282427375,
      "grad_norm": 0.04696277156472206,
      "learning_rate": 0.0008872648835202762,
      "loss": 0.0018,
      "step": 19600
    },
    {
      "epoch": 5.651423641069888,
      "grad_norm": 5.06071955896914e-05,
      "learning_rate": 0.0008869772792637332,
      "loss": 0.0025,
      "step": 19650
    },
    {
      "epoch": 5.665803853897038,
      "grad_norm": 7.874579023336992e-05,
      "learning_rate": 0.0008866896750071901,
      "loss": 0.0024,
      "step": 19700
    },
    {
      "epoch": 5.680184066724188,
      "grad_norm": 0.00018294004257768393,
      "learning_rate": 0.0008864020707506472,
      "loss": 0.0034,
      "step": 19750
    },
    {
      "epoch": 5.694564279551337,
      "grad_norm": 0.0006989532266743481,
      "learning_rate": 0.0008861144664941041,
      "loss": 0.0011,
      "step": 19800
    },
    {
      "epoch": 5.708944492378487,
      "grad_norm": 0.00038166093872860074,
      "learning_rate": 0.0008858268622375611,
      "loss": 0.0064,
      "step": 19850
    },
    {
      "epoch": 5.723324705205637,
      "grad_norm": 0.00015804007125552744,
      "learning_rate": 0.0008855392579810181,
      "loss": 0.002,
      "step": 19900
    },
    {
      "epoch": 5.737704918032787,
      "grad_norm": 0.02120274119079113,
      "learning_rate": 0.0008852516537244751,
      "loss": 0.001,
      "step": 19950
    },
    {
      "epoch": 5.752085130859937,
      "grad_norm": 0.005140317138284445,
      "learning_rate": 0.0008849640494679322,
      "loss": 0.002,
      "step": 20000
    },
    {
      "epoch": 5.766465343687086,
      "grad_norm": 0.009331444278359413,
      "learning_rate": 0.0008846764452113892,
      "loss": 0.0036,
      "step": 20050
    },
    {
      "epoch": 5.780845556514237,
      "grad_norm": 0.000196047083591111,
      "learning_rate": 0.0008843888409548462,
      "loss": 0.0055,
      "step": 20100
    },
    {
      "epoch": 5.7952257693413864,
      "grad_norm": 0.00031652741017751396,
      "learning_rate": 0.0008841012366983031,
      "loss": 0.0016,
      "step": 20150
    },
    {
      "epoch": 5.809605982168536,
      "grad_norm": 0.0331573411822319,
      "learning_rate": 0.0008838136324417602,
      "loss": 0.0019,
      "step": 20200
    },
    {
      "epoch": 5.823986194995686,
      "grad_norm": 0.010307552292943,
      "learning_rate": 0.0008835260281852172,
      "loss": 0.0022,
      "step": 20250
    },
    {
      "epoch": 5.838366407822836,
      "grad_norm": 0.0018126447685062885,
      "learning_rate": 0.0008832384239286741,
      "loss": 0.0037,
      "step": 20300
    },
    {
      "epoch": 5.852746620649985,
      "grad_norm": 5.6101907830452546e-05,
      "learning_rate": 0.0008829508196721312,
      "loss": 0.0026,
      "step": 20350
    },
    {
      "epoch": 5.867126833477135,
      "grad_norm": 6.726763240294531e-05,
      "learning_rate": 0.0008826632154155881,
      "loss": 0.0027,
      "step": 20400
    },
    {
      "epoch": 5.881507046304286,
      "grad_norm": 0.027061978355050087,
      "learning_rate": 0.0008823756111590452,
      "loss": 0.0051,
      "step": 20450
    },
    {
      "epoch": 5.8958872591314355,
      "grad_norm": 0.04855760559439659,
      "learning_rate": 0.0008820880069025022,
      "loss": 0.001,
      "step": 20500
    },
    {
      "epoch": 5.910267471958585,
      "grad_norm": 0.05498829111456871,
      "learning_rate": 0.0008818004026459592,
      "loss": 0.0049,
      "step": 20550
    },
    {
      "epoch": 5.924647684785735,
      "grad_norm": 0.0008604974136687815,
      "learning_rate": 0.0008815127983894162,
      "loss": 0.0018,
      "step": 20600
    },
    {
      "epoch": 5.939027897612885,
      "grad_norm": 7.824681961210445e-05,
      "learning_rate": 0.0008812251941328732,
      "loss": 0.0027,
      "step": 20650
    },
    {
      "epoch": 5.9534081104400345,
      "grad_norm": 0.00013391472748480737,
      "learning_rate": 0.0008809375898763302,
      "loss": 0.0037,
      "step": 20700
    },
    {
      "epoch": 5.967788323267184,
      "grad_norm": 0.00011113298387499526,
      "learning_rate": 0.0008806499856197871,
      "loss": 0.0016,
      "step": 20750
    },
    {
      "epoch": 5.982168536094334,
      "grad_norm": 0.00032768354867585003,
      "learning_rate": 0.0008803623813632442,
      "loss": 0.0057,
      "step": 20800
    },
    {
      "epoch": 5.996548748921484,
      "grad_norm": 0.035113248974084854,
      "learning_rate": 0.0008800747771067013,
      "loss": 0.0024,
      "step": 20850
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.01085981447249651,
      "eval_runtime": 16.6241,
      "eval_samples_per_second": 2870.471,
      "eval_steps_per_second": 44.875,
      "step": 20862
    },
    {
      "epoch": 6.0109289617486334,
      "grad_norm": 5.9973699535476044e-05,
      "learning_rate": 0.0008797871728501582,
      "loss": 0.0033,
      "step": 20900
    },
    {
      "epoch": 6.025309174575784,
      "grad_norm": 8.055672515183687e-05,
      "learning_rate": 0.0008794995685936153,
      "loss": 0.0039,
      "step": 20950
    },
    {
      "epoch": 6.039689387402934,
      "grad_norm": 0.0031957130413502455,
      "learning_rate": 0.0008792119643370722,
      "loss": 0.0024,
      "step": 21000
    },
    {
      "epoch": 6.054069600230084,
      "grad_norm": 0.009780347347259521,
      "learning_rate": 0.0008789243600805292,
      "loss": 0.0037,
      "step": 21050
    },
    {
      "epoch": 6.068449813057233,
      "grad_norm": 0.07059728354215622,
      "learning_rate": 0.0008786367558239862,
      "loss": 0.0013,
      "step": 21100
    },
    {
      "epoch": 6.082830025884383,
      "grad_norm": 0.1095634177327156,
      "learning_rate": 0.0008783491515674432,
      "loss": 0.0025,
      "step": 21150
    },
    {
      "epoch": 6.097210238711533,
      "grad_norm": 0.00417976314201951,
      "learning_rate": 0.0008780615473109002,
      "loss": 0.0017,
      "step": 21200
    },
    {
      "epoch": 6.1115904515386825,
      "grad_norm": 8.121495193336159e-05,
      "learning_rate": 0.0008777739430543572,
      "loss": 0.0026,
      "step": 21250
    },
    {
      "epoch": 6.125970664365832,
      "grad_norm": 0.0007858405588194728,
      "learning_rate": 0.0008774863387978143,
      "loss": 0.0024,
      "step": 21300
    },
    {
      "epoch": 6.140350877192983,
      "grad_norm": 4.169813109911047e-05,
      "learning_rate": 0.0008771987345412712,
      "loss": 0.0025,
      "step": 21350
    },
    {
      "epoch": 6.154731090020133,
      "grad_norm": 0.012493764981627464,
      "learning_rate": 0.0008769111302847283,
      "loss": 0.0021,
      "step": 21400
    },
    {
      "epoch": 6.169111302847282,
      "grad_norm": 0.000598577898927033,
      "learning_rate": 0.0008766235260281852,
      "loss": 0.0004,
      "step": 21450
    },
    {
      "epoch": 6.183491515674432,
      "grad_norm": 0.09811895340681076,
      "learning_rate": 0.0008763359217716422,
      "loss": 0.0036,
      "step": 21500
    },
    {
      "epoch": 6.197871728501582,
      "grad_norm": 0.03709612786769867,
      "learning_rate": 0.0008760483175150993,
      "loss": 0.0042,
      "step": 21550
    },
    {
      "epoch": 6.212251941328732,
      "grad_norm": 6.801459676353261e-05,
      "learning_rate": 0.0008757607132585562,
      "loss": 0.0008,
      "step": 21600
    },
    {
      "epoch": 6.226632154155881,
      "grad_norm": 0.0021278676576912403,
      "learning_rate": 0.0008754731090020132,
      "loss": 0.004,
      "step": 21650
    },
    {
      "epoch": 6.241012366983031,
      "grad_norm": 0.23327261209487915,
      "learning_rate": 0.0008751855047454703,
      "loss": 0.0018,
      "step": 21700
    },
    {
      "epoch": 6.255392579810181,
      "grad_norm": 0.027655014768242836,
      "learning_rate": 0.0008748979004889273,
      "loss": 0.0037,
      "step": 21750
    },
    {
      "epoch": 6.2697727926373314,
      "grad_norm": 0.012235480360686779,
      "learning_rate": 0.0008746102962323843,
      "loss": 0.0086,
      "step": 21800
    },
    {
      "epoch": 6.284153005464481,
      "grad_norm": 0.0011346845421940088,
      "learning_rate": 0.0008743226919758413,
      "loss": 0.0032,
      "step": 21850
    },
    {
      "epoch": 6.298533218291631,
      "grad_norm": 0.05182584002614021,
      "learning_rate": 0.0008740350877192983,
      "loss": 0.003,
      "step": 21900
    },
    {
      "epoch": 6.312913431118781,
      "grad_norm": 0.018633602187037468,
      "learning_rate": 0.0008737474834627552,
      "loss": 0.0043,
      "step": 21950
    },
    {
      "epoch": 6.32729364394593,
      "grad_norm": 0.00014948243915569037,
      "learning_rate": 0.0008734598792062123,
      "loss": 0.0012,
      "step": 22000
    },
    {
      "epoch": 6.34167385677308,
      "grad_norm": 0.00032500954694114625,
      "learning_rate": 0.0008731722749496692,
      "loss": 0.0045,
      "step": 22050
    },
    {
      "epoch": 6.35605406960023,
      "grad_norm": 0.0008773379377089441,
      "learning_rate": 0.0008728846706931262,
      "loss": 0.0042,
      "step": 22100
    },
    {
      "epoch": 6.37043428242738,
      "grad_norm": 0.00019649726164061576,
      "learning_rate": 0.0008725970664365834,
      "loss": 0.0046,
      "step": 22150
    },
    {
      "epoch": 6.384814495254529,
      "grad_norm": 0.07679947465658188,
      "learning_rate": 0.0008723094621800403,
      "loss": 0.0036,
      "step": 22200
    },
    {
      "epoch": 6.39919470808168,
      "grad_norm": 0.00018177741731051356,
      "learning_rate": 0.0008720218579234973,
      "loss": 0.0022,
      "step": 22250
    },
    {
      "epoch": 6.41357492090883,
      "grad_norm": 0.12817178666591644,
      "learning_rate": 0.0008717342536669543,
      "loss": 0.0043,
      "step": 22300
    },
    {
      "epoch": 6.4279551337359795,
      "grad_norm": 0.0002680501784197986,
      "learning_rate": 0.0008714466494104113,
      "loss": 0.0014,
      "step": 22350
    },
    {
      "epoch": 6.442335346563129,
      "grad_norm": 7.826262299204245e-05,
      "learning_rate": 0.0008711590451538683,
      "loss": 0.0028,
      "step": 22400
    },
    {
      "epoch": 6.456715559390279,
      "grad_norm": 0.0010373290861025453,
      "learning_rate": 0.0008708714408973253,
      "loss": 0.0032,
      "step": 22450
    },
    {
      "epoch": 6.471095772217429,
      "grad_norm": 0.05377786234021187,
      "learning_rate": 0.0008705838366407823,
      "loss": 0.0025,
      "step": 22500
    },
    {
      "epoch": 6.4854759850445785,
      "grad_norm": 0.01991412043571472,
      "learning_rate": 0.0008702962323842392,
      "loss": 0.0023,
      "step": 22550
    },
    {
      "epoch": 6.499856197871728,
      "grad_norm": 0.03651691600680351,
      "learning_rate": 0.0008700086281276964,
      "loss": 0.0011,
      "step": 22600
    },
    {
      "epoch": 6.514236410698878,
      "grad_norm": 0.0026385930832475424,
      "learning_rate": 0.0008697210238711533,
      "loss": 0.0022,
      "step": 22650
    },
    {
      "epoch": 6.528616623526029,
      "grad_norm": 0.0018928444478660822,
      "learning_rate": 0.0008694334196146103,
      "loss": 0.0037,
      "step": 22700
    },
    {
      "epoch": 6.542996836353178,
      "grad_norm": 9.198639600072056e-05,
      "learning_rate": 0.0008691458153580674,
      "loss": 0.002,
      "step": 22750
    },
    {
      "epoch": 6.557377049180328,
      "grad_norm": 0.017732886597514153,
      "learning_rate": 0.0008688582111015243,
      "loss": 0.0034,
      "step": 22800
    },
    {
      "epoch": 6.571757262007478,
      "grad_norm": 0.0005159546271897852,
      "learning_rate": 0.0008685706068449813,
      "loss": 0.0068,
      "step": 22850
    },
    {
      "epoch": 6.5861374748346275,
      "grad_norm": 0.00023213868553284556,
      "learning_rate": 0.0008682830025884383,
      "loss": 0.0038,
      "step": 22900
    },
    {
      "epoch": 6.600517687661777,
      "grad_norm": 7.166984869400039e-05,
      "learning_rate": 0.0008679953983318953,
      "loss": 0.0053,
      "step": 22950
    },
    {
      "epoch": 6.614897900488927,
      "grad_norm": 0.020906962454319,
      "learning_rate": 0.0008677077940753524,
      "loss": 0.0031,
      "step": 23000
    },
    {
      "epoch": 6.629278113316077,
      "grad_norm": 0.07006175816059113,
      "learning_rate": 0.0008674201898188094,
      "loss": 0.0059,
      "step": 23050
    },
    {
      "epoch": 6.6436583261432265,
      "grad_norm": 0.00013844098430126905,
      "learning_rate": 0.0008671325855622664,
      "loss": 0.0038,
      "step": 23100
    },
    {
      "epoch": 6.658038538970377,
      "grad_norm": 0.0029806725215166807,
      "learning_rate": 0.0008668449813057233,
      "loss": 0.0043,
      "step": 23150
    },
    {
      "epoch": 6.672418751797527,
      "grad_norm": 0.0006840127753093839,
      "learning_rate": 0.0008665573770491804,
      "loss": 0.0013,
      "step": 23200
    },
    {
      "epoch": 6.686798964624677,
      "grad_norm": 0.0459090992808342,
      "learning_rate": 0.0008662697727926373,
      "loss": 0.0019,
      "step": 23250
    },
    {
      "epoch": 6.701179177451826,
      "grad_norm": 0.0021388614550232887,
      "learning_rate": 0.0008659821685360943,
      "loss": 0.0026,
      "step": 23300
    },
    {
      "epoch": 6.715559390278976,
      "grad_norm": 0.00017930494504980743,
      "learning_rate": 0.0008656945642795514,
      "loss": 0.0025,
      "step": 23350
    },
    {
      "epoch": 6.729939603106126,
      "grad_norm": 0.00010390298120910302,
      "learning_rate": 0.0008654069600230083,
      "loss": 0.0039,
      "step": 23400
    },
    {
      "epoch": 6.744319815933276,
      "grad_norm": 0.0005602281889878213,
      "learning_rate": 0.0008651193557664654,
      "loss": 0.001,
      "step": 23450
    },
    {
      "epoch": 6.758700028760425,
      "grad_norm": 0.001564567442983389,
      "learning_rate": 0.0008648317515099224,
      "loss": 0.0047,
      "step": 23500
    },
    {
      "epoch": 6.773080241587575,
      "grad_norm": 0.0004367312940303236,
      "learning_rate": 0.0008645441472533794,
      "loss": 0.0037,
      "step": 23550
    },
    {
      "epoch": 6.787460454414726,
      "grad_norm": 0.0016041933558881283,
      "learning_rate": 0.0008642565429968364,
      "loss": 0.0033,
      "step": 23600
    },
    {
      "epoch": 6.801840667241875,
      "grad_norm": 0.016310319304466248,
      "learning_rate": 0.0008639689387402934,
      "loss": 0.0032,
      "step": 23650
    },
    {
      "epoch": 6.816220880069025,
      "grad_norm": 0.0003262086829636246,
      "learning_rate": 0.0008636813344837504,
      "loss": 0.0027,
      "step": 23700
    },
    {
      "epoch": 6.830601092896175,
      "grad_norm": 6.585462688235566e-05,
      "learning_rate": 0.0008633937302272073,
      "loss": 0.0063,
      "step": 23750
    },
    {
      "epoch": 6.844981305723325,
      "grad_norm": 0.00024369538004975766,
      "learning_rate": 0.0008631061259706644,
      "loss": 0.0023,
      "step": 23800
    },
    {
      "epoch": 6.859361518550474,
      "grad_norm": 0.0003267924766987562,
      "learning_rate": 0.0008628185217141213,
      "loss": 0.0015,
      "step": 23850
    },
    {
      "epoch": 6.873741731377624,
      "grad_norm": 0.007636547554284334,
      "learning_rate": 0.0008625309174575784,
      "loss": 0.0019,
      "step": 23900
    },
    {
      "epoch": 6.888121944204774,
      "grad_norm": 0.017050722613930702,
      "learning_rate": 0.0008622433132010355,
      "loss": 0.0063,
      "step": 23950
    },
    {
      "epoch": 6.902502157031924,
      "grad_norm": 0.0008221339085139334,
      "learning_rate": 0.0008619557089444924,
      "loss": 0.0015,
      "step": 24000
    },
    {
      "epoch": 6.916882369859074,
      "grad_norm": 0.00021509382349904627,
      "learning_rate": 0.0008616681046879494,
      "loss": 0.0027,
      "step": 24050
    },
    {
      "epoch": 6.931262582686224,
      "grad_norm": 0.02328905463218689,
      "learning_rate": 0.0008613805004314064,
      "loss": 0.0036,
      "step": 24100
    },
    {
      "epoch": 6.945642795513374,
      "grad_norm": 0.002230367623269558,
      "learning_rate": 0.0008610928961748634,
      "loss": 0.0037,
      "step": 24150
    },
    {
      "epoch": 6.9600230083405235,
      "grad_norm": 0.036466773599386215,
      "learning_rate": 0.0008608052919183203,
      "loss": 0.0021,
      "step": 24200
    },
    {
      "epoch": 6.974403221167673,
      "grad_norm": 0.03361192345619202,
      "learning_rate": 0.0008605176876617774,
      "loss": 0.0028,
      "step": 24250
    },
    {
      "epoch": 6.988783433994823,
      "grad_norm": 0.00027069475618191063,
      "learning_rate": 0.0008602300834052345,
      "loss": 0.0028,
      "step": 24300
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.011578783392906189,
      "eval_runtime": 17.2144,
      "eval_samples_per_second": 2772.033,
      "eval_steps_per_second": 43.336,
      "step": 24339
    },
    {
      "epoch": 7.003163646821973,
      "grad_norm": 0.009754079394042492,
      "learning_rate": 0.0008599424791486914,
      "loss": 0.002,
      "step": 24350
    },
    {
      "epoch": 7.017543859649122,
      "grad_norm": 0.00016870972467586398,
      "learning_rate": 0.0008596548748921485,
      "loss": 0.0084,
      "step": 24400
    },
    {
      "epoch": 7.031924072476273,
      "grad_norm": 0.004120220430195332,
      "learning_rate": 0.0008593672706356054,
      "loss": 0.0035,
      "step": 24450
    },
    {
      "epoch": 7.046304285303423,
      "grad_norm": 0.21372345089912415,
      "learning_rate": 0.0008590796663790624,
      "loss": 0.0042,
      "step": 24500
    },
    {
      "epoch": 7.0606844981305725,
      "grad_norm": 0.00037669329321943223,
      "learning_rate": 0.0008587920621225195,
      "loss": 0.0033,
      "step": 24550
    },
    {
      "epoch": 7.075064710957722,
      "grad_norm": 0.0003105586511082947,
      "learning_rate": 0.0008585044578659764,
      "loss": 0.0041,
      "step": 24600
    },
    {
      "epoch": 7.089444923784872,
      "grad_norm": 0.00042421059333719313,
      "learning_rate": 0.0008582168536094334,
      "loss": 0.0034,
      "step": 24650
    },
    {
      "epoch": 7.103825136612022,
      "grad_norm": 0.00012402729771565646,
      "learning_rate": 0.0008579292493528904,
      "loss": 0.0022,
      "step": 24700
    },
    {
      "epoch": 7.1182053494391715,
      "grad_norm": 0.021494902670383453,
      "learning_rate": 0.0008576416450963475,
      "loss": 0.0025,
      "step": 24750
    },
    {
      "epoch": 7.132585562266321,
      "grad_norm": 7.450112025253475e-05,
      "learning_rate": 0.0008573540408398044,
      "loss": 0.0015,
      "step": 24800
    },
    {
      "epoch": 7.146965775093471,
      "grad_norm": 0.00022708246251568198,
      "learning_rate": 0.0008570664365832615,
      "loss": 0.0022,
      "step": 24850
    },
    {
      "epoch": 7.161345987920622,
      "grad_norm": 0.00012226657418068498,
      "learning_rate": 0.0008567788323267185,
      "loss": 0.0027,
      "step": 24900
    },
    {
      "epoch": 7.175726200747771,
      "grad_norm": 8.421481470577419e-05,
      "learning_rate": 0.0008564912280701754,
      "loss": 0.0049,
      "step": 24950
    },
    {
      "epoch": 7.190106413574921,
      "grad_norm": 0.0006823559524491429,
      "learning_rate": 0.0008562036238136325,
      "loss": 0.0037,
      "step": 25000
    },
    {
      "epoch": 7.204486626402071,
      "grad_norm": 0.002603864064440131,
      "learning_rate": 0.0008559160195570894,
      "loss": 0.0023,
      "step": 25050
    },
    {
      "epoch": 7.218866839229221,
      "grad_norm": 7.987719436641783e-05,
      "learning_rate": 0.0008556284153005464,
      "loss": 0.0021,
      "step": 25100
    },
    {
      "epoch": 7.23324705205637,
      "grad_norm": 0.0283669400960207,
      "learning_rate": 0.0008553408110440036,
      "loss": 0.0038,
      "step": 25150
    },
    {
      "epoch": 7.24762726488352,
      "grad_norm": 0.0003421324072405696,
      "learning_rate": 0.0008550532067874605,
      "loss": 0.002,
      "step": 25200
    },
    {
      "epoch": 7.26200747771067,
      "grad_norm": 0.00035473829484544694,
      "learning_rate": 0.0008547656025309175,
      "loss": 0.0038,
      "step": 25250
    },
    {
      "epoch": 7.27638769053782,
      "grad_norm": 7.854375144233927e-05,
      "learning_rate": 0.0008544779982743745,
      "loss": 0.0071,
      "step": 25300
    },
    {
      "epoch": 7.29076790336497,
      "grad_norm": 0.013543768785893917,
      "learning_rate": 0.0008541903940178315,
      "loss": 0.0062,
      "step": 25350
    },
    {
      "epoch": 7.30514811619212,
      "grad_norm": 7.093767635524273e-05,
      "learning_rate": 0.0008539027897612884,
      "loss": 0.0066,
      "step": 25400
    },
    {
      "epoch": 7.31952832901927,
      "grad_norm": 4.121452002436854e-05,
      "learning_rate": 0.0008536151855047455,
      "loss": 0.0028,
      "step": 25450
    },
    {
      "epoch": 7.333908541846419,
      "grad_norm": 6.830050551798195e-05,
      "learning_rate": 0.0008533275812482025,
      "loss": 0.0046,
      "step": 25500
    },
    {
      "epoch": 7.348288754673569,
      "grad_norm": 0.0003041376476176083,
      "learning_rate": 0.0008530399769916594,
      "loss": 0.0042,
      "step": 25550
    },
    {
      "epoch": 7.362668967500719,
      "grad_norm": 0.0032063291873782873,
      "learning_rate": 0.0008527523727351166,
      "loss": 0.0035,
      "step": 25600
    },
    {
      "epoch": 7.377049180327869,
      "grad_norm": 0.0481349341571331,
      "learning_rate": 0.0008524647684785735,
      "loss": 0.0018,
      "step": 25650
    },
    {
      "epoch": 7.391429393155018,
      "grad_norm": 7.037047180347145e-05,
      "learning_rate": 0.0008521771642220305,
      "loss": 0.0015,
      "step": 25700
    },
    {
      "epoch": 7.405809605982169,
      "grad_norm": 0.003030819585546851,
      "learning_rate": 0.0008518895599654876,
      "loss": 0.0024,
      "step": 25750
    },
    {
      "epoch": 7.420189818809319,
      "grad_norm": 0.00020556169329211116,
      "learning_rate": 0.0008516019557089445,
      "loss": 0.0043,
      "step": 25800
    },
    {
      "epoch": 7.4345700316364685,
      "grad_norm": 0.00015078789147082716,
      "learning_rate": 0.0008513143514524015,
      "loss": 0.0033,
      "step": 25850
    },
    {
      "epoch": 7.448950244463618,
      "grad_norm": 0.00013333243259694427,
      "learning_rate": 0.0008510267471958585,
      "loss": 0.0018,
      "step": 25900
    },
    {
      "epoch": 7.463330457290768,
      "grad_norm": 0.0007550056325271726,
      "learning_rate": 0.0008507391429393155,
      "loss": 0.0037,
      "step": 25950
    },
    {
      "epoch": 7.477710670117918,
      "grad_norm": 0.024762829765677452,
      "learning_rate": 0.0008504515386827724,
      "loss": 0.001,
      "step": 26000
    },
    {
      "epoch": 7.492090882945067,
      "grad_norm": 0.00018274798640049994,
      "learning_rate": 0.0008501639344262296,
      "loss": 0.0009,
      "step": 26050
    },
    {
      "epoch": 7.506471095772217,
      "grad_norm": 0.03763497993350029,
      "learning_rate": 0.0008498763301696866,
      "loss": 0.002,
      "step": 26100
    },
    {
      "epoch": 7.520851308599367,
      "grad_norm": 0.00039112468948587775,
      "learning_rate": 0.0008495887259131435,
      "loss": 0.0026,
      "step": 26150
    },
    {
      "epoch": 7.5352315214265175,
      "grad_norm": 0.017476728186011314,
      "learning_rate": 0.0008493011216566006,
      "loss": 0.0022,
      "step": 26200
    },
    {
      "epoch": 7.549611734253667,
      "grad_norm": 0.04858997091650963,
      "learning_rate": 0.0008490135174000575,
      "loss": 0.0034,
      "step": 26250
    },
    {
      "epoch": 7.563991947080817,
      "grad_norm": 5.375291584641673e-05,
      "learning_rate": 0.0008487259131435145,
      "loss": 0.0038,
      "step": 26300
    },
    {
      "epoch": 7.578372159907967,
      "grad_norm": 0.03319820016622543,
      "learning_rate": 0.0008484383088869716,
      "loss": 0.0017,
      "step": 26350
    },
    {
      "epoch": 7.5927523727351165,
      "grad_norm": 0.0001313781540375203,
      "learning_rate": 0.0008481507046304285,
      "loss": 0.0042,
      "step": 26400
    },
    {
      "epoch": 7.607132585562266,
      "grad_norm": 0.0011998863192275167,
      "learning_rate": 0.0008478631003738856,
      "loss": 0.0041,
      "step": 26450
    },
    {
      "epoch": 7.621512798389416,
      "grad_norm": 0.014852158725261688,
      "learning_rate": 0.0008475754961173426,
      "loss": 0.004,
      "step": 26500
    },
    {
      "epoch": 7.635893011216566,
      "grad_norm": 0.0008241491159424186,
      "learning_rate": 0.0008472878918607996,
      "loss": 0.0016,
      "step": 26550
    },
    {
      "epoch": 7.6502732240437155,
      "grad_norm": 0.0004715865070465952,
      "learning_rate": 0.0008470002876042565,
      "loss": 0.0027,
      "step": 26600
    },
    {
      "epoch": 7.664653436870866,
      "grad_norm": 0.02592996321618557,
      "learning_rate": 0.0008467126833477136,
      "loss": 0.0024,
      "step": 26650
    },
    {
      "epoch": 7.679033649698016,
      "grad_norm": 0.00023041348322294652,
      "learning_rate": 0.0008464250790911706,
      "loss": 0.0013,
      "step": 26700
    },
    {
      "epoch": 7.693413862525166,
      "grad_norm": 0.00013950625725556165,
      "learning_rate": 0.0008461374748346275,
      "loss": 0.0048,
      "step": 26750
    },
    {
      "epoch": 7.707794075352315,
      "grad_norm": 9.522875916445628e-05,
      "learning_rate": 0.0008458498705780846,
      "loss": 0.0032,
      "step": 26800
    },
    {
      "epoch": 7.722174288179465,
      "grad_norm": 0.003044324228540063,
      "learning_rate": 0.0008455622663215415,
      "loss": 0.0014,
      "step": 26850
    },
    {
      "epoch": 7.736554501006615,
      "grad_norm": 0.00035776745062321424,
      "learning_rate": 0.0008452746620649986,
      "loss": 0.0013,
      "step": 26900
    },
    {
      "epoch": 7.7509347138337645,
      "grad_norm": 0.03257514536380768,
      "learning_rate": 0.0008449870578084556,
      "loss": 0.0041,
      "step": 26950
    },
    {
      "epoch": 7.765314926660914,
      "grad_norm": 0.0010126375127583742,
      "learning_rate": 0.0008446994535519126,
      "loss": 0.004,
      "step": 27000
    },
    {
      "epoch": 7.779695139488064,
      "grad_norm": 0.005191863980144262,
      "learning_rate": 0.0008444118492953696,
      "loss": 0.0054,
      "step": 27050
    },
    {
      "epoch": 7.794075352315215,
      "grad_norm": 0.0001748388312989846,
      "learning_rate": 0.0008441242450388266,
      "loss": 0.0016,
      "step": 27100
    },
    {
      "epoch": 7.808455565142364,
      "grad_norm": 0.013624866493046284,
      "learning_rate": 0.0008438366407822836,
      "loss": 0.0013,
      "step": 27150
    },
    {
      "epoch": 7.822835777969514,
      "grad_norm": 0.018289368599653244,
      "learning_rate": 0.0008435490365257405,
      "loss": 0.0006,
      "step": 27200
    },
    {
      "epoch": 7.837215990796664,
      "grad_norm": 0.000855161459185183,
      "learning_rate": 0.0008432614322691976,
      "loss": 0.0032,
      "step": 27250
    },
    {
      "epoch": 7.851596203623814,
      "grad_norm": 0.0003448536735959351,
      "learning_rate": 0.0008429738280126547,
      "loss": 0.0014,
      "step": 27300
    },
    {
      "epoch": 7.865976416450963,
      "grad_norm": 0.00046413621748797596,
      "learning_rate": 0.0008426862237561116,
      "loss": 0.0028,
      "step": 27350
    },
    {
      "epoch": 7.880356629278113,
      "grad_norm": 0.0021737616043537855,
      "learning_rate": 0.0008423986194995687,
      "loss": 0.0028,
      "step": 27400
    },
    {
      "epoch": 7.894736842105263,
      "grad_norm": 0.0004183656710665673,
      "learning_rate": 0.0008421110152430256,
      "loss": 0.0032,
      "step": 27450
    },
    {
      "epoch": 7.909117054932413,
      "grad_norm": 0.0015282060485333204,
      "learning_rate": 0.0008418234109864826,
      "loss": 0.002,
      "step": 27500
    },
    {
      "epoch": 7.923497267759563,
      "grad_norm": 0.00014221516903489828,
      "learning_rate": 0.0008415358067299396,
      "loss": 0.0019,
      "step": 27550
    },
    {
      "epoch": 7.937877480586713,
      "grad_norm": 9.3833077698946e-05,
      "learning_rate": 0.0008412482024733966,
      "loss": 0.0013,
      "step": 27600
    },
    {
      "epoch": 7.952257693413863,
      "grad_norm": 0.015604568645358086,
      "learning_rate": 0.0008409605982168536,
      "loss": 0.0014,
      "step": 27650
    },
    {
      "epoch": 7.966637906241012,
      "grad_norm": 0.004324750974774361,
      "learning_rate": 0.0008406729939603106,
      "loss": 0.0048,
      "step": 27700
    },
    {
      "epoch": 7.981018119068162,
      "grad_norm": 0.00029340366018004715,
      "learning_rate": 0.0008403853897037677,
      "loss": 0.0027,
      "step": 27750
    },
    {
      "epoch": 7.995398331895312,
      "grad_norm": 0.03278956189751625,
      "learning_rate": 0.0008400977854472246,
      "loss": 0.0017,
      "step": 27800
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.010107924230396748,
      "eval_runtime": 17.8288,
      "eval_samples_per_second": 2676.513,
      "eval_steps_per_second": 41.842,
      "step": 27816
    },
    {
      "epoch": 8.009778544722462,
      "grad_norm": 8.569195779273286e-05,
      "learning_rate": 0.0008398101811906817,
      "loss": 0.0057,
      "step": 27850
    },
    {
      "epoch": 8.024158757549612,
      "grad_norm": 0.00014072777412366122,
      "learning_rate": 0.0008395225769341387,
      "loss": 0.0044,
      "step": 27900
    },
    {
      "epoch": 8.038538970376761,
      "grad_norm": 4.666618406190537e-05,
      "learning_rate": 0.0008392349726775956,
      "loss": 0.0038,
      "step": 27950
    },
    {
      "epoch": 8.052919183203912,
      "grad_norm": 0.00015729255392216146,
      "learning_rate": 0.0008389473684210527,
      "loss": 0.0029,
      "step": 28000
    },
    {
      "epoch": 8.06729939603106,
      "grad_norm": 0.010589832440018654,
      "learning_rate": 0.0008386597641645096,
      "loss": 0.0022,
      "step": 28050
    },
    {
      "epoch": 8.081679608858211,
      "grad_norm": 8.10734200058505e-05,
      "learning_rate": 0.0008383721599079666,
      "loss": 0.0017,
      "step": 28100
    },
    {
      "epoch": 8.09605982168536,
      "grad_norm": 0.0012592754792422056,
      "learning_rate": 0.0008380845556514236,
      "loss": 0.0019,
      "step": 28150
    },
    {
      "epoch": 8.11044003451251,
      "grad_norm": 0.010075397789478302,
      "learning_rate": 0.0008377969513948807,
      "loss": 0.0024,
      "step": 28200
    },
    {
      "epoch": 8.124820247339661,
      "grad_norm": 0.00768042728304863,
      "learning_rate": 0.0008375093471383377,
      "loss": 0.0034,
      "step": 28250
    },
    {
      "epoch": 8.13920046016681,
      "grad_norm": 0.0007317232666537166,
      "learning_rate": 0.0008372217428817947,
      "loss": 0.0024,
      "step": 28300
    },
    {
      "epoch": 8.15358067299396,
      "grad_norm": 0.00044885434908792377,
      "learning_rate": 0.0008369341386252517,
      "loss": 0.0029,
      "step": 28350
    },
    {
      "epoch": 8.16796088582111,
      "grad_norm": 0.018434979021549225,
      "learning_rate": 0.0008366465343687086,
      "loss": 0.0043,
      "step": 28400
    },
    {
      "epoch": 8.18234109864826,
      "grad_norm": 0.06191692873835564,
      "learning_rate": 0.0008363589301121657,
      "loss": 0.0025,
      "step": 28450
    },
    {
      "epoch": 8.19672131147541,
      "grad_norm": 0.00013411663530860096,
      "learning_rate": 0.0008360713258556227,
      "loss": 0.0026,
      "step": 28500
    },
    {
      "epoch": 8.21110152430256,
      "grad_norm": 0.006143226288259029,
      "learning_rate": 0.0008357837215990796,
      "loss": 0.0016,
      "step": 28550
    },
    {
      "epoch": 8.225481737129709,
      "grad_norm": 0.00046226047561503947,
      "learning_rate": 0.0008354961173425368,
      "loss": 0.0022,
      "step": 28600
    },
    {
      "epoch": 8.23986194995686,
      "grad_norm": 0.00029423393425531685,
      "learning_rate": 0.0008352085130859937,
      "loss": 0.001,
      "step": 28650
    },
    {
      "epoch": 8.25424216278401,
      "grad_norm": 0.0463840588927269,
      "learning_rate": 0.0008349209088294507,
      "loss": 0.0019,
      "step": 28700
    },
    {
      "epoch": 8.268622375611159,
      "grad_norm": 0.0018656406318768859,
      "learning_rate": 0.0008346333045729077,
      "loss": 0.0018,
      "step": 28750
    },
    {
      "epoch": 8.28300258843831,
      "grad_norm": 0.00010166675201617181,
      "learning_rate": 0.0008343457003163647,
      "loss": 0.0027,
      "step": 28800
    },
    {
      "epoch": 8.297382801265458,
      "grad_norm": 0.043275486677885056,
      "learning_rate": 0.0008340580960598217,
      "loss": 0.0034,
      "step": 28850
    },
    {
      "epoch": 8.311763014092609,
      "grad_norm": 0.002404438564553857,
      "learning_rate": 0.0008337704918032787,
      "loss": 0.0016,
      "step": 28900
    },
    {
      "epoch": 8.326143226919758,
      "grad_norm": 0.0002219980233348906,
      "learning_rate": 0.0008334828875467357,
      "loss": 0.0046,
      "step": 28950
    },
    {
      "epoch": 8.340523439746908,
      "grad_norm": 0.001827285741455853,
      "learning_rate": 0.0008331952832901926,
      "loss": 0.0079,
      "step": 29000
    },
    {
      "epoch": 8.354903652574059,
      "grad_norm": 0.00013804907212033868,
      "learning_rate": 0.0008329076790336498,
      "loss": 0.0023,
      "step": 29050
    },
    {
      "epoch": 8.369283865401208,
      "grad_norm": 0.0011090193875133991,
      "learning_rate": 0.0008326200747771068,
      "loss": 0.0023,
      "step": 29100
    },
    {
      "epoch": 8.383664078228358,
      "grad_norm": 0.00010805068450281397,
      "learning_rate": 0.0008323324705205637,
      "loss": 0.0017,
      "step": 29150
    },
    {
      "epoch": 8.398044291055507,
      "grad_norm": 0.012080955319106579,
      "learning_rate": 0.0008320448662640208,
      "loss": 0.0031,
      "step": 29200
    },
    {
      "epoch": 8.412424503882658,
      "grad_norm": 4.2132738599320874e-05,
      "learning_rate": 0.0008317572620074777,
      "loss": 0.003,
      "step": 29250
    },
    {
      "epoch": 8.426804716709807,
      "grad_norm": 0.015392140485346317,
      "learning_rate": 0.0008314696577509347,
      "loss": 0.004,
      "step": 29300
    },
    {
      "epoch": 8.441184929536957,
      "grad_norm": 0.0004239684494677931,
      "learning_rate": 0.0008311820534943917,
      "loss": 0.0023,
      "step": 29350
    },
    {
      "epoch": 8.455565142364106,
      "grad_norm": 4.6113153075566515e-05,
      "learning_rate": 0.0008308944492378487,
      "loss": 0.002,
      "step": 29400
    },
    {
      "epoch": 8.469945355191257,
      "grad_norm": 0.0017347413813695312,
      "learning_rate": 0.0008306068449813057,
      "loss": 0.0015,
      "step": 29450
    },
    {
      "epoch": 8.484325568018406,
      "grad_norm": 0.017723893746733665,
      "learning_rate": 0.0008303192407247628,
      "loss": 0.0034,
      "step": 29500
    },
    {
      "epoch": 8.498705780845556,
      "grad_norm": 0.05394537374377251,
      "learning_rate": 0.0008300316364682198,
      "loss": 0.0009,
      "step": 29550
    },
    {
      "epoch": 8.513085993672707,
      "grad_norm": 8.538027032045648e-05,
      "learning_rate": 0.0008297440322116767,
      "loss": 0.0031,
      "step": 29600
    },
    {
      "epoch": 8.527466206499856,
      "grad_norm": 0.00014858387294225395,
      "learning_rate": 0.0008294564279551338,
      "loss": 0.0017,
      "step": 29650
    },
    {
      "epoch": 8.541846419327007,
      "grad_norm": 9.558629972161725e-05,
      "learning_rate": 0.0008291688236985907,
      "loss": 0.0062,
      "step": 29700
    },
    {
      "epoch": 8.556226632154155,
      "grad_norm": 8.837724453769624e-05,
      "learning_rate": 0.0008288812194420477,
      "loss": 0.0026,
      "step": 29750
    },
    {
      "epoch": 8.570606844981306,
      "grad_norm": 0.00032545652356930077,
      "learning_rate": 0.0008285936151855048,
      "loss": 0.0028,
      "step": 29800
    },
    {
      "epoch": 8.584987057808455,
      "grad_norm": 0.015663091093301773,
      "learning_rate": 0.0008283060109289617,
      "loss": 0.001,
      "step": 29850
    },
    {
      "epoch": 8.599367270635605,
      "grad_norm": 0.006686702370643616,
      "learning_rate": 0.0008280184066724187,
      "loss": 0.0022,
      "step": 29900
    },
    {
      "epoch": 8.613747483462756,
      "grad_norm": 0.00429856963455677,
      "learning_rate": 0.0008277308024158758,
      "loss": 0.0027,
      "step": 29950
    },
    {
      "epoch": 8.628127696289905,
      "grad_norm": 0.0010179819073528051,
      "learning_rate": 0.0008274431981593328,
      "loss": 0.006,
      "step": 30000
    },
    {
      "epoch": 8.642507909117056,
      "grad_norm": 0.015963593497872353,
      "learning_rate": 0.0008271555939027898,
      "loss": 0.002,
      "step": 30050
    },
    {
      "epoch": 8.656888121944204,
      "grad_norm": 6.543017661897466e-05,
      "learning_rate": 0.0008268679896462468,
      "loss": 0.004,
      "step": 30100
    },
    {
      "epoch": 8.671268334771355,
      "grad_norm": 0.032820940017700195,
      "learning_rate": 0.0008265803853897038,
      "loss": 0.0018,
      "step": 30150
    },
    {
      "epoch": 8.685648547598504,
      "grad_norm": 9.41212274483405e-05,
      "learning_rate": 0.0008262927811331607,
      "loss": 0.0035,
      "step": 30200
    },
    {
      "epoch": 8.700028760425655,
      "grad_norm": 0.06739427149295807,
      "learning_rate": 0.0008260051768766178,
      "loss": 0.0068,
      "step": 30250
    },
    {
      "epoch": 8.714408973252803,
      "grad_norm": 0.005380234215408564,
      "learning_rate": 0.0008257175726200747,
      "loss": 0.001,
      "step": 30300
    },
    {
      "epoch": 8.728789186079954,
      "grad_norm": 0.06757379323244095,
      "learning_rate": 0.0008254299683635318,
      "loss": 0.0033,
      "step": 30350
    },
    {
      "epoch": 8.743169398907105,
      "grad_norm": 3.4884214983321726e-05,
      "learning_rate": 0.0008251423641069889,
      "loss": 0.0025,
      "step": 30400
    },
    {
      "epoch": 8.757549611734254,
      "grad_norm": 0.055976253002882004,
      "learning_rate": 0.0008248547598504458,
      "loss": 0.002,
      "step": 30450
    },
    {
      "epoch": 8.771929824561404,
      "grad_norm": 8.51995064294897e-05,
      "learning_rate": 0.0008245671555939028,
      "loss": 0.0047,
      "step": 30500
    },
    {
      "epoch": 8.786310037388553,
      "grad_norm": 0.00022993420134298503,
      "learning_rate": 0.0008242795513373598,
      "loss": 0.0025,
      "step": 30550
    },
    {
      "epoch": 8.800690250215704,
      "grad_norm": 0.00013077707262709737,
      "learning_rate": 0.0008239919470808168,
      "loss": 0.0009,
      "step": 30600
    },
    {
      "epoch": 8.815070463042852,
      "grad_norm": 0.054708290845155716,
      "learning_rate": 0.0008237043428242738,
      "loss": 0.0041,
      "step": 30650
    },
    {
      "epoch": 8.829450675870003,
      "grad_norm": 0.02001318149268627,
      "learning_rate": 0.0008234167385677308,
      "loss": 0.003,
      "step": 30700
    },
    {
      "epoch": 8.843830888697152,
      "grad_norm": 0.052526846528053284,
      "learning_rate": 0.0008231291343111878,
      "loss": 0.002,
      "step": 30750
    },
    {
      "epoch": 8.858211101524303,
      "grad_norm": 0.0008092389325611293,
      "learning_rate": 0.0008228415300546448,
      "loss": 0.0015,
      "step": 30800
    },
    {
      "epoch": 8.872591314351453,
      "grad_norm": 0.0002152070519514382,
      "learning_rate": 0.0008225539257981019,
      "loss": 0.0054,
      "step": 30850
    },
    {
      "epoch": 8.886971527178602,
      "grad_norm": 0.0007964104297570884,
      "learning_rate": 0.0008222663215415588,
      "loss": 0.0032,
      "step": 30900
    },
    {
      "epoch": 8.901351740005753,
      "grad_norm": 0.030125506222248077,
      "learning_rate": 0.0008219787172850158,
      "loss": 0.0016,
      "step": 30950
    },
    {
      "epoch": 8.915731952832902,
      "grad_norm": 0.00020506781584117562,
      "learning_rate": 0.0008216911130284729,
      "loss": 0.0046,
      "step": 31000
    },
    {
      "epoch": 8.930112165660052,
      "grad_norm": 0.005469565279781818,
      "learning_rate": 0.0008214035087719298,
      "loss": 0.0025,
      "step": 31050
    },
    {
      "epoch": 8.944492378487201,
      "grad_norm": 0.00734791811555624,
      "learning_rate": 0.0008211159045153868,
      "loss": 0.0032,
      "step": 31100
    },
    {
      "epoch": 8.958872591314352,
      "grad_norm": 0.0001421265333192423,
      "learning_rate": 0.0008208283002588438,
      "loss": 0.0023,
      "step": 31150
    },
    {
      "epoch": 8.9732528041415,
      "grad_norm": 0.003098836401477456,
      "learning_rate": 0.0008205406960023009,
      "loss": 0.0042,
      "step": 31200
    },
    {
      "epoch": 8.987633016968651,
      "grad_norm": 0.00682911928743124,
      "learning_rate": 0.0008202530917457579,
      "loss": 0.0044,
      "step": 31250
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.010119139216840267,
      "eval_runtime": 17.8748,
      "eval_samples_per_second": 2669.621,
      "eval_steps_per_second": 41.735,
      "step": 31293
    },
    {
      "epoch": 9.002013229795802,
      "grad_norm": 0.001522201462648809,
      "learning_rate": 0.0008199654874892149,
      "loss": 0.0017,
      "step": 31300
    },
    {
      "epoch": 9.01639344262295,
      "grad_norm": 0.00012263297685422003,
      "learning_rate": 0.0008196778832326719,
      "loss": 0.0027,
      "step": 31350
    },
    {
      "epoch": 9.030773655450101,
      "grad_norm": 0.02597815915942192,
      "learning_rate": 0.0008193902789761288,
      "loss": 0.0037,
      "step": 31400
    },
    {
      "epoch": 9.04515386827725,
      "grad_norm": 0.000212443686905317,
      "learning_rate": 0.0008191026747195859,
      "loss": 0.0018,
      "step": 31450
    },
    {
      "epoch": 9.0595340811044,
      "grad_norm": 0.04072248190641403,
      "learning_rate": 0.0008188150704630428,
      "loss": 0.0015,
      "step": 31500
    },
    {
      "epoch": 9.07391429393155,
      "grad_norm": 0.002030220115557313,
      "learning_rate": 0.0008185274662064998,
      "loss": 0.0037,
      "step": 31550
    },
    {
      "epoch": 9.0882945067587,
      "grad_norm": 0.0009146637166850269,
      "learning_rate": 0.000818239861949957,
      "loss": 0.0022,
      "step": 31600
    },
    {
      "epoch": 9.102674719585849,
      "grad_norm": 0.05133943632245064,
      "learning_rate": 0.0008179522576934139,
      "loss": 0.0012,
      "step": 31650
    },
    {
      "epoch": 9.117054932413,
      "grad_norm": 0.00028722829301841557,
      "learning_rate": 0.0008176646534368709,
      "loss": 0.0018,
      "step": 31700
    },
    {
      "epoch": 9.13143514524015,
      "grad_norm": 5.801158113172278e-05,
      "learning_rate": 0.0008173770491803279,
      "loss": 0.0032,
      "step": 31750
    },
    {
      "epoch": 9.1458153580673,
      "grad_norm": 0.01397138461470604,
      "learning_rate": 0.0008170894449237849,
      "loss": 0.0052,
      "step": 31800
    },
    {
      "epoch": 9.16019557089445,
      "grad_norm": 9.913937537930906e-05,
      "learning_rate": 0.0008168018406672419,
      "loss": 0.0023,
      "step": 31850
    },
    {
      "epoch": 9.174575783721599,
      "grad_norm": 0.00013129974831826985,
      "learning_rate": 0.0008165142364106989,
      "loss": 0.0013,
      "step": 31900
    },
    {
      "epoch": 9.18895599654875,
      "grad_norm": 0.07055073976516724,
      "learning_rate": 0.0008162266321541559,
      "loss": 0.0013,
      "step": 31950
    },
    {
      "epoch": 9.203336209375898,
      "grad_norm": 0.08460196852684021,
      "learning_rate": 0.0008159390278976128,
      "loss": 0.0033,
      "step": 32000
    },
    {
      "epoch": 9.217716422203049,
      "grad_norm": 0.002212642226368189,
      "learning_rate": 0.00081565142364107,
      "loss": 0.0031,
      "step": 32050
    },
    {
      "epoch": 9.232096635030198,
      "grad_norm": 0.0175271425396204,
      "learning_rate": 0.0008153638193845269,
      "loss": 0.0022,
      "step": 32100
    },
    {
      "epoch": 9.246476847857348,
      "grad_norm": 6.620207568630576e-05,
      "learning_rate": 0.0008150762151279839,
      "loss": 0.0012,
      "step": 32150
    },
    {
      "epoch": 9.260857060684499,
      "grad_norm": 0.011880013160407543,
      "learning_rate": 0.000814788610871441,
      "loss": 0.0013,
      "step": 32200
    },
    {
      "epoch": 9.275237273511648,
      "grad_norm": 0.00035384876537136734,
      "learning_rate": 0.0008145010066148979,
      "loss": 0.0044,
      "step": 32250
    },
    {
      "epoch": 9.289617486338798,
      "grad_norm": 0.0009250915609300137,
      "learning_rate": 0.0008142134023583549,
      "loss": 0.0022,
      "step": 32300
    },
    {
      "epoch": 9.303997699165947,
      "grad_norm": 0.0018944917246699333,
      "learning_rate": 0.0008139257981018119,
      "loss": 0.0032,
      "step": 32350
    },
    {
      "epoch": 9.318377911993098,
      "grad_norm": 0.00042470701737329364,
      "learning_rate": 0.0008136381938452689,
      "loss": 0.0042,
      "step": 32400
    },
    {
      "epoch": 9.332758124820247,
      "grad_norm": 0.0005497575039044023,
      "learning_rate": 0.0008133505895887258,
      "loss": 0.003,
      "step": 32450
    },
    {
      "epoch": 9.347138337647397,
      "grad_norm": 0.07942121475934982,
      "learning_rate": 0.000813062985332183,
      "loss": 0.0029,
      "step": 32500
    },
    {
      "epoch": 9.361518550474546,
      "grad_norm": 0.029151806607842445,
      "learning_rate": 0.00081277538107564,
      "loss": 0.0026,
      "step": 32550
    },
    {
      "epoch": 9.375898763301697,
      "grad_norm": 0.0007534715696237981,
      "learning_rate": 0.0008124877768190969,
      "loss": 0.0018,
      "step": 32600
    },
    {
      "epoch": 9.390278976128847,
      "grad_norm": 0.02280822955071926,
      "learning_rate": 0.000812200172562554,
      "loss": 0.0022,
      "step": 32650
    },
    {
      "epoch": 9.404659188955996,
      "grad_norm": 0.0011120140552520752,
      "learning_rate": 0.0008119125683060109,
      "loss": 0.0049,
      "step": 32700
    },
    {
      "epoch": 9.419039401783147,
      "grad_norm": 0.1515447050333023,
      "learning_rate": 0.0008116249640494679,
      "loss": 0.0025,
      "step": 32750
    },
    {
      "epoch": 9.433419614610296,
      "grad_norm": 0.03805330768227577,
      "learning_rate": 0.000811337359792925,
      "loss": 0.0039,
      "step": 32800
    },
    {
      "epoch": 9.447799827437446,
      "grad_norm": 0.0002293201832799241,
      "learning_rate": 0.0008110497555363819,
      "loss": 0.0045,
      "step": 32850
    },
    {
      "epoch": 9.462180040264595,
      "grad_norm": 7.930246647447348e-05,
      "learning_rate": 0.0008107621512798389,
      "loss": 0.0023,
      "step": 32900
    },
    {
      "epoch": 9.476560253091746,
      "grad_norm": 0.0012110167881473899,
      "learning_rate": 0.000810474547023296,
      "loss": 0.003,
      "step": 32950
    },
    {
      "epoch": 9.490940465918897,
      "grad_norm": 0.0007872541318647563,
      "learning_rate": 0.000810186942766753,
      "loss": 0.0028,
      "step": 33000
    },
    {
      "epoch": 9.505320678746045,
      "grad_norm": 0.009946064092218876,
      "learning_rate": 0.0008098993385102099,
      "loss": 0.003,
      "step": 33050
    },
    {
      "epoch": 9.519700891573196,
      "grad_norm": 0.01595281809568405,
      "learning_rate": 0.000809611734253667,
      "loss": 0.0021,
      "step": 33100
    },
    {
      "epoch": 9.534081104400345,
      "grad_norm": 0.007557578384876251,
      "learning_rate": 0.000809324129997124,
      "loss": 0.0033,
      "step": 33150
    },
    {
      "epoch": 9.548461317227495,
      "grad_norm": 0.0004360245366115123,
      "learning_rate": 0.0008090365257405809,
      "loss": 0.0017,
      "step": 33200
    },
    {
      "epoch": 9.562841530054644,
      "grad_norm": 0.1273406594991684,
      "learning_rate": 0.000808748921484038,
      "loss": 0.0016,
      "step": 33250
    },
    {
      "epoch": 9.577221742881795,
      "grad_norm": 0.0002762821677606553,
      "learning_rate": 0.0008084613172274949,
      "loss": 0.0057,
      "step": 33300
    },
    {
      "epoch": 9.591601955708944,
      "grad_norm": 0.0002472974010743201,
      "learning_rate": 0.000808173712970952,
      "loss": 0.0034,
      "step": 33350
    },
    {
      "epoch": 9.605982168536094,
      "grad_norm": 0.0362185463309288,
      "learning_rate": 0.0008078861087144091,
      "loss": 0.0037,
      "step": 33400
    },
    {
      "epoch": 9.620362381363243,
      "grad_norm": 0.0005886814906261861,
      "learning_rate": 0.000807598504457866,
      "loss": 0.0057,
      "step": 33450
    },
    {
      "epoch": 9.634742594190394,
      "grad_norm": 0.0035535478964447975,
      "learning_rate": 0.000807310900201323,
      "loss": 0.0059,
      "step": 33500
    },
    {
      "epoch": 9.649122807017545,
      "grad_norm": 0.0072201625443995,
      "learning_rate": 0.00080702329594478,
      "loss": 0.0013,
      "step": 33550
    },
    {
      "epoch": 9.663503019844693,
      "grad_norm": 0.01038944162428379,
      "learning_rate": 0.000806735691688237,
      "loss": 0.0019,
      "step": 33600
    },
    {
      "epoch": 9.677883232671844,
      "grad_norm": 0.00023465331469196826,
      "learning_rate": 0.0008064480874316939,
      "loss": 0.0012,
      "step": 33650
    },
    {
      "epoch": 9.692263445498993,
      "grad_norm": 0.00019358753343112767,
      "learning_rate": 0.000806160483175151,
      "loss": 0.0026,
      "step": 33700
    },
    {
      "epoch": 9.706643658326144,
      "grad_norm": 0.000691175926476717,
      "learning_rate": 0.000805872878918608,
      "loss": 0.005,
      "step": 33750
    },
    {
      "epoch": 9.721023871153292,
      "grad_norm": 4.833668208448216e-05,
      "learning_rate": 0.000805585274662065,
      "loss": 0.0019,
      "step": 33800
    },
    {
      "epoch": 9.735404083980443,
      "grad_norm": 0.0018085531191900373,
      "learning_rate": 0.0008052976704055221,
      "loss": 0.0034,
      "step": 33850
    },
    {
      "epoch": 9.749784296807594,
      "grad_norm": 2.4725039111217484e-05,
      "learning_rate": 0.000805010066148979,
      "loss": 0.0041,
      "step": 33900
    },
    {
      "epoch": 9.764164509634742,
      "grad_norm": 4.639727558242157e-05,
      "learning_rate": 0.000804722461892436,
      "loss": 0.001,
      "step": 33950
    },
    {
      "epoch": 9.778544722461893,
      "grad_norm": 0.011112361215054989,
      "learning_rate": 0.0008044348576358931,
      "loss": 0.0029,
      "step": 34000
    },
    {
      "epoch": 9.792924935289042,
      "grad_norm": 6.915684934938326e-05,
      "learning_rate": 0.00080414725337935,
      "loss": 0.0016,
      "step": 34050
    },
    {
      "epoch": 9.807305148116193,
      "grad_norm": 0.012181060388684273,
      "learning_rate": 0.000803859649122807,
      "loss": 0.0025,
      "step": 34100
    },
    {
      "epoch": 9.821685360943341,
      "grad_norm": 0.00786938238888979,
      "learning_rate": 0.000803572044866264,
      "loss": 0.0047,
      "step": 34150
    },
    {
      "epoch": 9.836065573770492,
      "grad_norm": 0.0022998149506747723,
      "learning_rate": 0.000803284440609721,
      "loss": 0.0041,
      "step": 34200
    },
    {
      "epoch": 9.850445786597641,
      "grad_norm": 0.019781744107604027,
      "learning_rate": 0.000802996836353178,
      "loss": 0.0019,
      "step": 34250
    },
    {
      "epoch": 9.864825999424792,
      "grad_norm": 0.0003833493101410568,
      "learning_rate": 0.0008027092320966351,
      "loss": 0.0009,
      "step": 34300
    },
    {
      "epoch": 9.87920621225194,
      "grad_norm": 0.0029509589076042175,
      "learning_rate": 0.0008024216278400921,
      "loss": 0.0017,
      "step": 34350
    },
    {
      "epoch": 9.893586425079091,
      "grad_norm": 0.07981658726930618,
      "learning_rate": 0.000802134023583549,
      "loss": 0.0036,
      "step": 34400
    },
    {
      "epoch": 9.907966637906242,
      "grad_norm": 0.0002830137964338064,
      "learning_rate": 0.0008018464193270061,
      "loss": 0.0045,
      "step": 34450
    },
    {
      "epoch": 9.92234685073339,
      "grad_norm": 5.9831159887835383e-05,
      "learning_rate": 0.000801558815070463,
      "loss": 0.0005,
      "step": 34500
    },
    {
      "epoch": 9.936727063560541,
      "grad_norm": 0.0008078758837655187,
      "learning_rate": 0.00080127121081392,
      "loss": 0.0017,
      "step": 34550
    },
    {
      "epoch": 9.95110727638769,
      "grad_norm": 0.010386566631495953,
      "learning_rate": 0.0008009836065573771,
      "loss": 0.003,
      "step": 34600
    },
    {
      "epoch": 9.96548748921484,
      "grad_norm": 0.00038183582364581525,
      "learning_rate": 0.000800696002300834,
      "loss": 0.002,
      "step": 34650
    },
    {
      "epoch": 9.97986770204199,
      "grad_norm": 0.00015204613737296313,
      "learning_rate": 0.0008004083980442911,
      "loss": 0.0033,
      "step": 34700
    },
    {
      "epoch": 9.99424791486914,
      "grad_norm": 0.10547620803117752,
      "learning_rate": 0.0008001207937877481,
      "loss": 0.006,
      "step": 34750
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.010688338428735733,
      "eval_runtime": 16.6972,
      "eval_samples_per_second": 2857.902,
      "eval_steps_per_second": 44.678,
      "step": 34770
    },
    {
      "epoch": 10.00862812769629,
      "grad_norm": 0.00035518870572559536,
      "learning_rate": 0.0007998331895312051,
      "loss": 0.0007,
      "step": 34800
    },
    {
      "epoch": 10.02300834052344,
      "grad_norm": 0.003703045193105936,
      "learning_rate": 0.000799545585274662,
      "loss": 0.0052,
      "step": 34850
    },
    {
      "epoch": 10.03738855335059,
      "grad_norm": 0.0006976541480980814,
      "learning_rate": 0.0007992579810181191,
      "loss": 0.0011,
      "step": 34900
    },
    {
      "epoch": 10.051768766177739,
      "grad_norm": 0.00013386172940954566,
      "learning_rate": 0.0007989703767615761,
      "loss": 0.0023,
      "step": 34950
    },
    {
      "epoch": 10.06614897900489,
      "grad_norm": 5.022510595154017e-05,
      "learning_rate": 0.000798682772505033,
      "loss": 0.0025,
      "step": 35000
    },
    {
      "epoch": 10.080529191832039,
      "grad_norm": 0.0001030837811413221,
      "learning_rate": 0.0007983951682484901,
      "loss": 0.0033,
      "step": 35050
    },
    {
      "epoch": 10.09490940465919,
      "grad_norm": 8.58498242450878e-05,
      "learning_rate": 0.000798107563991947,
      "loss": 0.0043,
      "step": 35100
    },
    {
      "epoch": 10.109289617486338,
      "grad_norm": 0.0712445005774498,
      "learning_rate": 0.0007978199597354041,
      "loss": 0.0031,
      "step": 35150
    },
    {
      "epoch": 10.123669830313489,
      "grad_norm": 0.00010981396917486563,
      "learning_rate": 0.0007975323554788611,
      "loss": 0.0042,
      "step": 35200
    },
    {
      "epoch": 10.13805004314064,
      "grad_norm": 0.06105087697505951,
      "learning_rate": 0.0007972447512223181,
      "loss": 0.0041,
      "step": 35250
    },
    {
      "epoch": 10.152430255967788,
      "grad_norm": 0.002104156883433461,
      "learning_rate": 0.0007969571469657751,
      "loss": 0.0016,
      "step": 35300
    },
    {
      "epoch": 10.166810468794939,
      "grad_norm": 0.00017721139010973275,
      "learning_rate": 0.0007966695427092321,
      "loss": 0.004,
      "step": 35350
    },
    {
      "epoch": 10.181190681622088,
      "grad_norm": 0.00045773701276630163,
      "learning_rate": 0.0007963819384526891,
      "loss": 0.0033,
      "step": 35400
    },
    {
      "epoch": 10.195570894449238,
      "grad_norm": 0.0006141564808785915,
      "learning_rate": 0.000796094334196146,
      "loss": 0.0034,
      "step": 35450
    },
    {
      "epoch": 10.209951107276387,
      "grad_norm": 0.00011044686107197776,
      "learning_rate": 0.0007958067299396031,
      "loss": 0.0012,
      "step": 35500
    },
    {
      "epoch": 10.224331320103538,
      "grad_norm": 3.475914127193391e-05,
      "learning_rate": 0.0007955191256830602,
      "loss": 0.0034,
      "step": 35550
    },
    {
      "epoch": 10.238711532930687,
      "grad_norm": 0.001724206143990159,
      "learning_rate": 0.0007952315214265171,
      "loss": 0.0032,
      "step": 35600
    },
    {
      "epoch": 10.253091745757837,
      "grad_norm": 0.03318091854453087,
      "learning_rate": 0.0007949439171699742,
      "loss": 0.0013,
      "step": 35650
    },
    {
      "epoch": 10.267471958584988,
      "grad_norm": 0.060839489102363586,
      "learning_rate": 0.0007946563129134311,
      "loss": 0.0038,
      "step": 35700
    },
    {
      "epoch": 10.281852171412137,
      "grad_norm": 8.300586341647431e-05,
      "learning_rate": 0.0007943687086568881,
      "loss": 0.002,
      "step": 35750
    },
    {
      "epoch": 10.296232384239287,
      "grad_norm": 0.00032795604784041643,
      "learning_rate": 0.0007940811044003451,
      "loss": 0.0025,
      "step": 35800
    },
    {
      "epoch": 10.310612597066436,
      "grad_norm": 0.000387870823033154,
      "learning_rate": 0.0007937935001438021,
      "loss": 0.0018,
      "step": 35850
    },
    {
      "epoch": 10.324992809893587,
      "grad_norm": 0.02625163085758686,
      "learning_rate": 0.0007935058958872591,
      "loss": 0.0016,
      "step": 35900
    },
    {
      "epoch": 10.339373022720736,
      "grad_norm": 0.031516872346401215,
      "learning_rate": 0.0007932182916307162,
      "loss": 0.0051,
      "step": 35950
    },
    {
      "epoch": 10.353753235547886,
      "grad_norm": 0.0001664043520577252,
      "learning_rate": 0.0007929306873741732,
      "loss": 0.002,
      "step": 36000
    },
    {
      "epoch": 10.368133448375035,
      "grad_norm": 0.0016694212099537253,
      "learning_rate": 0.0007926430831176301,
      "loss": 0.0019,
      "step": 36050
    },
    {
      "epoch": 10.382513661202186,
      "grad_norm": 0.0001862446079030633,
      "learning_rate": 0.0007923554788610872,
      "loss": 0.003,
      "step": 36100
    },
    {
      "epoch": 10.396893874029336,
      "grad_norm": 0.01956656388938427,
      "learning_rate": 0.0007920678746045442,
      "loss": 0.0027,
      "step": 36150
    },
    {
      "epoch": 10.411274086856485,
      "grad_norm": 0.0006457757554017007,
      "learning_rate": 0.0007917802703480011,
      "loss": 0.0013,
      "step": 36200
    },
    {
      "epoch": 10.425654299683636,
      "grad_norm": 0.00021265141549520195,
      "learning_rate": 0.0007914926660914582,
      "loss": 0.0005,
      "step": 36250
    },
    {
      "epoch": 10.440034512510785,
      "grad_norm": 0.0005061018746346235,
      "learning_rate": 0.0007912050618349151,
      "loss": 0.0033,
      "step": 36300
    },
    {
      "epoch": 10.454414725337935,
      "grad_norm": 0.0010290940990671515,
      "learning_rate": 0.0007909174575783721,
      "loss": 0.0006,
      "step": 36350
    },
    {
      "epoch": 10.468794938165084,
      "grad_norm": 0.009825412184000015,
      "learning_rate": 0.0007906298533218292,
      "loss": 0.0034,
      "step": 36400
    },
    {
      "epoch": 10.483175150992235,
      "grad_norm": 0.024965617805719376,
      "learning_rate": 0.0007903422490652862,
      "loss": 0.0033,
      "step": 36450
    },
    {
      "epoch": 10.497555363819384,
      "grad_norm": 0.0005115268286317587,
      "learning_rate": 0.0007900546448087432,
      "loss": 0.0015,
      "step": 36500
    },
    {
      "epoch": 10.511935576646534,
      "grad_norm": 0.0009276428027078509,
      "learning_rate": 0.0007897670405522002,
      "loss": 0.0021,
      "step": 36550
    },
    {
      "epoch": 10.526315789473685,
      "grad_norm": 0.00022765334870200604,
      "learning_rate": 0.0007894794362956572,
      "loss": 0.0046,
      "step": 36600
    },
    {
      "epoch": 10.540696002300834,
      "grad_norm": 0.5100651979446411,
      "learning_rate": 0.0007891918320391141,
      "loss": 0.0046,
      "step": 36650
    },
    {
      "epoch": 10.555076215127984,
      "grad_norm": 0.00024737141211517155,
      "learning_rate": 0.0007889042277825712,
      "loss": 0.0026,
      "step": 36700
    },
    {
      "epoch": 10.569456427955133,
      "grad_norm": 0.0009087255457416177,
      "learning_rate": 0.0007886166235260282,
      "loss": 0.0021,
      "step": 36750
    },
    {
      "epoch": 10.583836640782284,
      "grad_norm": 0.0001201220802613534,
      "learning_rate": 0.0007883290192694851,
      "loss": 0.003,
      "step": 36800
    },
    {
      "epoch": 10.598216853609433,
      "grad_norm": 0.0006993893184699118,
      "learning_rate": 0.0007880414150129423,
      "loss": 0.0023,
      "step": 36850
    },
    {
      "epoch": 10.612597066436583,
      "grad_norm": 0.0012911894591525197,
      "learning_rate": 0.0007877538107563992,
      "loss": 0.0032,
      "step": 36900
    },
    {
      "epoch": 10.626977279263734,
      "grad_norm": 0.00032682609162293375,
      "learning_rate": 0.0007874662064998562,
      "loss": 0.0047,
      "step": 36950
    },
    {
      "epoch": 10.641357492090883,
      "grad_norm": 0.025210795924067497,
      "learning_rate": 0.0007871786022433132,
      "loss": 0.0014,
      "step": 37000
    },
    {
      "epoch": 10.655737704918034,
      "grad_norm": 8.91114177647978e-05,
      "learning_rate": 0.0007868909979867702,
      "loss": 0.0017,
      "step": 37050
    },
    {
      "epoch": 10.670117917745182,
      "grad_norm": 0.05481588840484619,
      "learning_rate": 0.0007866033937302272,
      "loss": 0.0025,
      "step": 37100
    },
    {
      "epoch": 10.684498130572333,
      "grad_norm": 7.662829739274457e-05,
      "learning_rate": 0.0007863157894736842,
      "loss": 0.0035,
      "step": 37150
    },
    {
      "epoch": 10.698878343399482,
      "grad_norm": 2.210898674093187e-05,
      "learning_rate": 0.0007860281852171412,
      "loss": 0.0045,
      "step": 37200
    },
    {
      "epoch": 10.713258556226632,
      "grad_norm": 0.0002219092712039128,
      "learning_rate": 0.0007857405809605981,
      "loss": 0.0028,
      "step": 37250
    },
    {
      "epoch": 10.727638769053781,
      "grad_norm": 0.0968523770570755,
      "learning_rate": 0.0007854529767040553,
      "loss": 0.0035,
      "step": 37300
    },
    {
      "epoch": 10.742018981880932,
      "grad_norm": 0.0005049652536399662,
      "learning_rate": 0.0007851653724475123,
      "loss": 0.0039,
      "step": 37350
    },
    {
      "epoch": 10.75639919470808,
      "grad_norm": 0.0006550721009261906,
      "learning_rate": 0.0007848777681909692,
      "loss": 0.005,
      "step": 37400
    },
    {
      "epoch": 10.770779407535231,
      "grad_norm": 0.02485927753150463,
      "learning_rate": 0.0007845901639344263,
      "loss": 0.0028,
      "step": 37450
    },
    {
      "epoch": 10.785159620362382,
      "grad_norm": 0.0005944346776232123,
      "learning_rate": 0.0007843025596778832,
      "loss": 0.0029,
      "step": 37500
    },
    {
      "epoch": 10.799539833189531,
      "grad_norm": 0.02074199542403221,
      "learning_rate": 0.0007840149554213402,
      "loss": 0.0029,
      "step": 37550
    },
    {
      "epoch": 10.813920046016682,
      "grad_norm": 0.005947203375399113,
      "learning_rate": 0.0007837273511647972,
      "loss": 0.0037,
      "step": 37600
    },
    {
      "epoch": 10.82830025884383,
      "grad_norm": 0.012536655180156231,
      "learning_rate": 0.0007834397469082542,
      "loss": 0.0022,
      "step": 37650
    },
    {
      "epoch": 10.842680471670981,
      "grad_norm": 0.0005312237772159278,
      "learning_rate": 0.0007831521426517113,
      "loss": 0.0013,
      "step": 37700
    },
    {
      "epoch": 10.85706068449813,
      "grad_norm": 0.0057496074587106705,
      "learning_rate": 0.0007828645383951683,
      "loss": 0.0015,
      "step": 37750
    },
    {
      "epoch": 10.87144089732528,
      "grad_norm": 0.00011867678404087201,
      "learning_rate": 0.0007825769341386253,
      "loss": 0.0033,
      "step": 37800
    },
    {
      "epoch": 10.885821110152431,
      "grad_norm": 6.336693331832066e-05,
      "learning_rate": 0.0007822893298820822,
      "loss": 0.0033,
      "step": 37850
    },
    {
      "epoch": 10.90020132297958,
      "grad_norm": 6.419047713279724e-05,
      "learning_rate": 0.0007820017256255393,
      "loss": 0.0016,
      "step": 37900
    },
    {
      "epoch": 10.91458153580673,
      "grad_norm": 9.23018014873378e-05,
      "learning_rate": 0.0007817141213689962,
      "loss": 0.0053,
      "step": 37950
    },
    {
      "epoch": 10.92896174863388,
      "grad_norm": 0.002019282663241029,
      "learning_rate": 0.0007814265171124532,
      "loss": 0.0044,
      "step": 38000
    },
    {
      "epoch": 10.94334196146103,
      "grad_norm": 0.0012435776880010962,
      "learning_rate": 0.0007811389128559103,
      "loss": 0.005,
      "step": 38050
    },
    {
      "epoch": 10.957722174288179,
      "grad_norm": 0.0002966994361486286,
      "learning_rate": 0.0007808513085993672,
      "loss": 0.0056,
      "step": 38100
    },
    {
      "epoch": 10.97210238711533,
      "grad_norm": 0.0009918335126712918,
      "learning_rate": 0.0007805637043428243,
      "loss": 0.0018,
      "step": 38150
    },
    {
      "epoch": 10.986482599942478,
      "grad_norm": 9.361038246424869e-05,
      "learning_rate": 0.0007802761000862813,
      "loss": 0.0039,
      "step": 38200
    },
    {
      "epoch": 11.0,
      "eval_loss": 0.010118232108652592,
      "eval_runtime": 17.1619,
      "eval_samples_per_second": 2780.518,
      "eval_steps_per_second": 43.468,
      "step": 38247
    },
    {
      "epoch": 11.000862812769629,
      "grad_norm": 0.001091805170290172,
      "learning_rate": 0.0007799884958297383,
      "loss": 0.0016,
      "step": 38250
    },
    {
      "epoch": 11.01524302559678,
      "grad_norm": 0.1405005156993866,
      "learning_rate": 0.0007797008915731953,
      "loss": 0.0033,
      "step": 38300
    },
    {
      "epoch": 11.029623238423929,
      "grad_norm": 0.0011260781902819872,
      "learning_rate": 0.0007794132873166523,
      "loss": 0.0042,
      "step": 38350
    },
    {
      "epoch": 11.04400345125108,
      "grad_norm": 0.030150607228279114,
      "learning_rate": 0.0007791256830601093,
      "loss": 0.0019,
      "step": 38400
    },
    {
      "epoch": 11.058383664078228,
      "grad_norm": 0.0011494996724650264,
      "learning_rate": 0.0007788380788035662,
      "loss": 0.0021,
      "step": 38450
    },
    {
      "epoch": 11.072763876905379,
      "grad_norm": 0.01416943222284317,
      "learning_rate": 0.0007785504745470233,
      "loss": 0.0031,
      "step": 38500
    },
    {
      "epoch": 11.087144089732528,
      "grad_norm": 2.550033423176501e-05,
      "learning_rate": 0.0007782628702904802,
      "loss": 0.0027,
      "step": 38550
    },
    {
      "epoch": 11.101524302559678,
      "grad_norm": 7.261029531946406e-05,
      "learning_rate": 0.0007779752660339373,
      "loss": 0.0043,
      "step": 38600
    },
    {
      "epoch": 11.115904515386827,
      "grad_norm": 0.0002071902999887243,
      "learning_rate": 0.0007776876617773944,
      "loss": 0.0026,
      "step": 38650
    },
    {
      "epoch": 11.130284728213978,
      "grad_norm": 0.00037309169420041144,
      "learning_rate": 0.0007774000575208513,
      "loss": 0.0027,
      "step": 38700
    },
    {
      "epoch": 11.144664941041128,
      "grad_norm": 0.10091090947389603,
      "learning_rate": 0.0007771124532643083,
      "loss": 0.001,
      "step": 38750
    },
    {
      "epoch": 11.159045153868277,
      "grad_norm": 0.0014172146329656243,
      "learning_rate": 0.0007768248490077653,
      "loss": 0.0024,
      "step": 38800
    },
    {
      "epoch": 11.173425366695428,
      "grad_norm": 0.0001864185614977032,
      "learning_rate": 0.0007765372447512223,
      "loss": 0.004,
      "step": 38850
    },
    {
      "epoch": 11.187805579522577,
      "grad_norm": 0.00016787719505373389,
      "learning_rate": 0.0007762496404946793,
      "loss": 0.0018,
      "step": 38900
    },
    {
      "epoch": 11.202185792349727,
      "grad_norm": 0.00048451751354150474,
      "learning_rate": 0.0007759620362381363,
      "loss": 0.0012,
      "step": 38950
    },
    {
      "epoch": 11.216566005176876,
      "grad_norm": 0.0003045004268642515,
      "learning_rate": 0.0007756744319815934,
      "loss": 0.0033,
      "step": 39000
    },
    {
      "epoch": 11.230946218004027,
      "grad_norm": 0.00166068144608289,
      "learning_rate": 0.0007753868277250503,
      "loss": 0.0039,
      "step": 39050
    },
    {
      "epoch": 11.245326430831176,
      "grad_norm": 0.0006113291601650417,
      "learning_rate": 0.0007750992234685074,
      "loss": 0.0023,
      "step": 39100
    },
    {
      "epoch": 11.259706643658326,
      "grad_norm": 0.0011896262876689434,
      "learning_rate": 0.0007748116192119643,
      "loss": 0.0011,
      "step": 39150
    },
    {
      "epoch": 11.274086856485477,
      "grad_norm": 5.1765513489954174e-05,
      "learning_rate": 0.0007745240149554213,
      "loss": 0.0015,
      "step": 39200
    },
    {
      "epoch": 11.288467069312626,
      "grad_norm": 0.0002286544331582263,
      "learning_rate": 0.0007742364106988784,
      "loss": 0.0054,
      "step": 39250
    },
    {
      "epoch": 11.302847282139776,
      "grad_norm": 0.0005376158514991403,
      "learning_rate": 0.0007739488064423353,
      "loss": 0.0064,
      "step": 39300
    },
    {
      "epoch": 11.317227494966925,
      "grad_norm": 0.00013662305718753487,
      "learning_rate": 0.0007736612021857923,
      "loss": 0.0052,
      "step": 39350
    },
    {
      "epoch": 11.331607707794076,
      "grad_norm": 0.022294403985142708,
      "learning_rate": 0.0007733735979292493,
      "loss": 0.002,
      "step": 39400
    },
    {
      "epoch": 11.345987920621225,
      "grad_norm": 0.0404188334941864,
      "learning_rate": 0.0007730859936727064,
      "loss": 0.0051,
      "step": 39450
    },
    {
      "epoch": 11.360368133448375,
      "grad_norm": 4.837474989471957e-05,
      "learning_rate": 0.0007727983894161634,
      "loss": 0.0038,
      "step": 39500
    },
    {
      "epoch": 11.374748346275524,
      "grad_norm": 0.0008926891605369747,
      "learning_rate": 0.0007725107851596204,
      "loss": 0.002,
      "step": 39550
    },
    {
      "epoch": 11.389128559102675,
      "grad_norm": 0.0003989867109339684,
      "learning_rate": 0.0007722231809030774,
      "loss": 0.0017,
      "step": 39600
    },
    {
      "epoch": 11.403508771929825,
      "grad_norm": 0.004934265743941069,
      "learning_rate": 0.0007719355766465343,
      "loss": 0.0013,
      "step": 39650
    },
    {
      "epoch": 11.417888984756974,
      "grad_norm": 0.002123258076608181,
      "learning_rate": 0.0007716479723899914,
      "loss": 0.0019,
      "step": 39700
    },
    {
      "epoch": 11.432269197584125,
      "grad_norm": 0.000355070224031806,
      "learning_rate": 0.0007713603681334483,
      "loss": 0.0011,
      "step": 39750
    },
    {
      "epoch": 11.446649410411274,
      "grad_norm": 0.0006846394971944392,
      "learning_rate": 0.0007710727638769053,
      "loss": 0.0014,
      "step": 39800
    },
    {
      "epoch": 11.461029623238424,
      "grad_norm": 0.010495350696146488,
      "learning_rate": 0.0007707851596203625,
      "loss": 0.0025,
      "step": 39850
    },
    {
      "epoch": 11.475409836065573,
      "grad_norm": 0.0031712695490568876,
      "learning_rate": 0.0007704975553638194,
      "loss": 0.0052,
      "step": 39900
    },
    {
      "epoch": 11.489790048892724,
      "grad_norm": 0.00021373078925535083,
      "learning_rate": 0.0007702099511072764,
      "loss": 0.0026,
      "step": 39950
    },
    {
      "epoch": 11.504170261719873,
      "grad_norm": 0.0012712753377854824,
      "learning_rate": 0.0007699223468507334,
      "loss": 0.0048,
      "step": 40000
    },
    {
      "epoch": 11.518550474547023,
      "grad_norm": 0.0001318836584687233,
      "learning_rate": 0.0007696347425941904,
      "loss": 0.0029,
      "step": 40050
    },
    {
      "epoch": 11.532930687374174,
      "grad_norm": 0.00010742988524725661,
      "learning_rate": 0.0007693471383376474,
      "loss": 0.0014,
      "step": 40100
    },
    {
      "epoch": 11.547310900201323,
      "grad_norm": 0.02232772298157215,
      "learning_rate": 0.0007690595340811044,
      "loss": 0.0039,
      "step": 40150
    },
    {
      "epoch": 11.561691113028473,
      "grad_norm": 0.00852663442492485,
      "learning_rate": 0.0007687719298245614,
      "loss": 0.0032,
      "step": 40200
    },
    {
      "epoch": 11.576071325855622,
      "grad_norm": 0.00033828994492068887,
      "learning_rate": 0.0007684843255680183,
      "loss": 0.0028,
      "step": 40250
    },
    {
      "epoch": 11.590451538682773,
      "grad_norm": 0.12354712188243866,
      "learning_rate": 0.0007681967213114755,
      "loss": 0.0024,
      "step": 40300
    },
    {
      "epoch": 11.604831751509922,
      "grad_norm": 0.0015518363798037171,
      "learning_rate": 0.0007679091170549324,
      "loss": 0.0036,
      "step": 40350
    },
    {
      "epoch": 11.619211964337072,
      "grad_norm": 0.00039571302477270365,
      "learning_rate": 0.0007676215127983894,
      "loss": 0.0037,
      "step": 40400
    },
    {
      "epoch": 11.633592177164221,
      "grad_norm": 0.011966343969106674,
      "learning_rate": 0.0007673339085418465,
      "loss": 0.0021,
      "step": 40450
    },
    {
      "epoch": 11.647972389991372,
      "grad_norm": 0.0005315582384355366,
      "learning_rate": 0.0007670463042853034,
      "loss": 0.0016,
      "step": 40500
    },
    {
      "epoch": 11.662352602818522,
      "grad_norm": 5.374029933591373e-05,
      "learning_rate": 0.0007667587000287604,
      "loss": 0.0059,
      "step": 40550
    },
    {
      "epoch": 11.676732815645671,
      "grad_norm": 0.01900019496679306,
      "learning_rate": 0.0007664710957722174,
      "loss": 0.0013,
      "step": 40600
    },
    {
      "epoch": 11.691113028472822,
      "grad_norm": 3.3951582736335695e-05,
      "learning_rate": 0.0007661834915156744,
      "loss": 0.0025,
      "step": 40650
    },
    {
      "epoch": 11.70549324129997,
      "grad_norm": 0.0022859557066112757,
      "learning_rate": 0.0007658958872591313,
      "loss": 0.0007,
      "step": 40700
    },
    {
      "epoch": 11.719873454127121,
      "grad_norm": 0.0009549000533297658,
      "learning_rate": 0.0007656082830025885,
      "loss": 0.0017,
      "step": 40750
    },
    {
      "epoch": 11.73425366695427,
      "grad_norm": 0.0003960754838772118,
      "learning_rate": 0.0007653206787460455,
      "loss": 0.0018,
      "step": 40800
    },
    {
      "epoch": 11.748633879781421,
      "grad_norm": 7.058516348479316e-05,
      "learning_rate": 0.0007650330744895024,
      "loss": 0.0037,
      "step": 40850
    },
    {
      "epoch": 11.76301409260857,
      "grad_norm": 0.0001210483314935118,
      "learning_rate": 0.0007647454702329595,
      "loss": 0.0031,
      "step": 40900
    },
    {
      "epoch": 11.77739430543572,
      "grad_norm": 0.00040668825386092067,
      "learning_rate": 0.0007644578659764164,
      "loss": 0.0011,
      "step": 40950
    },
    {
      "epoch": 11.791774518262871,
      "grad_norm": 0.00021672934235539287,
      "learning_rate": 0.0007641702617198734,
      "loss": 0.0012,
      "step": 41000
    },
    {
      "epoch": 11.80615473109002,
      "grad_norm": 0.0008622069144621491,
      "learning_rate": 0.0007638826574633305,
      "loss": 0.0033,
      "step": 41050
    },
    {
      "epoch": 11.82053494391717,
      "grad_norm": 0.04492278769612312,
      "learning_rate": 0.0007635950532067874,
      "loss": 0.0058,
      "step": 41100
    },
    {
      "epoch": 11.83491515674432,
      "grad_norm": 0.011075545102357864,
      "learning_rate": 0.0007633074489502446,
      "loss": 0.0018,
      "step": 41150
    },
    {
      "epoch": 11.84929536957147,
      "grad_norm": 0.05370718613266945,
      "learning_rate": 0.0007630198446937015,
      "loss": 0.004,
      "step": 41200
    },
    {
      "epoch": 11.863675582398619,
      "grad_norm": 0.059873081743717194,
      "learning_rate": 0.0007627322404371585,
      "loss": 0.0063,
      "step": 41250
    },
    {
      "epoch": 11.87805579522577,
      "grad_norm": 0.11958729475736618,
      "learning_rate": 0.0007624446361806154,
      "loss": 0.0019,
      "step": 41300
    },
    {
      "epoch": 11.892436008052918,
      "grad_norm": 0.0005762169021181762,
      "learning_rate": 0.0007621570319240725,
      "loss": 0.0016,
      "step": 41350
    },
    {
      "epoch": 11.906816220880069,
      "grad_norm": 0.005230579525232315,
      "learning_rate": 0.0007618694276675295,
      "loss": 0.0043,
      "step": 41400
    },
    {
      "epoch": 11.92119643370722,
      "grad_norm": 0.03756919875741005,
      "learning_rate": 0.0007615818234109864,
      "loss": 0.0018,
      "step": 41450
    },
    {
      "epoch": 11.935576646534368,
      "grad_norm": 0.0007381989271380007,
      "learning_rate": 0.0007612942191544435,
      "loss": 0.0035,
      "step": 41500
    },
    {
      "epoch": 11.949956859361519,
      "grad_norm": 0.05441545695066452,
      "learning_rate": 0.0007610066148979004,
      "loss": 0.0032,
      "step": 41550
    },
    {
      "epoch": 11.964337072188668,
      "grad_norm": 0.03704094514250755,
      "learning_rate": 0.0007607190106413576,
      "loss": 0.0048,
      "step": 41600
    },
    {
      "epoch": 11.978717285015819,
      "grad_norm": 0.0006984772626310587,
      "learning_rate": 0.0007604314063848146,
      "loss": 0.0029,
      "step": 41650
    },
    {
      "epoch": 11.993097497842967,
      "grad_norm": 0.00027163862250745296,
      "learning_rate": 0.0007601438021282715,
      "loss": 0.002,
      "step": 41700
    },
    {
      "epoch": 12.0,
      "eval_loss": 0.010513859800994396,
      "eval_runtime": 17.6205,
      "eval_samples_per_second": 2708.15,
      "eval_steps_per_second": 42.337,
      "step": 41724
    },
    {
      "epoch": 12.007477710670118,
      "grad_norm": 0.0015171482227742672,
      "learning_rate": 0.0007598561978717286,
      "loss": 0.0014,
      "step": 41750
    },
    {
      "epoch": 12.021857923497267,
      "grad_norm": 0.38988757133483887,
      "learning_rate": 0.0007595685936151855,
      "loss": 0.0012,
      "step": 41800
    },
    {
      "epoch": 12.036238136324418,
      "grad_norm": 0.004802215378731489,
      "learning_rate": 0.0007592809893586425,
      "loss": 0.003,
      "step": 41850
    },
    {
      "epoch": 12.050618349151568,
      "grad_norm": 0.2478807419538498,
      "learning_rate": 0.0007589933851020994,
      "loss": 0.0027,
      "step": 41900
    },
    {
      "epoch": 12.064998561978717,
      "grad_norm": 0.28472384810447693,
      "learning_rate": 0.0007587057808455565,
      "loss": 0.0016,
      "step": 41950
    },
    {
      "epoch": 12.079378774805868,
      "grad_norm": 0.02074051834642887,
      "learning_rate": 0.0007584181765890136,
      "loss": 0.0019,
      "step": 42000
    },
    {
      "epoch": 12.093758987633016,
      "grad_norm": 0.00035310949897393584,
      "learning_rate": 0.0007581305723324706,
      "loss": 0.0033,
      "step": 42050
    },
    {
      "epoch": 12.108139200460167,
      "grad_norm": 0.002149483421817422,
      "learning_rate": 0.0007578429680759276,
      "loss": 0.0047,
      "step": 42100
    },
    {
      "epoch": 12.122519413287316,
      "grad_norm": 0.05140945315361023,
      "learning_rate": 0.0007575553638193845,
      "loss": 0.0037,
      "step": 42150
    },
    {
      "epoch": 12.136899626114467,
      "grad_norm": 0.0003058384754694998,
      "learning_rate": 0.0007572677595628416,
      "loss": 0.0038,
      "step": 42200
    },
    {
      "epoch": 12.151279838941615,
      "grad_norm": 0.0005071377381682396,
      "learning_rate": 0.0007569801553062986,
      "loss": 0.0031,
      "step": 42250
    },
    {
      "epoch": 12.165660051768766,
      "grad_norm": 0.0020559942349791527,
      "learning_rate": 0.0007566925510497555,
      "loss": 0.0034,
      "step": 42300
    },
    {
      "epoch": 12.180040264595917,
      "grad_norm": 0.015395848080515862,
      "learning_rate": 0.0007564049467932126,
      "loss": 0.0026,
      "step": 42350
    },
    {
      "epoch": 12.194420477423066,
      "grad_norm": 0.00011128338519483805,
      "learning_rate": 0.0007561173425366695,
      "loss": 0.0026,
      "step": 42400
    },
    {
      "epoch": 12.208800690250216,
      "grad_norm": 0.0014804091770201921,
      "learning_rate": 0.0007558297382801266,
      "loss": 0.0013,
      "step": 42450
    },
    {
      "epoch": 12.223180903077365,
      "grad_norm": 0.0004004664078820497,
      "learning_rate": 0.0007555421340235836,
      "loss": 0.0026,
      "step": 42500
    },
    {
      "epoch": 12.237561115904516,
      "grad_norm": 0.12286345660686493,
      "learning_rate": 0.0007552545297670406,
      "loss": 0.006,
      "step": 42550
    },
    {
      "epoch": 12.251941328731665,
      "grad_norm": 0.00010399183520348743,
      "learning_rate": 0.0007549669255104976,
      "loss": 0.003,
      "step": 42600
    },
    {
      "epoch": 12.266321541558815,
      "grad_norm": 0.0001558804651722312,
      "learning_rate": 0.0007546793212539546,
      "loss": 0.0011,
      "step": 42650
    },
    {
      "epoch": 12.280701754385966,
      "grad_norm": 0.0030071332585066557,
      "learning_rate": 0.0007543917169974116,
      "loss": 0.0038,
      "step": 42700
    },
    {
      "epoch": 12.295081967213115,
      "grad_norm": 0.00014274500426836312,
      "learning_rate": 0.0007541041127408685,
      "loss": 0.0022,
      "step": 42750
    },
    {
      "epoch": 12.309462180040265,
      "grad_norm": 0.00016369613877031952,
      "learning_rate": 0.0007538165084843256,
      "loss": 0.0031,
      "step": 42800
    },
    {
      "epoch": 12.323842392867414,
      "grad_norm": 0.0002988576015923172,
      "learning_rate": 0.0007535289042277827,
      "loss": 0.0017,
      "step": 42850
    },
    {
      "epoch": 12.338222605694565,
      "grad_norm": 0.0018983890768140554,
      "learning_rate": 0.0007532412999712396,
      "loss": 0.0035,
      "step": 42900
    },
    {
      "epoch": 12.352602818521714,
      "grad_norm": 0.0932469367980957,
      "learning_rate": 0.0007529536957146967,
      "loss": 0.001,
      "step": 42950
    },
    {
      "epoch": 12.366983031348864,
      "grad_norm": 0.0008002789109013975,
      "learning_rate": 0.0007526660914581536,
      "loss": 0.0032,
      "step": 43000
    },
    {
      "epoch": 12.381363244176013,
      "grad_norm": 0.00011168653145432472,
      "learning_rate": 0.0007523784872016106,
      "loss": 0.0025,
      "step": 43050
    },
    {
      "epoch": 12.395743457003164,
      "grad_norm": 0.0013070804998278618,
      "learning_rate": 0.0007520908829450676,
      "loss": 0.0016,
      "step": 43100
    },
    {
      "epoch": 12.410123669830314,
      "grad_norm": 0.00010800906602526084,
      "learning_rate": 0.0007518032786885246,
      "loss": 0.0016,
      "step": 43150
    },
    {
      "epoch": 12.424503882657463,
      "grad_norm": 0.0008104369626380503,
      "learning_rate": 0.0007515156744319816,
      "loss": 0.0019,
      "step": 43200
    },
    {
      "epoch": 12.438884095484614,
      "grad_norm": 0.03362879902124405,
      "learning_rate": 0.0007512280701754386,
      "loss": 0.0062,
      "step": 43250
    },
    {
      "epoch": 12.453264308311763,
      "grad_norm": 0.00029774781432934105,
      "learning_rate": 0.0007509404659188957,
      "loss": 0.0034,
      "step": 43300
    },
    {
      "epoch": 12.467644521138913,
      "grad_norm": 0.00034858210710808635,
      "learning_rate": 0.0007506528616623526,
      "loss": 0.0041,
      "step": 43350
    },
    {
      "epoch": 12.482024733966062,
      "grad_norm": 5.835791671415791e-05,
      "learning_rate": 0.0007503652574058097,
      "loss": 0.002,
      "step": 43400
    },
    {
      "epoch": 12.496404946793213,
      "grad_norm": 0.0004783621116075665,
      "learning_rate": 0.0007500776531492666,
      "loss": 0.0035,
      "step": 43450
    },
    {
      "epoch": 12.510785159620362,
      "grad_norm": 0.008077808655798435,
      "learning_rate": 0.0007497900488927236,
      "loss": 0.0018,
      "step": 43500
    },
    {
      "epoch": 12.525165372447512,
      "grad_norm": 0.0011750480625778437,
      "learning_rate": 0.0007495024446361807,
      "loss": 0.0053,
      "step": 43550
    },
    {
      "epoch": 12.539545585274663,
      "grad_norm": 0.020098140463232994,
      "learning_rate": 0.0007492148403796376,
      "loss": 0.0032,
      "step": 43600
    },
    {
      "epoch": 12.553925798101812,
      "grad_norm": 0.0004972106544300914,
      "learning_rate": 0.0007489272361230946,
      "loss": 0.0019,
      "step": 43650
    },
    {
      "epoch": 12.568306010928962,
      "grad_norm": 0.00021896041289437562,
      "learning_rate": 0.0007486396318665516,
      "loss": 0.003,
      "step": 43700
    },
    {
      "epoch": 12.582686223756111,
      "grad_norm": 0.0018646224634721875,
      "learning_rate": 0.0007483520276100087,
      "loss": 0.0012,
      "step": 43750
    },
    {
      "epoch": 12.597066436583262,
      "grad_norm": 8.119593985611573e-05,
      "learning_rate": 0.0007480644233534657,
      "loss": 0.0011,
      "step": 43800
    },
    {
      "epoch": 12.61144664941041,
      "grad_norm": 0.00024328006838914007,
      "learning_rate": 0.0007477768190969227,
      "loss": 0.0049,
      "step": 43850
    },
    {
      "epoch": 12.625826862237561,
      "grad_norm": 0.12450522184371948,
      "learning_rate": 0.0007474892148403797,
      "loss": 0.0033,
      "step": 43900
    },
    {
      "epoch": 12.64020707506471,
      "grad_norm": 2.2544800231116824e-05,
      "learning_rate": 0.0007472016105838366,
      "loss": 0.0018,
      "step": 43950
    },
    {
      "epoch": 12.65458728789186,
      "grad_norm": 0.0021188484970480204,
      "learning_rate": 0.0007469140063272937,
      "loss": 0.0048,
      "step": 44000
    },
    {
      "epoch": 12.668967500719011,
      "grad_norm": 0.0021465695463120937,
      "learning_rate": 0.0007466264020707506,
      "loss": 0.0008,
      "step": 44050
    },
    {
      "epoch": 12.68334771354616,
      "grad_norm": 0.00044244169839657843,
      "learning_rate": 0.0007463387978142076,
      "loss": 0.0012,
      "step": 44100
    },
    {
      "epoch": 12.697727926373311,
      "grad_norm": 0.00011868294677697122,
      "learning_rate": 0.0007460511935576648,
      "loss": 0.0052,
      "step": 44150
    },
    {
      "epoch": 12.71210813920046,
      "grad_norm": 0.051351889967918396,
      "learning_rate": 0.0007457635893011217,
      "loss": 0.0038,
      "step": 44200
    },
    {
      "epoch": 12.72648835202761,
      "grad_norm": 0.005884475540369749,
      "learning_rate": 0.0007454759850445787,
      "loss": 0.0032,
      "step": 44250
    },
    {
      "epoch": 12.74086856485476,
      "grad_norm": 0.0009459096472710371,
      "learning_rate": 0.0007451883807880357,
      "loss": 0.0033,
      "step": 44300
    },
    {
      "epoch": 12.75524877768191,
      "grad_norm": 0.01761573739349842,
      "learning_rate": 0.0007449007765314927,
      "loss": 0.004,
      "step": 44350
    },
    {
      "epoch": 12.769628990509059,
      "grad_norm": 0.0005902909324504435,
      "learning_rate": 0.0007446131722749497,
      "loss": 0.0034,
      "step": 44400
    },
    {
      "epoch": 12.78400920333621,
      "grad_norm": 0.07450678199529648,
      "learning_rate": 0.0007443255680184067,
      "loss": 0.0036,
      "step": 44450
    },
    {
      "epoch": 12.79838941616336,
      "grad_norm": 0.22392956912517548,
      "learning_rate": 0.0007440379637618637,
      "loss": 0.0021,
      "step": 44500
    },
    {
      "epoch": 12.812769628990509,
      "grad_norm": 7.299608114408329e-05,
      "learning_rate": 0.0007437503595053206,
      "loss": 0.0047,
      "step": 44550
    },
    {
      "epoch": 12.82714984181766,
      "grad_norm": 0.06617068499326706,
      "learning_rate": 0.0007434627552487778,
      "loss": 0.0056,
      "step": 44600
    },
    {
      "epoch": 12.841530054644808,
      "grad_norm": 0.03508348762989044,
      "learning_rate": 0.0007431751509922347,
      "loss": 0.0028,
      "step": 44650
    },
    {
      "epoch": 12.855910267471959,
      "grad_norm": 0.06423962861299515,
      "learning_rate": 0.0007428875467356917,
      "loss": 0.004,
      "step": 44700
    },
    {
      "epoch": 12.870290480299108,
      "grad_norm": 0.000130016531329602,
      "learning_rate": 0.0007425999424791488,
      "loss": 0.0034,
      "step": 44750
    },
    {
      "epoch": 12.884670693126258,
      "grad_norm": 0.0003006431506946683,
      "learning_rate": 0.0007423123382226057,
      "loss": 0.0008,
      "step": 44800
    },
    {
      "epoch": 12.899050905953407,
      "grad_norm": 0.0004365714266896248,
      "learning_rate": 0.0007420247339660627,
      "loss": 0.0012,
      "step": 44850
    },
    {
      "epoch": 12.913431118780558,
      "grad_norm": 0.0001552171161165461,
      "learning_rate": 0.0007417371297095197,
      "loss": 0.0037,
      "step": 44900
    },
    {
      "epoch": 12.927811331607709,
      "grad_norm": 0.0001280260767089203,
      "learning_rate": 0.0007414495254529767,
      "loss": 0.0007,
      "step": 44950
    },
    {
      "epoch": 12.942191544434857,
      "grad_norm": 0.016809049993753433,
      "learning_rate": 0.0007411619211964338,
      "loss": 0.0008,
      "step": 45000
    },
    {
      "epoch": 12.956571757262008,
      "grad_norm": 0.00020030837913509458,
      "learning_rate": 0.0007408743169398908,
      "loss": 0.0029,
      "step": 45050
    },
    {
      "epoch": 12.970951970089157,
      "grad_norm": 8.451858593616635e-05,
      "learning_rate": 0.0007405867126833478,
      "loss": 0.0058,
      "step": 45100
    },
    {
      "epoch": 12.985332182916308,
      "grad_norm": 0.00016526221588719636,
      "learning_rate": 0.0007402991084268047,
      "loss": 0.0011,
      "step": 45150
    },
    {
      "epoch": 12.999712395743456,
      "grad_norm": 6.481164018623531e-05,
      "learning_rate": 0.0007400115041702618,
      "loss": 0.0025,
      "step": 45200
    },
    {
      "epoch": 13.0,
      "eval_loss": 0.010095086880028248,
      "eval_runtime": 16.8006,
      "eval_samples_per_second": 2840.307,
      "eval_steps_per_second": 44.403,
      "step": 45201
    },
    {
      "epoch": 13.014092608570607,
      "grad_norm": 8.806295227259398e-05,
      "learning_rate": 0.0007397238999137187,
      "loss": 0.0041,
      "step": 45250
    },
    {
      "epoch": 13.028472821397756,
      "grad_norm": 4.095115582458675e-05,
      "learning_rate": 0.0007394362956571757,
      "loss": 0.003,
      "step": 45300
    },
    {
      "epoch": 13.042853034224906,
      "grad_norm": 0.0005480592953972518,
      "learning_rate": 0.0007391486914006328,
      "loss": 0.0022,
      "step": 45350
    },
    {
      "epoch": 13.057233247052057,
      "grad_norm": 0.00019647969747893512,
      "learning_rate": 0.0007388610871440897,
      "loss": 0.0029,
      "step": 45400
    },
    {
      "epoch": 13.071613459879206,
      "grad_norm": 0.015428195707499981,
      "learning_rate": 0.0007385734828875468,
      "loss": 0.0056,
      "step": 45450
    },
    {
      "epoch": 13.085993672706357,
      "grad_norm": 0.0005122525617480278,
      "learning_rate": 0.0007382858786310038,
      "loss": 0.0018,
      "step": 45500
    },
    {
      "epoch": 13.100373885533505,
      "grad_norm": 6.895078695379198e-05,
      "learning_rate": 0.0007379982743744608,
      "loss": 0.0037,
      "step": 45550
    },
    {
      "epoch": 13.114754098360656,
      "grad_norm": 0.00013173757179174572,
      "learning_rate": 0.0007377106701179178,
      "loss": 0.0017,
      "step": 45600
    },
    {
      "epoch": 13.129134311187805,
      "grad_norm": 0.0001380941685056314,
      "learning_rate": 0.0007374230658613748,
      "loss": 0.002,
      "step": 45650
    },
    {
      "epoch": 13.143514524014956,
      "grad_norm": 0.00010604510316625237,
      "learning_rate": 0.0007371354616048318,
      "loss": 0.006,
      "step": 45700
    },
    {
      "epoch": 13.157894736842104,
      "grad_norm": 0.020905951038002968,
      "learning_rate": 0.0007368478573482887,
      "loss": 0.0018,
      "step": 45750
    },
    {
      "epoch": 13.172274949669255,
      "grad_norm": 0.0031814754474908113,
      "learning_rate": 0.0007365602530917458,
      "loss": 0.0012,
      "step": 45800
    },
    {
      "epoch": 13.186655162496406,
      "grad_norm": 0.0015128408558666706,
      "learning_rate": 0.0007362726488352027,
      "loss": 0.0021,
      "step": 45850
    },
    {
      "epoch": 13.201035375323555,
      "grad_norm": 0.14162549376487732,
      "learning_rate": 0.0007359850445786598,
      "loss": 0.0023,
      "step": 45900
    },
    {
      "epoch": 13.215415588150705,
      "grad_norm": 0.00031423932523466647,
      "learning_rate": 0.0007356974403221169,
      "loss": 0.0041,
      "step": 45950
    },
    {
      "epoch": 13.229795800977854,
      "grad_norm": 0.0005319103365764022,
      "learning_rate": 0.0007354098360655738,
      "loss": 0.0026,
      "step": 46000
    },
    {
      "epoch": 13.244176013805005,
      "grad_norm": 0.005127901677042246,
      "learning_rate": 0.0007351222318090308,
      "loss": 0.005,
      "step": 46050
    },
    {
      "epoch": 13.258556226632153,
      "grad_norm": 0.00048800132935866714,
      "learning_rate": 0.0007348346275524878,
      "loss": 0.0017,
      "step": 46100
    },
    {
      "epoch": 13.272936439459304,
      "grad_norm": 0.025963498279452324,
      "learning_rate": 0.0007345470232959448,
      "loss": 0.003,
      "step": 46150
    },
    {
      "epoch": 13.287316652286453,
      "grad_norm": 0.13013139367103577,
      "learning_rate": 0.0007342594190394017,
      "loss": 0.0022,
      "step": 46200
    },
    {
      "epoch": 13.301696865113604,
      "grad_norm": 0.025094395503401756,
      "learning_rate": 0.0007339718147828588,
      "loss": 0.0042,
      "step": 46250
    },
    {
      "epoch": 13.316077077940754,
      "grad_norm": 0.0015728489961475134,
      "learning_rate": 0.0007336842105263159,
      "loss": 0.0019,
      "step": 46300
    },
    {
      "epoch": 13.330457290767903,
      "grad_norm": 0.01236893329769373,
      "learning_rate": 0.0007333966062697728,
      "loss": 0.0017,
      "step": 46350
    },
    {
      "epoch": 13.344837503595054,
      "grad_norm": 0.010603026486933231,
      "learning_rate": 0.0007331090020132299,
      "loss": 0.0018,
      "step": 46400
    },
    {
      "epoch": 13.359217716422203,
      "grad_norm": 0.029054593294858932,
      "learning_rate": 0.0007328213977566868,
      "loss": 0.0039,
      "step": 46450
    },
    {
      "epoch": 13.373597929249353,
      "grad_norm": 0.0004781407187692821,
      "learning_rate": 0.0007325337935001438,
      "loss": 0.0026,
      "step": 46500
    },
    {
      "epoch": 13.387978142076502,
      "grad_norm": 0.0013221859699115157,
      "learning_rate": 0.0007322461892436009,
      "loss": 0.0032,
      "step": 46550
    },
    {
      "epoch": 13.402358354903653,
      "grad_norm": 0.009707300923764706,
      "learning_rate": 0.0007319585849870578,
      "loss": 0.0047,
      "step": 46600
    },
    {
      "epoch": 13.416738567730803,
      "grad_norm": 7.509942952310666e-05,
      "learning_rate": 0.0007316709807305148,
      "loss": 0.0021,
      "step": 46650
    },
    {
      "epoch": 13.431118780557952,
      "grad_norm": 0.0007984969415701926,
      "learning_rate": 0.0007313833764739718,
      "loss": 0.0023,
      "step": 46700
    },
    {
      "epoch": 13.445498993385103,
      "grad_norm": 0.0004934023600071669,
      "learning_rate": 0.0007310957722174289,
      "loss": 0.0042,
      "step": 46750
    },
    {
      "epoch": 13.459879206212252,
      "grad_norm": 0.001331995939835906,
      "learning_rate": 0.0007308081679608858,
      "loss": 0.0018,
      "step": 46800
    },
    {
      "epoch": 13.474259419039402,
      "grad_norm": 0.004955501761287451,
      "learning_rate": 0.0007305205637043429,
      "loss": 0.0018,
      "step": 46850
    },
    {
      "epoch": 13.488639631866551,
      "grad_norm": 0.00011337378964526579,
      "learning_rate": 0.0007302329594477999,
      "loss": 0.0026,
      "step": 46900
    },
    {
      "epoch": 13.503019844693702,
      "grad_norm": 0.0007548898574896157,
      "learning_rate": 0.0007299453551912568,
      "loss": 0.0037,
      "step": 46950
    },
    {
      "epoch": 13.51740005752085,
      "grad_norm": 4.550748781184666e-05,
      "learning_rate": 0.0007296577509347139,
      "loss": 0.0046,
      "step": 47000
    },
    {
      "epoch": 13.531780270348001,
      "grad_norm": 0.017784442752599716,
      "learning_rate": 0.0007293701466781708,
      "loss": 0.0008,
      "step": 47050
    },
    {
      "epoch": 13.54616048317515,
      "grad_norm": 3.469633884378709e-05,
      "learning_rate": 0.0007290825424216278,
      "loss": 0.0046,
      "step": 47100
    },
    {
      "epoch": 13.5605406960023,
      "grad_norm": 6.418021803256124e-05,
      "learning_rate": 0.000728794938165085,
      "loss": 0.0013,
      "step": 47150
    },
    {
      "epoch": 13.574920908829451,
      "grad_norm": 0.0009068872896023095,
      "learning_rate": 0.0007285073339085419,
      "loss": 0.0007,
      "step": 47200
    },
    {
      "epoch": 13.5893011216566,
      "grad_norm": 0.0010081370128318667,
      "learning_rate": 0.0007282197296519989,
      "loss": 0.0028,
      "step": 47250
    },
    {
      "epoch": 13.60368133448375,
      "grad_norm": 4.3360181734897196e-05,
      "learning_rate": 0.0007279321253954559,
      "loss": 0.0012,
      "step": 47300
    },
    {
      "epoch": 13.6180615473109,
      "grad_norm": 0.0009730939054861665,
      "learning_rate": 0.0007276445211389129,
      "loss": 0.0048,
      "step": 47350
    },
    {
      "epoch": 13.63244176013805,
      "grad_norm": 0.006750252563506365,
      "learning_rate": 0.0007273569168823698,
      "loss": 0.0036,
      "step": 47400
    },
    {
      "epoch": 13.6468219729652,
      "grad_norm": 0.00010978661885019392,
      "learning_rate": 0.0007270693126258269,
      "loss": 0.0023,
      "step": 47450
    },
    {
      "epoch": 13.66120218579235,
      "grad_norm": 0.010200731456279755,
      "learning_rate": 0.0007267817083692839,
      "loss": 0.0019,
      "step": 47500
    },
    {
      "epoch": 13.6755823986195,
      "grad_norm": 0.00027484894962981343,
      "learning_rate": 0.0007264941041127408,
      "loss": 0.0025,
      "step": 47550
    },
    {
      "epoch": 13.68996261144665,
      "grad_norm": 0.0001827478699851781,
      "learning_rate": 0.000726206499856198,
      "loss": 0.0039,
      "step": 47600
    },
    {
      "epoch": 13.7043428242738,
      "grad_norm": 0.00035797085729427636,
      "learning_rate": 0.0007259188955996549,
      "loss": 0.0007,
      "step": 47650
    },
    {
      "epoch": 13.718723037100949,
      "grad_norm": 0.07673130184412003,
      "learning_rate": 0.0007256312913431119,
      "loss": 0.0011,
      "step": 47700
    },
    {
      "epoch": 13.7331032499281,
      "grad_norm": 0.035928722470998764,
      "learning_rate": 0.000725343687086569,
      "loss": 0.004,
      "step": 47750
    },
    {
      "epoch": 13.747483462755248,
      "grad_norm": 0.30563101172447205,
      "learning_rate": 0.0007250560828300259,
      "loss": 0.0037,
      "step": 47800
    },
    {
      "epoch": 13.761863675582399,
      "grad_norm": 9.893417882267386e-05,
      "learning_rate": 0.0007247684785734829,
      "loss": 0.0022,
      "step": 47850
    },
    {
      "epoch": 13.776243888409548,
      "grad_norm": 0.00016360290464945138,
      "learning_rate": 0.0007244808743169399,
      "loss": 0.0013,
      "step": 47900
    },
    {
      "epoch": 13.790624101236698,
      "grad_norm": 0.022912491112947464,
      "learning_rate": 0.0007241932700603969,
      "loss": 0.0017,
      "step": 47950
    },
    {
      "epoch": 13.805004314063847,
      "grad_norm": 0.0010062626097351313,
      "learning_rate": 0.0007239056658038538,
      "loss": 0.0009,
      "step": 48000
    },
    {
      "epoch": 13.819384526890998,
      "grad_norm": 0.0006592860445380211,
      "learning_rate": 0.000723618061547311,
      "loss": 0.0028,
      "step": 48050
    },
    {
      "epoch": 13.833764739718148,
      "grad_norm": 0.0007517279009334743,
      "learning_rate": 0.000723330457290768,
      "loss": 0.0057,
      "step": 48100
    },
    {
      "epoch": 13.848144952545297,
      "grad_norm": 5.858821532456204e-05,
      "learning_rate": 0.0007230428530342249,
      "loss": 0.0037,
      "step": 48150
    },
    {
      "epoch": 13.862525165372448,
      "grad_norm": 0.0002688676177058369,
      "learning_rate": 0.000722755248777682,
      "loss": 0.0013,
      "step": 48200
    },
    {
      "epoch": 13.876905378199597,
      "grad_norm": 8.441110549028963e-05,
      "learning_rate": 0.0007224676445211389,
      "loss": 0.0025,
      "step": 48250
    },
    {
      "epoch": 13.891285591026747,
      "grad_norm": 0.04874742031097412,
      "learning_rate": 0.0007221800402645959,
      "loss": 0.0023,
      "step": 48300
    },
    {
      "epoch": 13.905665803853896,
      "grad_norm": 0.00019009760580956936,
      "learning_rate": 0.000721892436008053,
      "loss": 0.0027,
      "step": 48350
    },
    {
      "epoch": 13.920046016681047,
      "grad_norm": 0.08402974158525467,
      "learning_rate": 0.0007216048317515099,
      "loss": 0.0024,
      "step": 48400
    },
    {
      "epoch": 13.934426229508198,
      "grad_norm": 0.013028716668486595,
      "learning_rate": 0.000721317227494967,
      "loss": 0.0047,
      "step": 48450
    },
    {
      "epoch": 13.948806442335346,
      "grad_norm": 0.000178106376552023,
      "learning_rate": 0.000721029623238424,
      "loss": 0.0038,
      "step": 48500
    },
    {
      "epoch": 13.963186655162497,
      "grad_norm": 0.01147377584129572,
      "learning_rate": 0.000720742018981881,
      "loss": 0.0009,
      "step": 48550
    },
    {
      "epoch": 13.977566867989646,
      "grad_norm": 0.04230134189128876,
      "learning_rate": 0.0007204544147253379,
      "loss": 0.0027,
      "step": 48600
    },
    {
      "epoch": 13.991947080816797,
      "grad_norm": 6.347298040054739e-05,
      "learning_rate": 0.000720166810468795,
      "loss": 0.0022,
      "step": 48650
    },
    {
      "epoch": 14.0,
      "eval_loss": 0.010121594183146954,
      "eval_runtime": 16.9632,
      "eval_samples_per_second": 2813.092,
      "eval_steps_per_second": 43.978,
      "step": 48678
    },
    {
      "epoch": 14.006327293643945,
      "grad_norm": 7.095341425156221e-05,
      "learning_rate": 0.000719879206212252,
      "loss": 0.0041,
      "step": 48700
    },
    {
      "epoch": 14.020707506471096,
      "grad_norm": 0.01464686170220375,
      "learning_rate": 0.0007195916019557089,
      "loss": 0.0037,
      "step": 48750
    },
    {
      "epoch": 14.035087719298245,
      "grad_norm": 8.690952381584793e-05,
      "learning_rate": 0.000719303997699166,
      "loss": 0.003,
      "step": 48800
    },
    {
      "epoch": 14.049467932125395,
      "grad_norm": 0.00019184421398676932,
      "learning_rate": 0.0007190163934426229,
      "loss": 0.0017,
      "step": 48850
    },
    {
      "epoch": 14.063848144952546,
      "grad_norm": 0.00088161148596555,
      "learning_rate": 0.00071872878918608,
      "loss": 0.0026,
      "step": 48900
    },
    {
      "epoch": 14.078228357779695,
      "grad_norm": 0.030776912346482277,
      "learning_rate": 0.000718441184929537,
      "loss": 0.0041,
      "step": 48950
    },
    {
      "epoch": 14.092608570606846,
      "grad_norm": 8.735980372875929e-05,
      "learning_rate": 0.000718153580672994,
      "loss": 0.0023,
      "step": 49000
    },
    {
      "epoch": 14.106988783433994,
      "grad_norm": 8.460096432827413e-05,
      "learning_rate": 0.000717865976416451,
      "loss": 0.0035,
      "step": 49050
    },
    {
      "epoch": 14.121368996261145,
      "grad_norm": 0.0001538334327051416,
      "learning_rate": 0.000717578372159908,
      "loss": 0.0027,
      "step": 49100
    },
    {
      "epoch": 14.135749209088294,
      "grad_norm": 0.0018824032740667462,
      "learning_rate": 0.000717290767903365,
      "loss": 0.0019,
      "step": 49150
    },
    {
      "epoch": 14.150129421915445,
      "grad_norm": 0.0004927221452817321,
      "learning_rate": 0.0007170031636468219,
      "loss": 0.0057,
      "step": 49200
    },
    {
      "epoch": 14.164509634742593,
      "grad_norm": 0.00010375052806921303,
      "learning_rate": 0.000716715559390279,
      "loss": 0.003,
      "step": 49250
    },
    {
      "epoch": 14.178889847569744,
      "grad_norm": 0.00011644732876447961,
      "learning_rate": 0.000716427955133736,
      "loss": 0.0066,
      "step": 49300
    },
    {
      "epoch": 14.193270060396895,
      "grad_norm": 0.00011299726611468941,
      "learning_rate": 0.000716140350877193,
      "loss": 0.002,
      "step": 49350
    },
    {
      "epoch": 14.207650273224044,
      "grad_norm": 0.00023894959304016083,
      "learning_rate": 0.0007158527466206501,
      "loss": 0.0035,
      "step": 49400
    },
    {
      "epoch": 14.222030486051194,
      "grad_norm": 0.0282049048691988,
      "learning_rate": 0.000715565142364107,
      "loss": 0.0023,
      "step": 49450
    },
    {
      "epoch": 14.236410698878343,
      "grad_norm": 0.000767593621276319,
      "learning_rate": 0.000715277538107564,
      "loss": 0.0005,
      "step": 49500
    },
    {
      "epoch": 14.250790911705494,
      "grad_norm": 7.811823161318898e-05,
      "learning_rate": 0.000714989933851021,
      "loss": 0.0049,
      "step": 49550
    },
    {
      "epoch": 14.265171124532642,
      "grad_norm": 0.09202826023101807,
      "learning_rate": 0.000714702329594478,
      "loss": 0.0052,
      "step": 49600
    },
    {
      "epoch": 14.279551337359793,
      "grad_norm": 0.0003822711587417871,
      "learning_rate": 0.000714414725337935,
      "loss": 0.0032,
      "step": 49650
    },
    {
      "epoch": 14.293931550186942,
      "grad_norm": 0.010807639919221401,
      "learning_rate": 0.000714127121081392,
      "loss": 0.0013,
      "step": 49700
    },
    {
      "epoch": 14.308311763014093,
      "grad_norm": 0.08595257997512817,
      "learning_rate": 0.000713839516824849,
      "loss": 0.0031,
      "step": 49750
    },
    {
      "epoch": 14.322691975841243,
      "grad_norm": 9.682318341219798e-05,
      "learning_rate": 0.000713551912568306,
      "loss": 0.0029,
      "step": 49800
    },
    {
      "epoch": 14.337072188668392,
      "grad_norm": 0.0008089170441962779,
      "learning_rate": 0.0007132643083117631,
      "loss": 0.0008,
      "step": 49850
    },
    {
      "epoch": 14.351452401495543,
      "grad_norm": 0.06613045185804367,
      "learning_rate": 0.0007129767040552201,
      "loss": 0.0009,
      "step": 49900
    },
    {
      "epoch": 14.365832614322692,
      "grad_norm": 0.0001180041508632712,
      "learning_rate": 0.000712689099798677,
      "loss": 0.0028,
      "step": 49950
    },
    {
      "epoch": 14.380212827149842,
      "grad_norm": 0.034158945083618164,
      "learning_rate": 0.0007124014955421341,
      "loss": 0.002,
      "step": 50000
    },
    {
      "epoch": 14.394593039976991,
      "grad_norm": 0.00010631800250848755,
      "learning_rate": 0.000712113891285591,
      "loss": 0.0009,
      "step": 50050
    },
    {
      "epoch": 14.408973252804142,
      "grad_norm": 0.0002485252625774592,
      "learning_rate": 0.000711826287029048,
      "loss": 0.0016,
      "step": 50100
    },
    {
      "epoch": 14.42335346563129,
      "grad_norm": 0.012842011637985706,
      "learning_rate": 0.000711538682772505,
      "loss": 0.0024,
      "step": 50150
    },
    {
      "epoch": 14.437733678458441,
      "grad_norm": 0.0027167610824108124,
      "learning_rate": 0.000711251078515962,
      "loss": 0.0007,
      "step": 50200
    },
    {
      "epoch": 14.452113891285592,
      "grad_norm": 0.008856314234435558,
      "learning_rate": 0.0007109634742594191,
      "loss": 0.0023,
      "step": 50250
    },
    {
      "epoch": 14.46649410411274,
      "grad_norm": 0.00023430302098859102,
      "learning_rate": 0.0007106758700028761,
      "loss": 0.0037,
      "step": 50300
    },
    {
      "epoch": 14.480874316939891,
      "grad_norm": 0.00016705237794667482,
      "learning_rate": 0.0007103882657463331,
      "loss": 0.0029,
      "step": 50350
    },
    {
      "epoch": 14.49525452976704,
      "grad_norm": 8.623234316473827e-05,
      "learning_rate": 0.00071010066148979,
      "loss": 0.001,
      "step": 50400
    },
    {
      "epoch": 14.50963474259419,
      "grad_norm": 8.347137190867215e-05,
      "learning_rate": 0.0007098130572332471,
      "loss": 0.0039,
      "step": 50450
    },
    {
      "epoch": 14.52401495542134,
      "grad_norm": 0.010873892344534397,
      "learning_rate": 0.0007095254529767041,
      "loss": 0.0048,
      "step": 50500
    },
    {
      "epoch": 14.53839516824849,
      "grad_norm": 0.016145706176757812,
      "learning_rate": 0.000709237848720161,
      "loss": 0.0015,
      "step": 50550
    },
    {
      "epoch": 14.55277538107564,
      "grad_norm": 0.0023462800309062004,
      "learning_rate": 0.0007089502444636182,
      "loss": 0.0019,
      "step": 50600
    },
    {
      "epoch": 14.56715559390279,
      "grad_norm": 0.0007200088584795594,
      "learning_rate": 0.0007086626402070751,
      "loss": 0.0028,
      "step": 50650
    },
    {
      "epoch": 14.58153580672994,
      "grad_norm": 0.010514777153730392,
      "learning_rate": 0.0007083750359505321,
      "loss": 0.0031,
      "step": 50700
    },
    {
      "epoch": 14.59591601955709,
      "grad_norm": 6.931283132871613e-05,
      "learning_rate": 0.0007080874316939891,
      "loss": 0.0038,
      "step": 50750
    },
    {
      "epoch": 14.61029623238424,
      "grad_norm": 0.01586047373712063,
      "learning_rate": 0.0007077998274374461,
      "loss": 0.0049,
      "step": 50800
    },
    {
      "epoch": 14.624676445211389,
      "grad_norm": 0.012420915067195892,
      "learning_rate": 0.0007075122231809031,
      "loss": 0.0026,
      "step": 50850
    },
    {
      "epoch": 14.63905665803854,
      "grad_norm": 9.144891373580322e-05,
      "learning_rate": 0.0007072246189243601,
      "loss": 0.0047,
      "step": 50900
    },
    {
      "epoch": 14.653436870865688,
      "grad_norm": 0.00010318616114091128,
      "learning_rate": 0.0007069370146678171,
      "loss": 0.0028,
      "step": 50950
    },
    {
      "epoch": 14.667817083692839,
      "grad_norm": 6.857032713014632e-05,
      "learning_rate": 0.000706649410411274,
      "loss": 0.0037,
      "step": 51000
    },
    {
      "epoch": 14.682197296519988,
      "grad_norm": 8.861696551321074e-05,
      "learning_rate": 0.0007063618061547312,
      "loss": 0.0015,
      "step": 51050
    },
    {
      "epoch": 14.696577509347138,
      "grad_norm": 0.034788601100444794,
      "learning_rate": 0.0007060742018981882,
      "loss": 0.0006,
      "step": 51100
    },
    {
      "epoch": 14.710957722174289,
      "grad_norm": 0.03311377391219139,
      "learning_rate": 0.0007057865976416451,
      "loss": 0.0013,
      "step": 51150
    },
    {
      "epoch": 14.725337935001438,
      "grad_norm": 6.422165461117402e-05,
      "learning_rate": 0.0007054989933851022,
      "loss": 0.0009,
      "step": 51200
    },
    {
      "epoch": 14.739718147828588,
      "grad_norm": 0.009108253754675388,
      "learning_rate": 0.0007052113891285591,
      "loss": 0.0031,
      "step": 51250
    },
    {
      "epoch": 14.754098360655737,
      "grad_norm": 0.006763712037354708,
      "learning_rate": 0.0007049237848720161,
      "loss": 0.0006,
      "step": 51300
    },
    {
      "epoch": 14.768478573482888,
      "grad_norm": 0.00858769379556179,
      "learning_rate": 0.0007046361806154731,
      "loss": 0.0026,
      "step": 51350
    },
    {
      "epoch": 14.782858786310037,
      "grad_norm": 0.04845685884356499,
      "learning_rate": 0.0007043485763589301,
      "loss": 0.0041,
      "step": 51400
    },
    {
      "epoch": 14.797238999137187,
      "grad_norm": 0.00030857938691042364,
      "learning_rate": 0.0007040609721023871,
      "loss": 0.0036,
      "step": 51450
    },
    {
      "epoch": 14.811619211964338,
      "grad_norm": 3.960704998462461e-05,
      "learning_rate": 0.0007037733678458442,
      "loss": 0.0048,
      "step": 51500
    },
    {
      "epoch": 14.825999424791487,
      "grad_norm": 0.001194793963804841,
      "learning_rate": 0.0007034857635893012,
      "loss": 0.0024,
      "step": 51550
    },
    {
      "epoch": 14.840379637618637,
      "grad_norm": 0.17588014900684357,
      "learning_rate": 0.0007031981593327581,
      "loss": 0.0027,
      "step": 51600
    },
    {
      "epoch": 14.854759850445786,
      "grad_norm": 0.0037511999253183603,
      "learning_rate": 0.0007029105550762152,
      "loss": 0.0019,
      "step": 51650
    },
    {
      "epoch": 14.869140063272937,
      "grad_norm": 0.0004633500357158482,
      "learning_rate": 0.0007026229508196721,
      "loss": 0.0062,
      "step": 51700
    },
    {
      "epoch": 14.883520276100086,
      "grad_norm": 3.1116451282287017e-05,
      "learning_rate": 0.0007023353465631291,
      "loss": 0.002,
      "step": 51750
    },
    {
      "epoch": 14.897900488927236,
      "grad_norm": 3.6089222703594714e-05,
      "learning_rate": 0.0007020477423065862,
      "loss": 0.0017,
      "step": 51800
    },
    {
      "epoch": 14.912280701754385,
      "grad_norm": 0.011494722217321396,
      "learning_rate": 0.0007017601380500431,
      "loss": 0.0021,
      "step": 51850
    },
    {
      "epoch": 14.926660914581536,
      "grad_norm": 0.0015931109664961696,
      "learning_rate": 0.0007014725337935001,
      "loss": 0.0041,
      "step": 51900
    },
    {
      "epoch": 14.941041127408685,
      "grad_norm": 0.013979778625071049,
      "learning_rate": 0.0007011849295369572,
      "loss": 0.0035,
      "step": 51950
    },
    {
      "epoch": 14.955421340235835,
      "grad_norm": 0.005032101646065712,
      "learning_rate": 0.0007008973252804142,
      "loss": 0.0025,
      "step": 52000
    },
    {
      "epoch": 14.969801553062986,
      "grad_norm": 9.890427463687956e-05,
      "learning_rate": 0.0007006097210238712,
      "loss": 0.0014,
      "step": 52050
    },
    {
      "epoch": 14.984181765890135,
      "grad_norm": 7.47047015465796e-05,
      "learning_rate": 0.0007003221167673282,
      "loss": 0.0034,
      "step": 52100
    },
    {
      "epoch": 14.998561978717285,
      "grad_norm": 0.00023194703680928797,
      "learning_rate": 0.0007000345125107852,
      "loss": 0.0016,
      "step": 52150
    },
    {
      "epoch": 15.0,
      "eval_loss": 0.010704448446631432,
      "eval_runtime": 17.9907,
      "eval_samples_per_second": 2652.424,
      "eval_steps_per_second": 41.466,
      "step": 52155
    },
    {
      "epoch": 15.012942191544434,
      "grad_norm": 0.00024109159130603075,
      "learning_rate": 0.0006997469082542421,
      "loss": 0.0031,
      "step": 52200
    },
    {
      "epoch": 15.027322404371585,
      "grad_norm": 0.0013745776377618313,
      "learning_rate": 0.0006994593039976992,
      "loss": 0.0031,
      "step": 52250
    },
    {
      "epoch": 15.041702617198734,
      "grad_norm": 6.500232120743021e-05,
      "learning_rate": 0.0006991716997411561,
      "loss": 0.0025,
      "step": 52300
    },
    {
      "epoch": 15.056082830025884,
      "grad_norm": 0.00013632849731948227,
      "learning_rate": 0.0006988840954846131,
      "loss": 0.0025,
      "step": 52350
    },
    {
      "epoch": 15.070463042853035,
      "grad_norm": 0.0004820712492801249,
      "learning_rate": 0.0006985964912280703,
      "loss": 0.0026,
      "step": 52400
    },
    {
      "epoch": 15.084843255680184,
      "grad_norm": 9.448804485145956e-05,
      "learning_rate": 0.0006983088869715272,
      "loss": 0.0031,
      "step": 52450
    },
    {
      "epoch": 15.099223468507335,
      "grad_norm": 0.0009998504538089037,
      "learning_rate": 0.0006980212827149842,
      "loss": 0.0018,
      "step": 52500
    },
    {
      "epoch": 15.113603681334483,
      "grad_norm": 1.5164897376962472e-05,
      "learning_rate": 0.0006977336784584412,
      "loss": 0.0015,
      "step": 52550
    },
    {
      "epoch": 15.127983894161634,
      "grad_norm": 0.0006210637511685491,
      "learning_rate": 0.0006974460742018982,
      "loss": 0.0023,
      "step": 52600
    },
    {
      "epoch": 15.142364106988783,
      "grad_norm": 0.002393734408542514,
      "learning_rate": 0.0006971584699453552,
      "loss": 0.0017,
      "step": 52650
    },
    {
      "epoch": 15.156744319815934,
      "grad_norm": 0.0023532998748123646,
      "learning_rate": 0.0006968708656888122,
      "loss": 0.0026,
      "step": 52700
    },
    {
      "epoch": 15.171124532643082,
      "grad_norm": 6.63020764477551e-05,
      "learning_rate": 0.0006965832614322692,
      "loss": 0.0038,
      "step": 52750
    },
    {
      "epoch": 15.185504745470233,
      "grad_norm": 0.07769913971424103,
      "learning_rate": 0.0006962956571757262,
      "loss": 0.0018,
      "step": 52800
    },
    {
      "epoch": 15.199884958297384,
      "grad_norm": 0.00016310723731294274,
      "learning_rate": 0.0006960080529191833,
      "loss": 0.0013,
      "step": 52850
    },
    {
      "epoch": 15.214265171124532,
      "grad_norm": 2.7933694582316093e-05,
      "learning_rate": 0.0006957204486626402,
      "loss": 0.0022,
      "step": 52900
    },
    {
      "epoch": 15.228645383951683,
      "grad_norm": 0.0004068228299729526,
      "learning_rate": 0.0006954328444060972,
      "loss": 0.0017,
      "step": 52950
    },
    {
      "epoch": 15.243025596778832,
      "grad_norm": 0.00013727889745496213,
      "learning_rate": 0.0006951452401495543,
      "loss": 0.0064,
      "step": 53000
    },
    {
      "epoch": 15.257405809605983,
      "grad_norm": 4.37936614616774e-05,
      "learning_rate": 0.0006948576358930112,
      "loss": 0.0044,
      "step": 53050
    },
    {
      "epoch": 15.271786022433131,
      "grad_norm": 0.04351843520998955,
      "learning_rate": 0.0006945700316364682,
      "loss": 0.0023,
      "step": 53100
    },
    {
      "epoch": 15.286166235260282,
      "grad_norm": 0.009537818841636181,
      "learning_rate": 0.0006942824273799252,
      "loss": 0.0016,
      "step": 53150
    },
    {
      "epoch": 15.300546448087431,
      "grad_norm": 9.836138633545488e-05,
      "learning_rate": 0.0006939948231233822,
      "loss": 0.001,
      "step": 53200
    },
    {
      "epoch": 15.314926660914582,
      "grad_norm": 0.015961969271302223,
      "learning_rate": 0.0006937072188668393,
      "loss": 0.0022,
      "step": 53250
    },
    {
      "epoch": 15.329306873741732,
      "grad_norm": 0.0001221627026097849,
      "learning_rate": 0.0006934196146102963,
      "loss": 0.0027,
      "step": 53300
    },
    {
      "epoch": 15.343687086568881,
      "grad_norm": 0.00017775414744392037,
      "learning_rate": 0.0006931320103537533,
      "loss": 0.0012,
      "step": 53350
    },
    {
      "epoch": 15.358067299396032,
      "grad_norm": 6.23072191956453e-05,
      "learning_rate": 0.0006928444060972102,
      "loss": 0.0025,
      "step": 53400
    },
    {
      "epoch": 15.37244751222318,
      "grad_norm": 0.002537327818572521,
      "learning_rate": 0.0006925568018406673,
      "loss": 0.0008,
      "step": 53450
    },
    {
      "epoch": 15.386827725050331,
      "grad_norm": 0.00035442167427390814,
      "learning_rate": 0.0006922691975841242,
      "loss": 0.0011,
      "step": 53500
    },
    {
      "epoch": 15.40120793787748,
      "grad_norm": 0.00018858941621147096,
      "learning_rate": 0.0006919815933275812,
      "loss": 0.0055,
      "step": 53550
    },
    {
      "epoch": 15.41558815070463,
      "grad_norm": 0.0009178047184832394,
      "learning_rate": 0.0006916939890710383,
      "loss": 0.0028,
      "step": 53600
    },
    {
      "epoch": 15.42996836353178,
      "grad_norm": 0.015183777548372746,
      "learning_rate": 0.0006914063848144953,
      "loss": 0.0024,
      "step": 53650
    },
    {
      "epoch": 15.44434857635893,
      "grad_norm": 8.600937871960923e-05,
      "learning_rate": 0.0006911187805579523,
      "loss": 0.0027,
      "step": 53700
    },
    {
      "epoch": 15.45872878918608,
      "grad_norm": 6.454095273511484e-05,
      "learning_rate": 0.0006908311763014093,
      "loss": 0.0017,
      "step": 53750
    },
    {
      "epoch": 15.47310900201323,
      "grad_norm": 0.03294958919286728,
      "learning_rate": 0.0006905435720448663,
      "loss": 0.0012,
      "step": 53800
    },
    {
      "epoch": 15.48748921484038,
      "grad_norm": 0.000112269633973483,
      "learning_rate": 0.0006902559677883233,
      "loss": 0.0009,
      "step": 53850
    },
    {
      "epoch": 15.501869427667529,
      "grad_norm": 9.564796346239746e-05,
      "learning_rate": 0.0006899683635317803,
      "loss": 0.0027,
      "step": 53900
    },
    {
      "epoch": 15.51624964049468,
      "grad_norm": 0.0037679318338632584,
      "learning_rate": 0.0006896807592752373,
      "loss": 0.0047,
      "step": 53950
    },
    {
      "epoch": 15.530629853321829,
      "grad_norm": 0.000627289293333888,
      "learning_rate": 0.0006893931550186942,
      "loss": 0.0029,
      "step": 54000
    },
    {
      "epoch": 15.54501006614898,
      "grad_norm": 0.00510791689157486,
      "learning_rate": 0.0006891055507621513,
      "loss": 0.0007,
      "step": 54050
    },
    {
      "epoch": 15.559390278976128,
      "grad_norm": 1.910920036607422e-05,
      "learning_rate": 0.0006888179465056083,
      "loss": 0.0033,
      "step": 54100
    },
    {
      "epoch": 15.573770491803279,
      "grad_norm": 0.00014019646914675832,
      "learning_rate": 0.0006885303422490653,
      "loss": 0.0026,
      "step": 54150
    },
    {
      "epoch": 15.58815070463043,
      "grad_norm": 2.9500655728043057e-05,
      "learning_rate": 0.0006882427379925224,
      "loss": 0.0013,
      "step": 54200
    },
    {
      "epoch": 15.602530917457578,
      "grad_norm": 0.021775897592306137,
      "learning_rate": 0.0006879551337359793,
      "loss": 0.0039,
      "step": 54250
    },
    {
      "epoch": 15.616911130284729,
      "grad_norm": 0.00014152450603432953,
      "learning_rate": 0.0006876675294794363,
      "loss": 0.0023,
      "step": 54300
    },
    {
      "epoch": 15.631291343111878,
      "grad_norm": 0.012508152984082699,
      "learning_rate": 0.0006873799252228933,
      "loss": 0.0013,
      "step": 54350
    },
    {
      "epoch": 15.645671555939028,
      "grad_norm": 0.0007983148097991943,
      "learning_rate": 0.0006870923209663503,
      "loss": 0.0022,
      "step": 54400
    },
    {
      "epoch": 15.660051768766177,
      "grad_norm": 0.00020759030303452164,
      "learning_rate": 0.0006868047167098072,
      "loss": 0.0009,
      "step": 54450
    },
    {
      "epoch": 15.674431981593328,
      "grad_norm": 0.0131964897736907,
      "learning_rate": 0.0006865171124532644,
      "loss": 0.0018,
      "step": 54500
    },
    {
      "epoch": 15.688812194420478,
      "grad_norm": 9.51966067077592e-05,
      "learning_rate": 0.0006862295081967214,
      "loss": 0.0035,
      "step": 54550
    },
    {
      "epoch": 15.703192407247627,
      "grad_norm": 0.05340036377310753,
      "learning_rate": 0.0006859419039401783,
      "loss": 0.0024,
      "step": 54600
    },
    {
      "epoch": 15.717572620074778,
      "grad_norm": 0.00042348267743363976,
      "learning_rate": 0.0006856542996836354,
      "loss": 0.0014,
      "step": 54650
    },
    {
      "epoch": 15.731952832901927,
      "grad_norm": 0.013997702859342098,
      "learning_rate": 0.0006853666954270923,
      "loss": 0.0029,
      "step": 54700
    },
    {
      "epoch": 15.746333045729077,
      "grad_norm": 4.310600706958212e-05,
      "learning_rate": 0.0006850790911705493,
      "loss": 0.0037,
      "step": 54750
    },
    {
      "epoch": 15.760713258556226,
      "grad_norm": 2.322550062672235e-05,
      "learning_rate": 0.0006847914869140064,
      "loss": 0.0024,
      "step": 54800
    },
    {
      "epoch": 15.775093471383377,
      "grad_norm": 0.007217413745820522,
      "learning_rate": 0.0006845038826574633,
      "loss": 0.0011,
      "step": 54850
    },
    {
      "epoch": 15.789473684210526,
      "grad_norm": 0.00046952240518294275,
      "learning_rate": 0.0006842162784009203,
      "loss": 0.0016,
      "step": 54900
    },
    {
      "epoch": 15.803853897037676,
      "grad_norm": 3.790240953094326e-05,
      "learning_rate": 0.0006839286741443774,
      "loss": 0.0032,
      "step": 54950
    },
    {
      "epoch": 15.818234109864825,
      "grad_norm": 0.11399652808904648,
      "learning_rate": 0.0006836410698878344,
      "loss": 0.0029,
      "step": 55000
    },
    {
      "epoch": 15.832614322691976,
      "grad_norm": 0.00665480550378561,
      "learning_rate": 0.0006833534656312913,
      "loss": 0.0043,
      "step": 55050
    },
    {
      "epoch": 15.846994535519126,
      "grad_norm": 0.0001774043048499152,
      "learning_rate": 0.0006830658613747484,
      "loss": 0.0021,
      "step": 55100
    },
    {
      "epoch": 15.861374748346275,
      "grad_norm": 0.00030151952523738146,
      "learning_rate": 0.0006827782571182054,
      "loss": 0.0045,
      "step": 55150
    },
    {
      "epoch": 15.875754961173426,
      "grad_norm": 9.686959674581885e-05,
      "learning_rate": 0.0006824906528616623,
      "loss": 0.0095,
      "step": 55200
    },
    {
      "epoch": 15.890135174000575,
      "grad_norm": 7.699892012169585e-05,
      "learning_rate": 0.0006822030486051194,
      "loss": 0.0035,
      "step": 55250
    },
    {
      "epoch": 15.904515386827725,
      "grad_norm": 0.0004155485949013382,
      "learning_rate": 0.0006819154443485763,
      "loss": 0.0021,
      "step": 55300
    },
    {
      "epoch": 15.918895599654874,
      "grad_norm": 0.0013953251764178276,
      "learning_rate": 0.0006816278400920333,
      "loss": 0.0008,
      "step": 55350
    },
    {
      "epoch": 15.933275812482025,
      "grad_norm": 3.177444887114689e-05,
      "learning_rate": 0.0006813402358354905,
      "loss": 0.0039,
      "step": 55400
    },
    {
      "epoch": 15.947656025309175,
      "grad_norm": 0.0001322162279393524,
      "learning_rate": 0.0006810526315789474,
      "loss": 0.0056,
      "step": 55450
    },
    {
      "epoch": 15.962036238136324,
      "grad_norm": 0.05337142571806908,
      "learning_rate": 0.0006807650273224044,
      "loss": 0.0022,
      "step": 55500
    },
    {
      "epoch": 15.976416450963475,
      "grad_norm": 0.09156110882759094,
      "learning_rate": 0.0006804774230658614,
      "loss": 0.0014,
      "step": 55550
    },
    {
      "epoch": 15.990796663790624,
      "grad_norm": 0.002088802168145776,
      "learning_rate": 0.0006801898188093184,
      "loss": 0.0071,
      "step": 55600
    },
    {
      "epoch": 16.0,
      "eval_loss": 0.01055669691413641,
      "eval_runtime": 17.4348,
      "eval_samples_per_second": 2736.991,
      "eval_steps_per_second": 42.788,
      "step": 55632
    },
    {
      "epoch": 16.005176876617774,
      "grad_norm": 0.014127989299595356,
      "learning_rate": 0.0006799022145527753,
      "loss": 0.0015,
      "step": 55650
    },
    {
      "epoch": 16.019557089444923,
      "grad_norm": 7.1578127972316e-05,
      "learning_rate": 0.0006796146102962324,
      "loss": 0.0015,
      "step": 55700
    },
    {
      "epoch": 16.033937302272072,
      "grad_norm": 0.00011873134644702077,
      "learning_rate": 0.0006793270060396894,
      "loss": 0.0023,
      "step": 55750
    },
    {
      "epoch": 16.048317515099225,
      "grad_norm": 0.01959490403532982,
      "learning_rate": 0.0006790394017831463,
      "loss": 0.0025,
      "step": 55800
    },
    {
      "epoch": 16.062697727926373,
      "grad_norm": 9.439692803425714e-05,
      "learning_rate": 0.0006787517975266035,
      "loss": 0.0035,
      "step": 55850
    },
    {
      "epoch": 16.077077940753522,
      "grad_norm": 0.001879348186776042,
      "learning_rate": 0.0006784641932700604,
      "loss": 0.0006,
      "step": 55900
    },
    {
      "epoch": 16.091458153580675,
      "grad_norm": 0.025956274941563606,
      "learning_rate": 0.0006781765890135174,
      "loss": 0.0023,
      "step": 55950
    },
    {
      "epoch": 16.105838366407824,
      "grad_norm": 0.03416465222835541,
      "learning_rate": 0.0006778889847569745,
      "loss": 0.0018,
      "step": 56000
    },
    {
      "epoch": 16.120218579234972,
      "grad_norm": 0.0006503253825940192,
      "learning_rate": 0.0006776013805004314,
      "loss": 0.0017,
      "step": 56050
    },
    {
      "epoch": 16.13459879206212,
      "grad_norm": 0.023369209840893745,
      "learning_rate": 0.0006773137762438884,
      "loss": 0.0019,
      "step": 56100
    },
    {
      "epoch": 16.148979004889274,
      "grad_norm": 0.00042768591083586216,
      "learning_rate": 0.0006770261719873454,
      "loss": 0.0035,
      "step": 56150
    },
    {
      "epoch": 16.163359217716422,
      "grad_norm": 0.00041946687269955873,
      "learning_rate": 0.0006767385677308024,
      "loss": 0.0049,
      "step": 56200
    },
    {
      "epoch": 16.17773943054357,
      "grad_norm": 0.036301229149103165,
      "learning_rate": 0.0006764509634742593,
      "loss": 0.0032,
      "step": 56250
    },
    {
      "epoch": 16.19211964337072,
      "grad_norm": 6.983258936088532e-05,
      "learning_rate": 0.0006761633592177165,
      "loss": 0.0019,
      "step": 56300
    },
    {
      "epoch": 16.206499856197873,
      "grad_norm": 0.0025177656207233667,
      "learning_rate": 0.0006758757549611735,
      "loss": 0.0044,
      "step": 56350
    },
    {
      "epoch": 16.22088006902502,
      "grad_norm": 0.00020244112238287926,
      "learning_rate": 0.0006755881507046304,
      "loss": 0.0048,
      "step": 56400
    },
    {
      "epoch": 16.23526028185217,
      "grad_norm": 0.0002126446197507903,
      "learning_rate": 0.0006753005464480875,
      "loss": 0.0024,
      "step": 56450
    },
    {
      "epoch": 16.249640494679323,
      "grad_norm": 0.0004999325610697269,
      "learning_rate": 0.0006750129421915444,
      "loss": 0.0005,
      "step": 56500
    },
    {
      "epoch": 16.26402070750647,
      "grad_norm": 0.00695119658485055,
      "learning_rate": 0.0006747253379350014,
      "loss": 0.0045,
      "step": 56550
    },
    {
      "epoch": 16.27840092033362,
      "grad_norm": 0.0024163227062672377,
      "learning_rate": 0.0006744377336784585,
      "loss": 0.0022,
      "step": 56600
    },
    {
      "epoch": 16.29278113316077,
      "grad_norm": 0.0001933359308168292,
      "learning_rate": 0.0006741501294219154,
      "loss": 0.0034,
      "step": 56650
    },
    {
      "epoch": 16.30716134598792,
      "grad_norm": 4.758977229357697e-05,
      "learning_rate": 0.0006738625251653725,
      "loss": 0.0012,
      "step": 56700
    },
    {
      "epoch": 16.32154155881507,
      "grad_norm": 0.011189139448106289,
      "learning_rate": 0.0006735749209088295,
      "loss": 0.0013,
      "step": 56750
    },
    {
      "epoch": 16.33592177164222,
      "grad_norm": 0.007806303910911083,
      "learning_rate": 0.0006732873166522865,
      "loss": 0.0004,
      "step": 56800
    },
    {
      "epoch": 16.35030198446937,
      "grad_norm": 3.844649108941667e-05,
      "learning_rate": 0.0006729997123957434,
      "loss": 0.0033,
      "step": 56850
    },
    {
      "epoch": 16.36468219729652,
      "grad_norm": 0.0007924828678369522,
      "learning_rate": 0.0006727121081392005,
      "loss": 0.0013,
      "step": 56900
    },
    {
      "epoch": 16.37906241012367,
      "grad_norm": 0.10324583202600479,
      "learning_rate": 0.0006724245038826575,
      "loss": 0.0051,
      "step": 56950
    },
    {
      "epoch": 16.39344262295082,
      "grad_norm": 0.007035170216113329,
      "learning_rate": 0.0006721368996261144,
      "loss": 0.0047,
      "step": 57000
    },
    {
      "epoch": 16.40782283577797,
      "grad_norm": 0.009008524008095264,
      "learning_rate": 0.0006718492953695715,
      "loss": 0.0056,
      "step": 57050
    },
    {
      "epoch": 16.42220304860512,
      "grad_norm": 0.00019581343804020435,
      "learning_rate": 0.0006715616911130284,
      "loss": 0.0022,
      "step": 57100
    },
    {
      "epoch": 16.43658326143227,
      "grad_norm": 0.00018940019072033465,
      "learning_rate": 0.0006712740868564855,
      "loss": 0.0028,
      "step": 57150
    },
    {
      "epoch": 16.450963474259417,
      "grad_norm": 0.06397582590579987,
      "learning_rate": 0.0006709864825999425,
      "loss": 0.0066,
      "step": 57200
    },
    {
      "epoch": 16.46534368708657,
      "grad_norm": 0.00011405671830289066,
      "learning_rate": 0.0006706988783433995,
      "loss": 0.0046,
      "step": 57250
    },
    {
      "epoch": 16.47972389991372,
      "grad_norm": 0.001241234946064651,
      "learning_rate": 0.0006704112740868565,
      "loss": 0.0022,
      "step": 57300
    },
    {
      "epoch": 16.494104112740867,
      "grad_norm": 0.005490153096616268,
      "learning_rate": 0.0006701236698303135,
      "loss": 0.0011,
      "step": 57350
    },
    {
      "epoch": 16.50848432556802,
      "grad_norm": 0.05541163682937622,
      "learning_rate": 0.0006698360655737705,
      "loss": 0.0037,
      "step": 57400
    },
    {
      "epoch": 16.52286453839517,
      "grad_norm": 0.00013216309889685363,
      "learning_rate": 0.0006695484613172274,
      "loss": 0.0011,
      "step": 57450
    },
    {
      "epoch": 16.537244751222318,
      "grad_norm": 0.01125423051416874,
      "learning_rate": 0.0006692608570606845,
      "loss": 0.0013,
      "step": 57500
    },
    {
      "epoch": 16.551624964049466,
      "grad_norm": 0.0003929653612431139,
      "learning_rate": 0.0006689732528041416,
      "loss": 0.0007,
      "step": 57550
    },
    {
      "epoch": 16.56600517687662,
      "grad_norm": 5.950260310783051e-05,
      "learning_rate": 0.0006686856485475985,
      "loss": 0.0011,
      "step": 57600
    },
    {
      "epoch": 16.580385389703768,
      "grad_norm": 0.002979454817250371,
      "learning_rate": 0.0006683980442910556,
      "loss": 0.0019,
      "step": 57650
    },
    {
      "epoch": 16.594765602530916,
      "grad_norm": 0.00017387577099725604,
      "learning_rate": 0.0006681104400345125,
      "loss": 0.0016,
      "step": 57700
    },
    {
      "epoch": 16.60914581535807,
      "grad_norm": 0.005528382956981659,
      "learning_rate": 0.0006678228357779695,
      "loss": 0.0023,
      "step": 57750
    },
    {
      "epoch": 16.623526028185218,
      "grad_norm": 9.069086809176952e-05,
      "learning_rate": 0.0006675352315214265,
      "loss": 0.0031,
      "step": 57800
    },
    {
      "epoch": 16.637906241012367,
      "grad_norm": 0.07012143731117249,
      "learning_rate": 0.0006672476272648835,
      "loss": 0.0029,
      "step": 57850
    },
    {
      "epoch": 16.652286453839515,
      "grad_norm": 3.0226847229641862e-05,
      "learning_rate": 0.0006669600230083405,
      "loss": 0.0009,
      "step": 57900
    },
    {
      "epoch": 16.666666666666668,
      "grad_norm": 4.201383489998989e-05,
      "learning_rate": 0.0006666724187517975,
      "loss": 0.0023,
      "step": 57950
    },
    {
      "epoch": 16.681046879493817,
      "grad_norm": 0.00018869354971684515,
      "learning_rate": 0.0006663848144952546,
      "loss": 0.0039,
      "step": 58000
    },
    {
      "epoch": 16.695427092320966,
      "grad_norm": 0.00014533357170876116,
      "learning_rate": 0.0006660972102387115,
      "loss": 0.0043,
      "step": 58050
    },
    {
      "epoch": 16.709807305148118,
      "grad_norm": 0.0010661343112587929,
      "learning_rate": 0.0006658096059821686,
      "loss": 0.0041,
      "step": 58100
    },
    {
      "epoch": 16.724187517975267,
      "grad_norm": 7.05830316292122e-05,
      "learning_rate": 0.0006655220017256256,
      "loss": 0.0012,
      "step": 58150
    },
    {
      "epoch": 16.738567730802416,
      "grad_norm": 0.0005425884737633169,
      "learning_rate": 0.0006652343974690825,
      "loss": 0.0022,
      "step": 58200
    },
    {
      "epoch": 16.752947943629565,
      "grad_norm": 7.964914402691647e-05,
      "learning_rate": 0.0006649467932125396,
      "loss": 0.0029,
      "step": 58250
    },
    {
      "epoch": 16.767328156456717,
      "grad_norm": 0.03663986176252365,
      "learning_rate": 0.0006646591889559965,
      "loss": 0.0044,
      "step": 58300
    },
    {
      "epoch": 16.781708369283866,
      "grad_norm": 3.3051022910512984e-05,
      "learning_rate": 0.0006643715846994535,
      "loss": 0.0042,
      "step": 58350
    },
    {
      "epoch": 16.796088582111015,
      "grad_norm": 0.0025315035600215197,
      "learning_rate": 0.0006640839804429106,
      "loss": 0.0037,
      "step": 58400
    },
    {
      "epoch": 16.810468794938163,
      "grad_norm": 0.0003160382329951972,
      "learning_rate": 0.0006637963761863676,
      "loss": 0.0037,
      "step": 58450
    },
    {
      "epoch": 16.824849007765316,
      "grad_norm": 0.02536614052951336,
      "learning_rate": 0.0006635087719298246,
      "loss": 0.0022,
      "step": 58500
    },
    {
      "epoch": 16.839229220592465,
      "grad_norm": 0.00028015137650072575,
      "learning_rate": 0.0006632211676732816,
      "loss": 0.0014,
      "step": 58550
    },
    {
      "epoch": 16.853609433419614,
      "grad_norm": 0.0001987947034649551,
      "learning_rate": 0.0006629335634167386,
      "loss": 0.0011,
      "step": 58600
    },
    {
      "epoch": 16.867989646246766,
      "grad_norm": 0.0006231135921552777,
      "learning_rate": 0.0006626459591601955,
      "loss": 0.0051,
      "step": 58650
    },
    {
      "epoch": 16.882369859073915,
      "grad_norm": 6.933219992788509e-05,
      "learning_rate": 0.0006623583549036526,
      "loss": 0.0027,
      "step": 58700
    },
    {
      "epoch": 16.896750071901064,
      "grad_norm": 0.05766957253217697,
      "learning_rate": 0.0006620707506471096,
      "loss": 0.0011,
      "step": 58750
    },
    {
      "epoch": 16.911130284728213,
      "grad_norm": 0.00330581096932292,
      "learning_rate": 0.0006617831463905665,
      "loss": 0.0021,
      "step": 58800
    },
    {
      "epoch": 16.925510497555365,
      "grad_norm": 0.0015721683157607913,
      "learning_rate": 0.0006614955421340237,
      "loss": 0.0039,
      "step": 58850
    },
    {
      "epoch": 16.939890710382514,
      "grad_norm": 0.004595316015183926,
      "learning_rate": 0.0006612079378774806,
      "loss": 0.0011,
      "step": 58900
    },
    {
      "epoch": 16.954270923209663,
      "grad_norm": 0.000875660392921418,
      "learning_rate": 0.0006609203336209376,
      "loss": 0.0023,
      "step": 58950
    },
    {
      "epoch": 16.96865113603681,
      "grad_norm": 6.190579733811319e-05,
      "learning_rate": 0.0006606327293643946,
      "loss": 0.0015,
      "step": 59000
    },
    {
      "epoch": 16.983031348863964,
      "grad_norm": 0.003960070200264454,
      "learning_rate": 0.0006603451251078516,
      "loss": 0.0013,
      "step": 59050
    },
    {
      "epoch": 16.997411561691113,
      "grad_norm": 0.01429204922169447,
      "learning_rate": 0.0006600575208513086,
      "loss": 0.0022,
      "step": 59100
    },
    {
      "epoch": 17.0,
      "eval_loss": 0.012085789814591408,
      "eval_runtime": 17.7124,
      "eval_samples_per_second": 2694.104,
      "eval_steps_per_second": 42.117,
      "step": 59109
    },
    {
      "epoch": 17.01179177451826,
      "grad_norm": 0.0121421217918396,
      "learning_rate": 0.0006597699165947656,
      "loss": 0.0032,
      "step": 59150
    },
    {
      "epoch": 17.026171987345414,
      "grad_norm": 0.01705240271985531,
      "learning_rate": 0.0006594823123382226,
      "loss": 0.002,
      "step": 59200
    },
    {
      "epoch": 17.040552200172563,
      "grad_norm": 0.0002325663808733225,
      "learning_rate": 0.0006591947080816795,
      "loss": 0.0029,
      "step": 59250
    },
    {
      "epoch": 17.05493241299971,
      "grad_norm": 0.0014471589820459485,
      "learning_rate": 0.0006589071038251367,
      "loss": 0.002,
      "step": 59300
    },
    {
      "epoch": 17.06931262582686,
      "grad_norm": 0.00017706411017570645,
      "learning_rate": 0.0006586194995685937,
      "loss": 0.004,
      "step": 59350
    },
    {
      "epoch": 17.083692838654013,
      "grad_norm": 0.0001451388088753447,
      "learning_rate": 0.0006583318953120506,
      "loss": 0.0035,
      "step": 59400
    },
    {
      "epoch": 17.098073051481162,
      "grad_norm": 0.0001375000865664333,
      "learning_rate": 0.0006580442910555077,
      "loss": 0.0011,
      "step": 59450
    },
    {
      "epoch": 17.11245326430831,
      "grad_norm": 0.013361566700041294,
      "learning_rate": 0.0006577566867989646,
      "loss": 0.0015,
      "step": 59500
    },
    {
      "epoch": 17.126833477135463,
      "grad_norm": 0.006084388587623835,
      "learning_rate": 0.0006574690825424216,
      "loss": 0.002,
      "step": 59550
    },
    {
      "epoch": 17.141213689962612,
      "grad_norm": 0.00012977859296370298,
      "learning_rate": 0.0006571814782858786,
      "loss": 0.0006,
      "step": 59600
    },
    {
      "epoch": 17.15559390278976,
      "grad_norm": 0.001190241309814155,
      "learning_rate": 0.0006568938740293356,
      "loss": 0.0034,
      "step": 59650
    },
    {
      "epoch": 17.16997411561691,
      "grad_norm": 0.0009024696773849428,
      "learning_rate": 0.0006566062697727927,
      "loss": 0.0043,
      "step": 59700
    },
    {
      "epoch": 17.184354328444062,
      "grad_norm": 0.03260176628828049,
      "learning_rate": 0.0006563186655162497,
      "loss": 0.0037,
      "step": 59750
    },
    {
      "epoch": 17.19873454127121,
      "grad_norm": 0.0061872247606515884,
      "learning_rate": 0.0006560310612597067,
      "loss": 0.0013,
      "step": 59800
    },
    {
      "epoch": 17.21311475409836,
      "grad_norm": 0.00019711742061190307,
      "learning_rate": 0.0006557434570031636,
      "loss": 0.0012,
      "step": 59850
    },
    {
      "epoch": 17.227494966925512,
      "grad_norm": 0.06701621413230896,
      "learning_rate": 0.0006554558527466207,
      "loss": 0.0044,
      "step": 59900
    },
    {
      "epoch": 17.24187517975266,
      "grad_norm": 0.0003216719487681985,
      "learning_rate": 0.0006551682484900776,
      "loss": 0.0011,
      "step": 59950
    },
    {
      "epoch": 17.25625539257981,
      "grad_norm": 0.00020286098879296333,
      "learning_rate": 0.0006548806442335346,
      "loss": 0.0019,
      "step": 60000
    },
    {
      "epoch": 17.27063560540696,
      "grad_norm": 0.0006967287044972181,
      "learning_rate": 0.0006545930399769917,
      "loss": 0.002,
      "step": 60050
    },
    {
      "epoch": 17.28501581823411,
      "grad_norm": 0.14566709101200104,
      "learning_rate": 0.0006543054357204486,
      "loss": 0.0034,
      "step": 60100
    },
    {
      "epoch": 17.29939603106126,
      "grad_norm": 5.7852532336255535e-05,
      "learning_rate": 0.0006540178314639057,
      "loss": 0.0004,
      "step": 60150
    },
    {
      "epoch": 17.31377624388841,
      "grad_norm": 0.00017657339049037546,
      "learning_rate": 0.0006537302272073627,
      "loss": 0.0014,
      "step": 60200
    },
    {
      "epoch": 17.328156456715558,
      "grad_norm": 3.0702278309036046e-05,
      "learning_rate": 0.0006534426229508197,
      "loss": 0.0031,
      "step": 60250
    },
    {
      "epoch": 17.34253666954271,
      "grad_norm": 0.002495260676369071,
      "learning_rate": 0.0006531550186942767,
      "loss": 0.0031,
      "step": 60300
    },
    {
      "epoch": 17.35691688236986,
      "grad_norm": 0.0006869570352137089,
      "learning_rate": 0.0006528674144377337,
      "loss": 0.0037,
      "step": 60350
    },
    {
      "epoch": 17.371297095197008,
      "grad_norm": 0.02879953570663929,
      "learning_rate": 0.0006525798101811907,
      "loss": 0.0017,
      "step": 60400
    },
    {
      "epoch": 17.38567730802416,
      "grad_norm": 0.003924893215298653,
      "learning_rate": 0.0006522922059246476,
      "loss": 0.0014,
      "step": 60450
    },
    {
      "epoch": 17.40005752085131,
      "grad_norm": 0.06059670075774193,
      "learning_rate": 0.0006520046016681047,
      "loss": 0.0013,
      "step": 60500
    },
    {
      "epoch": 17.414437733678458,
      "grad_norm": 6.566157389897853e-05,
      "learning_rate": 0.0006517169974115616,
      "loss": 0.0015,
      "step": 60550
    },
    {
      "epoch": 17.428817946505607,
      "grad_norm": 6.550987018272281e-05,
      "learning_rate": 0.0006514293931550187,
      "loss": 0.0016,
      "step": 60600
    },
    {
      "epoch": 17.44319815933276,
      "grad_norm": 0.00011117960093542933,
      "learning_rate": 0.0006511417888984758,
      "loss": 0.0029,
      "step": 60650
    },
    {
      "epoch": 17.457578372159908,
      "grad_norm": 0.0004265346797183156,
      "learning_rate": 0.0006508541846419327,
      "loss": 0.0017,
      "step": 60700
    },
    {
      "epoch": 17.471958584987057,
      "grad_norm": 0.1396568864583969,
      "learning_rate": 0.0006505665803853897,
      "loss": 0.0037,
      "step": 60750
    },
    {
      "epoch": 17.48633879781421,
      "grad_norm": 0.0010540783405303955,
      "learning_rate": 0.0006502789761288467,
      "loss": 0.0012,
      "step": 60800
    },
    {
      "epoch": 17.500719010641358,
      "grad_norm": 0.00018140324391424656,
      "learning_rate": 0.0006499913718723037,
      "loss": 0.0042,
      "step": 60850
    },
    {
      "epoch": 17.515099223468507,
      "grad_norm": 0.009877900592982769,
      "learning_rate": 0.0006497037676157607,
      "loss": 0.0084,
      "step": 60900
    },
    {
      "epoch": 17.529479436295656,
      "grad_norm": 4.376519427751191e-05,
      "learning_rate": 0.0006494161633592177,
      "loss": 0.0021,
      "step": 60950
    },
    {
      "epoch": 17.54385964912281,
      "grad_norm": 0.02612912841141224,
      "learning_rate": 0.0006491285591026748,
      "loss": 0.0038,
      "step": 61000
    },
    {
      "epoch": 17.558239861949957,
      "grad_norm": 0.0009717741631902754,
      "learning_rate": 0.0006488409548461317,
      "loss": 0.0011,
      "step": 61050
    },
    {
      "epoch": 17.572620074777106,
      "grad_norm": 0.0011812766315415502,
      "learning_rate": 0.0006485533505895888,
      "loss": 0.0033,
      "step": 61100
    },
    {
      "epoch": 17.587000287604255,
      "grad_norm": 0.0001610432518646121,
      "learning_rate": 0.0006482657463330457,
      "loss": 0.0026,
      "step": 61150
    },
    {
      "epoch": 17.601380500431407,
      "grad_norm": 0.0005642137839458883,
      "learning_rate": 0.0006479781420765027,
      "loss": 0.0023,
      "step": 61200
    },
    {
      "epoch": 17.615760713258556,
      "grad_norm": 0.0016548526473343372,
      "learning_rate": 0.0006476905378199598,
      "loss": 0.0019,
      "step": 61250
    },
    {
      "epoch": 17.630140926085705,
      "grad_norm": 0.00011270479444647208,
      "learning_rate": 0.0006474029335634167,
      "loss": 0.003,
      "step": 61300
    },
    {
      "epoch": 17.644521138912857,
      "grad_norm": 0.00048588577192276716,
      "learning_rate": 0.0006471153293068737,
      "loss": 0.004,
      "step": 61350
    },
    {
      "epoch": 17.658901351740006,
      "grad_norm": 0.00022471997363027185,
      "learning_rate": 0.0006468277250503307,
      "loss": 0.0043,
      "step": 61400
    },
    {
      "epoch": 17.673281564567155,
      "grad_norm": 0.00023990996123757213,
      "learning_rate": 0.0006465401207937878,
      "loss": 0.0012,
      "step": 61450
    },
    {
      "epoch": 17.687661777394304,
      "grad_norm": 0.0018682052614167333,
      "learning_rate": 0.0006462525165372448,
      "loss": 0.0035,
      "step": 61500
    },
    {
      "epoch": 17.702041990221456,
      "grad_norm": 0.0028114865999668837,
      "learning_rate": 0.0006459649122807018,
      "loss": 0.0026,
      "step": 61550
    },
    {
      "epoch": 17.716422203048605,
      "grad_norm": 0.0045173633843660355,
      "learning_rate": 0.0006456773080241588,
      "loss": 0.0041,
      "step": 61600
    },
    {
      "epoch": 17.730802415875754,
      "grad_norm": 3.971996557083912e-05,
      "learning_rate": 0.0006453897037676157,
      "loss": 0.0013,
      "step": 61650
    },
    {
      "epoch": 17.745182628702906,
      "grad_norm": 0.0003399204579181969,
      "learning_rate": 0.0006451020995110728,
      "loss": 0.0005,
      "step": 61700
    },
    {
      "epoch": 17.759562841530055,
      "grad_norm": 0.007216483820229769,
      "learning_rate": 0.0006448144952545297,
      "loss": 0.0015,
      "step": 61750
    },
    {
      "epoch": 17.773943054357204,
      "grad_norm": 0.0038085253909230232,
      "learning_rate": 0.0006445268909979867,
      "loss": 0.0034,
      "step": 61800
    },
    {
      "epoch": 17.788323267184353,
      "grad_norm": 0.0084668705239892,
      "learning_rate": 0.0006442392867414439,
      "loss": 0.0027,
      "step": 61850
    },
    {
      "epoch": 17.802703480011505,
      "grad_norm": 5.347153637558222e-05,
      "learning_rate": 0.0006439516824849008,
      "loss": 0.0034,
      "step": 61900
    },
    {
      "epoch": 17.817083692838654,
      "grad_norm": 0.00021979243319947273,
      "learning_rate": 0.0006436640782283578,
      "loss": 0.0058,
      "step": 61950
    },
    {
      "epoch": 17.831463905665803,
      "grad_norm": 0.0001882213109638542,
      "learning_rate": 0.0006433764739718148,
      "loss": 0.0019,
      "step": 62000
    },
    {
      "epoch": 17.845844118492955,
      "grad_norm": 2.356246841372922e-05,
      "learning_rate": 0.0006430888697152718,
      "loss": 0.0022,
      "step": 62050
    },
    {
      "epoch": 17.860224331320104,
      "grad_norm": 0.000572029675822705,
      "learning_rate": 0.0006428012654587288,
      "loss": 0.0028,
      "step": 62100
    },
    {
      "epoch": 17.874604544147253,
      "grad_norm": 0.00014303030911833048,
      "learning_rate": 0.0006425136612021858,
      "loss": 0.0011,
      "step": 62150
    },
    {
      "epoch": 17.888984756974402,
      "grad_norm": 0.010564323514699936,
      "learning_rate": 0.0006422260569456428,
      "loss": 0.0026,
      "step": 62200
    },
    {
      "epoch": 17.903364969801554,
      "grad_norm": 5.968807454337366e-05,
      "learning_rate": 0.0006419384526890997,
      "loss": 0.0009,
      "step": 62250
    },
    {
      "epoch": 17.917745182628703,
      "grad_norm": 0.0004384508356451988,
      "learning_rate": 0.0006416508484325569,
      "loss": 0.0012,
      "step": 62300
    },
    {
      "epoch": 17.932125395455852,
      "grad_norm": 0.000656874559354037,
      "learning_rate": 0.0006413632441760138,
      "loss": 0.0006,
      "step": 62350
    },
    {
      "epoch": 17.946505608283,
      "grad_norm": 0.00017746762023307383,
      "learning_rate": 0.0006410756399194708,
      "loss": 0.0028,
      "step": 62400
    },
    {
      "epoch": 17.960885821110153,
      "grad_norm": 4.585329588735476e-05,
      "learning_rate": 0.0006407880356629279,
      "loss": 0.0054,
      "step": 62450
    },
    {
      "epoch": 17.975266033937302,
      "grad_norm": 0.0014680057065561414,
      "learning_rate": 0.0006405004314063848,
      "loss": 0.0059,
      "step": 62500
    },
    {
      "epoch": 17.98964624676445,
      "grad_norm": 0.12780341506004333,
      "learning_rate": 0.0006402128271498418,
      "loss": 0.006,
      "step": 62550
    },
    {
      "epoch": 18.0,
      "eval_loss": 0.0104155782610178,
      "eval_runtime": 17.6366,
      "eval_samples_per_second": 2705.688,
      "eval_steps_per_second": 42.299,
      "step": 62586
    },
    {
      "epoch": 18.004026459591604,
      "grad_norm": 0.00042347461567260325,
      "learning_rate": 0.0006399252228932988,
      "loss": 0.0053,
      "step": 62600
    },
    {
      "epoch": 18.018406672418752,
      "grad_norm": 0.005223872605711222,
      "learning_rate": 0.0006396376186367558,
      "loss": 0.0014,
      "step": 62650
    },
    {
      "epoch": 18.0327868852459,
      "grad_norm": 0.0857328251004219,
      "learning_rate": 0.0006393500143802127,
      "loss": 0.0028,
      "step": 62700
    },
    {
      "epoch": 18.04716709807305,
      "grad_norm": 4.6689245209563524e-05,
      "learning_rate": 0.0006390624101236699,
      "loss": 0.0017,
      "step": 62750
    },
    {
      "epoch": 18.061547310900202,
      "grad_norm": 0.009856835938990116,
      "learning_rate": 0.0006387748058671269,
      "loss": 0.0019,
      "step": 62800
    },
    {
      "epoch": 18.07592752372735,
      "grad_norm": 0.00023275376588571817,
      "learning_rate": 0.0006384872016105838,
      "loss": 0.0036,
      "step": 62850
    },
    {
      "epoch": 18.0903077365545,
      "grad_norm": 0.006298055872321129,
      "learning_rate": 0.0006381995973540409,
      "loss": 0.0016,
      "step": 62900
    },
    {
      "epoch": 18.104687949381653,
      "grad_norm": 0.0002389423898421228,
      "learning_rate": 0.0006379119930974978,
      "loss": 0.0017,
      "step": 62950
    },
    {
      "epoch": 18.1190681622088,
      "grad_norm": 0.00013169675366953015,
      "learning_rate": 0.0006376243888409548,
      "loss": 0.0024,
      "step": 63000
    },
    {
      "epoch": 18.13344837503595,
      "grad_norm": 0.000339625432388857,
      "learning_rate": 0.0006373367845844119,
      "loss": 0.0045,
      "step": 63050
    },
    {
      "epoch": 18.1478285878631,
      "grad_norm": 6.372982170432806e-05,
      "learning_rate": 0.0006370491803278688,
      "loss": 0.0043,
      "step": 63100
    },
    {
      "epoch": 18.16220880069025,
      "grad_norm": 6.26607725280337e-05,
      "learning_rate": 0.0006367615760713259,
      "loss": 0.0003,
      "step": 63150
    },
    {
      "epoch": 18.1765890135174,
      "grad_norm": 0.0003503094194456935,
      "learning_rate": 0.0006364739718147829,
      "loss": 0.0004,
      "step": 63200
    },
    {
      "epoch": 18.19096922634455,
      "grad_norm": 0.0020383137743920088,
      "learning_rate": 0.0006361863675582399,
      "loss": 0.004,
      "step": 63250
    },
    {
      "epoch": 18.205349439171698,
      "grad_norm": 3.9120692235883325e-05,
      "learning_rate": 0.0006358987633016968,
      "loss": 0.0013,
      "step": 63300
    },
    {
      "epoch": 18.21972965199885,
      "grad_norm": 0.0005553813534788787,
      "learning_rate": 0.0006356111590451539,
      "loss": 0.0025,
      "step": 63350
    },
    {
      "epoch": 18.234109864826,
      "grad_norm": 0.00035095930797979236,
      "learning_rate": 0.0006353235547886109,
      "loss": 0.0027,
      "step": 63400
    },
    {
      "epoch": 18.24849007765315,
      "grad_norm": 0.0035238887649029493,
      "learning_rate": 0.0006350359505320678,
      "loss": 0.0027,
      "step": 63450
    },
    {
      "epoch": 18.2628702904803,
      "grad_norm": 0.00023617770057171583,
      "learning_rate": 0.0006347483462755249,
      "loss": 0.0033,
      "step": 63500
    },
    {
      "epoch": 18.27725050330745,
      "grad_norm": 0.000388496060622856,
      "learning_rate": 0.0006344607420189818,
      "loss": 0.0043,
      "step": 63550
    },
    {
      "epoch": 18.2916307161346,
      "grad_norm": 0.004191071726381779,
      "learning_rate": 0.0006341731377624389,
      "loss": 0.0051,
      "step": 63600
    },
    {
      "epoch": 18.306010928961747,
      "grad_norm": 0.0059987581335008144,
      "learning_rate": 0.000633885533505896,
      "loss": 0.0024,
      "step": 63650
    },
    {
      "epoch": 18.3203911417889,
      "grad_norm": 0.0001774666307028383,
      "learning_rate": 0.0006335979292493529,
      "loss": 0.0026,
      "step": 63700
    },
    {
      "epoch": 18.33477135461605,
      "grad_norm": 0.00013134549953974783,
      "learning_rate": 0.0006333103249928099,
      "loss": 0.0012,
      "step": 63750
    },
    {
      "epoch": 18.349151567443197,
      "grad_norm": 0.0005539011908695102,
      "learning_rate": 0.0006330227207362669,
      "loss": 0.0029,
      "step": 63800
    },
    {
      "epoch": 18.36353178027035,
      "grad_norm": 0.02272934466600418,
      "learning_rate": 0.0006327351164797239,
      "loss": 0.0025,
      "step": 63850
    },
    {
      "epoch": 18.3779119930975,
      "grad_norm": 0.00013365494669415057,
      "learning_rate": 0.0006324475122231808,
      "loss": 0.0035,
      "step": 63900
    },
    {
      "epoch": 18.392292205924647,
      "grad_norm": 0.0012021412840113044,
      "learning_rate": 0.0006321599079666379,
      "loss": 0.0023,
      "step": 63950
    },
    {
      "epoch": 18.406672418751796,
      "grad_norm": 0.00017361788195557892,
      "learning_rate": 0.000631872303710095,
      "loss": 0.0008,
      "step": 64000
    },
    {
      "epoch": 18.42105263157895,
      "grad_norm": 4.158327283221297e-05,
      "learning_rate": 0.0006315846994535519,
      "loss": 0.0025,
      "step": 64050
    },
    {
      "epoch": 18.435432844406098,
      "grad_norm": 0.0296725332736969,
      "learning_rate": 0.000631297095197009,
      "loss": 0.0047,
      "step": 64100
    },
    {
      "epoch": 18.449813057233246,
      "grad_norm": 0.004378391429781914,
      "learning_rate": 0.0006310094909404659,
      "loss": 0.0037,
      "step": 64150
    },
    {
      "epoch": 18.464193270060395,
      "grad_norm": 0.000900579325389117,
      "learning_rate": 0.0006307218866839229,
      "loss": 0.003,
      "step": 64200
    },
    {
      "epoch": 18.478573482887548,
      "grad_norm": 0.00027000700356438756,
      "learning_rate": 0.00063043428242738,
      "loss": 0.0012,
      "step": 64250
    },
    {
      "epoch": 18.492953695714696,
      "grad_norm": 0.00043540005572140217,
      "learning_rate": 0.0006301466781708369,
      "loss": 0.0024,
      "step": 64300
    },
    {
      "epoch": 18.507333908541845,
      "grad_norm": 0.0003045559860765934,
      "learning_rate": 0.0006298590739142939,
      "loss": 0.0024,
      "step": 64350
    },
    {
      "epoch": 18.521714121368998,
      "grad_norm": 0.00756089948117733,
      "learning_rate": 0.0006295714696577509,
      "loss": 0.0021,
      "step": 64400
    },
    {
      "epoch": 18.536094334196147,
      "grad_norm": 0.0031122644431889057,
      "learning_rate": 0.000629283865401208,
      "loss": 0.0041,
      "step": 64450
    },
    {
      "epoch": 18.550474547023295,
      "grad_norm": 0.007200997322797775,
      "learning_rate": 0.0006289962611446649,
      "loss": 0.0018,
      "step": 64500
    },
    {
      "epoch": 18.564854759850444,
      "grad_norm": 0.006040268577635288,
      "learning_rate": 0.000628708656888122,
      "loss": 0.0031,
      "step": 64550
    },
    {
      "epoch": 18.579234972677597,
      "grad_norm": 0.0002251893311040476,
      "learning_rate": 0.000628421052631579,
      "loss": 0.0005,
      "step": 64600
    },
    {
      "epoch": 18.593615185504746,
      "grad_norm": 0.027691464871168137,
      "learning_rate": 0.0006281334483750359,
      "loss": 0.0041,
      "step": 64650
    },
    {
      "epoch": 18.607995398331894,
      "grad_norm": 0.00020629209757316858,
      "learning_rate": 0.000627845844118493,
      "loss": 0.0058,
      "step": 64700
    },
    {
      "epoch": 18.622375611159047,
      "grad_norm": 0.00021750944142695516,
      "learning_rate": 0.0006275582398619499,
      "loss": 0.0017,
      "step": 64750
    },
    {
      "epoch": 18.636755823986196,
      "grad_norm": 0.000685886072460562,
      "learning_rate": 0.0006272706356054069,
      "loss": 0.0028,
      "step": 64800
    },
    {
      "epoch": 18.651136036813345,
      "grad_norm": 0.00014399558131117374,
      "learning_rate": 0.000626983031348864,
      "loss": 0.0006,
      "step": 64850
    },
    {
      "epoch": 18.665516249640493,
      "grad_norm": 0.0001390021643601358,
      "learning_rate": 0.000626695427092321,
      "loss": 0.0025,
      "step": 64900
    },
    {
      "epoch": 18.679896462467646,
      "grad_norm": 0.00039309149724431336,
      "learning_rate": 0.000626407822835778,
      "loss": 0.001,
      "step": 64950
    },
    {
      "epoch": 18.694276675294795,
      "grad_norm": 7.550274312961847e-05,
      "learning_rate": 0.000626120218579235,
      "loss": 0.002,
      "step": 65000
    },
    {
      "epoch": 18.708656888121943,
      "grad_norm": 0.000924441555980593,
      "learning_rate": 0.000625832614322692,
      "loss": 0.0007,
      "step": 65050
    },
    {
      "epoch": 18.723037100949092,
      "grad_norm": 0.018311891704797745,
      "learning_rate": 0.0006255450100661489,
      "loss": 0.0011,
      "step": 65100
    },
    {
      "epoch": 18.737417313776245,
      "grad_norm": 0.00014691051910631359,
      "learning_rate": 0.000625257405809606,
      "loss": 0.0047,
      "step": 65150
    },
    {
      "epoch": 18.751797526603394,
      "grad_norm": 4.1360253817401826e-05,
      "learning_rate": 0.000624969801553063,
      "loss": 0.0039,
      "step": 65200
    },
    {
      "epoch": 18.766177739430542,
      "grad_norm": 5.104204683448188e-05,
      "learning_rate": 0.0006246821972965199,
      "loss": 0.0044,
      "step": 65250
    },
    {
      "epoch": 18.780557952257695,
      "grad_norm": 0.002450250554829836,
      "learning_rate": 0.0006243945930399771,
      "loss": 0.0012,
      "step": 65300
    },
    {
      "epoch": 18.794938165084844,
      "grad_norm": 5.349700222723186e-05,
      "learning_rate": 0.000624106988783434,
      "loss": 0.0022,
      "step": 65350
    },
    {
      "epoch": 18.809318377911993,
      "grad_norm": 0.02586391568183899,
      "learning_rate": 0.000623819384526891,
      "loss": 0.0027,
      "step": 65400
    },
    {
      "epoch": 18.82369859073914,
      "grad_norm": 0.008039448410272598,
      "learning_rate": 0.000623531780270348,
      "loss": 0.0041,
      "step": 65450
    },
    {
      "epoch": 18.838078803566294,
      "grad_norm": 5.502307249116711e-05,
      "learning_rate": 0.000623244176013805,
      "loss": 0.0043,
      "step": 65500
    },
    {
      "epoch": 18.852459016393443,
      "grad_norm": 0.016616078093647957,
      "learning_rate": 0.000622956571757262,
      "loss": 0.0028,
      "step": 65550
    },
    {
      "epoch": 18.86683922922059,
      "grad_norm": 0.010843075811862946,
      "learning_rate": 0.000622668967500719,
      "loss": 0.0019,
      "step": 65600
    },
    {
      "epoch": 18.881219442047744,
      "grad_norm": 0.0005789453280158341,
      "learning_rate": 0.000622381363244176,
      "loss": 0.001,
      "step": 65650
    },
    {
      "epoch": 18.895599654874893,
      "grad_norm": 0.005104714538902044,
      "learning_rate": 0.0006220937589876329,
      "loss": 0.0015,
      "step": 65700
    },
    {
      "epoch": 18.90997986770204,
      "grad_norm": 0.11273177713155746,
      "learning_rate": 0.0006218061547310901,
      "loss": 0.0034,
      "step": 65750
    },
    {
      "epoch": 18.92436008052919,
      "grad_norm": 0.0071210116147994995,
      "learning_rate": 0.0006215185504745471,
      "loss": 0.0024,
      "step": 65800
    },
    {
      "epoch": 18.938740293356343,
      "grad_norm": 0.010453219525516033,
      "learning_rate": 0.000621230946218004,
      "loss": 0.0044,
      "step": 65850
    },
    {
      "epoch": 18.95312050618349,
      "grad_norm": 2.329079325136263e-05,
      "learning_rate": 0.0006209433419614611,
      "loss": 0.0008,
      "step": 65900
    },
    {
      "epoch": 18.96750071901064,
      "grad_norm": 4.0794537198962644e-05,
      "learning_rate": 0.000620655737704918,
      "loss": 0.0038,
      "step": 65950
    },
    {
      "epoch": 18.981880931837793,
      "grad_norm": 0.00010920088971033692,
      "learning_rate": 0.000620368133448375,
      "loss": 0.0024,
      "step": 66000
    },
    {
      "epoch": 18.996261144664942,
      "grad_norm": 0.0002667458902578801,
      "learning_rate": 0.000620080529191832,
      "loss": 0.0023,
      "step": 66050
    },
    {
      "epoch": 19.0,
      "eval_loss": 0.010255473665893078,
      "eval_runtime": 17.7868,
      "eval_samples_per_second": 2682.835,
      "eval_steps_per_second": 41.941,
      "step": 66063
    },
    {
      "epoch": 19.01064135749209,
      "grad_norm": 0.0008251366089098155,
      "learning_rate": 0.000619792924935289,
      "loss": 0.0016,
      "step": 66100
    },
    {
      "epoch": 19.02502157031924,
      "grad_norm": 6.426471372833475e-05,
      "learning_rate": 0.000619505320678746,
      "loss": 0.0018,
      "step": 66150
    },
    {
      "epoch": 19.039401783146392,
      "grad_norm": 0.1020364984869957,
      "learning_rate": 0.0006192177164222031,
      "loss": 0.0016,
      "step": 66200
    },
    {
      "epoch": 19.05378199597354,
      "grad_norm": 9.358666284242645e-05,
      "learning_rate": 0.0006189301121656601,
      "loss": 0.001,
      "step": 66250
    },
    {
      "epoch": 19.06816220880069,
      "grad_norm": 0.003581792348995805,
      "learning_rate": 0.000618642507909117,
      "loss": 0.003,
      "step": 66300
    },
    {
      "epoch": 19.08254242162784,
      "grad_norm": 0.008256053552031517,
      "learning_rate": 0.0006183549036525741,
      "loss": 0.0029,
      "step": 66350
    },
    {
      "epoch": 19.09692263445499,
      "grad_norm": 6.343564746202901e-05,
      "learning_rate": 0.0006180672993960311,
      "loss": 0.0024,
      "step": 66400
    },
    {
      "epoch": 19.11130284728214,
      "grad_norm": 2.6582303689792752e-05,
      "learning_rate": 0.000617779695139488,
      "loss": 0.0033,
      "step": 66450
    },
    {
      "epoch": 19.12568306010929,
      "grad_norm": 0.0002621403837110847,
      "learning_rate": 0.0006174920908829451,
      "loss": 0.0023,
      "step": 66500
    },
    {
      "epoch": 19.14006327293644,
      "grad_norm": 8.335158054251224e-05,
      "learning_rate": 0.000617204486626402,
      "loss": 0.0061,
      "step": 66550
    },
    {
      "epoch": 19.15444348576359,
      "grad_norm": 0.005646935664117336,
      "learning_rate": 0.000616916882369859,
      "loss": 0.0013,
      "step": 66600
    },
    {
      "epoch": 19.16882369859074,
      "grad_norm": 0.00024089902581181377,
      "learning_rate": 0.0006166292781133161,
      "loss": 0.0024,
      "step": 66650
    },
    {
      "epoch": 19.183203911417888,
      "grad_norm": 9.514016710454598e-05,
      "learning_rate": 0.0006163416738567731,
      "loss": 0.0012,
      "step": 66700
    },
    {
      "epoch": 19.19758412424504,
      "grad_norm": 4.000999251729809e-05,
      "learning_rate": 0.0006160540696002301,
      "loss": 0.0025,
      "step": 66750
    },
    {
      "epoch": 19.21196433707219,
      "grad_norm": 0.011637515388429165,
      "learning_rate": 0.0006157664653436871,
      "loss": 0.0045,
      "step": 66800
    },
    {
      "epoch": 19.226344549899338,
      "grad_norm": 0.00035210608621127903,
      "learning_rate": 0.0006154788610871441,
      "loss": 0.0013,
      "step": 66850
    },
    {
      "epoch": 19.240724762726487,
      "grad_norm": 0.0008939633262343705,
      "learning_rate": 0.000615191256830601,
      "loss": 0.0038,
      "step": 66900
    },
    {
      "epoch": 19.25510497555364,
      "grad_norm": 9.036174014909193e-05,
      "learning_rate": 0.0006149036525740581,
      "loss": 0.0039,
      "step": 66950
    },
    {
      "epoch": 19.269485188380788,
      "grad_norm": 4.4674641685560346e-05,
      "learning_rate": 0.0006146160483175151,
      "loss": 0.0023,
      "step": 67000
    },
    {
      "epoch": 19.283865401207937,
      "grad_norm": 0.00012077621067874134,
      "learning_rate": 0.000614328444060972,
      "loss": 0.0013,
      "step": 67050
    },
    {
      "epoch": 19.29824561403509,
      "grad_norm": 0.017911193892359734,
      "learning_rate": 0.0006140408398044292,
      "loss": 0.0024,
      "step": 67100
    },
    {
      "epoch": 19.312625826862238,
      "grad_norm": 3.726532304426655e-05,
      "learning_rate": 0.0006137532355478861,
      "loss": 0.0026,
      "step": 67150
    },
    {
      "epoch": 19.327006039689387,
      "grad_norm": 0.005830354057252407,
      "learning_rate": 0.0006134656312913431,
      "loss": 0.0035,
      "step": 67200
    },
    {
      "epoch": 19.341386252516536,
      "grad_norm": 0.00042786833364516497,
      "learning_rate": 0.0006131780270348001,
      "loss": 0.003,
      "step": 67250
    },
    {
      "epoch": 19.355766465343688,
      "grad_norm": 6.648741691606119e-05,
      "learning_rate": 0.0006128904227782571,
      "loss": 0.0011,
      "step": 67300
    },
    {
      "epoch": 19.370146678170837,
      "grad_norm": 5.197141945245676e-05,
      "learning_rate": 0.0006126028185217141,
      "loss": 0.0002,
      "step": 67350
    },
    {
      "epoch": 19.384526890997986,
      "grad_norm": 0.009594653733074665,
      "learning_rate": 0.0006123152142651711,
      "loss": 0.0028,
      "step": 67400
    },
    {
      "epoch": 19.398907103825138,
      "grad_norm": 0.00027098777354694903,
      "learning_rate": 0.0006120276100086282,
      "loss": 0.0046,
      "step": 67450
    },
    {
      "epoch": 19.413287316652287,
      "grad_norm": 0.021539771929383278,
      "learning_rate": 0.0006117400057520851,
      "loss": 0.0031,
      "step": 67500
    },
    {
      "epoch": 19.427667529479436,
      "grad_norm": 0.00040652777533978224,
      "learning_rate": 0.0006114524014955422,
      "loss": 0.0017,
      "step": 67550
    },
    {
      "epoch": 19.442047742306585,
      "grad_norm": 0.005941498558968306,
      "learning_rate": 0.0006111647972389992,
      "loss": 0.0027,
      "step": 67600
    },
    {
      "epoch": 19.456427955133737,
      "grad_norm": 0.002126981969922781,
      "learning_rate": 0.0006108771929824561,
      "loss": 0.0015,
      "step": 67650
    },
    {
      "epoch": 19.470808167960886,
      "grad_norm": 4.649277252610773e-05,
      "learning_rate": 0.0006105895887259132,
      "loss": 0.0016,
      "step": 67700
    },
    {
      "epoch": 19.485188380788035,
      "grad_norm": 0.0003527673543430865,
      "learning_rate": 0.0006103019844693701,
      "loss": 0.0014,
      "step": 67750
    },
    {
      "epoch": 19.499568593615187,
      "grad_norm": 5.157006671652198e-05,
      "learning_rate": 0.0006100143802128271,
      "loss": 0.0056,
      "step": 67800
    },
    {
      "epoch": 19.513948806442336,
      "grad_norm": 0.0011770298006013036,
      "learning_rate": 0.0006097267759562841,
      "loss": 0.0037,
      "step": 67850
    },
    {
      "epoch": 19.528329019269485,
      "grad_norm": 0.00023496591893490404,
      "learning_rate": 0.0006094391716997412,
      "loss": 0.0021,
      "step": 67900
    },
    {
      "epoch": 19.542709232096634,
      "grad_norm": 0.00034044301719404757,
      "learning_rate": 0.0006091515674431982,
      "loss": 0.0012,
      "step": 67950
    },
    {
      "epoch": 19.557089444923786,
      "grad_norm": 4.4490610889624804e-05,
      "learning_rate": 0.0006088639631866552,
      "loss": 0.001,
      "step": 68000
    },
    {
      "epoch": 19.571469657750935,
      "grad_norm": 0.001051299856044352,
      "learning_rate": 0.0006085763589301122,
      "loss": 0.0028,
      "step": 68050
    },
    {
      "epoch": 19.585849870578084,
      "grad_norm": 0.0031426523346453905,
      "learning_rate": 0.0006082887546735691,
      "loss": 0.0028,
      "step": 68100
    },
    {
      "epoch": 19.600230083405233,
      "grad_norm": 0.0017608033958822489,
      "learning_rate": 0.0006080011504170262,
      "loss": 0.0034,
      "step": 68150
    },
    {
      "epoch": 19.614610296232385,
      "grad_norm": 9.115896682487801e-05,
      "learning_rate": 0.0006077135461604831,
      "loss": 0.0031,
      "step": 68200
    },
    {
      "epoch": 19.628990509059534,
      "grad_norm": 0.07035233825445175,
      "learning_rate": 0.0006074259419039401,
      "loss": 0.0017,
      "step": 68250
    },
    {
      "epoch": 19.643370721886683,
      "grad_norm": 0.0003657073830254376,
      "learning_rate": 0.0006071383376473973,
      "loss": 0.0054,
      "step": 68300
    },
    {
      "epoch": 19.657750934713835,
      "grad_norm": 0.008289340883493423,
      "learning_rate": 0.0006068507333908542,
      "loss": 0.0008,
      "step": 68350
    },
    {
      "epoch": 19.672131147540984,
      "grad_norm": 0.00010949767602141947,
      "learning_rate": 0.0006065631291343112,
      "loss": 0.0056,
      "step": 68400
    },
    {
      "epoch": 19.686511360368133,
      "grad_norm": 0.06526459008455276,
      "learning_rate": 0.0006062755248777682,
      "loss": 0.0014,
      "step": 68450
    },
    {
      "epoch": 19.700891573195282,
      "grad_norm": 0.00043895686394535005,
      "learning_rate": 0.0006059879206212252,
      "loss": 0.001,
      "step": 68500
    },
    {
      "epoch": 19.715271786022434,
      "grad_norm": 9.032081288751215e-05,
      "learning_rate": 0.0006057003163646822,
      "loss": 0.0025,
      "step": 68550
    },
    {
      "epoch": 19.729651998849583,
      "grad_norm": 0.00345592456869781,
      "learning_rate": 0.0006054127121081392,
      "loss": 0.0012,
      "step": 68600
    },
    {
      "epoch": 19.744032211676732,
      "grad_norm": 5.324922676663846e-05,
      "learning_rate": 0.0006051251078515962,
      "loss": 0.0017,
      "step": 68650
    },
    {
      "epoch": 19.758412424503884,
      "grad_norm": 0.00012332315964158624,
      "learning_rate": 0.0006048375035950531,
      "loss": 0.0015,
      "step": 68700
    },
    {
      "epoch": 19.772792637331033,
      "grad_norm": 0.008373026736080647,
      "learning_rate": 0.0006045498993385103,
      "loss": 0.0026,
      "step": 68750
    },
    {
      "epoch": 19.787172850158182,
      "grad_norm": 0.035059183835983276,
      "learning_rate": 0.0006042622950819672,
      "loss": 0.001,
      "step": 68800
    },
    {
      "epoch": 19.80155306298533,
      "grad_norm": 7.336674752878025e-05,
      "learning_rate": 0.0006039746908254242,
      "loss": 0.0029,
      "step": 68850
    },
    {
      "epoch": 19.815933275812483,
      "grad_norm": 8.084151340881363e-05,
      "learning_rate": 0.0006036870865688813,
      "loss": 0.0034,
      "step": 68900
    },
    {
      "epoch": 19.830313488639632,
      "grad_norm": 0.017570486292243004,
      "learning_rate": 0.0006033994823123382,
      "loss": 0.0012,
      "step": 68950
    },
    {
      "epoch": 19.84469370146678,
      "grad_norm": 3.498164369375445e-05,
      "learning_rate": 0.0006031118780557952,
      "loss": 0.0022,
      "step": 69000
    },
    {
      "epoch": 19.85907391429393,
      "grad_norm": 5.368467100197449e-05,
      "learning_rate": 0.0006028242737992522,
      "loss": 0.0056,
      "step": 69050
    },
    {
      "epoch": 19.873454127121082,
      "grad_norm": 4.669449481298216e-05,
      "learning_rate": 0.0006025366695427092,
      "loss": 0.0012,
      "step": 69100
    },
    {
      "epoch": 19.88783433994823,
      "grad_norm": 0.0008740182383917272,
      "learning_rate": 0.0006022490652861662,
      "loss": 0.0047,
      "step": 69150
    },
    {
      "epoch": 19.90221455277538,
      "grad_norm": 0.002979670651257038,
      "learning_rate": 0.0006019614610296233,
      "loss": 0.0017,
      "step": 69200
    },
    {
      "epoch": 19.916594765602532,
      "grad_norm": 0.0004127979918848723,
      "learning_rate": 0.0006016738567730803,
      "loss": 0.0039,
      "step": 69250
    },
    {
      "epoch": 19.93097497842968,
      "grad_norm": 0.0002246805961476639,
      "learning_rate": 0.0006013862525165372,
      "loss": 0.0021,
      "step": 69300
    },
    {
      "epoch": 19.94535519125683,
      "grad_norm": 0.0004002712666988373,
      "learning_rate": 0.0006010986482599943,
      "loss": 0.004,
      "step": 69350
    },
    {
      "epoch": 19.95973540408398,
      "grad_norm": 0.002819082699716091,
      "learning_rate": 0.0006008110440034512,
      "loss": 0.0032,
      "step": 69400
    },
    {
      "epoch": 19.97411561691113,
      "grad_norm": 4.676010939874686e-05,
      "learning_rate": 0.0006005234397469082,
      "loss": 0.0031,
      "step": 69450
    },
    {
      "epoch": 19.98849582973828,
      "grad_norm": 0.011261326260864735,
      "learning_rate": 0.0006002358354903653,
      "loss": 0.0019,
      "step": 69500
    },
    {
      "epoch": 20.0,
      "eval_loss": 0.011390823870897293,
      "eval_runtime": 16.7102,
      "eval_samples_per_second": 2855.688,
      "eval_steps_per_second": 44.644,
      "step": 69540
    },
    {
      "epoch": 20.00287604256543,
      "grad_norm": 0.02398146688938141,
      "learning_rate": 0.0005999482312338222,
      "loss": 0.0038,
      "step": 69550
    },
    {
      "epoch": 20.01725625539258,
      "grad_norm": 0.021375928074121475,
      "learning_rate": 0.0005996606269772792,
      "loss": 0.0015,
      "step": 69600
    },
    {
      "epoch": 20.03163646821973,
      "grad_norm": 0.1426592618227005,
      "learning_rate": 0.0005993730227207363,
      "loss": 0.0024,
      "step": 69650
    },
    {
      "epoch": 20.04601668104688,
      "grad_norm": 0.0004604344430845231,
      "learning_rate": 0.0005990854184641933,
      "loss": 0.0014,
      "step": 69700
    },
    {
      "epoch": 20.060396893874028,
      "grad_norm": 0.0001858213945524767,
      "learning_rate": 0.0005987978142076504,
      "loss": 0.0015,
      "step": 69750
    },
    {
      "epoch": 20.07477710670118,
      "grad_norm": 7.179418753366917e-05,
      "learning_rate": 0.0005985102099511073,
      "loss": 0.0026,
      "step": 69800
    },
    {
      "epoch": 20.08915731952833,
      "grad_norm": 0.00025724145234562457,
      "learning_rate": 0.0005982226056945643,
      "loss": 0.0043,
      "step": 69850
    },
    {
      "epoch": 20.103537532355478,
      "grad_norm": 0.0008292751153931022,
      "learning_rate": 0.0005979350014380212,
      "loss": 0.0062,
      "step": 69900
    },
    {
      "epoch": 20.117917745182627,
      "grad_norm": 0.0004982547252438962,
      "learning_rate": 0.0005976473971814783,
      "loss": 0.003,
      "step": 69950
    },
    {
      "epoch": 20.13229795800978,
      "grad_norm": 0.006532896775752306,
      "learning_rate": 0.0005973597929249352,
      "loss": 0.0022,
      "step": 70000
    },
    {
      "epoch": 20.14667817083693,
      "grad_norm": 0.009016868658363819,
      "learning_rate": 0.0005970721886683922,
      "loss": 0.0016,
      "step": 70050
    },
    {
      "epoch": 20.161058383664077,
      "grad_norm": 3.292776455054991e-05,
      "learning_rate": 0.0005967845844118494,
      "loss": 0.0013,
      "step": 70100
    },
    {
      "epoch": 20.17543859649123,
      "grad_norm": 0.024843890219926834,
      "learning_rate": 0.0005964969801553063,
      "loss": 0.003,
      "step": 70150
    },
    {
      "epoch": 20.18981880931838,
      "grad_norm": 0.00032740889582782984,
      "learning_rate": 0.0005962093758987634,
      "loss": 0.0011,
      "step": 70200
    },
    {
      "epoch": 20.204199022145527,
      "grad_norm": 0.00020387186668813229,
      "learning_rate": 0.0005959217716422203,
      "loss": 0.0014,
      "step": 70250
    },
    {
      "epoch": 20.218579234972676,
      "grad_norm": 0.0007350905798375607,
      "learning_rate": 0.0005956341673856773,
      "loss": 0.0021,
      "step": 70300
    },
    {
      "epoch": 20.23295944779983,
      "grad_norm": 9.636926552047953e-05,
      "learning_rate": 0.0005953465631291344,
      "loss": 0.0038,
      "step": 70350
    },
    {
      "epoch": 20.247339660626977,
      "grad_norm": 0.00031555703026242554,
      "learning_rate": 0.0005950589588725913,
      "loss": 0.0018,
      "step": 70400
    },
    {
      "epoch": 20.261719873454126,
      "grad_norm": 0.006698841229081154,
      "learning_rate": 0.0005947713546160483,
      "loss": 0.0011,
      "step": 70450
    },
    {
      "epoch": 20.27610008628128,
      "grad_norm": 0.0018858356634154916,
      "learning_rate": 0.0005944837503595053,
      "loss": 0.0042,
      "step": 70500
    },
    {
      "epoch": 20.290480299108427,
      "grad_norm": 0.00036746790283359587,
      "learning_rate": 0.0005941961461029624,
      "loss": 0.0021,
      "step": 70550
    },
    {
      "epoch": 20.304860511935576,
      "grad_norm": 8.794394670985639e-05,
      "learning_rate": 0.0005939085418464193,
      "loss": 0.0015,
      "step": 70600
    },
    {
      "epoch": 20.319240724762725,
      "grad_norm": 0.003847428597509861,
      "learning_rate": 0.0005936209375898764,
      "loss": 0.0033,
      "step": 70650
    },
    {
      "epoch": 20.333620937589878,
      "grad_norm": 0.0009312496986240149,
      "learning_rate": 0.0005933333333333334,
      "loss": 0.0049,
      "step": 70700
    },
    {
      "epoch": 20.348001150417026,
      "grad_norm": 4.6258508518803865e-05,
      "learning_rate": 0.0005930457290767903,
      "loss": 0.0047,
      "step": 70750
    },
    {
      "epoch": 20.362381363244175,
      "grad_norm": 0.0349765419960022,
      "learning_rate": 0.0005927581248202474,
      "loss": 0.0024,
      "step": 70800
    },
    {
      "epoch": 20.376761576071324,
      "grad_norm": 0.031014051288366318,
      "learning_rate": 0.0005924705205637043,
      "loss": 0.0046,
      "step": 70850
    },
    {
      "epoch": 20.391141788898477,
      "grad_norm": 7.261813880177215e-05,
      "learning_rate": 0.0005921829163071613,
      "loss": 0.0021,
      "step": 70900
    },
    {
      "epoch": 20.405522001725625,
      "grad_norm": 0.0013134335167706013,
      "learning_rate": 0.0005918953120506183,
      "loss": 0.0026,
      "step": 70950
    },
    {
      "epoch": 20.419902214552774,
      "grad_norm": 0.03801315650343895,
      "learning_rate": 0.0005916077077940754,
      "loss": 0.0017,
      "step": 71000
    },
    {
      "epoch": 20.434282427379927,
      "grad_norm": 0.00021247993572615087,
      "learning_rate": 0.0005913201035375324,
      "loss": 0.0033,
      "step": 71050
    },
    {
      "epoch": 20.448662640207075,
      "grad_norm": 4.4859112676931545e-05,
      "learning_rate": 0.0005910324992809894,
      "loss": 0.0007,
      "step": 71100
    },
    {
      "epoch": 20.463042853034224,
      "grad_norm": 0.0067259385250508785,
      "learning_rate": 0.0005907448950244464,
      "loss": 0.001,
      "step": 71150
    },
    {
      "epoch": 20.477423065861373,
      "grad_norm": 0.000561912078410387,
      "learning_rate": 0.0005904572907679033,
      "loss": 0.0034,
      "step": 71200
    },
    {
      "epoch": 20.491803278688526,
      "grad_norm": 0.011320792138576508,
      "learning_rate": 0.0005901696865113604,
      "loss": 0.0032,
      "step": 71250
    },
    {
      "epoch": 20.506183491515674,
      "grad_norm": 0.00013621531252283603,
      "learning_rate": 0.0005898820822548174,
      "loss": 0.0011,
      "step": 71300
    },
    {
      "epoch": 20.520563704342823,
      "grad_norm": 0.03463377058506012,
      "learning_rate": 0.0005895944779982744,
      "loss": 0.0019,
      "step": 71350
    },
    {
      "epoch": 20.534943917169976,
      "grad_norm": 0.06420747935771942,
      "learning_rate": 0.0005893068737417315,
      "loss": 0.0019,
      "step": 71400
    },
    {
      "epoch": 20.549324129997125,
      "grad_norm": 0.0006851740181446075,
      "learning_rate": 0.0005890192694851884,
      "loss": 0.0019,
      "step": 71450
    },
    {
      "epoch": 20.563704342824273,
      "grad_norm": 0.00021655923046637326,
      "learning_rate": 0.0005887316652286454,
      "loss": 0.002,
      "step": 71500
    },
    {
      "epoch": 20.578084555651422,
      "grad_norm": 0.0001633194915484637,
      "learning_rate": 0.0005884440609721024,
      "loss": 0.0017,
      "step": 71550
    },
    {
      "epoch": 20.592464768478575,
      "grad_norm": 0.0008354855817742646,
      "learning_rate": 0.0005881564567155594,
      "loss": 0.0021,
      "step": 71600
    },
    {
      "epoch": 20.606844981305724,
      "grad_norm": 6.727512663928792e-05,
      "learning_rate": 0.0005878688524590164,
      "loss": 0.0037,
      "step": 71650
    },
    {
      "epoch": 20.621225194132872,
      "grad_norm": 0.017029698938131332,
      "learning_rate": 0.0005875812482024734,
      "loss": 0.0045,
      "step": 71700
    },
    {
      "epoch": 20.635605406960025,
      "grad_norm": 0.00018866494065150619,
      "learning_rate": 0.0005872936439459304,
      "loss": 0.0016,
      "step": 71750
    },
    {
      "epoch": 20.649985619787174,
      "grad_norm": 0.00023751931439619511,
      "learning_rate": 0.0005870060396893874,
      "loss": 0.0042,
      "step": 71800
    },
    {
      "epoch": 20.664365832614322,
      "grad_norm": 7.412234845105559e-05,
      "learning_rate": 0.0005867184354328445,
      "loss": 0.001,
      "step": 71850
    },
    {
      "epoch": 20.67874604544147,
      "grad_norm": 0.007997879758477211,
      "learning_rate": 0.0005864308311763015,
      "loss": 0.0035,
      "step": 71900
    },
    {
      "epoch": 20.693126258268624,
      "grad_norm": 0.010832371190190315,
      "learning_rate": 0.0005861432269197584,
      "loss": 0.0008,
      "step": 71950
    },
    {
      "epoch": 20.707506471095773,
      "grad_norm": 0.00016257091192528605,
      "learning_rate": 0.0005858556226632155,
      "loss": 0.0013,
      "step": 72000
    },
    {
      "epoch": 20.72188668392292,
      "grad_norm": 5.168944699107669e-05,
      "learning_rate": 0.0005855680184066724,
      "loss": 0.001,
      "step": 72050
    },
    {
      "epoch": 20.73626689675007,
      "grad_norm": 0.0005294102593325078,
      "learning_rate": 0.0005852804141501294,
      "loss": 0.0035,
      "step": 72100
    },
    {
      "epoch": 20.750647109577223,
      "grad_norm": 0.0018681060755625367,
      "learning_rate": 0.0005849928098935864,
      "loss": 0.0032,
      "step": 72150
    },
    {
      "epoch": 20.76502732240437,
      "grad_norm": 0.0003635311149992049,
      "learning_rate": 0.0005847052056370435,
      "loss": 0.0032,
      "step": 72200
    },
    {
      "epoch": 20.77940753523152,
      "grad_norm": 0.06094362214207649,
      "learning_rate": 0.0005844176013805005,
      "loss": 0.0006,
      "step": 72250
    },
    {
      "epoch": 20.793787748058673,
      "grad_norm": 0.0013100570067763329,
      "learning_rate": 0.0005841299971239575,
      "loss": 0.0067,
      "step": 72300
    },
    {
      "epoch": 20.80816796088582,
      "grad_norm": 0.000143543365993537,
      "learning_rate": 0.0005838423928674145,
      "loss": 0.0016,
      "step": 72350
    },
    {
      "epoch": 20.82254817371297,
      "grad_norm": 5.021523611503653e-05,
      "learning_rate": 0.0005835547886108714,
      "loss": 0.0028,
      "step": 72400
    },
    {
      "epoch": 20.83692838654012,
      "grad_norm": 5.568801861954853e-05,
      "learning_rate": 0.0005832671843543285,
      "loss": 0.0038,
      "step": 72450
    },
    {
      "epoch": 20.85130859936727,
      "grad_norm": 0.0006514082779176533,
      "learning_rate": 0.0005829795800977855,
      "loss": 0.0035,
      "step": 72500
    },
    {
      "epoch": 20.86568881219442,
      "grad_norm": 0.027603717520833015,
      "learning_rate": 0.0005826919758412424,
      "loss": 0.0027,
      "step": 72550
    },
    {
      "epoch": 20.88006902502157,
      "grad_norm": 2.7521549782250077e-05,
      "learning_rate": 0.0005824043715846995,
      "loss": 0.0005,
      "step": 72600
    },
    {
      "epoch": 20.894449237848722,
      "grad_norm": 0.05227629467844963,
      "learning_rate": 0.0005821167673281565,
      "loss": 0.0014,
      "step": 72650
    },
    {
      "epoch": 20.90882945067587,
      "grad_norm": 0.00014448548608925194,
      "learning_rate": 0.0005818291630716135,
      "loss": 0.0008,
      "step": 72700
    },
    {
      "epoch": 20.92320966350302,
      "grad_norm": 0.008604593575000763,
      "learning_rate": 0.0005815415588150705,
      "loss": 0.0027,
      "step": 72750
    },
    {
      "epoch": 20.93758987633017,
      "grad_norm": 0.00048243542551063,
      "learning_rate": 0.0005812539545585275,
      "loss": 0.0012,
      "step": 72800
    },
    {
      "epoch": 20.95197008915732,
      "grad_norm": 0.02516039088368416,
      "learning_rate": 0.0005809663503019845,
      "loss": 0.005,
      "step": 72850
    },
    {
      "epoch": 20.96635030198447,
      "grad_norm": 0.010742731392383575,
      "learning_rate": 0.0005806787460454415,
      "loss": 0.0019,
      "step": 72900
    },
    {
      "epoch": 20.98073051481162,
      "grad_norm": 0.021261140704154968,
      "learning_rate": 0.0005803911417888985,
      "loss": 0.007,
      "step": 72950
    },
    {
      "epoch": 20.995110727638767,
      "grad_norm": 0.012547492980957031,
      "learning_rate": 0.0005801035375323554,
      "loss": 0.0015,
      "step": 73000
    },
    {
      "epoch": 21.0,
      "eval_loss": 0.01032276265323162,
      "eval_runtime": 16.8697,
      "eval_samples_per_second": 2828.683,
      "eval_steps_per_second": 44.221,
      "step": 73017
    },
    {
      "epoch": 21.00949094046592,
      "grad_norm": 0.001999099738895893,
      "learning_rate": 0.0005798159332758126,
      "loss": 0.0023,
      "step": 73050
    },
    {
      "epoch": 21.02387115329307,
      "grad_norm": 0.0005161383305676281,
      "learning_rate": 0.0005795283290192696,
      "loss": 0.001,
      "step": 73100
    },
    {
      "epoch": 21.038251366120218,
      "grad_norm": 0.0010020375484600663,
      "learning_rate": 0.0005792407247627265,
      "loss": 0.0017,
      "step": 73150
    },
    {
      "epoch": 21.05263157894737,
      "grad_norm": 8.958294347394258e-05,
      "learning_rate": 0.0005789531205061836,
      "loss": 0.0055,
      "step": 73200
    },
    {
      "epoch": 21.06701179177452,
      "grad_norm": 0.05133955925703049,
      "learning_rate": 0.0005786655162496405,
      "loss": 0.002,
      "step": 73250
    },
    {
      "epoch": 21.081392004601668,
      "grad_norm": 0.01762956753373146,
      "learning_rate": 0.0005783779119930975,
      "loss": 0.0021,
      "step": 73300
    },
    {
      "epoch": 21.095772217428816,
      "grad_norm": 4.520924267126247e-05,
      "learning_rate": 0.0005780903077365545,
      "loss": 0.0027,
      "step": 73350
    },
    {
      "epoch": 21.11015243025597,
      "grad_norm": 0.017829498276114464,
      "learning_rate": 0.0005778027034800115,
      "loss": 0.0035,
      "step": 73400
    },
    {
      "epoch": 21.124532643083118,
      "grad_norm": 0.01965201273560524,
      "learning_rate": 0.0005775150992234685,
      "loss": 0.0024,
      "step": 73450
    },
    {
      "epoch": 21.138912855910267,
      "grad_norm": 0.00013084130478091538,
      "learning_rate": 0.0005772274949669256,
      "loss": 0.0022,
      "step": 73500
    },
    {
      "epoch": 21.15329306873742,
      "grad_norm": 0.02769005484879017,
      "learning_rate": 0.0005769398907103826,
      "loss": 0.0048,
      "step": 73550
    },
    {
      "epoch": 21.167673281564568,
      "grad_norm": 0.00010807470243889838,
      "learning_rate": 0.0005766522864538395,
      "loss": 0.0013,
      "step": 73600
    },
    {
      "epoch": 21.182053494391717,
      "grad_norm": 0.02265099808573723,
      "learning_rate": 0.0005763646821972966,
      "loss": 0.0013,
      "step": 73650
    },
    {
      "epoch": 21.196433707218866,
      "grad_norm": 0.00733977323397994,
      "learning_rate": 0.0005760770779407535,
      "loss": 0.0031,
      "step": 73700
    },
    {
      "epoch": 21.210813920046018,
      "grad_norm": 0.0017578941769897938,
      "learning_rate": 0.0005757894736842105,
      "loss": 0.0025,
      "step": 73750
    },
    {
      "epoch": 21.225194132873167,
      "grad_norm": 0.00023581256391480565,
      "learning_rate": 0.0005755018694276676,
      "loss": 0.0052,
      "step": 73800
    },
    {
      "epoch": 21.239574345700316,
      "grad_norm": 0.00013241068518254906,
      "learning_rate": 0.0005752142651711245,
      "loss": 0.0013,
      "step": 73850
    },
    {
      "epoch": 21.253954558527465,
      "grad_norm": 8.728561078896746e-05,
      "learning_rate": 0.0005749266609145815,
      "loss": 0.0033,
      "step": 73900
    },
    {
      "epoch": 21.268334771354617,
      "grad_norm": 0.00042878190288320184,
      "learning_rate": 0.0005746390566580386,
      "loss": 0.0012,
      "step": 73950
    },
    {
      "epoch": 21.282714984181766,
      "grad_norm": 6.39871577732265e-05,
      "learning_rate": 0.0005743514524014956,
      "loss": 0.0011,
      "step": 74000
    },
    {
      "epoch": 21.297095197008915,
      "grad_norm": 0.0002412342291790992,
      "learning_rate": 0.0005740638481449526,
      "loss": 0.001,
      "step": 74050
    },
    {
      "epoch": 21.311475409836067,
      "grad_norm": 0.012208182364702225,
      "learning_rate": 0.0005737762438884096,
      "loss": 0.0027,
      "step": 74100
    },
    {
      "epoch": 21.325855622663216,
      "grad_norm": 8.0803845776245e-05,
      "learning_rate": 0.0005734886396318666,
      "loss": 0.002,
      "step": 74150
    },
    {
      "epoch": 21.340235835490365,
      "grad_norm": 0.0002079945697914809,
      "learning_rate": 0.0005732010353753235,
      "loss": 0.0034,
      "step": 74200
    },
    {
      "epoch": 21.354616048317514,
      "grad_norm": 4.813099803868681e-05,
      "learning_rate": 0.0005729134311187806,
      "loss": 0.0011,
      "step": 74250
    },
    {
      "epoch": 21.368996261144666,
      "grad_norm": 5.3478321206057444e-05,
      "learning_rate": 0.0005726258268622375,
      "loss": 0.0005,
      "step": 74300
    },
    {
      "epoch": 21.383376473971815,
      "grad_norm": 0.021436693146824837,
      "learning_rate": 0.0005723382226056945,
      "loss": 0.0018,
      "step": 74350
    },
    {
      "epoch": 21.397756686798964,
      "grad_norm": 0.00044139634701423347,
      "learning_rate": 0.0005720506183491517,
      "loss": 0.005,
      "step": 74400
    },
    {
      "epoch": 21.412136899626116,
      "grad_norm": 0.0001901994546642527,
      "learning_rate": 0.0005717630140926086,
      "loss": 0.0034,
      "step": 74450
    },
    {
      "epoch": 21.426517112453265,
      "grad_norm": 9.245896944776177e-05,
      "learning_rate": 0.0005714754098360656,
      "loss": 0.0007,
      "step": 74500
    },
    {
      "epoch": 21.440897325280414,
      "grad_norm": 0.00013210908218752593,
      "learning_rate": 0.0005711878055795226,
      "loss": 0.0032,
      "step": 74550
    },
    {
      "epoch": 21.455277538107563,
      "grad_norm": 0.0001495608885306865,
      "learning_rate": 0.0005709002013229796,
      "loss": 0.0015,
      "step": 74600
    },
    {
      "epoch": 21.469657750934715,
      "grad_norm": 0.0006947072688490152,
      "learning_rate": 0.0005706125970664366,
      "loss": 0.0021,
      "step": 74650
    },
    {
      "epoch": 21.484037963761864,
      "grad_norm": 4.6207569539546967e-05,
      "learning_rate": 0.0005703249928098936,
      "loss": 0.002,
      "step": 74700
    },
    {
      "epoch": 21.498418176589013,
      "grad_norm": 0.012906714342534542,
      "learning_rate": 0.0005700373885533506,
      "loss": 0.0015,
      "step": 74750
    },
    {
      "epoch": 21.51279838941616,
      "grad_norm": 3.4841523302020505e-05,
      "learning_rate": 0.0005697497842968075,
      "loss": 0.0042,
      "step": 74800
    },
    {
      "epoch": 21.527178602243314,
      "grad_norm": 6.366468733176589e-05,
      "learning_rate": 0.0005694621800402647,
      "loss": 0.0015,
      "step": 74850
    },
    {
      "epoch": 21.541558815070463,
      "grad_norm": 0.00018227369582746178,
      "learning_rate": 0.0005691745757837216,
      "loss": 0.0003,
      "step": 74900
    },
    {
      "epoch": 21.55593902789761,
      "grad_norm": 0.001414331141859293,
      "learning_rate": 0.0005688869715271786,
      "loss": 0.0036,
      "step": 74950
    },
    {
      "epoch": 21.570319240724764,
      "grad_norm": 0.006012971978634596,
      "learning_rate": 0.0005685993672706357,
      "loss": 0.0021,
      "step": 75000
    },
    {
      "epoch": 21.584699453551913,
      "grad_norm": 0.00022604063269682229,
      "learning_rate": 0.0005683117630140926,
      "loss": 0.0039,
      "step": 75050
    },
    {
      "epoch": 21.599079666379062,
      "grad_norm": 9.469476208323613e-05,
      "learning_rate": 0.0005680241587575496,
      "loss": 0.0023,
      "step": 75100
    },
    {
      "epoch": 21.61345987920621,
      "grad_norm": 8.050002361414954e-05,
      "learning_rate": 0.0005677365545010066,
      "loss": 0.0031,
      "step": 75150
    },
    {
      "epoch": 21.627840092033363,
      "grad_norm": 0.04658273980021477,
      "learning_rate": 0.0005674489502444636,
      "loss": 0.0006,
      "step": 75200
    },
    {
      "epoch": 21.642220304860512,
      "grad_norm": 0.00287332059815526,
      "learning_rate": 0.0005671613459879207,
      "loss": 0.0034,
      "step": 75250
    },
    {
      "epoch": 21.65660051768766,
      "grad_norm": 0.000672814145218581,
      "learning_rate": 0.0005668737417313777,
      "loss": 0.0013,
      "step": 75300
    },
    {
      "epoch": 21.670980730514813,
      "grad_norm": 0.000198341891518794,
      "learning_rate": 0.0005665861374748347,
      "loss": 0.0035,
      "step": 75350
    },
    {
      "epoch": 21.685360943341962,
      "grad_norm": 0.0002742742362897843,
      "learning_rate": 0.0005662985332182916,
      "loss": 0.0017,
      "step": 75400
    },
    {
      "epoch": 21.69974115616911,
      "grad_norm": 3.490657400107011e-05,
      "learning_rate": 0.0005660109289617487,
      "loss": 0.0018,
      "step": 75450
    },
    {
      "epoch": 21.71412136899626,
      "grad_norm": 0.005669700913131237,
      "learning_rate": 0.0005657233247052056,
      "loss": 0.0043,
      "step": 75500
    },
    {
      "epoch": 21.728501581823412,
      "grad_norm": 6.223546370165423e-05,
      "learning_rate": 0.0005654357204486626,
      "loss": 0.0039,
      "step": 75550
    },
    {
      "epoch": 21.74288179465056,
      "grad_norm": 0.00026163854636251926,
      "learning_rate": 0.0005651481161921197,
      "loss": 0.0022,
      "step": 75600
    },
    {
      "epoch": 21.75726200747771,
      "grad_norm": 0.015827009454369545,
      "learning_rate": 0.0005648605119355766,
      "loss": 0.0033,
      "step": 75650
    },
    {
      "epoch": 21.771642220304862,
      "grad_norm": 0.002896219491958618,
      "learning_rate": 0.0005645729076790337,
      "loss": 0.0011,
      "step": 75700
    },
    {
      "epoch": 21.78602243313201,
      "grad_norm": 0.022408269345760345,
      "learning_rate": 0.0005642853034224907,
      "loss": 0.0032,
      "step": 75750
    },
    {
      "epoch": 21.80040264595916,
      "grad_norm": 7.297988486243412e-05,
      "learning_rate": 0.0005639976991659477,
      "loss": 0.0024,
      "step": 75800
    },
    {
      "epoch": 21.81478285878631,
      "grad_norm": 0.03533713147044182,
      "learning_rate": 0.0005637100949094047,
      "loss": 0.0053,
      "step": 75850
    },
    {
      "epoch": 21.82916307161346,
      "grad_norm": 0.005500136408954859,
      "learning_rate": 0.0005634224906528617,
      "loss": 0.0015,
      "step": 75900
    },
    {
      "epoch": 21.84354328444061,
      "grad_norm": 0.0010666142916306853,
      "learning_rate": 0.0005631348863963187,
      "loss": 0.0013,
      "step": 75950
    },
    {
      "epoch": 21.85792349726776,
      "grad_norm": 6.734504131600261e-05,
      "learning_rate": 0.0005628472821397756,
      "loss": 0.0051,
      "step": 76000
    },
    {
      "epoch": 21.872303710094908,
      "grad_norm": 0.05558865889906883,
      "learning_rate": 0.0005625596778832327,
      "loss": 0.0036,
      "step": 76050
    },
    {
      "epoch": 21.88668392292206,
      "grad_norm": 3.335777364554815e-05,
      "learning_rate": 0.0005622720736266897,
      "loss": 0.0014,
      "step": 76100
    },
    {
      "epoch": 21.90106413574921,
      "grad_norm": 0.00012740127567667514,
      "learning_rate": 0.0005619844693701467,
      "loss": 0.0046,
      "step": 76150
    },
    {
      "epoch": 21.915444348576358,
      "grad_norm": 0.051688939332962036,
      "learning_rate": 0.0005616968651136038,
      "loss": 0.0011,
      "step": 76200
    },
    {
      "epoch": 21.92982456140351,
      "grad_norm": 0.00020524646970443428,
      "learning_rate": 0.0005614092608570607,
      "loss": 0.0024,
      "step": 76250
    },
    {
      "epoch": 21.94420477423066,
      "grad_norm": 0.0005865413695573807,
      "learning_rate": 0.0005611216566005177,
      "loss": 0.0039,
      "step": 76300
    },
    {
      "epoch": 21.958584987057808,
      "grad_norm": 0.0013208567397668958,
      "learning_rate": 0.0005608340523439747,
      "loss": 0.0029,
      "step": 76350
    },
    {
      "epoch": 21.972965199884957,
      "grad_norm": 0.008073561824858189,
      "learning_rate": 0.0005605464480874317,
      "loss": 0.002,
      "step": 76400
    },
    {
      "epoch": 21.98734541271211,
      "grad_norm": 6.391035276465118e-05,
      "learning_rate": 0.0005602588438308886,
      "loss": 0.0028,
      "step": 76450
    },
    {
      "epoch": 22.0,
      "eval_loss": 0.010879108682274818,
      "eval_runtime": 18.0656,
      "eval_samples_per_second": 2641.424,
      "eval_steps_per_second": 41.294,
      "step": 76494
    },
    {
      "epoch": 22.001725625539258,
      "grad_norm": 0.003928579855710268,
      "learning_rate": 0.0005599712395743457,
      "loss": 0.0026,
      "step": 76500
    },
    {
      "epoch": 22.016105838366407,
      "grad_norm": 1.1027414984710049e-05,
      "learning_rate": 0.0005596836353178028,
      "loss": 0.0042,
      "step": 76550
    },
    {
      "epoch": 22.03048605119356,
      "grad_norm": 0.00542675144970417,
      "learning_rate": 0.0005593960310612597,
      "loss": 0.0018,
      "step": 76600
    },
    {
      "epoch": 22.04486626402071,
      "grad_norm": 0.017601504921913147,
      "learning_rate": 0.0005591084268047168,
      "loss": 0.0043,
      "step": 76650
    },
    {
      "epoch": 22.059246476847857,
      "grad_norm": 0.020146649330854416,
      "learning_rate": 0.0005588208225481737,
      "loss": 0.0021,
      "step": 76700
    },
    {
      "epoch": 22.073626689675006,
      "grad_norm": 6.809255137341097e-05,
      "learning_rate": 0.0005585332182916307,
      "loss": 0.0034,
      "step": 76750
    },
    {
      "epoch": 22.08800690250216,
      "grad_norm": 0.004966170527040958,
      "learning_rate": 0.0005582456140350878,
      "loss": 0.0028,
      "step": 76800
    },
    {
      "epoch": 22.102387115329307,
      "grad_norm": 0.0004802261828444898,
      "learning_rate": 0.0005579580097785447,
      "loss": 0.0028,
      "step": 76850
    },
    {
      "epoch": 22.116767328156456,
      "grad_norm": 0.00404942175373435,
      "learning_rate": 0.0005576704055220017,
      "loss": 0.0009,
      "step": 76900
    },
    {
      "epoch": 22.131147540983605,
      "grad_norm": 0.0005726073286496103,
      "learning_rate": 0.0005573828012654588,
      "loss": 0.004,
      "step": 76950
    },
    {
      "epoch": 22.145527753810757,
      "grad_norm": 0.02372501604259014,
      "learning_rate": 0.0005570951970089158,
      "loss": 0.0026,
      "step": 77000
    },
    {
      "epoch": 22.159907966637906,
      "grad_norm": 0.01386434119194746,
      "learning_rate": 0.0005568075927523727,
      "loss": 0.004,
      "step": 77050
    },
    {
      "epoch": 22.174288179465055,
      "grad_norm": 0.00968886073678732,
      "learning_rate": 0.0005565199884958298,
      "loss": 0.0008,
      "step": 77100
    },
    {
      "epoch": 22.188668392292207,
      "grad_norm": 0.0021125981584191322,
      "learning_rate": 0.0005562323842392868,
      "loss": 0.0013,
      "step": 77150
    },
    {
      "epoch": 22.203048605119356,
      "grad_norm": 0.006177081260830164,
      "learning_rate": 0.0005559447799827437,
      "loss": 0.0005,
      "step": 77200
    },
    {
      "epoch": 22.217428817946505,
      "grad_norm": 0.0010944573441520333,
      "learning_rate": 0.0005556571757262008,
      "loss": 0.0045,
      "step": 77250
    },
    {
      "epoch": 22.231809030773654,
      "grad_norm": 0.00021197544992901385,
      "learning_rate": 0.0005553695714696577,
      "loss": 0.0028,
      "step": 77300
    },
    {
      "epoch": 22.246189243600806,
      "grad_norm": 0.00010621226829243824,
      "learning_rate": 0.0005550819672131147,
      "loss": 0.0042,
      "step": 77350
    },
    {
      "epoch": 22.260569456427955,
      "grad_norm": 0.0014913927298039198,
      "learning_rate": 0.0005547943629565719,
      "loss": 0.0032,
      "step": 77400
    },
    {
      "epoch": 22.274949669255104,
      "grad_norm": 0.0001823817437980324,
      "learning_rate": 0.0005545067587000288,
      "loss": 0.0033,
      "step": 77450
    },
    {
      "epoch": 22.289329882082257,
      "grad_norm": 0.00025331220240332186,
      "learning_rate": 0.0005542191544434858,
      "loss": 0.0053,
      "step": 77500
    },
    {
      "epoch": 22.303710094909405,
      "grad_norm": 0.0002166191698051989,
      "learning_rate": 0.0005539315501869428,
      "loss": 0.0054,
      "step": 77550
    },
    {
      "epoch": 22.318090307736554,
      "grad_norm": 0.00025794110842980444,
      "learning_rate": 0.0005536439459303998,
      "loss": 0.002,
      "step": 77600
    },
    {
      "epoch": 22.332470520563703,
      "grad_norm": 0.00019287185568828136,
      "learning_rate": 0.0005533563416738567,
      "loss": 0.0024,
      "step": 77650
    },
    {
      "epoch": 22.346850733390855,
      "grad_norm": 3.533335620886646e-05,
      "learning_rate": 0.0005530687374173138,
      "loss": 0.0018,
      "step": 77700
    },
    {
      "epoch": 22.361230946218004,
      "grad_norm": 2.899646460718941e-05,
      "learning_rate": 0.0005527811331607708,
      "loss": 0.0017,
      "step": 77750
    },
    {
      "epoch": 22.375611159045153,
      "grad_norm": 0.0003499862796161324,
      "learning_rate": 0.0005524935289042277,
      "loss": 0.0019,
      "step": 77800
    },
    {
      "epoch": 22.389991371872302,
      "grad_norm": 0.00012762760161422193,
      "learning_rate": 0.0005522059246476849,
      "loss": 0.0046,
      "step": 77850
    },
    {
      "epoch": 22.404371584699454,
      "grad_norm": 0.0002789729041978717,
      "learning_rate": 0.0005519183203911418,
      "loss": 0.0011,
      "step": 77900
    },
    {
      "epoch": 22.418751797526603,
      "grad_norm": 0.003887656144797802,
      "learning_rate": 0.0005516307161345988,
      "loss": 0.0022,
      "step": 77950
    },
    {
      "epoch": 22.433132010353752,
      "grad_norm": 0.0002112863294314593,
      "learning_rate": 0.0005513431118780559,
      "loss": 0.0012,
      "step": 78000
    },
    {
      "epoch": 22.447512223180905,
      "grad_norm": 0.0004177417140454054,
      "learning_rate": 0.0005510555076215128,
      "loss": 0.0014,
      "step": 78050
    },
    {
      "epoch": 22.461892436008053,
      "grad_norm": 4.879258995060809e-05,
      "learning_rate": 0.0005507679033649698,
      "loss": 0.0021,
      "step": 78100
    },
    {
      "epoch": 22.476272648835202,
      "grad_norm": 4.640886254492216e-05,
      "learning_rate": 0.0005504802991084268,
      "loss": 0.0018,
      "step": 78150
    },
    {
      "epoch": 22.49065286166235,
      "grad_norm": 0.00822623260319233,
      "learning_rate": 0.0005501926948518838,
      "loss": 0.0025,
      "step": 78200
    },
    {
      "epoch": 22.505033074489504,
      "grad_norm": 0.0005954678636044264,
      "learning_rate": 0.0005499050905953407,
      "loss": 0.0019,
      "step": 78250
    },
    {
      "epoch": 22.519413287316652,
      "grad_norm": 0.00455147260800004,
      "learning_rate": 0.0005496174863387979,
      "loss": 0.0027,
      "step": 78300
    },
    {
      "epoch": 22.5337935001438,
      "grad_norm": 9.876572585199028e-05,
      "learning_rate": 0.0005493298820822549,
      "loss": 0.0021,
      "step": 78350
    },
    {
      "epoch": 22.548173712970954,
      "grad_norm": 0.009014843963086605,
      "learning_rate": 0.0005490422778257118,
      "loss": 0.0009,
      "step": 78400
    },
    {
      "epoch": 22.562553925798102,
      "grad_norm": 0.004998141899704933,
      "learning_rate": 0.0005487546735691689,
      "loss": 0.0011,
      "step": 78450
    },
    {
      "epoch": 22.57693413862525,
      "grad_norm": 0.005135865416377783,
      "learning_rate": 0.0005484670693126258,
      "loss": 0.0016,
      "step": 78500
    },
    {
      "epoch": 22.5913143514524,
      "grad_norm": 8.495274960296229e-05,
      "learning_rate": 0.0005481794650560828,
      "loss": 0.0033,
      "step": 78550
    },
    {
      "epoch": 22.605694564279553,
      "grad_norm": 0.00027996476273983717,
      "learning_rate": 0.0005478918607995399,
      "loss": 0.0008,
      "step": 78600
    },
    {
      "epoch": 22.6200747771067,
      "grad_norm": 4.993234688299708e-05,
      "learning_rate": 0.0005476042565429968,
      "loss": 0.0029,
      "step": 78650
    },
    {
      "epoch": 22.63445498993385,
      "grad_norm": 0.006116501521319151,
      "learning_rate": 0.0005473166522864539,
      "loss": 0.0058,
      "step": 78700
    },
    {
      "epoch": 22.648835202761,
      "grad_norm": 0.00011567658657440916,
      "learning_rate": 0.0005470290480299109,
      "loss": 0.0023,
      "step": 78750
    },
    {
      "epoch": 22.66321541558815,
      "grad_norm": 3.195234603481367e-05,
      "learning_rate": 0.0005467414437733679,
      "loss": 0.0029,
      "step": 78800
    },
    {
      "epoch": 22.6775956284153,
      "grad_norm": 7.53424028516747e-05,
      "learning_rate": 0.0005464538395168248,
      "loss": 0.0048,
      "step": 78850
    },
    {
      "epoch": 22.69197584124245,
      "grad_norm": 0.0005493361968547106,
      "learning_rate": 0.0005461662352602819,
      "loss": 0.0047,
      "step": 78900
    },
    {
      "epoch": 22.7063560540696,
      "grad_norm": 0.008116528391838074,
      "learning_rate": 0.0005458786310037389,
      "loss": 0.003,
      "step": 78950
    },
    {
      "epoch": 22.72073626689675,
      "grad_norm": 0.00021026354806963354,
      "learning_rate": 0.0005455910267471958,
      "loss": 0.0007,
      "step": 79000
    },
    {
      "epoch": 22.7351164797239,
      "grad_norm": 0.00016949535347521305,
      "learning_rate": 0.0005453034224906529,
      "loss": 0.0027,
      "step": 79050
    },
    {
      "epoch": 22.74949669255105,
      "grad_norm": 5.1257018640171736e-05,
      "learning_rate": 0.0005450158182341098,
      "loss": 0.001,
      "step": 79100
    },
    {
      "epoch": 22.7638769053782,
      "grad_norm": 0.013458197936415672,
      "learning_rate": 0.0005447282139775669,
      "loss": 0.001,
      "step": 79150
    },
    {
      "epoch": 22.77825711820535,
      "grad_norm": 4.191830521449447e-05,
      "learning_rate": 0.0005444406097210239,
      "loss": 0.0008,
      "step": 79200
    },
    {
      "epoch": 22.7926373310325,
      "grad_norm": 0.00048037926899269223,
      "learning_rate": 0.0005441530054644809,
      "loss": 0.0035,
      "step": 79250
    },
    {
      "epoch": 22.80701754385965,
      "grad_norm": 0.0007156971259973943,
      "learning_rate": 0.0005438654012079379,
      "loss": 0.0024,
      "step": 79300
    },
    {
      "epoch": 22.8213977566868,
      "grad_norm": 0.002767187310382724,
      "learning_rate": 0.0005435777969513949,
      "loss": 0.0015,
      "step": 79350
    },
    {
      "epoch": 22.83577796951395,
      "grad_norm": 0.005087875761091709,
      "learning_rate": 0.0005432901926948519,
      "loss": 0.0007,
      "step": 79400
    },
    {
      "epoch": 22.850158182341097,
      "grad_norm": 0.0018051592633128166,
      "learning_rate": 0.0005430025884383088,
      "loss": 0.0032,
      "step": 79450
    },
    {
      "epoch": 22.86453839516825,
      "grad_norm": 8.61656226334162e-05,
      "learning_rate": 0.0005427149841817659,
      "loss": 0.0015,
      "step": 79500
    },
    {
      "epoch": 22.8789186079954,
      "grad_norm": 8.356874604942277e-05,
      "learning_rate": 0.000542427379925223,
      "loss": 0.0007,
      "step": 79550
    },
    {
      "epoch": 22.893298820822547,
      "grad_norm": 0.00037345883902162313,
      "learning_rate": 0.0005421397756686799,
      "loss": 0.0036,
      "step": 79600
    },
    {
      "epoch": 22.9076790336497,
      "grad_norm": 0.0002868078590836376,
      "learning_rate": 0.000541852171412137,
      "loss": 0.0022,
      "step": 79650
    },
    {
      "epoch": 22.92205924647685,
      "grad_norm": 0.0005062752170488238,
      "learning_rate": 0.0005415645671555939,
      "loss": 0.0004,
      "step": 79700
    },
    {
      "epoch": 22.936439459303998,
      "grad_norm": 0.001496757729910314,
      "learning_rate": 0.0005412769628990509,
      "loss": 0.0044,
      "step": 79750
    },
    {
      "epoch": 22.950819672131146,
      "grad_norm": 3.508307781885378e-05,
      "learning_rate": 0.0005409893586425079,
      "loss": 0.0024,
      "step": 79800
    },
    {
      "epoch": 22.9651998849583,
      "grad_norm": 0.009355907328426838,
      "learning_rate": 0.0005407017543859649,
      "loss": 0.0047,
      "step": 79850
    },
    {
      "epoch": 22.979580097785448,
      "grad_norm": 0.0011337475152686238,
      "learning_rate": 0.0005404141501294219,
      "loss": 0.0017,
      "step": 79900
    },
    {
      "epoch": 22.993960310612596,
      "grad_norm": 0.018458478152751923,
      "learning_rate": 0.000540126545872879,
      "loss": 0.002,
      "step": 79950
    },
    {
      "epoch": 23.0,
      "eval_loss": 0.01001352071762085,
      "eval_runtime": 16.8239,
      "eval_samples_per_second": 2836.373,
      "eval_steps_per_second": 44.342,
      "step": 79971
    },
    {
      "epoch": 23.008340523439745,
      "grad_norm": 9.657176997279748e-05,
      "learning_rate": 0.000539838941616336,
      "loss": 0.0018,
      "step": 80000
    },
    {
      "epoch": 23.022720736266898,
      "grad_norm": 0.037895604968070984,
      "learning_rate": 0.0005395513373597929,
      "loss": 0.0036,
      "step": 80050
    },
    {
      "epoch": 23.037100949094047,
      "grad_norm": 0.031164832413196564,
      "learning_rate": 0.00053926373310325,
      "loss": 0.003,
      "step": 80100
    },
    {
      "epoch": 23.051481161921195,
      "grad_norm": 0.0014314192812889814,
      "learning_rate": 0.000538976128846707,
      "loss": 0.0016,
      "step": 80150
    },
    {
      "epoch": 23.065861374748348,
      "grad_norm": 0.0003600596101023257,
      "learning_rate": 0.0005386885245901639,
      "loss": 0.0011,
      "step": 80200
    },
    {
      "epoch": 23.080241587575497,
      "grad_norm": 0.010071317665278912,
      "learning_rate": 0.000538400920333621,
      "loss": 0.0024,
      "step": 80250
    },
    {
      "epoch": 23.094621800402646,
      "grad_norm": 5.727052121073939e-05,
      "learning_rate": 0.0005381133160770779,
      "loss": 0.0059,
      "step": 80300
    },
    {
      "epoch": 23.109002013229794,
      "grad_norm": 0.0565517358481884,
      "learning_rate": 0.0005378257118205349,
      "loss": 0.0022,
      "step": 80350
    },
    {
      "epoch": 23.123382226056947,
      "grad_norm": 5.2395414968486875e-05,
      "learning_rate": 0.000537538107563992,
      "loss": 0.002,
      "step": 80400
    },
    {
      "epoch": 23.137762438884096,
      "grad_norm": 0.005637417081743479,
      "learning_rate": 0.000537250503307449,
      "loss": 0.0019,
      "step": 80450
    },
    {
      "epoch": 23.152142651711245,
      "grad_norm": 2.2993903257884085e-05,
      "learning_rate": 0.000536962899050906,
      "loss": 0.0027,
      "step": 80500
    },
    {
      "epoch": 23.166522864538393,
      "grad_norm": 0.005296687129884958,
      "learning_rate": 0.000536675294794363,
      "loss": 0.0056,
      "step": 80550
    },
    {
      "epoch": 23.180903077365546,
      "grad_norm": 0.0002402477985015139,
      "learning_rate": 0.00053638769053782,
      "loss": 0.005,
      "step": 80600
    },
    {
      "epoch": 23.195283290192695,
      "grad_norm": 0.00019245210569351912,
      "learning_rate": 0.0005361000862812769,
      "loss": 0.0041,
      "step": 80650
    },
    {
      "epoch": 23.209663503019843,
      "grad_norm": 0.0030509347561746836,
      "learning_rate": 0.000535812482024734,
      "loss": 0.0029,
      "step": 80700
    },
    {
      "epoch": 23.224043715846996,
      "grad_norm": 3.9370948798023164e-05,
      "learning_rate": 0.000535524877768191,
      "loss": 0.0028,
      "step": 80750
    },
    {
      "epoch": 23.238423928674145,
      "grad_norm": 5.757558756158687e-05,
      "learning_rate": 0.0005352372735116479,
      "loss": 0.0045,
      "step": 80800
    },
    {
      "epoch": 23.252804141501294,
      "grad_norm": 1.6997822967823595e-05,
      "learning_rate": 0.0005349496692551051,
      "loss": 0.0028,
      "step": 80850
    },
    {
      "epoch": 23.267184354328442,
      "grad_norm": 0.06507667899131775,
      "learning_rate": 0.000534662064998562,
      "loss": 0.0007,
      "step": 80900
    },
    {
      "epoch": 23.281564567155595,
      "grad_norm": 0.07535703480243683,
      "learning_rate": 0.000534374460742019,
      "loss": 0.0053,
      "step": 80950
    },
    {
      "epoch": 23.295944779982744,
      "grad_norm": 5.258390956441872e-05,
      "learning_rate": 0.000534086856485476,
      "loss": 0.0017,
      "step": 81000
    },
    {
      "epoch": 23.310324992809893,
      "grad_norm": 7.629510218976066e-05,
      "learning_rate": 0.000533799252228933,
      "loss": 0.0009,
      "step": 81050
    },
    {
      "epoch": 23.324705205637045,
      "grad_norm": 0.013375189155340195,
      "learning_rate": 0.00053351164797239,
      "loss": 0.0032,
      "step": 81100
    },
    {
      "epoch": 23.339085418464194,
      "grad_norm": 8.747796528041363e-05,
      "learning_rate": 0.000533224043715847,
      "loss": 0.0021,
      "step": 81150
    },
    {
      "epoch": 23.353465631291343,
      "grad_norm": 0.06162348389625549,
      "learning_rate": 0.000532936439459304,
      "loss": 0.0026,
      "step": 81200
    },
    {
      "epoch": 23.36784584411849,
      "grad_norm": 0.013328870758414268,
      "learning_rate": 0.0005326488352027609,
      "loss": 0.0023,
      "step": 81250
    },
    {
      "epoch": 23.382226056945644,
      "grad_norm": 0.0009490200318396091,
      "learning_rate": 0.0005323612309462181,
      "loss": 0.0011,
      "step": 81300
    },
    {
      "epoch": 23.396606269772793,
      "grad_norm": 0.0001480927603552118,
      "learning_rate": 0.0005320736266896751,
      "loss": 0.0047,
      "step": 81350
    },
    {
      "epoch": 23.41098648259994,
      "grad_norm": 0.02983405813574791,
      "learning_rate": 0.000531786022433132,
      "loss": 0.0026,
      "step": 81400
    },
    {
      "epoch": 23.425366695427094,
      "grad_norm": 0.009896333329379559,
      "learning_rate": 0.0005314984181765891,
      "loss": 0.0047,
      "step": 81450
    },
    {
      "epoch": 23.439746908254243,
      "grad_norm": 0.0007026488310657442,
      "learning_rate": 0.000531210813920046,
      "loss": 0.0019,
      "step": 81500
    },
    {
      "epoch": 23.45412712108139,
      "grad_norm": 9.686312114354223e-05,
      "learning_rate": 0.000530923209663503,
      "loss": 0.0011,
      "step": 81550
    },
    {
      "epoch": 23.46850733390854,
      "grad_norm": 0.0073569114319980145,
      "learning_rate": 0.00053063560540696,
      "loss": 0.0042,
      "step": 81600
    },
    {
      "epoch": 23.482887546735693,
      "grad_norm": 0.001903980621136725,
      "learning_rate": 0.000530348001150417,
      "loss": 0.002,
      "step": 81650
    },
    {
      "epoch": 23.497267759562842,
      "grad_norm": 0.0015404400182887912,
      "learning_rate": 0.000530060396893874,
      "loss": 0.0004,
      "step": 81700
    },
    {
      "epoch": 23.51164797238999,
      "grad_norm": 0.012534250505268574,
      "learning_rate": 0.0005297727926373311,
      "loss": 0.0013,
      "step": 81750
    },
    {
      "epoch": 23.52602818521714,
      "grad_norm": 0.0001406025403412059,
      "learning_rate": 0.0005294851883807881,
      "loss": 0.005,
      "step": 81800
    },
    {
      "epoch": 23.540408398044292,
      "grad_norm": 0.007059506140649319,
      "learning_rate": 0.000529197584124245,
      "loss": 0.0006,
      "step": 81850
    },
    {
      "epoch": 23.55478861087144,
      "grad_norm": 0.0005792200681753457,
      "learning_rate": 0.0005289099798677021,
      "loss": 0.0043,
      "step": 81900
    },
    {
      "epoch": 23.56916882369859,
      "grad_norm": 7.458574691554531e-05,
      "learning_rate": 0.000528622375611159,
      "loss": 0.0027,
      "step": 81950
    },
    {
      "epoch": 23.583549036525742,
      "grad_norm": 3.2550062314840034e-05,
      "learning_rate": 0.000528334771354616,
      "loss": 0.003,
      "step": 82000
    },
    {
      "epoch": 23.59792924935289,
      "grad_norm": 0.0012515619164332747,
      "learning_rate": 0.0005280471670980731,
      "loss": 0.0017,
      "step": 82050
    },
    {
      "epoch": 23.61230946218004,
      "grad_norm": 0.0003078416921198368,
      "learning_rate": 0.00052775956284153,
      "loss": 0.001,
      "step": 82100
    },
    {
      "epoch": 23.62668967500719,
      "grad_norm": 0.0010335813276469707,
      "learning_rate": 0.0005274719585849871,
      "loss": 0.0008,
      "step": 82150
    },
    {
      "epoch": 23.64106988783434,
      "grad_norm": 0.00277789868414402,
      "learning_rate": 0.0005271843543284441,
      "loss": 0.0043,
      "step": 82200
    },
    {
      "epoch": 23.65545010066149,
      "grad_norm": 3.113262209808454e-05,
      "learning_rate": 0.0005268967500719011,
      "loss": 0.0018,
      "step": 82250
    },
    {
      "epoch": 23.66983031348864,
      "grad_norm": 0.020558403804898262,
      "learning_rate": 0.0005266091458153581,
      "loss": 0.003,
      "step": 82300
    },
    {
      "epoch": 23.68421052631579,
      "grad_norm": 0.0006642966181971133,
      "learning_rate": 0.0005263215415588151,
      "loss": 0.0018,
      "step": 82350
    },
    {
      "epoch": 23.69859073914294,
      "grad_norm": 9.926492930389941e-05,
      "learning_rate": 0.0005260339373022721,
      "loss": 0.0031,
      "step": 82400
    },
    {
      "epoch": 23.71297095197009,
      "grad_norm": 4.571661338559352e-05,
      "learning_rate": 0.000525746333045729,
      "loss": 0.0002,
      "step": 82450
    },
    {
      "epoch": 23.727351164797238,
      "grad_norm": 0.00039993668906390667,
      "learning_rate": 0.0005254587287891861,
      "loss": 0.0015,
      "step": 82500
    },
    {
      "epoch": 23.74173137762439,
      "grad_norm": 0.0014663514448329806,
      "learning_rate": 0.000525171124532643,
      "loss": 0.0015,
      "step": 82550
    },
    {
      "epoch": 23.75611159045154,
      "grad_norm": 0.010958671569824219,
      "learning_rate": 0.0005248835202761001,
      "loss": 0.0028,
      "step": 82600
    },
    {
      "epoch": 23.770491803278688,
      "grad_norm": 0.0001743629400152713,
      "learning_rate": 0.0005245959160195572,
      "loss": 0.0017,
      "step": 82650
    },
    {
      "epoch": 23.784872016105837,
      "grad_norm": 0.00011298254685243592,
      "learning_rate": 0.0005243083117630141,
      "loss": 0.0038,
      "step": 82700
    },
    {
      "epoch": 23.79925222893299,
      "grad_norm": 0.01373132411390543,
      "learning_rate": 0.0005240207075064711,
      "loss": 0.0024,
      "step": 82750
    },
    {
      "epoch": 23.813632441760138,
      "grad_norm": 0.00032018250203691423,
      "learning_rate": 0.0005237331032499281,
      "loss": 0.0012,
      "step": 82800
    },
    {
      "epoch": 23.828012654587287,
      "grad_norm": 2.953529474325478e-05,
      "learning_rate": 0.0005234454989933851,
      "loss": 0.0006,
      "step": 82850
    },
    {
      "epoch": 23.84239286741444,
      "grad_norm": 0.0031476980075240135,
      "learning_rate": 0.0005231578947368421,
      "loss": 0.0026,
      "step": 82900
    },
    {
      "epoch": 23.856773080241588,
      "grad_norm": 0.0029012933373451233,
      "learning_rate": 0.0005228702904802991,
      "loss": 0.003,
      "step": 82950
    },
    {
      "epoch": 23.871153293068737,
      "grad_norm": 0.024964863434433937,
      "learning_rate": 0.0005225826862237562,
      "loss": 0.0039,
      "step": 83000
    },
    {
      "epoch": 23.885533505895886,
      "grad_norm": 0.0008574380772188306,
      "learning_rate": 0.0005222950819672131,
      "loss": 0.0022,
      "step": 83050
    },
    {
      "epoch": 23.899913718723038,
      "grad_norm": 0.03135884180665016,
      "learning_rate": 0.0005220074777106702,
      "loss": 0.0032,
      "step": 83100
    },
    {
      "epoch": 23.914293931550187,
      "grad_norm": 0.0002910234616138041,
      "learning_rate": 0.0005217198734541271,
      "loss": 0.0013,
      "step": 83150
    },
    {
      "epoch": 23.928674144377336,
      "grad_norm": 0.003983076196163893,
      "learning_rate": 0.0005214322691975841,
      "loss": 0.0021,
      "step": 83200
    },
    {
      "epoch": 23.94305435720449,
      "grad_norm": 5.9573860198725015e-05,
      "learning_rate": 0.0005211446649410412,
      "loss": 0.0026,
      "step": 83250
    },
    {
      "epoch": 23.957434570031637,
      "grad_norm": 0.00014449564332608134,
      "learning_rate": 0.0005208570606844981,
      "loss": 0.003,
      "step": 83300
    },
    {
      "epoch": 23.971814782858786,
      "grad_norm": 8.507886377628893e-05,
      "learning_rate": 0.0005205694564279551,
      "loss": 0.0016,
      "step": 83350
    },
    {
      "epoch": 23.986194995685935,
      "grad_norm": 4.811902545043267e-05,
      "learning_rate": 0.0005202818521714121,
      "loss": 0.0021,
      "step": 83400
    },
    {
      "epoch": 24.0,
      "eval_loss": 0.010085116140544415,
      "eval_runtime": 17.182,
      "eval_samples_per_second": 2777.259,
      "eval_steps_per_second": 43.417,
      "step": 83448
    },
    {
      "epoch": 24.000575208513087,
      "grad_norm": 7.122132956283167e-05,
      "learning_rate": 0.0005199942479148692,
      "loss": 0.0013,
      "step": 83450
    },
    {
      "epoch": 24.014955421340236,
      "grad_norm": 0.003110765479505062,
      "learning_rate": 0.0005197066436583262,
      "loss": 0.0032,
      "step": 83500
    },
    {
      "epoch": 24.029335634167385,
      "grad_norm": 0.0003714812337420881,
      "learning_rate": 0.0005194190394017832,
      "loss": 0.0002,
      "step": 83550
    },
    {
      "epoch": 24.043715846994534,
      "grad_norm": 0.0001637286477489397,
      "learning_rate": 0.0005191314351452402,
      "loss": 0.0019,
      "step": 83600
    },
    {
      "epoch": 24.058096059821686,
      "grad_norm": 2.899139690271113e-05,
      "learning_rate": 0.0005188438308886971,
      "loss": 0.0049,
      "step": 83650
    },
    {
      "epoch": 24.072476272648835,
      "grad_norm": 2.701259836612735e-05,
      "learning_rate": 0.0005185562266321542,
      "loss": 0.0021,
      "step": 83700
    },
    {
      "epoch": 24.086856485475984,
      "grad_norm": 0.0007252676878124475,
      "learning_rate": 0.0005182686223756111,
      "loss": 0.0039,
      "step": 83750
    },
    {
      "epoch": 24.101236698303136,
      "grad_norm": 0.007505110930651426,
      "learning_rate": 0.0005179810181190681,
      "loss": 0.0011,
      "step": 83800
    },
    {
      "epoch": 24.115616911130285,
      "grad_norm": 0.0002602317836135626,
      "learning_rate": 0.0005176934138625253,
      "loss": 0.0046,
      "step": 83850
    },
    {
      "epoch": 24.129997123957434,
      "grad_norm": 0.0018350580940023065,
      "learning_rate": 0.0005174058096059822,
      "loss": 0.0014,
      "step": 83900
    },
    {
      "epoch": 24.144377336784583,
      "grad_norm": 0.0005831165472045541,
      "learning_rate": 0.0005171182053494392,
      "loss": 0.0028,
      "step": 83950
    },
    {
      "epoch": 24.158757549611735,
      "grad_norm": 2.919106736953836e-05,
      "learning_rate": 0.0005168306010928962,
      "loss": 0.0031,
      "step": 84000
    },
    {
      "epoch": 24.173137762438884,
      "grad_norm": 0.0069007123820483685,
      "learning_rate": 0.0005165429968363532,
      "loss": 0.0031,
      "step": 84050
    },
    {
      "epoch": 24.187517975266033,
      "grad_norm": 0.015997087582945824,
      "learning_rate": 0.0005162553925798102,
      "loss": 0.0013,
      "step": 84100
    },
    {
      "epoch": 24.201898188093185,
      "grad_norm": 8.596807310823351e-05,
      "learning_rate": 0.0005159677883232672,
      "loss": 0.0008,
      "step": 84150
    },
    {
      "epoch": 24.216278400920334,
      "grad_norm": 0.0010993587784469128,
      "learning_rate": 0.0005156801840667242,
      "loss": 0.001,
      "step": 84200
    },
    {
      "epoch": 24.230658613747483,
      "grad_norm": 9.465631592320278e-05,
      "learning_rate": 0.0005153925798101811,
      "loss": 0.0036,
      "step": 84250
    },
    {
      "epoch": 24.245038826574632,
      "grad_norm": 0.00014341673522721976,
      "learning_rate": 0.0005151049755536383,
      "loss": 0.0043,
      "step": 84300
    },
    {
      "epoch": 24.259419039401784,
      "grad_norm": 0.0005884150159545243,
      "learning_rate": 0.0005148173712970952,
      "loss": 0.0061,
      "step": 84350
    },
    {
      "epoch": 24.273799252228933,
      "grad_norm": 0.0021320320665836334,
      "learning_rate": 0.0005145297670405522,
      "loss": 0.0002,
      "step": 84400
    },
    {
      "epoch": 24.288179465056082,
      "grad_norm": 0.00010486809333087876,
      "learning_rate": 0.0005142421627840093,
      "loss": 0.003,
      "step": 84450
    },
    {
      "epoch": 24.30255967788323,
      "grad_norm": 8.304010407300666e-05,
      "learning_rate": 0.0005139545585274662,
      "loss": 0.0032,
      "step": 84500
    },
    {
      "epoch": 24.316939890710383,
      "grad_norm": 0.03591439500451088,
      "learning_rate": 0.0005136669542709232,
      "loss": 0.004,
      "step": 84550
    },
    {
      "epoch": 24.331320103537532,
      "grad_norm": 0.023180965334177017,
      "learning_rate": 0.0005133793500143802,
      "loss": 0.0028,
      "step": 84600
    },
    {
      "epoch": 24.34570031636468,
      "grad_norm": 7.396040018647909e-05,
      "learning_rate": 0.0005130917457578372,
      "loss": 0.0025,
      "step": 84650
    },
    {
      "epoch": 24.360080529191833,
      "grad_norm": 0.003929730039089918,
      "learning_rate": 0.0005128041415012941,
      "loss": 0.0059,
      "step": 84700
    },
    {
      "epoch": 24.374460742018982,
      "grad_norm": 0.000163873570272699,
      "learning_rate": 0.0005125165372447513,
      "loss": 0.0055,
      "step": 84750
    },
    {
      "epoch": 24.38884095484613,
      "grad_norm": 0.01110121887177229,
      "learning_rate": 0.0005122289329882083,
      "loss": 0.0013,
      "step": 84800
    },
    {
      "epoch": 24.40322116767328,
      "grad_norm": 0.0026044384576380253,
      "learning_rate": 0.0005119413287316652,
      "loss": 0.0018,
      "step": 84850
    },
    {
      "epoch": 24.417601380500432,
      "grad_norm": 7.304306927835569e-05,
      "learning_rate": 0.0005116537244751223,
      "loss": 0.0045,
      "step": 84900
    },
    {
      "epoch": 24.43198159332758,
      "grad_norm": 9.104116179514676e-05,
      "learning_rate": 0.0005113661202185792,
      "loss": 0.0005,
      "step": 84950
    },
    {
      "epoch": 24.44636180615473,
      "grad_norm": 5.874408452655189e-05,
      "learning_rate": 0.0005110785159620362,
      "loss": 0.0013,
      "step": 85000
    },
    {
      "epoch": 24.460742018981882,
      "grad_norm": 0.0003911337989848107,
      "learning_rate": 0.0005107909117054933,
      "loss": 0.0014,
      "step": 85050
    },
    {
      "epoch": 24.47512223180903,
      "grad_norm": 3.876844129990786e-05,
      "learning_rate": 0.0005105033074489502,
      "loss": 0.003,
      "step": 85100
    },
    {
      "epoch": 24.48950244463618,
      "grad_norm": 0.000196493900148198,
      "learning_rate": 0.0005102157031924073,
      "loss": 0.002,
      "step": 85150
    },
    {
      "epoch": 24.50388265746333,
      "grad_norm": 0.0002892842749133706,
      "learning_rate": 0.0005099280989358643,
      "loss": 0.0034,
      "step": 85200
    },
    {
      "epoch": 24.51826287029048,
      "grad_norm": 7.477703911717981e-05,
      "learning_rate": 0.0005096404946793213,
      "loss": 0.003,
      "step": 85250
    },
    {
      "epoch": 24.53264308311763,
      "grad_norm": 0.00426126504316926,
      "learning_rate": 0.0005093528904227782,
      "loss": 0.0027,
      "step": 85300
    },
    {
      "epoch": 24.54702329594478,
      "grad_norm": 0.0012023320887237787,
      "learning_rate": 0.0005090652861662353,
      "loss": 0.0013,
      "step": 85350
    },
    {
      "epoch": 24.56140350877193,
      "grad_norm": 0.00012660805077757686,
      "learning_rate": 0.0005087776819096923,
      "loss": 0.0025,
      "step": 85400
    },
    {
      "epoch": 24.57578372159908,
      "grad_norm": 2.964492523460649e-05,
      "learning_rate": 0.0005084900776531492,
      "loss": 0.0006,
      "step": 85450
    },
    {
      "epoch": 24.59016393442623,
      "grad_norm": 0.00491236662492156,
      "learning_rate": 0.0005082024733966063,
      "loss": 0.0006,
      "step": 85500
    },
    {
      "epoch": 24.604544147253378,
      "grad_norm": 0.0057282657362520695,
      "learning_rate": 0.0005079148691400632,
      "loss": 0.0013,
      "step": 85550
    },
    {
      "epoch": 24.61892436008053,
      "grad_norm": 0.0018943875329568982,
      "learning_rate": 0.0005076272648835203,
      "loss": 0.0008,
      "step": 85600
    },
    {
      "epoch": 24.63330457290768,
      "grad_norm": 0.10209225118160248,
      "learning_rate": 0.0005073396606269774,
      "loss": 0.0011,
      "step": 85650
    },
    {
      "epoch": 24.64768478573483,
      "grad_norm": 4.0332852222491056e-05,
      "learning_rate": 0.0005070520563704343,
      "loss": 0.0029,
      "step": 85700
    },
    {
      "epoch": 24.662064998561977,
      "grad_norm": 0.0004959381185472012,
      "learning_rate": 0.0005067644521138913,
      "loss": 0.0016,
      "step": 85750
    },
    {
      "epoch": 24.67644521138913,
      "grad_norm": 0.017195304855704308,
      "learning_rate": 0.0005064768478573483,
      "loss": 0.0015,
      "step": 85800
    },
    {
      "epoch": 24.69082542421628,
      "grad_norm": 9.04402622836642e-05,
      "learning_rate": 0.0005061892436008053,
      "loss": 0.0022,
      "step": 85850
    },
    {
      "epoch": 24.705205637043427,
      "grad_norm": 0.0001117357678594999,
      "learning_rate": 0.0005059016393442622,
      "loss": 0.0045,
      "step": 85900
    },
    {
      "epoch": 24.71958584987058,
      "grad_norm": 0.00013133631728123873,
      "learning_rate": 0.0005056140350877193,
      "loss": 0.0009,
      "step": 85950
    },
    {
      "epoch": 24.73396606269773,
      "grad_norm": 5.0294453103560954e-05,
      "learning_rate": 0.0005053264308311764,
      "loss": 0.0045,
      "step": 86000
    },
    {
      "epoch": 24.748346275524877,
      "grad_norm": 0.0018901311559602618,
      "learning_rate": 0.0005050388265746333,
      "loss": 0.0006,
      "step": 86050
    },
    {
      "epoch": 24.762726488352026,
      "grad_norm": 0.009935437701642513,
      "learning_rate": 0.0005047512223180904,
      "loss": 0.003,
      "step": 86100
    },
    {
      "epoch": 24.77710670117918,
      "grad_norm": 0.06632100045681,
      "learning_rate": 0.0005044636180615473,
      "loss": 0.0048,
      "step": 86150
    },
    {
      "epoch": 24.791486914006327,
      "grad_norm": 0.00011133364751003683,
      "learning_rate": 0.0005041760138050043,
      "loss": 0.002,
      "step": 86200
    },
    {
      "epoch": 24.805867126833476,
      "grad_norm": 7.931404252303764e-05,
      "learning_rate": 0.0005038884095484614,
      "loss": 0.0017,
      "step": 86250
    },
    {
      "epoch": 24.82024733966063,
      "grad_norm": 0.01658199541270733,
      "learning_rate": 0.0005036008052919183,
      "loss": 0.0034,
      "step": 86300
    },
    {
      "epoch": 24.834627552487778,
      "grad_norm": 0.0006446245824918151,
      "learning_rate": 0.0005033132010353753,
      "loss": 0.0014,
      "step": 86350
    },
    {
      "epoch": 24.849007765314926,
      "grad_norm": 0.0002649909583851695,
      "learning_rate": 0.0005030255967788323,
      "loss": 0.0015,
      "step": 86400
    },
    {
      "epoch": 24.863387978142075,
      "grad_norm": 0.0032321917824447155,
      "learning_rate": 0.0005027379925222894,
      "loss": 0.0026,
      "step": 86450
    },
    {
      "epoch": 24.877768190969228,
      "grad_norm": 0.012084946036338806,
      "learning_rate": 0.0005024503882657463,
      "loss": 0.0003,
      "step": 86500
    },
    {
      "epoch": 24.892148403796376,
      "grad_norm": 0.0009112760308198631,
      "learning_rate": 0.0005021627840092034,
      "loss": 0.0022,
      "step": 86550
    },
    {
      "epoch": 24.906528616623525,
      "grad_norm": 0.037060294300317764,
      "learning_rate": 0.0005018751797526604,
      "loss": 0.0009,
      "step": 86600
    },
    {
      "epoch": 24.920908829450674,
      "grad_norm": 0.0010117717320099473,
      "learning_rate": 0.0005015875754961173,
      "loss": 0.0013,
      "step": 86650
    },
    {
      "epoch": 24.935289042277827,
      "grad_norm": 0.0011152831139042974,
      "learning_rate": 0.0005012999712395744,
      "loss": 0.0032,
      "step": 86700
    },
    {
      "epoch": 24.949669255104975,
      "grad_norm": 0.0023711055982857943,
      "learning_rate": 0.0005010123669830313,
      "loss": 0.0028,
      "step": 86750
    },
    {
      "epoch": 24.964049467932124,
      "grad_norm": 0.0017364148516207933,
      "learning_rate": 0.0005007247627264883,
      "loss": 0.0048,
      "step": 86800
    },
    {
      "epoch": 24.978429680759277,
      "grad_norm": 0.00019442303164396435,
      "learning_rate": 0.0005004371584699455,
      "loss": 0.0035,
      "step": 86850
    },
    {
      "epoch": 24.992809893586426,
      "grad_norm": 9.565870277583599e-05,
      "learning_rate": 0.0005001495542134024,
      "loss": 0.0003,
      "step": 86900
    },
    {
      "epoch": 25.0,
      "eval_loss": 0.009965748526155949,
      "eval_runtime": 17.573,
      "eval_samples_per_second": 2715.478,
      "eval_steps_per_second": 42.452,
      "step": 86925
    },
    {
      "epoch": 25.007190106413574,
      "grad_norm": 0.0004094669420737773,
      "learning_rate": 0.0004998619499568594,
      "loss": 0.0057,
      "step": 86950
    },
    {
      "epoch": 25.021570319240723,
      "grad_norm": 0.0003093726991210133,
      "learning_rate": 0.0004995743457003164,
      "loss": 0.002,
      "step": 87000
    },
    {
      "epoch": 25.035950532067876,
      "grad_norm": 0.00018952172831632197,
      "learning_rate": 0.0004992867414437734,
      "loss": 0.0047,
      "step": 87050
    },
    {
      "epoch": 25.050330744895025,
      "grad_norm": 0.000612124742474407,
      "learning_rate": 0.0004989991371872304,
      "loss": 0.0006,
      "step": 87100
    },
    {
      "epoch": 25.064710957722173,
      "grad_norm": 0.001309232204221189,
      "learning_rate": 0.0004987115329306874,
      "loss": 0.0041,
      "step": 87150
    },
    {
      "epoch": 25.079091170549326,
      "grad_norm": 0.016611553728580475,
      "learning_rate": 0.0004984239286741444,
      "loss": 0.0007,
      "step": 87200
    },
    {
      "epoch": 25.093471383376475,
      "grad_norm": 0.004989383742213249,
      "learning_rate": 0.0004981363244176013,
      "loss": 0.0041,
      "step": 87250
    },
    {
      "epoch": 25.107851596203624,
      "grad_norm": 4.867506504524499e-05,
      "learning_rate": 0.0004978487201610585,
      "loss": 0.0032,
      "step": 87300
    },
    {
      "epoch": 25.122231809030772,
      "grad_norm": 0.0009436877444386482,
      "learning_rate": 0.0004975611159045155,
      "loss": 0.0024,
      "step": 87350
    },
    {
      "epoch": 25.136612021857925,
      "grad_norm": 0.00036082311999052763,
      "learning_rate": 0.0004972735116479724,
      "loss": 0.001,
      "step": 87400
    },
    {
      "epoch": 25.150992234685074,
      "grad_norm": 0.00019176892237737775,
      "learning_rate": 0.0004969859073914294,
      "loss": 0.0039,
      "step": 87450
    },
    {
      "epoch": 25.165372447512222,
      "grad_norm": 0.0026742906775325537,
      "learning_rate": 0.0004966983031348864,
      "loss": 0.0025,
      "step": 87500
    },
    {
      "epoch": 25.17975266033937,
      "grad_norm": 0.0006499826558865607,
      "learning_rate": 0.0004964106988783434,
      "loss": 0.0005,
      "step": 87550
    },
    {
      "epoch": 25.194132873166524,
      "grad_norm": 2.5456454750383273e-05,
      "learning_rate": 0.0004961230946218004,
      "loss": 0.0017,
      "step": 87600
    },
    {
      "epoch": 25.208513085993673,
      "grad_norm": 7.770679076202214e-05,
      "learning_rate": 0.0004958354903652574,
      "loss": 0.0012,
      "step": 87650
    },
    {
      "epoch": 25.22289329882082,
      "grad_norm": 0.012506767176091671,
      "learning_rate": 0.0004955478861087144,
      "loss": 0.0026,
      "step": 87700
    },
    {
      "epoch": 25.237273511647974,
      "grad_norm": 0.0019676198717206717,
      "learning_rate": 0.0004952602818521715,
      "loss": 0.0018,
      "step": 87750
    },
    {
      "epoch": 25.251653724475123,
      "grad_norm": 0.00017629130161367357,
      "learning_rate": 0.0004949726775956285,
      "loss": 0.002,
      "step": 87800
    },
    {
      "epoch": 25.26603393730227,
      "grad_norm": 0.007889872416853905,
      "learning_rate": 0.0004946850733390854,
      "loss": 0.0014,
      "step": 87850
    },
    {
      "epoch": 25.28041415012942,
      "grad_norm": 0.009524887427687645,
      "learning_rate": 0.0004943974690825424,
      "loss": 0.0015,
      "step": 87900
    },
    {
      "epoch": 25.294794362956573,
      "grad_norm": 4.7270612412830815e-05,
      "learning_rate": 0.0004941098648259995,
      "loss": 0.0014,
      "step": 87950
    },
    {
      "epoch": 25.30917457578372,
      "grad_norm": 0.012920980341732502,
      "learning_rate": 0.0004938222605694564,
      "loss": 0.0026,
      "step": 88000
    },
    {
      "epoch": 25.32355478861087,
      "grad_norm": 0.056060634553432465,
      "learning_rate": 0.0004935346563129134,
      "loss": 0.004,
      "step": 88050
    },
    {
      "epoch": 25.337935001438023,
      "grad_norm": 0.0004665635642595589,
      "learning_rate": 0.0004932470520563704,
      "loss": 0.0017,
      "step": 88100
    },
    {
      "epoch": 25.35231521426517,
      "grad_norm": 2.9395998353720643e-05,
      "learning_rate": 0.0004929594477998274,
      "loss": 0.0041,
      "step": 88150
    },
    {
      "epoch": 25.36669542709232,
      "grad_norm": 0.0015101274475455284,
      "learning_rate": 0.0004926718435432845,
      "loss": 0.0026,
      "step": 88200
    },
    {
      "epoch": 25.38107563991947,
      "grad_norm": 0.00013376373681239784,
      "learning_rate": 0.0004923842392867415,
      "loss": 0.0008,
      "step": 88250
    },
    {
      "epoch": 25.395455852746622,
      "grad_norm": 0.0008473465568386018,
      "learning_rate": 0.0004920966350301985,
      "loss": 0.0072,
      "step": 88300
    },
    {
      "epoch": 25.40983606557377,
      "grad_norm": 0.0008569668279960752,
      "learning_rate": 0.0004918090307736555,
      "loss": 0.0019,
      "step": 88350
    },
    {
      "epoch": 25.42421627840092,
      "grad_norm": 0.003072000341489911,
      "learning_rate": 0.0004915214265171125,
      "loss": 0.0022,
      "step": 88400
    },
    {
      "epoch": 25.43859649122807,
      "grad_norm": 0.0006950779934413731,
      "learning_rate": 0.0004912338222605694,
      "loss": 0.0015,
      "step": 88450
    },
    {
      "epoch": 25.45297670405522,
      "grad_norm": 0.02489749900996685,
      "learning_rate": 0.0004909462180040264,
      "loss": 0.0022,
      "step": 88500
    },
    {
      "epoch": 25.46735691688237,
      "grad_norm": 0.0016353825340047479,
      "learning_rate": 0.0004906586137474835,
      "loss": 0.0011,
      "step": 88550
    },
    {
      "epoch": 25.48173712970952,
      "grad_norm": 0.005853016395121813,
      "learning_rate": 0.0004903710094909404,
      "loss": 0.001,
      "step": 88600
    },
    {
      "epoch": 25.49611734253667,
      "grad_norm": 0.029401173815131187,
      "learning_rate": 0.0004900834052343975,
      "loss": 0.0007,
      "step": 88650
    },
    {
      "epoch": 25.51049755536382,
      "grad_norm": 0.017671795561909676,
      "learning_rate": 0.0004897958009778545,
      "loss": 0.0043,
      "step": 88700
    },
    {
      "epoch": 25.52487776819097,
      "grad_norm": 5.643006079480983e-05,
      "learning_rate": 0.0004895081967213115,
      "loss": 0.0008,
      "step": 88750
    },
    {
      "epoch": 25.539257981018118,
      "grad_norm": 0.0012581399641931057,
      "learning_rate": 0.0004892205924647685,
      "loss": 0.0027,
      "step": 88800
    },
    {
      "epoch": 25.55363819384527,
      "grad_norm": 0.024947525933384895,
      "learning_rate": 0.0004889329882082255,
      "loss": 0.0037,
      "step": 88850
    },
    {
      "epoch": 25.56801840667242,
      "grad_norm": 6.577890599146485e-05,
      "learning_rate": 0.0004886453839516825,
      "loss": 0.0032,
      "step": 88900
    },
    {
      "epoch": 25.582398619499568,
      "grad_norm": 0.006528416648507118,
      "learning_rate": 0.0004883577796951395,
      "loss": 0.0045,
      "step": 88950
    },
    {
      "epoch": 25.59677883232672,
      "grad_norm": 0.00010959943028865382,
      "learning_rate": 0.0004880701754385965,
      "loss": 0.0013,
      "step": 89000
    },
    {
      "epoch": 25.61115904515387,
      "grad_norm": 0.0005720560438930988,
      "learning_rate": 0.0004877825711820535,
      "loss": 0.0013,
      "step": 89050
    },
    {
      "epoch": 25.625539257981018,
      "grad_norm": 0.00022691211779601872,
      "learning_rate": 0.00048749496692551046,
      "loss": 0.001,
      "step": 89100
    },
    {
      "epoch": 25.639919470808167,
      "grad_norm": 0.010988632217049599,
      "learning_rate": 0.00048720736266896753,
      "loss": 0.0006,
      "step": 89150
    },
    {
      "epoch": 25.65429968363532,
      "grad_norm": 6.156472954899073e-05,
      "learning_rate": 0.00048691975841242454,
      "loss": 0.0028,
      "step": 89200
    },
    {
      "epoch": 25.668679896462468,
      "grad_norm": 2.6807078029378317e-05,
      "learning_rate": 0.0004866321541558815,
      "loss": 0.0047,
      "step": 89250
    },
    {
      "epoch": 25.683060109289617,
      "grad_norm": 0.00017826927069108933,
      "learning_rate": 0.0004863445498993385,
      "loss": 0.0026,
      "step": 89300
    },
    {
      "epoch": 25.69744032211677,
      "grad_norm": 0.0014283395139500499,
      "learning_rate": 0.0004860569456427955,
      "loss": 0.0023,
      "step": 89350
    },
    {
      "epoch": 25.711820534943918,
      "grad_norm": 7.45969227864407e-05,
      "learning_rate": 0.0004857693413862525,
      "loss": 0.0026,
      "step": 89400
    },
    {
      "epoch": 25.726200747771067,
      "grad_norm": 0.03439560905098915,
      "learning_rate": 0.00048548173712970955,
      "loss": 0.0005,
      "step": 89450
    },
    {
      "epoch": 25.740580960598216,
      "grad_norm": 2.1027073671575636e-05,
      "learning_rate": 0.00048519413287316656,
      "loss": 0.0036,
      "step": 89500
    },
    {
      "epoch": 25.754961173425368,
      "grad_norm": 0.002069788984954357,
      "learning_rate": 0.0004849065286166235,
      "loss": 0.0055,
      "step": 89550
    },
    {
      "epoch": 25.769341386252517,
      "grad_norm": 0.003049906576052308,
      "learning_rate": 0.00048461892436008053,
      "loss": 0.0034,
      "step": 89600
    },
    {
      "epoch": 25.783721599079666,
      "grad_norm": 4.56495545222424e-05,
      "learning_rate": 0.00048433132010353754,
      "loss": 0.0015,
      "step": 89650
    },
    {
      "epoch": 25.798101811906815,
      "grad_norm": 0.18363644182682037,
      "learning_rate": 0.0004840437158469945,
      "loss": 0.0038,
      "step": 89700
    },
    {
      "epoch": 25.812482024733967,
      "grad_norm": 7.765725604258478e-05,
      "learning_rate": 0.00048375611159045157,
      "loss": 0.0015,
      "step": 89750
    },
    {
      "epoch": 25.826862237561116,
      "grad_norm": 0.00014229129010345787,
      "learning_rate": 0.0004834685073339086,
      "loss": 0.004,
      "step": 89800
    },
    {
      "epoch": 25.841242450388265,
      "grad_norm": 0.0002637817233335227,
      "learning_rate": 0.00048318090307736554,
      "loss": 0.0015,
      "step": 89850
    },
    {
      "epoch": 25.855622663215417,
      "grad_norm": 1.952695492946077e-05,
      "learning_rate": 0.00048289329882082255,
      "loss": 0.004,
      "step": 89900
    },
    {
      "epoch": 25.870002876042566,
      "grad_norm": 0.006391090806573629,
      "learning_rate": 0.00048260569456427956,
      "loss": 0.0043,
      "step": 89950
    },
    {
      "epoch": 25.884383088869715,
      "grad_norm": 0.0003489027149043977,
      "learning_rate": 0.0004823180903077365,
      "loss": 0.003,
      "step": 90000
    },
    {
      "epoch": 25.898763301696864,
      "grad_norm": 0.0001696772378636524,
      "learning_rate": 0.0004820304860511936,
      "loss": 0.0022,
      "step": 90050
    },
    {
      "epoch": 25.913143514524016,
      "grad_norm": 0.0439322404563427,
      "learning_rate": 0.0004817428817946506,
      "loss": 0.0039,
      "step": 90100
    },
    {
      "epoch": 25.927523727351165,
      "grad_norm": 0.002333888318389654,
      "learning_rate": 0.00048145527753810756,
      "loss": 0.0023,
      "step": 90150
    },
    {
      "epoch": 25.941903940178314,
      "grad_norm": 0.0008252047118730843,
      "learning_rate": 0.00048116767328156457,
      "loss": 0.002,
      "step": 90200
    },
    {
      "epoch": 25.956284153005463,
      "grad_norm": 0.0005062955315224826,
      "learning_rate": 0.0004808800690250216,
      "loss": 0.0007,
      "step": 90250
    },
    {
      "epoch": 25.970664365832615,
      "grad_norm": 9.351930930279195e-05,
      "learning_rate": 0.00048059246476847854,
      "loss": 0.0021,
      "step": 90300
    },
    {
      "epoch": 25.985044578659764,
      "grad_norm": 0.00043079626630060375,
      "learning_rate": 0.00048030486051193555,
      "loss": 0.0029,
      "step": 90350
    },
    {
      "epoch": 25.999424791486913,
      "grad_norm": 0.00018488049681764096,
      "learning_rate": 0.0004800172562553926,
      "loss": 0.0014,
      "step": 90400
    },
    {
      "epoch": 26.0,
      "eval_loss": 0.010513602755963802,
      "eval_runtime": 16.5955,
      "eval_samples_per_second": 2875.421,
      "eval_steps_per_second": 44.952,
      "step": 90402
    },
    {
      "epoch": 26.013805004314065,
      "grad_norm": 0.059867218136787415,
      "learning_rate": 0.0004797296519988496,
      "loss": 0.0025,
      "step": 90450
    },
    {
      "epoch": 26.028185217141214,
      "grad_norm": 0.0003491953248158097,
      "learning_rate": 0.0004794420477423066,
      "loss": 0.0003,
      "step": 90500
    },
    {
      "epoch": 26.042565429968363,
      "grad_norm": 0.005944396834820509,
      "learning_rate": 0.0004791544434857636,
      "loss": 0.0045,
      "step": 90550
    },
    {
      "epoch": 26.05694564279551,
      "grad_norm": 0.00404262263327837,
      "learning_rate": 0.00047886683922922056,
      "loss": 0.0017,
      "step": 90600
    },
    {
      "epoch": 26.071325855622664,
      "grad_norm": 2.9060864108032547e-05,
      "learning_rate": 0.00047857923497267757,
      "loss": 0.0024,
      "step": 90650
    },
    {
      "epoch": 26.085706068449813,
      "grad_norm": 0.07684890925884247,
      "learning_rate": 0.00047829163071613464,
      "loss": 0.001,
      "step": 90700
    },
    {
      "epoch": 26.100086281276962,
      "grad_norm": 3.840787394437939e-05,
      "learning_rate": 0.0004780040264595916,
      "loss": 0.0028,
      "step": 90750
    },
    {
      "epoch": 26.114466494104114,
      "grad_norm": 0.00014051252219360322,
      "learning_rate": 0.0004777164222030486,
      "loss": 0.007,
      "step": 90800
    },
    {
      "epoch": 26.128846706931263,
      "grad_norm": 0.0011691341642290354,
      "learning_rate": 0.0004774288179465056,
      "loss": 0.0016,
      "step": 90850
    },
    {
      "epoch": 26.143226919758412,
      "grad_norm": 0.000662691832985729,
      "learning_rate": 0.0004771412136899626,
      "loss": 0.0021,
      "step": 90900
    },
    {
      "epoch": 26.15760713258556,
      "grad_norm": 3.9908249164000154e-05,
      "learning_rate": 0.0004768536094334196,
      "loss": 0.0039,
      "step": 90950
    },
    {
      "epoch": 26.171987345412713,
      "grad_norm": 0.00019115774193778634,
      "learning_rate": 0.00047656600517687666,
      "loss": 0.002,
      "step": 91000
    },
    {
      "epoch": 26.186367558239862,
      "grad_norm": 0.01527486927807331,
      "learning_rate": 0.0004762784009203336,
      "loss": 0.0023,
      "step": 91050
    },
    {
      "epoch": 26.20074777106701,
      "grad_norm": 9.716672502690926e-05,
      "learning_rate": 0.00047599079666379063,
      "loss": 0.0041,
      "step": 91100
    },
    {
      "epoch": 26.215127983894163,
      "grad_norm": 7.546811684733257e-05,
      "learning_rate": 0.00047570319240724764,
      "loss": 0.0032,
      "step": 91150
    },
    {
      "epoch": 26.229508196721312,
      "grad_norm": 0.0006992886192165315,
      "learning_rate": 0.0004754155881507046,
      "loss": 0.0017,
      "step": 91200
    },
    {
      "epoch": 26.24388840954846,
      "grad_norm": 0.038979753851890564,
      "learning_rate": 0.0004751279838941616,
      "loss": 0.0048,
      "step": 91250
    },
    {
      "epoch": 26.25826862237561,
      "grad_norm": 0.00583154521882534,
      "learning_rate": 0.0004748403796376187,
      "loss": 0.0028,
      "step": 91300
    },
    {
      "epoch": 26.272648835202762,
      "grad_norm": 0.0004216995439492166,
      "learning_rate": 0.00047455277538107564,
      "loss": 0.0044,
      "step": 91350
    },
    {
      "epoch": 26.28702904802991,
      "grad_norm": 0.000701761688105762,
      "learning_rate": 0.00047426517112453265,
      "loss": 0.0005,
      "step": 91400
    },
    {
      "epoch": 26.30140926085706,
      "grad_norm": 0.004383735358715057,
      "learning_rate": 0.00047397756686798966,
      "loss": 0.0058,
      "step": 91450
    },
    {
      "epoch": 26.31578947368421,
      "grad_norm": 6.168589607113972e-05,
      "learning_rate": 0.0004736899626114466,
      "loss": 0.0004,
      "step": 91500
    },
    {
      "epoch": 26.33016968651136,
      "grad_norm": 0.03083604760468006,
      "learning_rate": 0.00047340235835490363,
      "loss": 0.0044,
      "step": 91550
    },
    {
      "epoch": 26.34454989933851,
      "grad_norm": 0.0003393790393602103,
      "learning_rate": 0.0004731147540983607,
      "loss": 0.0016,
      "step": 91600
    },
    {
      "epoch": 26.35893011216566,
      "grad_norm": 3.734648635145277e-05,
      "learning_rate": 0.0004728271498418177,
      "loss": 0.0016,
      "step": 91650
    },
    {
      "epoch": 26.37331032499281,
      "grad_norm": 0.010077795945107937,
      "learning_rate": 0.00047253954558527467,
      "loss": 0.0045,
      "step": 91700
    },
    {
      "epoch": 26.38769053781996,
      "grad_norm": 7.484263915102929e-05,
      "learning_rate": 0.0004722519413287317,
      "loss": 0.0032,
      "step": 91750
    },
    {
      "epoch": 26.40207075064711,
      "grad_norm": 0.0010915836319327354,
      "learning_rate": 0.00047196433707218864,
      "loss": 0.0032,
      "step": 91800
    },
    {
      "epoch": 26.416450963474258,
      "grad_norm": 0.0006640925421379507,
      "learning_rate": 0.00047167673281564565,
      "loss": 0.001,
      "step": 91850
    },
    {
      "epoch": 26.43083117630141,
      "grad_norm": 7.665598241146654e-05,
      "learning_rate": 0.0004713891285591027,
      "loss": 0.0024,
      "step": 91900
    },
    {
      "epoch": 26.44521138912856,
      "grad_norm": 0.00042368524009361863,
      "learning_rate": 0.00047110152430255973,
      "loss": 0.0035,
      "step": 91950
    },
    {
      "epoch": 26.459591601955708,
      "grad_norm": 0.0001502793893450871,
      "learning_rate": 0.0004708139200460167,
      "loss": 0.0032,
      "step": 92000
    },
    {
      "epoch": 26.47397181478286,
      "grad_norm": 3.1620853405911475e-05,
      "learning_rate": 0.0004705263157894737,
      "loss": 0.0003,
      "step": 92050
    },
    {
      "epoch": 26.48835202761001,
      "grad_norm": 0.004732917528599501,
      "learning_rate": 0.0004702387115329307,
      "loss": 0.0003,
      "step": 92100
    },
    {
      "epoch": 26.502732240437158,
      "grad_norm": 2.462818520143628e-05,
      "learning_rate": 0.00046995110727638767,
      "loss": 0.0062,
      "step": 92150
    },
    {
      "epoch": 26.517112453264307,
      "grad_norm": 2.6498693841858767e-05,
      "learning_rate": 0.00046966350301984474,
      "loss": 0.0014,
      "step": 92200
    },
    {
      "epoch": 26.53149266609146,
      "grad_norm": 0.002064475091174245,
      "learning_rate": 0.00046937589876330175,
      "loss": 0.0021,
      "step": 92250
    },
    {
      "epoch": 26.54587287891861,
      "grad_norm": 0.00024790610768832266,
      "learning_rate": 0.0004690882945067587,
      "loss": 0.0011,
      "step": 92300
    },
    {
      "epoch": 26.560253091745757,
      "grad_norm": 0.0001452615688322112,
      "learning_rate": 0.0004688006902502157,
      "loss": 0.0036,
      "step": 92350
    },
    {
      "epoch": 26.574633304572906,
      "grad_norm": 0.006143444217741489,
      "learning_rate": 0.00046851308599367273,
      "loss": 0.0011,
      "step": 92400
    },
    {
      "epoch": 26.58901351740006,
      "grad_norm": 0.00013599998783320189,
      "learning_rate": 0.0004682254817371297,
      "loss": 0.0038,
      "step": 92450
    },
    {
      "epoch": 26.603393730227207,
      "grad_norm": 5.4852949688211083e-05,
      "learning_rate": 0.00046793787748058676,
      "loss": 0.0042,
      "step": 92500
    },
    {
      "epoch": 26.617773943054356,
      "grad_norm": 0.0004249520134180784,
      "learning_rate": 0.00046765027322404377,
      "loss": 0.0025,
      "step": 92550
    },
    {
      "epoch": 26.63215415588151,
      "grad_norm": 0.0017791020218282938,
      "learning_rate": 0.0004673626689675007,
      "loss": 0.0007,
      "step": 92600
    },
    {
      "epoch": 26.646534368708657,
      "grad_norm": 8.09308621683158e-05,
      "learning_rate": 0.00046707506471095774,
      "loss": 0.0006,
      "step": 92650
    },
    {
      "epoch": 26.660914581535806,
      "grad_norm": 3.20287945214659e-05,
      "learning_rate": 0.00046678746045441475,
      "loss": 0.0015,
      "step": 92700
    },
    {
      "epoch": 26.675294794362955,
      "grad_norm": 0.0011121589923277497,
      "learning_rate": 0.0004664998561978717,
      "loss": 0.0016,
      "step": 92750
    },
    {
      "epoch": 26.689675007190107,
      "grad_norm": 8.597225678386167e-05,
      "learning_rate": 0.0004662122519413288,
      "loss": 0.0024,
      "step": 92800
    },
    {
      "epoch": 26.704055220017256,
      "grad_norm": 0.00010581736569292843,
      "learning_rate": 0.0004659246476847858,
      "loss": 0.0022,
      "step": 92850
    },
    {
      "epoch": 26.718435432844405,
      "grad_norm": 0.000638151599559933,
      "learning_rate": 0.00046563704342824275,
      "loss": 0.0025,
      "step": 92900
    },
    {
      "epoch": 26.732815645671558,
      "grad_norm": 4.206518497085199e-05,
      "learning_rate": 0.00046534943917169976,
      "loss": 0.0021,
      "step": 92950
    },
    {
      "epoch": 26.747195858498706,
      "grad_norm": 0.0005740890628658235,
      "learning_rate": 0.00046506183491515677,
      "loss": 0.001,
      "step": 93000
    },
    {
      "epoch": 26.761576071325855,
      "grad_norm": 0.002781304530799389,
      "learning_rate": 0.00046477423065861373,
      "loss": 0.0022,
      "step": 93050
    },
    {
      "epoch": 26.775956284153004,
      "grad_norm": 0.001413216581568122,
      "learning_rate": 0.00046448662640207074,
      "loss": 0.002,
      "step": 93100
    },
    {
      "epoch": 26.790336496980157,
      "grad_norm": 0.0033037748653441668,
      "learning_rate": 0.0004641990221455278,
      "loss": 0.0023,
      "step": 93150
    },
    {
      "epoch": 26.804716709807305,
      "grad_norm": 0.015788249671459198,
      "learning_rate": 0.00046391141788898477,
      "loss": 0.0032,
      "step": 93200
    },
    {
      "epoch": 26.819096922634454,
      "grad_norm": 0.005962631665170193,
      "learning_rate": 0.0004636238136324418,
      "loss": 0.0029,
      "step": 93250
    },
    {
      "epoch": 26.833477135461607,
      "grad_norm": 0.003860084805637598,
      "learning_rate": 0.0004633362093758988,
      "loss": 0.0013,
      "step": 93300
    },
    {
      "epoch": 26.847857348288755,
      "grad_norm": 0.0002147295599570498,
      "learning_rate": 0.00046304860511935575,
      "loss": 0.0036,
      "step": 93350
    },
    {
      "epoch": 26.862237561115904,
      "grad_norm": 0.0005642009200528264,
      "learning_rate": 0.00046276100086281276,
      "loss": 0.0012,
      "step": 93400
    },
    {
      "epoch": 26.876617773943053,
      "grad_norm": 0.012158973142504692,
      "learning_rate": 0.00046247339660626983,
      "loss": 0.0012,
      "step": 93450
    },
    {
      "epoch": 26.890997986770206,
      "grad_norm": 0.0008521700510755181,
      "learning_rate": 0.0004621857923497268,
      "loss": 0.0015,
      "step": 93500
    },
    {
      "epoch": 26.905378199597354,
      "grad_norm": 0.00011804742098320276,
      "learning_rate": 0.0004618981880931838,
      "loss": 0.0024,
      "step": 93550
    },
    {
      "epoch": 26.919758412424503,
      "grad_norm": 7.602015102747828e-05,
      "learning_rate": 0.0004616105838366408,
      "loss": 0.0017,
      "step": 93600
    },
    {
      "epoch": 26.934138625251652,
      "grad_norm": 0.002049622591584921,
      "learning_rate": 0.00046132297958009777,
      "loss": 0.0021,
      "step": 93650
    },
    {
      "epoch": 26.948518838078805,
      "grad_norm": 0.04606690630316734,
      "learning_rate": 0.0004610353753235548,
      "loss": 0.0032,
      "step": 93700
    },
    {
      "epoch": 26.962899050905953,
      "grad_norm": 4.720288052340038e-05,
      "learning_rate": 0.00046074777106701185,
      "loss": 0.0037,
      "step": 93750
    },
    {
      "epoch": 26.977279263733102,
      "grad_norm": 9.24914056668058e-05,
      "learning_rate": 0.0004604601668104688,
      "loss": 0.0018,
      "step": 93800
    },
    {
      "epoch": 26.991659476560255,
      "grad_norm": 0.00010232923523290083,
      "learning_rate": 0.0004601725625539258,
      "loss": 0.0006,
      "step": 93850
    },
    {
      "epoch": 27.0,
      "eval_loss": 0.01018139161169529,
      "eval_runtime": 16.6274,
      "eval_samples_per_second": 2869.894,
      "eval_steps_per_second": 44.866,
      "step": 93879
    },
    {
      "epoch": 27.006039689387404,
      "grad_norm": 0.06112473085522652,
      "learning_rate": 0.00045988495829738283,
      "loss": 0.0031,
      "step": 93900
    },
    {
      "epoch": 27.020419902214552,
      "grad_norm": 0.004131306428462267,
      "learning_rate": 0.0004595973540408398,
      "loss": 0.0029,
      "step": 93950
    },
    {
      "epoch": 27.0348001150417,
      "grad_norm": 0.0005352689768187702,
      "learning_rate": 0.0004593097497842968,
      "loss": 0.0036,
      "step": 94000
    },
    {
      "epoch": 27.049180327868854,
      "grad_norm": 0.0007058101473376155,
      "learning_rate": 0.00045902214552775387,
      "loss": 0.0004,
      "step": 94050
    },
    {
      "epoch": 27.063560540696002,
      "grad_norm": 6.452951492974535e-05,
      "learning_rate": 0.0004587345412712108,
      "loss": 0.0011,
      "step": 94100
    },
    {
      "epoch": 27.07794075352315,
      "grad_norm": 7.163960253819823e-05,
      "learning_rate": 0.00045844693701466784,
      "loss": 0.0024,
      "step": 94150
    },
    {
      "epoch": 27.092320966350304,
      "grad_norm": 0.00025197022478096187,
      "learning_rate": 0.00045815933275812485,
      "loss": 0.0027,
      "step": 94200
    },
    {
      "epoch": 27.106701179177453,
      "grad_norm": 0.01816357672214508,
      "learning_rate": 0.0004578717285015818,
      "loss": 0.0047,
      "step": 94250
    },
    {
      "epoch": 27.1210813920046,
      "grad_norm": 0.00024849275359883904,
      "learning_rate": 0.0004575841242450388,
      "loss": 0.0037,
      "step": 94300
    },
    {
      "epoch": 27.13546160483175,
      "grad_norm": 0.006104031577706337,
      "learning_rate": 0.0004572965199884959,
      "loss": 0.0014,
      "step": 94350
    },
    {
      "epoch": 27.149841817658903,
      "grad_norm": 0.0002594961551949382,
      "learning_rate": 0.00045700891573195284,
      "loss": 0.0036,
      "step": 94400
    },
    {
      "epoch": 27.16422203048605,
      "grad_norm": 0.0002600126317702234,
      "learning_rate": 0.00045672131147540986,
      "loss": 0.0016,
      "step": 94450
    },
    {
      "epoch": 27.1786022433132,
      "grad_norm": 0.0005513578653335571,
      "learning_rate": 0.00045643370721886687,
      "loss": 0.0004,
      "step": 94500
    },
    {
      "epoch": 27.19298245614035,
      "grad_norm": 0.0010892213322222233,
      "learning_rate": 0.0004561461029623238,
      "loss": 0.0034,
      "step": 94550
    },
    {
      "epoch": 27.2073626689675,
      "grad_norm": 8.949602488428354e-05,
      "learning_rate": 0.00045585849870578084,
      "loss": 0.001,
      "step": 94600
    },
    {
      "epoch": 27.22174288179465,
      "grad_norm": 0.00010871426638914272,
      "learning_rate": 0.0004555708944492379,
      "loss": 0.0027,
      "step": 94650
    },
    {
      "epoch": 27.2361230946218,
      "grad_norm": 0.00017835106700658798,
      "learning_rate": 0.00045528329019269486,
      "loss": 0.0049,
      "step": 94700
    },
    {
      "epoch": 27.25050330744895,
      "grad_norm": 0.001176253892481327,
      "learning_rate": 0.0004549956859361519,
      "loss": 0.0009,
      "step": 94750
    },
    {
      "epoch": 27.2648835202761,
      "grad_norm": 0.00011040014214813709,
      "learning_rate": 0.0004547080816796089,
      "loss": 0.0023,
      "step": 94800
    },
    {
      "epoch": 27.27926373310325,
      "grad_norm": 0.0003030775405932218,
      "learning_rate": 0.00045442047742306585,
      "loss": 0.0014,
      "step": 94850
    },
    {
      "epoch": 27.2936439459304,
      "grad_norm": 0.000448485923698172,
      "learning_rate": 0.00045413287316652286,
      "loss": 0.003,
      "step": 94900
    },
    {
      "epoch": 27.30802415875755,
      "grad_norm": 9.44262501434423e-05,
      "learning_rate": 0.0004538452689099799,
      "loss": 0.0049,
      "step": 94950
    },
    {
      "epoch": 27.3224043715847,
      "grad_norm": 0.0014963094145059586,
      "learning_rate": 0.0004535576646534369,
      "loss": 0.0031,
      "step": 95000
    },
    {
      "epoch": 27.33678458441185,
      "grad_norm": 1.4205495062924456e-05,
      "learning_rate": 0.0004532700603968939,
      "loss": 0.0036,
      "step": 95050
    },
    {
      "epoch": 27.351164797239,
      "grad_norm": 0.0008403887040913105,
      "learning_rate": 0.0004529824561403509,
      "loss": 0.0031,
      "step": 95100
    },
    {
      "epoch": 27.36554501006615,
      "grad_norm": 8.504475408699363e-05,
      "learning_rate": 0.00045269485188380787,
      "loss": 0.0025,
      "step": 95150
    },
    {
      "epoch": 27.3799252228933,
      "grad_norm": 0.018221180886030197,
      "learning_rate": 0.0004524072476272649,
      "loss": 0.0003,
      "step": 95200
    },
    {
      "epoch": 27.394305435720447,
      "grad_norm": 0.00010225074947811663,
      "learning_rate": 0.00045211964337072194,
      "loss": 0.0022,
      "step": 95250
    },
    {
      "epoch": 27.4086856485476,
      "grad_norm": 0.0002668265951797366,
      "learning_rate": 0.0004518320391141789,
      "loss": 0.0009,
      "step": 95300
    },
    {
      "epoch": 27.42306586137475,
      "grad_norm": 0.0002750588464550674,
      "learning_rate": 0.0004515444348576359,
      "loss": 0.0078,
      "step": 95350
    },
    {
      "epoch": 27.437446074201898,
      "grad_norm": 0.0005193598917685449,
      "learning_rate": 0.00045125683060109293,
      "loss": 0.0021,
      "step": 95400
    },
    {
      "epoch": 27.451826287029046,
      "grad_norm": 0.0004308051720727235,
      "learning_rate": 0.0004509692263445499,
      "loss": 0.0049,
      "step": 95450
    },
    {
      "epoch": 27.4662064998562,
      "grad_norm": 0.00019272950885351747,
      "learning_rate": 0.0004506816220880069,
      "loss": 0.0018,
      "step": 95500
    },
    {
      "epoch": 27.480586712683348,
      "grad_norm": 0.014450493268668652,
      "learning_rate": 0.00045039401783146396,
      "loss": 0.0016,
      "step": 95550
    },
    {
      "epoch": 27.494966925510496,
      "grad_norm": 0.0004530915175564587,
      "learning_rate": 0.0004501064135749209,
      "loss": 0.0004,
      "step": 95600
    },
    {
      "epoch": 27.50934713833765,
      "grad_norm": 0.0003434056125115603,
      "learning_rate": 0.00044981880931837793,
      "loss": 0.0007,
      "step": 95650
    },
    {
      "epoch": 27.523727351164798,
      "grad_norm": 0.0006334278150461614,
      "learning_rate": 0.00044953120506183495,
      "loss": 0.0032,
      "step": 95700
    },
    {
      "epoch": 27.538107563991947,
      "grad_norm": 3.0615556170232594e-05,
      "learning_rate": 0.0004492436008052919,
      "loss": 0.0036,
      "step": 95750
    },
    {
      "epoch": 27.552487776819095,
      "grad_norm": 0.00019191400497220457,
      "learning_rate": 0.0004489559965487489,
      "loss": 0.0035,
      "step": 95800
    },
    {
      "epoch": 27.566867989646248,
      "grad_norm": 4.1123210394289345e-05,
      "learning_rate": 0.00044866839229220593,
      "loss": 0.0032,
      "step": 95850
    },
    {
      "epoch": 27.581248202473397,
      "grad_norm": 9.69564207480289e-05,
      "learning_rate": 0.00044838078803566294,
      "loss": 0.0028,
      "step": 95900
    },
    {
      "epoch": 27.595628415300546,
      "grad_norm": 0.000170504383277148,
      "learning_rate": 0.00044809318377911995,
      "loss": 0.0017,
      "step": 95950
    },
    {
      "epoch": 27.610008628127698,
      "grad_norm": 0.0026102529373019934,
      "learning_rate": 0.00044780557952257697,
      "loss": 0.0014,
      "step": 96000
    },
    {
      "epoch": 27.624388840954847,
      "grad_norm": 0.00011654137779260054,
      "learning_rate": 0.0004475179752660339,
      "loss": 0.0017,
      "step": 96050
    },
    {
      "epoch": 27.638769053781996,
      "grad_norm": 0.00019376895215827972,
      "learning_rate": 0.00044723037100949094,
      "loss": 0.0013,
      "step": 96100
    },
    {
      "epoch": 27.653149266609145,
      "grad_norm": 0.002792282262817025,
      "learning_rate": 0.00044694276675294795,
      "loss": 0.006,
      "step": 96150
    },
    {
      "epoch": 27.667529479436297,
      "grad_norm": 0.0023184192832559347,
      "learning_rate": 0.00044665516249640496,
      "loss": 0.0033,
      "step": 96200
    },
    {
      "epoch": 27.681909692263446,
      "grad_norm": 0.031857576221227646,
      "learning_rate": 0.000446367558239862,
      "loss": 0.0013,
      "step": 96250
    },
    {
      "epoch": 27.696289905090595,
      "grad_norm": 0.0001530407025711611,
      "learning_rate": 0.000446079953983319,
      "loss": 0.0016,
      "step": 96300
    },
    {
      "epoch": 27.710670117917743,
      "grad_norm": 8.658976730657741e-05,
      "learning_rate": 0.00044579234972677594,
      "loss": 0.0026,
      "step": 96350
    },
    {
      "epoch": 27.725050330744896,
      "grad_norm": 0.00035261226003058255,
      "learning_rate": 0.00044550474547023296,
      "loss": 0.0012,
      "step": 96400
    },
    {
      "epoch": 27.739430543572045,
      "grad_norm": 0.00013944673992227763,
      "learning_rate": 0.00044521714121368997,
      "loss": 0.0009,
      "step": 96450
    },
    {
      "epoch": 27.753810756399194,
      "grad_norm": 0.00016419426538050175,
      "learning_rate": 0.000444929536957147,
      "loss": 0.007,
      "step": 96500
    },
    {
      "epoch": 27.768190969226346,
      "grad_norm": 0.016148727387189865,
      "learning_rate": 0.000444641932700604,
      "loss": 0.0042,
      "step": 96550
    },
    {
      "epoch": 27.782571182053495,
      "grad_norm": 0.027444321662187576,
      "learning_rate": 0.000444354328444061,
      "loss": 0.0046,
      "step": 96600
    },
    {
      "epoch": 27.796951394880644,
      "grad_norm": 2.8848700821981765e-05,
      "learning_rate": 0.00044406672418751796,
      "loss": 0.0035,
      "step": 96650
    },
    {
      "epoch": 27.811331607707793,
      "grad_norm": 9.584034705767408e-05,
      "learning_rate": 0.000443779119930975,
      "loss": 0.0024,
      "step": 96700
    },
    {
      "epoch": 27.825711820534945,
      "grad_norm": 0.00033753635943867266,
      "learning_rate": 0.000443491515674432,
      "loss": 0.003,
      "step": 96750
    },
    {
      "epoch": 27.840092033362094,
      "grad_norm": 6.278017826844007e-05,
      "learning_rate": 0.000443203911417889,
      "loss": 0.0004,
      "step": 96800
    },
    {
      "epoch": 27.854472246189243,
      "grad_norm": 2.9158101824577898e-05,
      "learning_rate": 0.000442916307161346,
      "loss": 0.0017,
      "step": 96850
    },
    {
      "epoch": 27.868852459016395,
      "grad_norm": 2.0412528101587668e-05,
      "learning_rate": 0.000442628702904803,
      "loss": 0.0011,
      "step": 96900
    },
    {
      "epoch": 27.883232671843544,
      "grad_norm": 0.00010578402725514024,
      "learning_rate": 0.00044234109864826,
      "loss": 0.001,
      "step": 96950
    },
    {
      "epoch": 27.897612884670693,
      "grad_norm": 0.0001443902001483366,
      "learning_rate": 0.000442053494391717,
      "loss": 0.0021,
      "step": 97000
    },
    {
      "epoch": 27.91199309749784,
      "grad_norm": 0.00017620909784454852,
      "learning_rate": 0.000441765890135174,
      "loss": 0.0014,
      "step": 97050
    },
    {
      "epoch": 27.926373310324994,
      "grad_norm": 0.0016659281682223082,
      "learning_rate": 0.000441478285878631,
      "loss": 0.0008,
      "step": 97100
    },
    {
      "epoch": 27.940753523152143,
      "grad_norm": 0.0006765216239728034,
      "learning_rate": 0.00044119068162208803,
      "loss": 0.0002,
      "step": 97150
    },
    {
      "epoch": 27.95513373597929,
      "grad_norm": 0.000276204344118014,
      "learning_rate": 0.00044090307736554504,
      "loss": 0.0013,
      "step": 97200
    },
    {
      "epoch": 27.969513948806444,
      "grad_norm": 0.0008479442331008613,
      "learning_rate": 0.000440615473109002,
      "loss": 0.0013,
      "step": 97250
    },
    {
      "epoch": 27.983894161633593,
      "grad_norm": 0.0004135049239266664,
      "learning_rate": 0.000440327868852459,
      "loss": 0.001,
      "step": 97300
    },
    {
      "epoch": 27.998274374460742,
      "grad_norm": 0.00043289424502290785,
      "learning_rate": 0.00044004026459591603,
      "loss": 0.0016,
      "step": 97350
    },
    {
      "epoch": 28.0,
      "eval_loss": 0.010310901328921318,
      "eval_runtime": 18.3504,
      "eval_samples_per_second": 2600.437,
      "eval_steps_per_second": 40.653,
      "step": 97356
    },
    {
      "epoch": 28.01265458728789,
      "grad_norm": 0.00012047240306856111,
      "learning_rate": 0.00043975266033937304,
      "loss": 0.0006,
      "step": 97400
    },
    {
      "epoch": 28.027034800115043,
      "grad_norm": 0.0024692716542631388,
      "learning_rate": 0.00043946505608283005,
      "loss": 0.002,
      "step": 97450
    },
    {
      "epoch": 28.041415012942192,
      "grad_norm": 7.886320963734761e-05,
      "learning_rate": 0.00043917745182628706,
      "loss": 0.0014,
      "step": 97500
    },
    {
      "epoch": 28.05579522576934,
      "grad_norm": 0.007407983299344778,
      "learning_rate": 0.000438889847569744,
      "loss": 0.0021,
      "step": 97550
    },
    {
      "epoch": 28.07017543859649,
      "grad_norm": 7.831439870642498e-05,
      "learning_rate": 0.00043860224331320103,
      "loss": 0.0008,
      "step": 97600
    },
    {
      "epoch": 28.084555651423642,
      "grad_norm": 0.00012552757107187063,
      "learning_rate": 0.00043831463905665805,
      "loss": 0.003,
      "step": 97650
    },
    {
      "epoch": 28.09893586425079,
      "grad_norm": 0.009124459698796272,
      "learning_rate": 0.00043802703480011506,
      "loss": 0.0008,
      "step": 97700
    },
    {
      "epoch": 28.11331607707794,
      "grad_norm": 0.00010222512355539948,
      "learning_rate": 0.00043773943054357207,
      "loss": 0.0016,
      "step": 97750
    },
    {
      "epoch": 28.127696289905092,
      "grad_norm": 0.0004568070871755481,
      "learning_rate": 0.0004374518262870291,
      "loss": 0.0022,
      "step": 97800
    },
    {
      "epoch": 28.14207650273224,
      "grad_norm": 0.0005640172748826444,
      "learning_rate": 0.00043716422203048604,
      "loss": 0.0052,
      "step": 97850
    },
    {
      "epoch": 28.15645671555939,
      "grad_norm": 0.0005318869953043759,
      "learning_rate": 0.00043687661777394305,
      "loss": 0.0019,
      "step": 97900
    },
    {
      "epoch": 28.17083692838654,
      "grad_norm": 0.03847998380661011,
      "learning_rate": 0.00043658901351740007,
      "loss": 0.0005,
      "step": 97950
    },
    {
      "epoch": 28.18521714121369,
      "grad_norm": 2.0251232854207046e-05,
      "learning_rate": 0.0004363014092608571,
      "loss": 0.0036,
      "step": 98000
    },
    {
      "epoch": 28.19959735404084,
      "grad_norm": 0.011956777423620224,
      "learning_rate": 0.0004360138050043141,
      "loss": 0.0026,
      "step": 98050
    },
    {
      "epoch": 28.21397756686799,
      "grad_norm": 0.0012150814291089773,
      "learning_rate": 0.0004357262007477711,
      "loss": 0.0048,
      "step": 98100
    },
    {
      "epoch": 28.228357779695138,
      "grad_norm": 0.00011409037688281387,
      "learning_rate": 0.00043543859649122806,
      "loss": 0.0022,
      "step": 98150
    },
    {
      "epoch": 28.24273799252229,
      "grad_norm": 5.649794184137136e-05,
      "learning_rate": 0.0004351509922346851,
      "loss": 0.002,
      "step": 98200
    },
    {
      "epoch": 28.25711820534944,
      "grad_norm": 0.0007688845507800579,
      "learning_rate": 0.0004348633879781421,
      "loss": 0.0022,
      "step": 98250
    },
    {
      "epoch": 28.271498418176588,
      "grad_norm": 0.008842147886753082,
      "learning_rate": 0.0004345757837215991,
      "loss": 0.0035,
      "step": 98300
    },
    {
      "epoch": 28.28587863100374,
      "grad_norm": 3.665094845928252e-05,
      "learning_rate": 0.0004342881794650561,
      "loss": 0.0006,
      "step": 98350
    },
    {
      "epoch": 28.30025884383089,
      "grad_norm": 0.008572505787014961,
      "learning_rate": 0.0004340005752085131,
      "loss": 0.0034,
      "step": 98400
    },
    {
      "epoch": 28.314639056658038,
      "grad_norm": 0.020834187045693398,
      "learning_rate": 0.0004337129709519701,
      "loss": 0.0015,
      "step": 98450
    },
    {
      "epoch": 28.329019269485187,
      "grad_norm": 4.9763119022827595e-05,
      "learning_rate": 0.0004334253666954271,
      "loss": 0.0025,
      "step": 98500
    },
    {
      "epoch": 28.34339948231234,
      "grad_norm": 7.752827514195815e-05,
      "learning_rate": 0.0004331377624388841,
      "loss": 0.0022,
      "step": 98550
    },
    {
      "epoch": 28.357779695139488,
      "grad_norm": 9.833373042056337e-05,
      "learning_rate": 0.00043285015818234106,
      "loss": 0.0016,
      "step": 98600
    },
    {
      "epoch": 28.372159907966637,
      "grad_norm": 0.004646908491849899,
      "learning_rate": 0.00043256255392579813,
      "loss": 0.0018,
      "step": 98650
    },
    {
      "epoch": 28.38654012079379,
      "grad_norm": 0.0003963612543884665,
      "learning_rate": 0.00043227494966925514,
      "loss": 0.0028,
      "step": 98700
    },
    {
      "epoch": 28.400920333620938,
      "grad_norm": 0.0021364022977650166,
      "learning_rate": 0.0004319873454127121,
      "loss": 0.0008,
      "step": 98750
    },
    {
      "epoch": 28.415300546448087,
      "grad_norm": 0.003674163017421961,
      "learning_rate": 0.0004316997411561691,
      "loss": 0.0055,
      "step": 98800
    },
    {
      "epoch": 28.429680759275236,
      "grad_norm": 0.00032086941064335406,
      "learning_rate": 0.0004314121368996261,
      "loss": 0.0036,
      "step": 98850
    },
    {
      "epoch": 28.44406097210239,
      "grad_norm": 0.0060160961002111435,
      "learning_rate": 0.0004311245326430831,
      "loss": 0.0021,
      "step": 98900
    },
    {
      "epoch": 28.458441184929537,
      "grad_norm": 0.0001331884559476748,
      "learning_rate": 0.00043083692838654015,
      "loss": 0.001,
      "step": 98950
    },
    {
      "epoch": 28.472821397756686,
      "grad_norm": 0.00019883956701960415,
      "learning_rate": 0.00043054932412999716,
      "loss": 0.0031,
      "step": 99000
    },
    {
      "epoch": 28.48720161058384,
      "grad_norm": 0.00024943429161794484,
      "learning_rate": 0.0004302617198734541,
      "loss": 0.0017,
      "step": 99050
    },
    {
      "epoch": 28.501581823410987,
      "grad_norm": 0.000410733453463763,
      "learning_rate": 0.00042997411561691113,
      "loss": 0.0012,
      "step": 99100
    },
    {
      "epoch": 28.515962036238136,
      "grad_norm": 0.0011632194509729743,
      "learning_rate": 0.00042968651136036814,
      "loss": 0.0018,
      "step": 99150
    },
    {
      "epoch": 28.530342249065285,
      "grad_norm": 0.0001254816452274099,
      "learning_rate": 0.0004293989071038251,
      "loss": 0.0006,
      "step": 99200
    },
    {
      "epoch": 28.544722461892437,
      "grad_norm": 0.0011242832988500595,
      "learning_rate": 0.00042911130284728217,
      "loss": 0.0018,
      "step": 99250
    },
    {
      "epoch": 28.559102674719586,
      "grad_norm": 0.022391285747289658,
      "learning_rate": 0.0004288236985907392,
      "loss": 0.0032,
      "step": 99300
    },
    {
      "epoch": 28.573482887546735,
      "grad_norm": 7.664545410079882e-05,
      "learning_rate": 0.00042853609433419614,
      "loss": 0.002,
      "step": 99350
    },
    {
      "epoch": 28.587863100373884,
      "grad_norm": 0.0049238079227507114,
      "learning_rate": 0.00042824849007765315,
      "loss": 0.0035,
      "step": 99400
    },
    {
      "epoch": 28.602243313201036,
      "grad_norm": 0.0007406108197756112,
      "learning_rate": 0.00042796088582111016,
      "loss": 0.0023,
      "step": 99450
    },
    {
      "epoch": 28.616623526028185,
      "grad_norm": 0.00022629614977631718,
      "learning_rate": 0.0004276732815645671,
      "loss": 0.0041,
      "step": 99500
    },
    {
      "epoch": 28.631003738855334,
      "grad_norm": 0.00040088349487632513,
      "learning_rate": 0.0004273856773080242,
      "loss": 0.0044,
      "step": 99550
    },
    {
      "epoch": 28.645383951682486,
      "grad_norm": 0.004574945196509361,
      "learning_rate": 0.0004270980730514812,
      "loss": 0.0034,
      "step": 99600
    },
    {
      "epoch": 28.659764164509635,
      "grad_norm": 2.3312255507335067e-05,
      "learning_rate": 0.00042681046879493816,
      "loss": 0.0029,
      "step": 99650
    },
    {
      "epoch": 28.674144377336784,
      "grad_norm": 0.0013632564805448055,
      "learning_rate": 0.00042652286453839517,
      "loss": 0.0007,
      "step": 99700
    },
    {
      "epoch": 28.688524590163933,
      "grad_norm": 0.014836247079074383,
      "learning_rate": 0.0004262352602818522,
      "loss": 0.0018,
      "step": 99750
    },
    {
      "epoch": 28.702904802991085,
      "grad_norm": 0.010347956791520119,
      "learning_rate": 0.00042594765602530914,
      "loss": 0.0025,
      "step": 99800
    },
    {
      "epoch": 28.717285015818234,
      "grad_norm": 0.00024575574207119644,
      "learning_rate": 0.0004256600517687662,
      "loss": 0.0016,
      "step": 99850
    },
    {
      "epoch": 28.731665228645383,
      "grad_norm": 3.118937820545398e-05,
      "learning_rate": 0.0004253724475122232,
      "loss": 0.0031,
      "step": 99900
    },
    {
      "epoch": 28.746045441472535,
      "grad_norm": 0.0007035997114144266,
      "learning_rate": 0.0004250848432556802,
      "loss": 0.0014,
      "step": 99950
    },
    {
      "epoch": 28.760425654299684,
      "grad_norm": 0.0056207794696092606,
      "learning_rate": 0.0004247972389991372,
      "loss": 0.0028,
      "step": 100000
    },
    {
      "epoch": 28.774805867126833,
      "grad_norm": 0.009825339540839195,
      "learning_rate": 0.0004245096347425942,
      "loss": 0.0038,
      "step": 100050
    },
    {
      "epoch": 28.789186079953982,
      "grad_norm": 4.006321614724584e-05,
      "learning_rate": 0.00042422203048605116,
      "loss": 0.0016,
      "step": 100100
    },
    {
      "epoch": 28.803566292781134,
      "grad_norm": 0.05240846052765846,
      "learning_rate": 0.00042393442622950823,
      "loss": 0.0019,
      "step": 100150
    },
    {
      "epoch": 28.817946505608283,
      "grad_norm": 8.010369492694736e-05,
      "learning_rate": 0.00042364682197296524,
      "loss": 0.0017,
      "step": 100200
    },
    {
      "epoch": 28.832326718435432,
      "grad_norm": 0.020068813115358353,
      "learning_rate": 0.0004233592177164222,
      "loss": 0.0038,
      "step": 100250
    },
    {
      "epoch": 28.84670693126258,
      "grad_norm": 0.01782916858792305,
      "learning_rate": 0.0004230716134598792,
      "loss": 0.0021,
      "step": 100300
    },
    {
      "epoch": 28.861087144089733,
      "grad_norm": 0.0004921096842736006,
      "learning_rate": 0.0004227840092033362,
      "loss": 0.0023,
      "step": 100350
    },
    {
      "epoch": 28.875467356916882,
      "grad_norm": 0.00011977157555520535,
      "learning_rate": 0.0004224964049467932,
      "loss": 0.0005,
      "step": 100400
    },
    {
      "epoch": 28.88984756974403,
      "grad_norm": 0.013778581283986568,
      "learning_rate": 0.00042220880069025025,
      "loss": 0.005,
      "step": 100450
    },
    {
      "epoch": 28.904227782571184,
      "grad_norm": 0.025986822322010994,
      "learning_rate": 0.00042192119643370726,
      "loss": 0.0027,
      "step": 100500
    },
    {
      "epoch": 28.918607995398332,
      "grad_norm": 3.0790757591603324e-05,
      "learning_rate": 0.0004216335921771642,
      "loss": 0.0031,
      "step": 100550
    },
    {
      "epoch": 28.93298820822548,
      "grad_norm": 0.004129617474973202,
      "learning_rate": 0.00042134598792062123,
      "loss": 0.0006,
      "step": 100600
    },
    {
      "epoch": 28.94736842105263,
      "grad_norm": 0.004811330698430538,
      "learning_rate": 0.00042105838366407824,
      "loss": 0.0042,
      "step": 100650
    },
    {
      "epoch": 28.961748633879782,
      "grad_norm": 0.0011397296329960227,
      "learning_rate": 0.0004207707794075352,
      "loss": 0.0059,
      "step": 100700
    },
    {
      "epoch": 28.97612884670693,
      "grad_norm": 5.8749417803483084e-05,
      "learning_rate": 0.00042048317515099227,
      "loss": 0.0014,
      "step": 100750
    },
    {
      "epoch": 28.99050905953408,
      "grad_norm": 3.1415467674378306e-05,
      "learning_rate": 0.0004201955708944493,
      "loss": 0.0025,
      "step": 100800
    },
    {
      "epoch": 29.0,
      "eval_loss": 0.009959611110389233,
      "eval_runtime": 17.3901,
      "eval_samples_per_second": 2744.025,
      "eval_steps_per_second": 42.898,
      "step": 100833
    },
    {
      "epoch": 29.004889272361233,
      "grad_norm": 1.6248592146439478e-05,
      "learning_rate": 0.00041990796663790624,
      "loss": 0.0025,
      "step": 100850
    },
    {
      "epoch": 29.01926948518838,
      "grad_norm": 0.0031261357944458723,
      "learning_rate": 0.00041962036238136325,
      "loss": 0.0025,
      "step": 100900
    },
    {
      "epoch": 29.03364969801553,
      "grad_norm": 0.017805321142077446,
      "learning_rate": 0.00041933275812482026,
      "loss": 0.0005,
      "step": 100950
    },
    {
      "epoch": 29.04802991084268,
      "grad_norm": 6.802597636124119e-05,
      "learning_rate": 0.0004190451538682772,
      "loss": 0.0006,
      "step": 101000
    },
    {
      "epoch": 29.06241012366983,
      "grad_norm": 0.00021321761596482247,
      "learning_rate": 0.0004187575496117343,
      "loss": 0.0032,
      "step": 101050
    },
    {
      "epoch": 29.07679033649698,
      "grad_norm": 0.0002933911746367812,
      "learning_rate": 0.0004184699453551913,
      "loss": 0.0032,
      "step": 101100
    },
    {
      "epoch": 29.09117054932413,
      "grad_norm": 3.638838461483829e-05,
      "learning_rate": 0.00041818234109864826,
      "loss": 0.0043,
      "step": 101150
    },
    {
      "epoch": 29.105550762151278,
      "grad_norm": 0.0002675021532922983,
      "learning_rate": 0.00041789473684210527,
      "loss": 0.0019,
      "step": 101200
    },
    {
      "epoch": 29.11993097497843,
      "grad_norm": 0.000372011010767892,
      "learning_rate": 0.0004176071325855623,
      "loss": 0.0056,
      "step": 101250
    },
    {
      "epoch": 29.13431118780558,
      "grad_norm": 8.227900980273262e-05,
      "learning_rate": 0.00041731952832901924,
      "loss": 0.0011,
      "step": 101300
    },
    {
      "epoch": 29.14869140063273,
      "grad_norm": 0.00017277075676247478,
      "learning_rate": 0.00041703192407247625,
      "loss": 0.0021,
      "step": 101350
    },
    {
      "epoch": 29.16307161345988,
      "grad_norm": 0.0025292111095041037,
      "learning_rate": 0.0004167443198159333,
      "loss": 0.0021,
      "step": 101400
    },
    {
      "epoch": 29.17745182628703,
      "grad_norm": 0.00018380882102064788,
      "learning_rate": 0.0004164567155593903,
      "loss": 0.003,
      "step": 101450
    },
    {
      "epoch": 29.19183203911418,
      "grad_norm": 0.013374656438827515,
      "learning_rate": 0.0004161691113028473,
      "loss": 0.0061,
      "step": 101500
    },
    {
      "epoch": 29.206212251941327,
      "grad_norm": 0.009302915073931217,
      "learning_rate": 0.0004158815070463043,
      "loss": 0.0016,
      "step": 101550
    },
    {
      "epoch": 29.22059246476848,
      "grad_norm": 0.003271674970164895,
      "learning_rate": 0.00041559390278976126,
      "loss": 0.005,
      "step": 101600
    },
    {
      "epoch": 29.23497267759563,
      "grad_norm": 0.007529269903898239,
      "learning_rate": 0.00041530629853321827,
      "loss": 0.0022,
      "step": 101650
    },
    {
      "epoch": 29.249352890422777,
      "grad_norm": 0.0003957060689572245,
      "learning_rate": 0.00041501869427667534,
      "loss": 0.0012,
      "step": 101700
    },
    {
      "epoch": 29.26373310324993,
      "grad_norm": 0.005828162655234337,
      "learning_rate": 0.0004147310900201323,
      "loss": 0.0013,
      "step": 101750
    },
    {
      "epoch": 29.27811331607708,
      "grad_norm": 0.014364287257194519,
      "learning_rate": 0.0004144434857635893,
      "loss": 0.0062,
      "step": 101800
    },
    {
      "epoch": 29.292493528904227,
      "grad_norm": 0.0001896284520626068,
      "learning_rate": 0.0004141558815070463,
      "loss": 0.002,
      "step": 101850
    },
    {
      "epoch": 29.306873741731376,
      "grad_norm": 3.409523196751252e-05,
      "learning_rate": 0.0004138682772505033,
      "loss": 0.001,
      "step": 101900
    },
    {
      "epoch": 29.32125395455853,
      "grad_norm": 0.0006067252834327519,
      "learning_rate": 0.0004135806729939603,
      "loss": 0.002,
      "step": 101950
    },
    {
      "epoch": 29.335634167385678,
      "grad_norm": 0.005071390885859728,
      "learning_rate": 0.00041329306873741736,
      "loss": 0.0032,
      "step": 102000
    },
    {
      "epoch": 29.350014380212826,
      "grad_norm": 0.013151600025594234,
      "learning_rate": 0.0004130054644808743,
      "loss": 0.0008,
      "step": 102050
    },
    {
      "epoch": 29.364394593039975,
      "grad_norm": 9.38129669521004e-05,
      "learning_rate": 0.00041271786022433133,
      "loss": 0.004,
      "step": 102100
    },
    {
      "epoch": 29.378774805867128,
      "grad_norm": 0.07603947818279266,
      "learning_rate": 0.00041243025596778834,
      "loss": 0.0007,
      "step": 102150
    },
    {
      "epoch": 29.393155018694276,
      "grad_norm": 0.0022872143890708685,
      "learning_rate": 0.0004121426517112453,
      "loss": 0.002,
      "step": 102200
    },
    {
      "epoch": 29.407535231521425,
      "grad_norm": 5.534366573556326e-05,
      "learning_rate": 0.0004118550474547023,
      "loss": 0.0041,
      "step": 102250
    },
    {
      "epoch": 29.421915444348578,
      "grad_norm": 0.016933390870690346,
      "learning_rate": 0.0004115674431981594,
      "loss": 0.0011,
      "step": 102300
    },
    {
      "epoch": 29.436295657175727,
      "grad_norm": 3.8864644011482596e-05,
      "learning_rate": 0.00041127983894161634,
      "loss": 0.0012,
      "step": 102350
    },
    {
      "epoch": 29.450675870002875,
      "grad_norm": 2.494372529326938e-05,
      "learning_rate": 0.00041099223468507335,
      "loss": 0.0016,
      "step": 102400
    },
    {
      "epoch": 29.465056082830024,
      "grad_norm": 0.042719677090644836,
      "learning_rate": 0.00041070463042853036,
      "loss": 0.0061,
      "step": 102450
    },
    {
      "epoch": 29.479436295657177,
      "grad_norm": 0.0006492017419077456,
      "learning_rate": 0.0004104170261719873,
      "loss": 0.0017,
      "step": 102500
    },
    {
      "epoch": 29.493816508484326,
      "grad_norm": 3.325044235680252e-05,
      "learning_rate": 0.00041012942191544433,
      "loss": 0.002,
      "step": 102550
    },
    {
      "epoch": 29.508196721311474,
      "grad_norm": 0.000352359376847744,
      "learning_rate": 0.0004098418176589014,
      "loss": 0.0018,
      "step": 102600
    },
    {
      "epoch": 29.522576934138627,
      "grad_norm": 9.649474668549374e-05,
      "learning_rate": 0.00040955421340235835,
      "loss": 0.0024,
      "step": 102650
    },
    {
      "epoch": 29.536957146965776,
      "grad_norm": 0.0001554259506519884,
      "learning_rate": 0.00040926660914581537,
      "loss": 0.001,
      "step": 102700
    },
    {
      "epoch": 29.551337359792925,
      "grad_norm": 0.0062026530504226685,
      "learning_rate": 0.0004089790048892724,
      "loss": 0.002,
      "step": 102750
    },
    {
      "epoch": 29.565717572620073,
      "grad_norm": 0.05090254545211792,
      "learning_rate": 0.00040869140063272934,
      "loss": 0.003,
      "step": 102800
    },
    {
      "epoch": 29.580097785447226,
      "grad_norm": 0.0012288687285035849,
      "learning_rate": 0.00040840379637618635,
      "loss": 0.0024,
      "step": 102850
    },
    {
      "epoch": 29.594477998274375,
      "grad_norm": 0.012206641025841236,
      "learning_rate": 0.0004081161921196434,
      "loss": 0.0029,
      "step": 102900
    },
    {
      "epoch": 29.608858211101523,
      "grad_norm": 0.006473998539149761,
      "learning_rate": 0.0004078285878631004,
      "loss": 0.0014,
      "step": 102950
    },
    {
      "epoch": 29.623238423928676,
      "grad_norm": 0.018299169838428497,
      "learning_rate": 0.0004075409836065574,
      "loss": 0.0002,
      "step": 103000
    },
    {
      "epoch": 29.637618636755825,
      "grad_norm": 0.001840647659264505,
      "learning_rate": 0.0004072533793500144,
      "loss": 0.0016,
      "step": 103050
    },
    {
      "epoch": 29.651998849582974,
      "grad_norm": 0.00016766719636507332,
      "learning_rate": 0.00040696577509347136,
      "loss": 0.0018,
      "step": 103100
    },
    {
      "epoch": 29.666379062410122,
      "grad_norm": 2.506826422177255e-05,
      "learning_rate": 0.00040667817083692837,
      "loss": 0.0039,
      "step": 103150
    },
    {
      "epoch": 29.680759275237275,
      "grad_norm": 0.0014343175571411848,
      "learning_rate": 0.00040639056658038544,
      "loss": 0.0026,
      "step": 103200
    },
    {
      "epoch": 29.695139488064424,
      "grad_norm": 0.002664418425410986,
      "learning_rate": 0.0004061029623238424,
      "loss": 0.002,
      "step": 103250
    },
    {
      "epoch": 29.709519700891573,
      "grad_norm": 0.0007257041870616376,
      "learning_rate": 0.0004058153580672994,
      "loss": 0.0013,
      "step": 103300
    },
    {
      "epoch": 29.72389991371872,
      "grad_norm": 0.001946849632076919,
      "learning_rate": 0.0004055277538107564,
      "loss": 0.0021,
      "step": 103350
    },
    {
      "epoch": 29.738280126545874,
      "grad_norm": 3.7694502680096775e-05,
      "learning_rate": 0.0004052401495542134,
      "loss": 0.0036,
      "step": 103400
    },
    {
      "epoch": 29.752660339373023,
      "grad_norm": 0.020060008391737938,
      "learning_rate": 0.0004049525452976704,
      "loss": 0.0047,
      "step": 103450
    },
    {
      "epoch": 29.76704055220017,
      "grad_norm": 6.612011202378199e-05,
      "learning_rate": 0.00040466494104112746,
      "loss": 0.0046,
      "step": 103500
    },
    {
      "epoch": 29.781420765027324,
      "grad_norm": 0.00011499259562697262,
      "learning_rate": 0.0004043773367845844,
      "loss": 0.0011,
      "step": 103550
    },
    {
      "epoch": 29.795800977854473,
      "grad_norm": 8.424076804658398e-05,
      "learning_rate": 0.0004040897325280414,
      "loss": 0.0013,
      "step": 103600
    },
    {
      "epoch": 29.81018119068162,
      "grad_norm": 0.0016389250522479415,
      "learning_rate": 0.00040380212827149844,
      "loss": 0.0049,
      "step": 103650
    },
    {
      "epoch": 29.82456140350877,
      "grad_norm": 0.00395714258775115,
      "learning_rate": 0.0004035145240149554,
      "loss": 0.003,
      "step": 103700
    },
    {
      "epoch": 29.838941616335923,
      "grad_norm": 0.0025636451318860054,
      "learning_rate": 0.0004032269197584124,
      "loss": 0.0028,
      "step": 103750
    },
    {
      "epoch": 29.85332182916307,
      "grad_norm": 3.005488179042004e-05,
      "learning_rate": 0.0004029393155018695,
      "loss": 0.0007,
      "step": 103800
    },
    {
      "epoch": 29.86770204199022,
      "grad_norm": 0.006152760703116655,
      "learning_rate": 0.00040265171124532643,
      "loss": 0.001,
      "step": 103850
    },
    {
      "epoch": 29.882082254817373,
      "grad_norm": 4.4906009861733764e-05,
      "learning_rate": 0.00040236410698878345,
      "loss": 0.0019,
      "step": 103900
    },
    {
      "epoch": 29.896462467644522,
      "grad_norm": 0.004818960092961788,
      "learning_rate": 0.00040207650273224046,
      "loss": 0.0028,
      "step": 103950
    },
    {
      "epoch": 29.91084268047167,
      "grad_norm": 0.0022484285291284323,
      "learning_rate": 0.0004017888984756974,
      "loss": 0.005,
      "step": 104000
    },
    {
      "epoch": 29.92522289329882,
      "grad_norm": 5.9496429457794875e-05,
      "learning_rate": 0.00040150129421915443,
      "loss": 0.0013,
      "step": 104050
    },
    {
      "epoch": 29.939603106125972,
      "grad_norm": 0.0009270153241232038,
      "learning_rate": 0.00040121368996261144,
      "loss": 0.0009,
      "step": 104100
    },
    {
      "epoch": 29.95398331895312,
      "grad_norm": 7.865531370043755e-05,
      "learning_rate": 0.00040092608570606845,
      "loss": 0.0011,
      "step": 104150
    },
    {
      "epoch": 29.96836353178027,
      "grad_norm": 0.0007691539940424263,
      "learning_rate": 0.00040063848144952546,
      "loss": 0.0029,
      "step": 104200
    },
    {
      "epoch": 29.98274374460742,
      "grad_norm": 0.021672027185559273,
      "learning_rate": 0.0004003508771929825,
      "loss": 0.0003,
      "step": 104250
    },
    {
      "epoch": 29.99712395743457,
      "grad_norm": 0.0025435597635805607,
      "learning_rate": 0.00040006327293643944,
      "loss": 0.0017,
      "step": 104300
    },
    {
      "epoch": 30.0,
      "eval_loss": 0.010465999133884907,
      "eval_runtime": 16.9035,
      "eval_samples_per_second": 2823.027,
      "eval_steps_per_second": 44.133,
      "step": 104310
    },
    {
      "epoch": 30.01150417026172,
      "grad_norm": 0.00017909253074321896,
      "learning_rate": 0.00039977566867989645,
      "loss": 0.0006,
      "step": 104350
    },
    {
      "epoch": 30.02588438308887,
      "grad_norm": 0.00013026421947870404,
      "learning_rate": 0.00039948806442335346,
      "loss": 0.0051,
      "step": 104400
    },
    {
      "epoch": 30.04026459591602,
      "grad_norm": 2.3398226403514855e-05,
      "learning_rate": 0.00039920046016681047,
      "loss": 0.0009,
      "step": 104450
    },
    {
      "epoch": 30.05464480874317,
      "grad_norm": 6.80767698213458e-05,
      "learning_rate": 0.0003989128559102675,
      "loss": 0.0011,
      "step": 104500
    },
    {
      "epoch": 30.06902502157032,
      "grad_norm": 6.238506466615945e-05,
      "learning_rate": 0.0003986252516537245,
      "loss": 0.0035,
      "step": 104550
    },
    {
      "epoch": 30.083405234397468,
      "grad_norm": 0.01584293693304062,
      "learning_rate": 0.00039833764739718145,
      "loss": 0.0027,
      "step": 104600
    },
    {
      "epoch": 30.09778544722462,
      "grad_norm": 0.002590752439573407,
      "learning_rate": 0.00039805004314063847,
      "loss": 0.001,
      "step": 104650
    },
    {
      "epoch": 30.11216566005177,
      "grad_norm": 4.608979361364618e-05,
      "learning_rate": 0.0003977624388840955,
      "loss": 0.0013,
      "step": 104700
    },
    {
      "epoch": 30.126545872878918,
      "grad_norm": 0.00013339295401237905,
      "learning_rate": 0.0003974748346275525,
      "loss": 0.0019,
      "step": 104750
    },
    {
      "epoch": 30.14092608570607,
      "grad_norm": 2.7475338356452994e-05,
      "learning_rate": 0.0003971872303710095,
      "loss": 0.0043,
      "step": 104800
    },
    {
      "epoch": 30.15530629853322,
      "grad_norm": 3.5207172913942486e-05,
      "learning_rate": 0.0003968996261144665,
      "loss": 0.0042,
      "step": 104850
    },
    {
      "epoch": 30.169686511360368,
      "grad_norm": 0.0035792330745607615,
      "learning_rate": 0.0003966120218579235,
      "loss": 0.0008,
      "step": 104900
    },
    {
      "epoch": 30.184066724187517,
      "grad_norm": 0.0016422928310930729,
      "learning_rate": 0.0003963244176013805,
      "loss": 0.0009,
      "step": 104950
    },
    {
      "epoch": 30.19844693701467,
      "grad_norm": 0.01022002100944519,
      "learning_rate": 0.0003960368133448375,
      "loss": 0.003,
      "step": 105000
    },
    {
      "epoch": 30.212827149841818,
      "grad_norm": 0.0014519343385472894,
      "learning_rate": 0.0003957492090882945,
      "loss": 0.0025,
      "step": 105050
    },
    {
      "epoch": 30.227207362668967,
      "grad_norm": 6.0487105656648055e-05,
      "learning_rate": 0.0003954616048317515,
      "loss": 0.002,
      "step": 105100
    },
    {
      "epoch": 30.241587575496116,
      "grad_norm": 0.006304015871137381,
      "learning_rate": 0.00039517400057520854,
      "loss": 0.0052,
      "step": 105150
    },
    {
      "epoch": 30.255967788323268,
      "grad_norm": 2.837341708072927e-05,
      "learning_rate": 0.0003948863963186655,
      "loss": 0.0025,
      "step": 105200
    },
    {
      "epoch": 30.270348001150417,
      "grad_norm": 3.346525409142487e-05,
      "learning_rate": 0.0003945987920621225,
      "loss": 0.0025,
      "step": 105250
    },
    {
      "epoch": 30.284728213977566,
      "grad_norm": 1.732657619868405e-05,
      "learning_rate": 0.0003943111878055795,
      "loss": 0.0038,
      "step": 105300
    },
    {
      "epoch": 30.299108426804718,
      "grad_norm": 0.00031426030909642577,
      "learning_rate": 0.00039402358354903653,
      "loss": 0.0016,
      "step": 105350
    },
    {
      "epoch": 30.313488639631867,
      "grad_norm": 1.6592228348599747e-05,
      "learning_rate": 0.00039373597929249354,
      "loss": 0.0048,
      "step": 105400
    },
    {
      "epoch": 30.327868852459016,
      "grad_norm": 0.00018059044668916613,
      "learning_rate": 0.00039344837503595056,
      "loss": 0.0023,
      "step": 105450
    },
    {
      "epoch": 30.342249065286165,
      "grad_norm": 0.007706746459007263,
      "learning_rate": 0.0003931607707794075,
      "loss": 0.006,
      "step": 105500
    },
    {
      "epoch": 30.356629278113317,
      "grad_norm": 5.597027120529674e-05,
      "learning_rate": 0.0003928731665228645,
      "loss": 0.0018,
      "step": 105550
    },
    {
      "epoch": 30.371009490940466,
      "grad_norm": 3.1553507142234594e-05,
      "learning_rate": 0.00039258556226632154,
      "loss": 0.0011,
      "step": 105600
    },
    {
      "epoch": 30.385389703767615,
      "grad_norm": 0.0010613322956487536,
      "learning_rate": 0.00039229795800977855,
      "loss": 0.0039,
      "step": 105650
    },
    {
      "epoch": 30.399769916594767,
      "grad_norm": 6.535626016557217e-05,
      "learning_rate": 0.00039201035375323556,
      "loss": 0.003,
      "step": 105700
    },
    {
      "epoch": 30.414150129421916,
      "grad_norm": 0.002970564877614379,
      "learning_rate": 0.0003917227494966926,
      "loss": 0.0024,
      "step": 105750
    },
    {
      "epoch": 30.428530342249065,
      "grad_norm": 0.0001376600266667083,
      "learning_rate": 0.00039143514524014953,
      "loss": 0.0058,
      "step": 105800
    },
    {
      "epoch": 30.442910555076214,
      "grad_norm": 0.000118450858280994,
      "learning_rate": 0.00039114754098360655,
      "loss": 0.0021,
      "step": 105850
    },
    {
      "epoch": 30.457290767903366,
      "grad_norm": 0.010167487896978855,
      "learning_rate": 0.00039085993672706356,
      "loss": 0.0009,
      "step": 105900
    },
    {
      "epoch": 30.471670980730515,
      "grad_norm": 0.002831205725669861,
      "learning_rate": 0.0003905723324705206,
      "loss": 0.0008,
      "step": 105950
    },
    {
      "epoch": 30.486051193557664,
      "grad_norm": 7.046416430966929e-05,
      "learning_rate": 0.0003902847282139776,
      "loss": 0.0032,
      "step": 106000
    },
    {
      "epoch": 30.500431406384813,
      "grad_norm": 0.004610269796103239,
      "learning_rate": 0.0003899971239574346,
      "loss": 0.0022,
      "step": 106050
    },
    {
      "epoch": 30.514811619211965,
      "grad_norm": 7.621700933668762e-05,
      "learning_rate": 0.00038970951970089155,
      "loss": 0.0005,
      "step": 106100
    },
    {
      "epoch": 30.529191832039114,
      "grad_norm": 0.0539090596139431,
      "learning_rate": 0.00038942191544434856,
      "loss": 0.0023,
      "step": 106150
    },
    {
      "epoch": 30.543572044866263,
      "grad_norm": 0.010241321288049221,
      "learning_rate": 0.0003891343111878056,
      "loss": 0.0007,
      "step": 106200
    },
    {
      "epoch": 30.557952257693415,
      "grad_norm": 3.7704874557675794e-05,
      "learning_rate": 0.00038884670693126264,
      "loss": 0.0027,
      "step": 106250
    },
    {
      "epoch": 30.572332470520564,
      "grad_norm": 0.00046436814591288567,
      "learning_rate": 0.0003885591026747196,
      "loss": 0.0032,
      "step": 106300
    },
    {
      "epoch": 30.586712683347713,
      "grad_norm": 0.000316528050461784,
      "learning_rate": 0.0003882714984181766,
      "loss": 0.0024,
      "step": 106350
    },
    {
      "epoch": 30.601092896174862,
      "grad_norm": 7.530424772994593e-05,
      "learning_rate": 0.0003879838941616336,
      "loss": 0.0013,
      "step": 106400
    },
    {
      "epoch": 30.615473109002014,
      "grad_norm": 7.138551154639572e-05,
      "learning_rate": 0.0003876962899050906,
      "loss": 0.0034,
      "step": 106450
    },
    {
      "epoch": 30.629853321829163,
      "grad_norm": 0.00017140332784038037,
      "learning_rate": 0.0003874086856485476,
      "loss": 0.0006,
      "step": 106500
    },
    {
      "epoch": 30.644233534656312,
      "grad_norm": 0.0009110156097449362,
      "learning_rate": 0.00038712108139200466,
      "loss": 0.0026,
      "step": 106550
    },
    {
      "epoch": 30.658613747483464,
      "grad_norm": 0.00010405695502413437,
      "learning_rate": 0.0003868334771354616,
      "loss": 0.0006,
      "step": 106600
    },
    {
      "epoch": 30.672993960310613,
      "grad_norm": 7.196259684860706e-05,
      "learning_rate": 0.00038654587287891863,
      "loss": 0.0036,
      "step": 106650
    },
    {
      "epoch": 30.687374173137762,
      "grad_norm": 0.00015925888146739453,
      "learning_rate": 0.00038625826862237565,
      "loss": 0.0028,
      "step": 106700
    },
    {
      "epoch": 30.70175438596491,
      "grad_norm": 0.0001506232365500182,
      "learning_rate": 0.0003859706643658326,
      "loss": 0.0031,
      "step": 106750
    },
    {
      "epoch": 30.716134598792063,
      "grad_norm": 8.19658234831877e-05,
      "learning_rate": 0.0003856830601092896,
      "loss": 0.0035,
      "step": 106800
    },
    {
      "epoch": 30.730514811619212,
      "grad_norm": 4.332120806793682e-05,
      "learning_rate": 0.0003853954558527466,
      "loss": 0.0006,
      "step": 106850
    },
    {
      "epoch": 30.74489502444636,
      "grad_norm": 1.6494353985763155e-05,
      "learning_rate": 0.00038510785159620364,
      "loss": 0.0013,
      "step": 106900
    },
    {
      "epoch": 30.759275237273513,
      "grad_norm": 2.9609813282149844e-05,
      "learning_rate": 0.00038482024733966065,
      "loss": 0.0033,
      "step": 106950
    },
    {
      "epoch": 30.773655450100662,
      "grad_norm": 0.0002761978539638221,
      "learning_rate": 0.00038453264308311767,
      "loss": 0.0024,
      "step": 107000
    },
    {
      "epoch": 30.78803566292781,
      "grad_norm": 0.00025673789787106216,
      "learning_rate": 0.0003842450388265746,
      "loss": 0.0011,
      "step": 107050
    },
    {
      "epoch": 30.80241587575496,
      "grad_norm": 0.0004046726389788091,
      "learning_rate": 0.00038395743457003164,
      "loss": 0.0003,
      "step": 107100
    },
    {
      "epoch": 30.816796088582112,
      "grad_norm": 0.00019228555902373046,
      "learning_rate": 0.00038366983031348865,
      "loss": 0.0021,
      "step": 107150
    },
    {
      "epoch": 30.83117630140926,
      "grad_norm": 5.41762201464735e-05,
      "learning_rate": 0.00038338222605694566,
      "loss": 0.001,
      "step": 107200
    },
    {
      "epoch": 30.84555651423641,
      "grad_norm": 0.02824113331735134,
      "learning_rate": 0.00038309462180040267,
      "loss": 0.0027,
      "step": 107250
    },
    {
      "epoch": 30.85993672706356,
      "grad_norm": 0.012676871381700039,
      "learning_rate": 0.0003828070175438597,
      "loss": 0.0055,
      "step": 107300
    },
    {
      "epoch": 30.87431693989071,
      "grad_norm": 0.0015489893266931176,
      "learning_rate": 0.00038251941328731664,
      "loss": 0.0011,
      "step": 107350
    },
    {
      "epoch": 30.88869715271786,
      "grad_norm": 0.0023819238413125277,
      "learning_rate": 0.00038223180903077366,
      "loss": 0.0016,
      "step": 107400
    },
    {
      "epoch": 30.90307736554501,
      "grad_norm": 8.543762669432908e-05,
      "learning_rate": 0.00038194420477423067,
      "loss": 0.0008,
      "step": 107450
    },
    {
      "epoch": 30.91745757837216,
      "grad_norm": 0.0026236642152071,
      "learning_rate": 0.0003816566005176877,
      "loss": 0.0017,
      "step": 107500
    },
    {
      "epoch": 30.93183779119931,
      "grad_norm": 0.0007330317748710513,
      "learning_rate": 0.0003813689962611447,
      "loss": 0.005,
      "step": 107550
    },
    {
      "epoch": 30.94621800402646,
      "grad_norm": 4.690406785812229e-05,
      "learning_rate": 0.0003810813920046017,
      "loss": 0.0015,
      "step": 107600
    },
    {
      "epoch": 30.960598216853608,
      "grad_norm": 0.011744186282157898,
      "learning_rate": 0.00038079378774805866,
      "loss": 0.0015,
      "step": 107650
    },
    {
      "epoch": 30.97497842968076,
      "grad_norm": 7.260916754603386e-05,
      "learning_rate": 0.0003805061834915157,
      "loss": 0.0016,
      "step": 107700
    },
    {
      "epoch": 30.98935864250791,
      "grad_norm": 0.0007578734075650573,
      "learning_rate": 0.0003802185792349727,
      "loss": 0.0022,
      "step": 107750
    },
    {
      "epoch": 31.0,
      "eval_loss": 0.010284772142767906,
      "eval_runtime": 17.7238,
      "eval_samples_per_second": 2692.369,
      "eval_steps_per_second": 42.09,
      "step": 107787
    },
    {
      "epoch": 31.003738855335058,
      "grad_norm": 0.0005565980682149529,
      "learning_rate": 0.0003799309749784297,
      "loss": 0.0042,
      "step": 107800
    },
    {
      "epoch": 31.01811906816221,
      "grad_norm": 3.365126030985266e-05,
      "learning_rate": 0.0003796433707218867,
      "loss": 0.0047,
      "step": 107850
    },
    {
      "epoch": 31.03249928098936,
      "grad_norm": 0.006833992898464203,
      "learning_rate": 0.0003793557664653437,
      "loss": 0.0044,
      "step": 107900
    },
    {
      "epoch": 31.04687949381651,
      "grad_norm": 0.001155687845312059,
      "learning_rate": 0.0003790681622088007,
      "loss": 0.0031,
      "step": 107950
    },
    {
      "epoch": 31.061259706643657,
      "grad_norm": 0.0004512753221206367,
      "learning_rate": 0.0003787805579522577,
      "loss": 0.0016,
      "step": 108000
    },
    {
      "epoch": 31.07563991947081,
      "grad_norm": 4.354895281721838e-05,
      "learning_rate": 0.0003784929536957147,
      "loss": 0.0005,
      "step": 108050
    },
    {
      "epoch": 31.09002013229796,
      "grad_norm": 0.00023601323482580483,
      "learning_rate": 0.0003782053494391717,
      "loss": 0.0008,
      "step": 108100
    },
    {
      "epoch": 31.104400345125107,
      "grad_norm": 4.621544576366432e-05,
      "learning_rate": 0.00037791774518262873,
      "loss": 0.001,
      "step": 108150
    },
    {
      "epoch": 31.118780557952256,
      "grad_norm": 0.03773055598139763,
      "learning_rate": 0.00037763014092608574,
      "loss": 0.0015,
      "step": 108200
    },
    {
      "epoch": 31.13316077077941,
      "grad_norm": 5.2186504035489634e-05,
      "learning_rate": 0.0003773425366695427,
      "loss": 0.0027,
      "step": 108250
    },
    {
      "epoch": 31.147540983606557,
      "grad_norm": 0.005375530105084181,
      "learning_rate": 0.0003770549324129997,
      "loss": 0.0036,
      "step": 108300
    },
    {
      "epoch": 31.161921196433706,
      "grad_norm": 7.458368054358289e-05,
      "learning_rate": 0.0003767673281564567,
      "loss": 0.0003,
      "step": 108350
    },
    {
      "epoch": 31.17630140926086,
      "grad_norm": 0.014799559488892555,
      "learning_rate": 0.00037647972389991374,
      "loss": 0.0015,
      "step": 108400
    },
    {
      "epoch": 31.190681622088007,
      "grad_norm": 3.115510844509117e-05,
      "learning_rate": 0.00037619211964337075,
      "loss": 0.0028,
      "step": 108450
    },
    {
      "epoch": 31.205061834915156,
      "grad_norm": 8.816492481855676e-05,
      "learning_rate": 0.00037590451538682776,
      "loss": 0.0007,
      "step": 108500
    },
    {
      "epoch": 31.219442047742305,
      "grad_norm": 0.000588536262512207,
      "learning_rate": 0.0003756169111302847,
      "loss": 0.0033,
      "step": 108550
    },
    {
      "epoch": 31.233822260569458,
      "grad_norm": 0.003741339547559619,
      "learning_rate": 0.00037532930687374173,
      "loss": 0.0018,
      "step": 108600
    },
    {
      "epoch": 31.248202473396606,
      "grad_norm": 0.004056625999510288,
      "learning_rate": 0.00037504170261719875,
      "loss": 0.0026,
      "step": 108650
    },
    {
      "epoch": 31.262582686223755,
      "grad_norm": 3.9604328776476905e-05,
      "learning_rate": 0.00037475409836065576,
      "loss": 0.0033,
      "step": 108700
    },
    {
      "epoch": 31.276962899050908,
      "grad_norm": 0.0003741993277799338,
      "learning_rate": 0.00037446649410411277,
      "loss": 0.002,
      "step": 108750
    },
    {
      "epoch": 31.291343111878057,
      "grad_norm": 0.0029048894066363573,
      "learning_rate": 0.0003741788898475698,
      "loss": 0.0011,
      "step": 108800
    },
    {
      "epoch": 31.305723324705205,
      "grad_norm": 0.0001977429201360792,
      "learning_rate": 0.00037389128559102674,
      "loss": 0.0029,
      "step": 108850
    },
    {
      "epoch": 31.320103537532354,
      "grad_norm": 0.02177857980132103,
      "learning_rate": 0.00037360368133448375,
      "loss": 0.0047,
      "step": 108900
    },
    {
      "epoch": 31.334483750359507,
      "grad_norm": 7.321731391130015e-05,
      "learning_rate": 0.00037331607707794077,
      "loss": 0.004,
      "step": 108950
    },
    {
      "epoch": 31.348863963186655,
      "grad_norm": 2.8967770049348474e-05,
      "learning_rate": 0.0003730284728213978,
      "loss": 0.0014,
      "step": 109000
    },
    {
      "epoch": 31.363244176013804,
      "grad_norm": 0.029924193397164345,
      "learning_rate": 0.0003727408685648548,
      "loss": 0.0007,
      "step": 109050
    },
    {
      "epoch": 31.377624388840953,
      "grad_norm": 0.00012064731708960608,
      "learning_rate": 0.0003724532643083118,
      "loss": 0.0009,
      "step": 109100
    },
    {
      "epoch": 31.392004601668106,
      "grad_norm": 0.0019738045521080494,
      "learning_rate": 0.00037216566005176876,
      "loss": 0.0008,
      "step": 109150
    },
    {
      "epoch": 31.406384814495254,
      "grad_norm": 0.00017426091653760523,
      "learning_rate": 0.00037187805579522577,
      "loss": 0.0018,
      "step": 109200
    },
    {
      "epoch": 31.420765027322403,
      "grad_norm": 0.0099185174331069,
      "learning_rate": 0.0003715904515386828,
      "loss": 0.0019,
      "step": 109250
    },
    {
      "epoch": 31.435145240149556,
      "grad_norm": 0.04460418224334717,
      "learning_rate": 0.0003713028472821398,
      "loss": 0.0015,
      "step": 109300
    },
    {
      "epoch": 31.449525452976705,
      "grad_norm": 0.0001218146353494376,
      "learning_rate": 0.0003710152430255968,
      "loss": 0.0011,
      "step": 109350
    },
    {
      "epoch": 31.463905665803853,
      "grad_norm": 0.002010919386520982,
      "learning_rate": 0.0003707276387690538,
      "loss": 0.0029,
      "step": 109400
    },
    {
      "epoch": 31.478285878631002,
      "grad_norm": 8.338221232406795e-05,
      "learning_rate": 0.0003704400345125108,
      "loss": 0.0045,
      "step": 109450
    },
    {
      "epoch": 31.492666091458155,
      "grad_norm": 5.6499244237784296e-05,
      "learning_rate": 0.0003701524302559678,
      "loss": 0.0026,
      "step": 109500
    },
    {
      "epoch": 31.507046304285304,
      "grad_norm": 0.0010532460873946548,
      "learning_rate": 0.0003698648259994248,
      "loss": 0.0046,
      "step": 109550
    },
    {
      "epoch": 31.521426517112452,
      "grad_norm": 0.0032727885991334915,
      "learning_rate": 0.00036957722174288176,
      "loss": 0.0023,
      "step": 109600
    },
    {
      "epoch": 31.535806729939605,
      "grad_norm": 3.677697532111779e-05,
      "learning_rate": 0.00036928961748633883,
      "loss": 0.0008,
      "step": 109650
    },
    {
      "epoch": 31.550186942766754,
      "grad_norm": 0.001226845197379589,
      "learning_rate": 0.00036900201322979584,
      "loss": 0.0023,
      "step": 109700
    },
    {
      "epoch": 31.564567155593902,
      "grad_norm": 0.01192750409245491,
      "learning_rate": 0.0003687144089732528,
      "loss": 0.0032,
      "step": 109750
    },
    {
      "epoch": 31.57894736842105,
      "grad_norm": 3.848327833111398e-05,
      "learning_rate": 0.0003684268047167098,
      "loss": 0.0022,
      "step": 109800
    },
    {
      "epoch": 31.593327581248204,
      "grad_norm": 0.011603707447648048,
      "learning_rate": 0.0003681392004601668,
      "loss": 0.0026,
      "step": 109850
    },
    {
      "epoch": 31.607707794075353,
      "grad_norm": 6.805914745200425e-05,
      "learning_rate": 0.0003678515962036238,
      "loss": 0.0036,
      "step": 109900
    },
    {
      "epoch": 31.6220880069025,
      "grad_norm": 5.520126433111727e-05,
      "learning_rate": 0.00036756399194708085,
      "loss": 0.0025,
      "step": 109950
    },
    {
      "epoch": 31.63646821972965,
      "grad_norm": 5.350702122086659e-05,
      "learning_rate": 0.00036727638769053786,
      "loss": 0.0027,
      "step": 110000
    },
    {
      "epoch": 31.650848432556803,
      "grad_norm": 3.527960507199168e-05,
      "learning_rate": 0.0003669887834339948,
      "loss": 0.0022,
      "step": 110050
    },
    {
      "epoch": 31.66522864538395,
      "grad_norm": 0.0034172614105045795,
      "learning_rate": 0.00036670117917745183,
      "loss": 0.0021,
      "step": 110100
    },
    {
      "epoch": 31.6796088582111,
      "grad_norm": 0.020374637097120285,
      "learning_rate": 0.00036641357492090884,
      "loss": 0.0021,
      "step": 110150
    },
    {
      "epoch": 31.693989071038253,
      "grad_norm": 0.00023937966034282,
      "learning_rate": 0.0003661259706643658,
      "loss": 0.0003,
      "step": 110200
    },
    {
      "epoch": 31.7083692838654,
      "grad_norm": 0.01453294139355421,
      "learning_rate": 0.00036583836640782287,
      "loss": 0.0031,
      "step": 110250
    },
    {
      "epoch": 31.72274949669255,
      "grad_norm": 0.000946496962569654,
      "learning_rate": 0.0003655507621512799,
      "loss": 0.0019,
      "step": 110300
    },
    {
      "epoch": 31.7371297095197,
      "grad_norm": 9.331277396995574e-05,
      "learning_rate": 0.00036526315789473684,
      "loss": 0.0002,
      "step": 110350
    },
    {
      "epoch": 31.75150992234685,
      "grad_norm": 5.230058741290122e-05,
      "learning_rate": 0.00036497555363819385,
      "loss": 0.0017,
      "step": 110400
    },
    {
      "epoch": 31.765890135174,
      "grad_norm": 0.026393482461571693,
      "learning_rate": 0.00036468794938165086,
      "loss": 0.0025,
      "step": 110450
    },
    {
      "epoch": 31.78027034800115,
      "grad_norm": 0.014865150675177574,
      "learning_rate": 0.0003644003451251078,
      "loss": 0.001,
      "step": 110500
    },
    {
      "epoch": 31.794650560828302,
      "grad_norm": 4.3124615331180394e-05,
      "learning_rate": 0.0003641127408685649,
      "loss": 0.0007,
      "step": 110550
    },
    {
      "epoch": 31.80903077365545,
      "grad_norm": 0.0016403457848355174,
      "learning_rate": 0.0003638251366120219,
      "loss": 0.0029,
      "step": 110600
    },
    {
      "epoch": 31.8234109864826,
      "grad_norm": 5.0318205467192456e-05,
      "learning_rate": 0.00036353753235547886,
      "loss": 0.0011,
      "step": 110650
    },
    {
      "epoch": 31.83779119930975,
      "grad_norm": 0.001237916061654687,
      "learning_rate": 0.00036324992809893587,
      "loss": 0.0036,
      "step": 110700
    },
    {
      "epoch": 31.8521714121369,
      "grad_norm": 0.0010866947704926133,
      "learning_rate": 0.0003629623238423929,
      "loss": 0.0051,
      "step": 110750
    },
    {
      "epoch": 31.86655162496405,
      "grad_norm": 1.2170267837063875e-05,
      "learning_rate": 0.00036267471958584984,
      "loss": 0.0016,
      "step": 110800
    },
    {
      "epoch": 31.8809318377912,
      "grad_norm": 0.02485170029103756,
      "learning_rate": 0.0003623871153293069,
      "loss": 0.0058,
      "step": 110850
    },
    {
      "epoch": 31.89531205061835,
      "grad_norm": 3.938973895856179e-05,
      "learning_rate": 0.0003620995110727639,
      "loss": 0.0008,
      "step": 110900
    },
    {
      "epoch": 31.9096922634455,
      "grad_norm": 8.013452315935865e-05,
      "learning_rate": 0.0003618119068162209,
      "loss": 0.0028,
      "step": 110950
    },
    {
      "epoch": 31.92407247627265,
      "grad_norm": 0.0013218186795711517,
      "learning_rate": 0.0003615243025596779,
      "loss": 0.0037,
      "step": 111000
    },
    {
      "epoch": 31.938452689099798,
      "grad_norm": 0.0035949277225881815,
      "learning_rate": 0.0003612366983031349,
      "loss": 0.0033,
      "step": 111050
    },
    {
      "epoch": 31.95283290192695,
      "grad_norm": 0.016418306156992912,
      "learning_rate": 0.00036094909404659186,
      "loss": 0.0022,
      "step": 111100
    },
    {
      "epoch": 31.9672131147541,
      "grad_norm": 0.0020098309032619,
      "learning_rate": 0.0003606614897900489,
      "loss": 0.0025,
      "step": 111150
    },
    {
      "epoch": 31.981593327581248,
      "grad_norm": 0.0003681130474433303,
      "learning_rate": 0.00036037388553350594,
      "loss": 0.0024,
      "step": 111200
    },
    {
      "epoch": 31.995973540408396,
      "grad_norm": 0.00019478793547023088,
      "learning_rate": 0.0003600862812769629,
      "loss": 0.0041,
      "step": 111250
    },
    {
      "epoch": 32.0,
      "eval_loss": 0.010199490003287792,
      "eval_runtime": 17.3778,
      "eval_samples_per_second": 2745.978,
      "eval_steps_per_second": 42.928,
      "step": 111264
    },
    {
      "epoch": 32.01035375323555,
      "grad_norm": 4.2410061723785475e-05,
      "learning_rate": 0.0003597986770204199,
      "loss": 0.0033,
      "step": 111300
    },
    {
      "epoch": 32.0247339660627,
      "grad_norm": 0.009282419458031654,
      "learning_rate": 0.0003595110727638769,
      "loss": 0.0033,
      "step": 111350
    },
    {
      "epoch": 32.03911417888985,
      "grad_norm": 0.003603119868785143,
      "learning_rate": 0.0003592234685073339,
      "loss": 0.0027,
      "step": 111400
    },
    {
      "epoch": 32.053494391716995,
      "grad_norm": 0.00041614106157794595,
      "learning_rate": 0.00035893586425079095,
      "loss": 0.0017,
      "step": 111450
    },
    {
      "epoch": 32.067874604544144,
      "grad_norm": 3.716500214068219e-05,
      "learning_rate": 0.00035864825999424796,
      "loss": 0.0018,
      "step": 111500
    },
    {
      "epoch": 32.0822548173713,
      "grad_norm": 0.006433672737330198,
      "learning_rate": 0.0003583606557377049,
      "loss": 0.0024,
      "step": 111550
    },
    {
      "epoch": 32.09663503019845,
      "grad_norm": 0.0001638846006244421,
      "learning_rate": 0.00035807305148116193,
      "loss": 0.0022,
      "step": 111600
    },
    {
      "epoch": 32.1110152430256,
      "grad_norm": 0.0019803778268396854,
      "learning_rate": 0.00035778544722461894,
      "loss": 0.0027,
      "step": 111650
    },
    {
      "epoch": 32.12539545585275,
      "grad_norm": 0.0273639764636755,
      "learning_rate": 0.0003574978429680759,
      "loss": 0.0033,
      "step": 111700
    },
    {
      "epoch": 32.139775668679896,
      "grad_norm": 6.380966806318611e-05,
      "learning_rate": 0.00035721023871153297,
      "loss": 0.0026,
      "step": 111750
    },
    {
      "epoch": 32.154155881507045,
      "grad_norm": 4.288117270334624e-05,
      "learning_rate": 0.00035692263445499,
      "loss": 0.0007,
      "step": 111800
    },
    {
      "epoch": 32.16853609433419,
      "grad_norm": 0.0008165149483829737,
      "learning_rate": 0.00035663503019844694,
      "loss": 0.003,
      "step": 111850
    },
    {
      "epoch": 32.18291630716135,
      "grad_norm": 8.155969408107921e-05,
      "learning_rate": 0.00035634742594190395,
      "loss": 0.0011,
      "step": 111900
    },
    {
      "epoch": 32.1972965199885,
      "grad_norm": 0.00687411380931735,
      "learning_rate": 0.00035605982168536096,
      "loss": 0.0013,
      "step": 111950
    },
    {
      "epoch": 32.21167673281565,
      "grad_norm": 0.010907430201768875,
      "learning_rate": 0.0003557722174288179,
      "loss": 0.0025,
      "step": 112000
    },
    {
      "epoch": 32.226056945642796,
      "grad_norm": 0.0018033424858003855,
      "learning_rate": 0.000355484613172275,
      "loss": 0.0022,
      "step": 112050
    },
    {
      "epoch": 32.240437158469945,
      "grad_norm": 0.008572926744818687,
      "learning_rate": 0.000355197008915732,
      "loss": 0.0032,
      "step": 112100
    },
    {
      "epoch": 32.254817371297094,
      "grad_norm": 0.00020235587726347148,
      "learning_rate": 0.00035490940465918896,
      "loss": 0.0021,
      "step": 112150
    },
    {
      "epoch": 32.26919758412424,
      "grad_norm": 0.0359254889190197,
      "learning_rate": 0.00035462180040264597,
      "loss": 0.0032,
      "step": 112200
    },
    {
      "epoch": 32.2835777969514,
      "grad_norm": 0.00027609479730017483,
      "learning_rate": 0.000354334196146103,
      "loss": 0.0029,
      "step": 112250
    },
    {
      "epoch": 32.29795800977855,
      "grad_norm": 0.001273558707907796,
      "learning_rate": 0.00035404659188955994,
      "loss": 0.005,
      "step": 112300
    },
    {
      "epoch": 32.312338222605696,
      "grad_norm": 0.007206672336906195,
      "learning_rate": 0.00035375898763301695,
      "loss": 0.0043,
      "step": 112350
    },
    {
      "epoch": 32.326718435432845,
      "grad_norm": 0.00974927470088005,
      "learning_rate": 0.000353471383376474,
      "loss": 0.0031,
      "step": 112400
    },
    {
      "epoch": 32.341098648259994,
      "grad_norm": 3.9780243241693825e-05,
      "learning_rate": 0.000353183779119931,
      "loss": 0.0019,
      "step": 112450
    },
    {
      "epoch": 32.35547886108714,
      "grad_norm": 0.003538090270012617,
      "learning_rate": 0.000352896174863388,
      "loss": 0.0026,
      "step": 112500
    },
    {
      "epoch": 32.36985907391429,
      "grad_norm": 0.0010633192723616958,
      "learning_rate": 0.000352608570606845,
      "loss": 0.002,
      "step": 112550
    },
    {
      "epoch": 32.38423928674144,
      "grad_norm": 3.97603762394283e-05,
      "learning_rate": 0.00035232096635030196,
      "loss": 0.0033,
      "step": 112600
    },
    {
      "epoch": 32.398619499568596,
      "grad_norm": 0.004998550750315189,
      "learning_rate": 0.00035203336209375897,
      "loss": 0.0035,
      "step": 112650
    },
    {
      "epoch": 32.412999712395745,
      "grad_norm": 0.00023232422245200723,
      "learning_rate": 0.00035174575783721604,
      "loss": 0.0017,
      "step": 112700
    },
    {
      "epoch": 32.427379925222894,
      "grad_norm": 4.0169808926293626e-05,
      "learning_rate": 0.000351458153580673,
      "loss": 0.0042,
      "step": 112750
    },
    {
      "epoch": 32.44176013805004,
      "grad_norm": 0.017239604145288467,
      "learning_rate": 0.00035117054932413,
      "loss": 0.0023,
      "step": 112800
    },
    {
      "epoch": 32.45614035087719,
      "grad_norm": 0.015939291566610336,
      "learning_rate": 0.000350882945067587,
      "loss": 0.0023,
      "step": 112850
    },
    {
      "epoch": 32.47052056370434,
      "grad_norm": 3.0870749469613656e-05,
      "learning_rate": 0.000350595340811044,
      "loss": 0.0028,
      "step": 112900
    },
    {
      "epoch": 32.48490077653149,
      "grad_norm": 3.2603122235741466e-05,
      "learning_rate": 0.000350307736554501,
      "loss": 0.0008,
      "step": 112950
    },
    {
      "epoch": 32.499280989358645,
      "grad_norm": 0.0006151134148240089,
      "learning_rate": 0.00035002013229795806,
      "loss": 0.0016,
      "step": 113000
    },
    {
      "epoch": 32.513661202185794,
      "grad_norm": 0.00010781027958728373,
      "learning_rate": 0.000349732528041415,
      "loss": 0.002,
      "step": 113050
    },
    {
      "epoch": 32.52804141501294,
      "grad_norm": 4.608776362147182e-05,
      "learning_rate": 0.000349444923784872,
      "loss": 0.0009,
      "step": 113100
    },
    {
      "epoch": 32.54242162784009,
      "grad_norm": 0.00017801995272748172,
      "learning_rate": 0.00034915731952832904,
      "loss": 0.0027,
      "step": 113150
    },
    {
      "epoch": 32.55680184066724,
      "grad_norm": 8.714909199625254e-05,
      "learning_rate": 0.000348869715271786,
      "loss": 0.0026,
      "step": 113200
    },
    {
      "epoch": 32.57118205349439,
      "grad_norm": 0.0008294550934806466,
      "learning_rate": 0.000348582111015243,
      "loss": 0.0024,
      "step": 113250
    },
    {
      "epoch": 32.58556226632154,
      "grad_norm": 5.728948235628195e-05,
      "learning_rate": 0.0003482945067587001,
      "loss": 0.0008,
      "step": 113300
    },
    {
      "epoch": 32.599942479148694,
      "grad_norm": 0.00011437159992055967,
      "learning_rate": 0.00034800690250215703,
      "loss": 0.0014,
      "step": 113350
    },
    {
      "epoch": 32.61432269197584,
      "grad_norm": 0.0004116786294616759,
      "learning_rate": 0.00034771929824561405,
      "loss": 0.0013,
      "step": 113400
    },
    {
      "epoch": 32.62870290480299,
      "grad_norm": 0.00019701512064784765,
      "learning_rate": 0.00034743169398907106,
      "loss": 0.0041,
      "step": 113450
    },
    {
      "epoch": 32.64308311763014,
      "grad_norm": 8.738772885408252e-05,
      "learning_rate": 0.000347144089732528,
      "loss": 0.0018,
      "step": 113500
    },
    {
      "epoch": 32.65746333045729,
      "grad_norm": 0.008897149935364723,
      "learning_rate": 0.00034685648547598503,
      "loss": 0.0027,
      "step": 113550
    },
    {
      "epoch": 32.67184354328444,
      "grad_norm": 0.00019837029685731977,
      "learning_rate": 0.0003465688812194421,
      "loss": 0.002,
      "step": 113600
    },
    {
      "epoch": 32.68622375611159,
      "grad_norm": 0.013384652324020863,
      "learning_rate": 0.00034628127696289905,
      "loss": 0.0031,
      "step": 113650
    },
    {
      "epoch": 32.70060396893874,
      "grad_norm": 0.0001881926873466,
      "learning_rate": 0.00034599367270635607,
      "loss": 0.0022,
      "step": 113700
    },
    {
      "epoch": 32.71498418176589,
      "grad_norm": 2.532611324568279e-05,
      "learning_rate": 0.0003457060684498131,
      "loss": 0.0013,
      "step": 113750
    },
    {
      "epoch": 32.72936439459304,
      "grad_norm": 0.02886541374027729,
      "learning_rate": 0.00034541846419327004,
      "loss": 0.0005,
      "step": 113800
    },
    {
      "epoch": 32.74374460742019,
      "grad_norm": 0.00042885274160653353,
      "learning_rate": 0.00034513085993672705,
      "loss": 0.0056,
      "step": 113850
    },
    {
      "epoch": 32.75812482024734,
      "grad_norm": 6.25207248958759e-05,
      "learning_rate": 0.0003448432556801841,
      "loss": 0.0024,
      "step": 113900
    },
    {
      "epoch": 32.77250503307449,
      "grad_norm": 0.0019934263546019793,
      "learning_rate": 0.0003445556514236411,
      "loss": 0.0022,
      "step": 113950
    },
    {
      "epoch": 32.78688524590164,
      "grad_norm": 0.0001856259914347902,
      "learning_rate": 0.0003442680471670981,
      "loss": 0.0039,
      "step": 114000
    },
    {
      "epoch": 32.80126545872879,
      "grad_norm": 0.0007184298592619598,
      "learning_rate": 0.0003439804429105551,
      "loss": 0.0028,
      "step": 114050
    },
    {
      "epoch": 32.81564567155594,
      "grad_norm": 0.030373133718967438,
      "learning_rate": 0.00034369283865401206,
      "loss": 0.0022,
      "step": 114100
    },
    {
      "epoch": 32.83002588438309,
      "grad_norm": 0.00014473343617282808,
      "learning_rate": 0.00034340523439746907,
      "loss": 0.001,
      "step": 114150
    },
    {
      "epoch": 32.84440609721024,
      "grad_norm": 7.884208753239363e-05,
      "learning_rate": 0.00034311763014092613,
      "loss": 0.0009,
      "step": 114200
    },
    {
      "epoch": 32.85878631003739,
      "grad_norm": 0.020387398079037666,
      "learning_rate": 0.0003428300258843831,
      "loss": 0.0031,
      "step": 114250
    },
    {
      "epoch": 32.87316652286454,
      "grad_norm": 6.377649697242305e-05,
      "learning_rate": 0.0003425424216278401,
      "loss": 0.0027,
      "step": 114300
    },
    {
      "epoch": 32.887546735691686,
      "grad_norm": 4.395277937874198e-05,
      "learning_rate": 0.0003422548173712971,
      "loss": 0.0014,
      "step": 114350
    },
    {
      "epoch": 32.901926948518835,
      "grad_norm": 1.7218582797795534e-05,
      "learning_rate": 0.0003419672131147541,
      "loss": 0.0014,
      "step": 114400
    },
    {
      "epoch": 32.91630716134599,
      "grad_norm": 0.00011412815365474671,
      "learning_rate": 0.0003416796088582111,
      "loss": 0.0003,
      "step": 114450
    },
    {
      "epoch": 32.93068737417314,
      "grad_norm": 0.0017790442798286676,
      "learning_rate": 0.00034139200460166815,
      "loss": 0.002,
      "step": 114500
    },
    {
      "epoch": 32.94506758700029,
      "grad_norm": 0.001832408714108169,
      "learning_rate": 0.0003411044003451251,
      "loss": 0.0039,
      "step": 114550
    },
    {
      "epoch": 32.95944779982744,
      "grad_norm": 0.017452022060751915,
      "learning_rate": 0.0003408167960885821,
      "loss": 0.0039,
      "step": 114600
    },
    {
      "epoch": 32.973828012654586,
      "grad_norm": 0.016817796975374222,
      "learning_rate": 0.00034052919183203914,
      "loss": 0.0017,
      "step": 114650
    },
    {
      "epoch": 32.988208225481735,
      "grad_norm": 3.1175357435131446e-05,
      "learning_rate": 0.0003402415875754961,
      "loss": 0.0011,
      "step": 114700
    },
    {
      "epoch": 33.0,
      "eval_loss": 0.010629634372889996,
      "eval_runtime": 16.9198,
      "eval_samples_per_second": 2820.302,
      "eval_steps_per_second": 44.09,
      "step": 114741
    },
    {
      "epoch": 33.002588438308884,
      "grad_norm": 0.000581408676225692,
      "learning_rate": 0.0003399539833189531,
      "loss": 0.0025,
      "step": 114750
    },
    {
      "epoch": 33.01696865113604,
      "grad_norm": 3.138239117106423e-05,
      "learning_rate": 0.0003396663790624102,
      "loss": 0.0034,
      "step": 114800
    },
    {
      "epoch": 33.03134886396319,
      "grad_norm": 7.959391950862482e-05,
      "learning_rate": 0.00033937877480586713,
      "loss": 0.0007,
      "step": 114850
    },
    {
      "epoch": 33.04572907679034,
      "grad_norm": 0.007535451557487249,
      "learning_rate": 0.00033909117054932414,
      "loss": 0.0026,
      "step": 114900
    },
    {
      "epoch": 33.060109289617486,
      "grad_norm": 0.0004752847889903933,
      "learning_rate": 0.00033880356629278116,
      "loss": 0.0017,
      "step": 114950
    },
    {
      "epoch": 33.074489502444635,
      "grad_norm": 0.0001554754562675953,
      "learning_rate": 0.0003385159620362381,
      "loss": 0.0022,
      "step": 115000
    },
    {
      "epoch": 33.088869715271784,
      "grad_norm": 0.00016772093658801168,
      "learning_rate": 0.0003382283577796951,
      "loss": 0.0011,
      "step": 115050
    },
    {
      "epoch": 33.10324992809893,
      "grad_norm": 0.012552552856504917,
      "learning_rate": 0.00033794075352315214,
      "loss": 0.0022,
      "step": 115100
    },
    {
      "epoch": 33.11763014092609,
      "grad_norm": 5.9961363149341196e-05,
      "learning_rate": 0.00033765314926660915,
      "loss": 0.0016,
      "step": 115150
    },
    {
      "epoch": 33.13201035375324,
      "grad_norm": 1.9032480850000866e-05,
      "learning_rate": 0.00033736554501006616,
      "loss": 0.0004,
      "step": 115200
    },
    {
      "epoch": 33.146390566580386,
      "grad_norm": 0.00011679343151627108,
      "learning_rate": 0.0003370779407535232,
      "loss": 0.0084,
      "step": 115250
    },
    {
      "epoch": 33.160770779407535,
      "grad_norm": 0.0130021832883358,
      "learning_rate": 0.00033679033649698013,
      "loss": 0.003,
      "step": 115300
    },
    {
      "epoch": 33.175150992234684,
      "grad_norm": 6.814035714342026e-06,
      "learning_rate": 0.00033650273224043715,
      "loss": 0.0008,
      "step": 115350
    },
    {
      "epoch": 33.18953120506183,
      "grad_norm": 0.00021877631661482155,
      "learning_rate": 0.00033621512798389416,
      "loss": 0.0013,
      "step": 115400
    },
    {
      "epoch": 33.20391141788898,
      "grad_norm": 2.647636028996203e-05,
      "learning_rate": 0.00033592752372735117,
      "loss": 0.002,
      "step": 115450
    },
    {
      "epoch": 33.21829163071614,
      "grad_norm": 0.005484189372509718,
      "learning_rate": 0.0003356399194708082,
      "loss": 0.0011,
      "step": 115500
    },
    {
      "epoch": 33.23267184354329,
      "grad_norm": 0.0008727310923859477,
      "learning_rate": 0.0003353523152142652,
      "loss": 0.0027,
      "step": 115550
    },
    {
      "epoch": 33.247052056370435,
      "grad_norm": 3.0087536288192496e-05,
      "learning_rate": 0.00033506471095772215,
      "loss": 0.004,
      "step": 115600
    },
    {
      "epoch": 33.261432269197584,
      "grad_norm": 3.564603684935719e-05,
      "learning_rate": 0.00033477710670117917,
      "loss": 0.0025,
      "step": 115650
    },
    {
      "epoch": 33.27581248202473,
      "grad_norm": 5.513162977877073e-05,
      "learning_rate": 0.0003344895024446362,
      "loss": 0.0018,
      "step": 115700
    },
    {
      "epoch": 33.29019269485188,
      "grad_norm": 0.00015945320774335414,
      "learning_rate": 0.0003342018981880932,
      "loss": 0.0031,
      "step": 115750
    },
    {
      "epoch": 33.30457290767903,
      "grad_norm": 0.0010371421230956912,
      "learning_rate": 0.0003339142939315502,
      "loss": 0.001,
      "step": 115800
    },
    {
      "epoch": 33.31895312050619,
      "grad_norm": 0.00028833924443461,
      "learning_rate": 0.0003336266896750072,
      "loss": 0.0037,
      "step": 115850
    },
    {
      "epoch": 33.333333333333336,
      "grad_norm": 0.007406903430819511,
      "learning_rate": 0.0003333390854184642,
      "loss": 0.0013,
      "step": 115900
    },
    {
      "epoch": 33.347713546160485,
      "grad_norm": 0.00025773991364985704,
      "learning_rate": 0.0003330514811619212,
      "loss": 0.0022,
      "step": 115950
    },
    {
      "epoch": 33.36209375898763,
      "grad_norm": 0.0029269293881952763,
      "learning_rate": 0.0003327638769053782,
      "loss": 0.0033,
      "step": 116000
    },
    {
      "epoch": 33.37647397181478,
      "grad_norm": 2.2208187147043645e-05,
      "learning_rate": 0.0003324762726488352,
      "loss": 0.0007,
      "step": 116050
    },
    {
      "epoch": 33.39085418464193,
      "grad_norm": 0.00046587217366322875,
      "learning_rate": 0.0003321886683922922,
      "loss": 0.0031,
      "step": 116100
    },
    {
      "epoch": 33.40523439746908,
      "grad_norm": 0.0016215367941185832,
      "learning_rate": 0.00033190106413574923,
      "loss": 0.0049,
      "step": 116150
    },
    {
      "epoch": 33.419614610296236,
      "grad_norm": 0.004278654232621193,
      "learning_rate": 0.0003316134598792062,
      "loss": 0.0021,
      "step": 116200
    },
    {
      "epoch": 33.433994823123385,
      "grad_norm": 0.00084167520981282,
      "learning_rate": 0.0003313258556226632,
      "loss": 0.0027,
      "step": 116250
    },
    {
      "epoch": 33.448375035950534,
      "grad_norm": 0.011942755430936813,
      "learning_rate": 0.0003310382513661202,
      "loss": 0.0026,
      "step": 116300
    },
    {
      "epoch": 33.46275524877768,
      "grad_norm": 5.458762461785227e-05,
      "learning_rate": 0.00033075064710957723,
      "loss": 0.001,
      "step": 116350
    },
    {
      "epoch": 33.47713546160483,
      "grad_norm": 0.0002482587588019669,
      "learning_rate": 0.00033046304285303424,
      "loss": 0.0035,
      "step": 116400
    },
    {
      "epoch": 33.49151567443198,
      "grad_norm": 4.672372233471833e-05,
      "learning_rate": 0.00033017543859649125,
      "loss": 0.0021,
      "step": 116450
    },
    {
      "epoch": 33.50589588725913,
      "grad_norm": 0.00018455024110153317,
      "learning_rate": 0.0003298878343399482,
      "loss": 0.0015,
      "step": 116500
    },
    {
      "epoch": 33.52027610008628,
      "grad_norm": 0.00012459057325031608,
      "learning_rate": 0.0003296002300834052,
      "loss": 0.001,
      "step": 116550
    },
    {
      "epoch": 33.534656312913434,
      "grad_norm": 0.016892291605472565,
      "learning_rate": 0.00032931262582686224,
      "loss": 0.0018,
      "step": 116600
    },
    {
      "epoch": 33.54903652574058,
      "grad_norm": 0.0001069072459358722,
      "learning_rate": 0.00032902502157031925,
      "loss": 0.0005,
      "step": 116650
    },
    {
      "epoch": 33.56341673856773,
      "grad_norm": 7.860739424359053e-05,
      "learning_rate": 0.00032873741731377626,
      "loss": 0.0018,
      "step": 116700
    },
    {
      "epoch": 33.57779695139488,
      "grad_norm": 0.0027320333756506443,
      "learning_rate": 0.0003284498130572333,
      "loss": 0.0032,
      "step": 116750
    },
    {
      "epoch": 33.59217716422203,
      "grad_norm": 0.00022589717991650105,
      "learning_rate": 0.00032816220880069023,
      "loss": 0.0017,
      "step": 116800
    },
    {
      "epoch": 33.60655737704918,
      "grad_norm": 1.1785545211751014e-05,
      "learning_rate": 0.00032787460454414724,
      "loss": 0.002,
      "step": 116850
    },
    {
      "epoch": 33.62093758987633,
      "grad_norm": 0.0047652157954871655,
      "learning_rate": 0.00032758700028760426,
      "loss": 0.0011,
      "step": 116900
    },
    {
      "epoch": 33.63531780270348,
      "grad_norm": 0.0010064601665362716,
      "learning_rate": 0.00032729939603106127,
      "loss": 0.0005,
      "step": 116950
    },
    {
      "epoch": 33.64969801553063,
      "grad_norm": 0.0002449755265843123,
      "learning_rate": 0.0003270117917745183,
      "loss": 0.0048,
      "step": 117000
    },
    {
      "epoch": 33.66407822835778,
      "grad_norm": 0.00011716108565451577,
      "learning_rate": 0.0003267241875179753,
      "loss": 0.0002,
      "step": 117050
    },
    {
      "epoch": 33.67845844118493,
      "grad_norm": 0.005545366555452347,
      "learning_rate": 0.00032643658326143225,
      "loss": 0.0035,
      "step": 117100
    },
    {
      "epoch": 33.69283865401208,
      "grad_norm": 0.002399213844910264,
      "learning_rate": 0.00032614897900488926,
      "loss": 0.0008,
      "step": 117150
    },
    {
      "epoch": 33.70721886683923,
      "grad_norm": 0.0002022180415224284,
      "learning_rate": 0.0003258613747483463,
      "loss": 0.0024,
      "step": 117200
    },
    {
      "epoch": 33.721599079666376,
      "grad_norm": 0.004597953520715237,
      "learning_rate": 0.0003255737704918033,
      "loss": 0.0013,
      "step": 117250
    },
    {
      "epoch": 33.73597929249353,
      "grad_norm": 0.00011328687833156437,
      "learning_rate": 0.0003252861662352603,
      "loss": 0.0026,
      "step": 117300
    },
    {
      "epoch": 33.75035950532068,
      "grad_norm": 4.3803440348710865e-05,
      "learning_rate": 0.0003249985619787173,
      "loss": 0.0024,
      "step": 117350
    },
    {
      "epoch": 33.76473971814783,
      "grad_norm": 0.00014254510460887104,
      "learning_rate": 0.00032471095772217427,
      "loss": 0.004,
      "step": 117400
    },
    {
      "epoch": 33.77911993097498,
      "grad_norm": 0.0009336139773949981,
      "learning_rate": 0.0003244233534656313,
      "loss": 0.0015,
      "step": 117450
    },
    {
      "epoch": 33.79350014380213,
      "grad_norm": 0.003688841126859188,
      "learning_rate": 0.0003241357492090883,
      "loss": 0.0028,
      "step": 117500
    },
    {
      "epoch": 33.807880356629276,
      "grad_norm": 0.0001731115044094622,
      "learning_rate": 0.0003238481449525453,
      "loss": 0.0028,
      "step": 117550
    },
    {
      "epoch": 33.822260569456425,
      "grad_norm": 8.083120337687433e-05,
      "learning_rate": 0.0003235605406960023,
      "loss": 0.0041,
      "step": 117600
    },
    {
      "epoch": 33.83664078228358,
      "grad_norm": 0.0016397725557908416,
      "learning_rate": 0.00032327293643945933,
      "loss": 0.0014,
      "step": 117650
    },
    {
      "epoch": 33.85102099511073,
      "grad_norm": 0.005287060979753733,
      "learning_rate": 0.0003229853321829163,
      "loss": 0.0023,
      "step": 117700
    },
    {
      "epoch": 33.86540120793788,
      "grad_norm": 0.0004415959701873362,
      "learning_rate": 0.0003226977279263733,
      "loss": 0.0047,
      "step": 117750
    },
    {
      "epoch": 33.87978142076503,
      "grad_norm": 0.00011019562225556001,
      "learning_rate": 0.0003224101236698303,
      "loss": 0.0032,
      "step": 117800
    },
    {
      "epoch": 33.89416163359218,
      "grad_norm": 0.0006432515801861882,
      "learning_rate": 0.0003221225194132873,
      "loss": 0.0037,
      "step": 117850
    },
    {
      "epoch": 33.908541846419325,
      "grad_norm": 4.893165532848798e-05,
      "learning_rate": 0.00032183491515674434,
      "loss": 0.0022,
      "step": 117900
    },
    {
      "epoch": 33.922922059246474,
      "grad_norm": 0.005366594064980745,
      "learning_rate": 0.00032154731090020135,
      "loss": 0.0011,
      "step": 117950
    },
    {
      "epoch": 33.93730227207363,
      "grad_norm": 0.0035187331959605217,
      "learning_rate": 0.0003212597066436583,
      "loss": 0.0037,
      "step": 118000
    },
    {
      "epoch": 33.95168248490078,
      "grad_norm": 2.2220323444344103e-05,
      "learning_rate": 0.0003209721023871153,
      "loss": 0.0028,
      "step": 118050
    },
    {
      "epoch": 33.96606269772793,
      "grad_norm": 7.039251795504242e-05,
      "learning_rate": 0.00032068449813057233,
      "loss": 0.0038,
      "step": 118100
    },
    {
      "epoch": 33.98044291055508,
      "grad_norm": 0.018677854910492897,
      "learning_rate": 0.0003203968938740293,
      "loss": 0.0022,
      "step": 118150
    },
    {
      "epoch": 33.994823123382226,
      "grad_norm": 0.0007286364561878145,
      "learning_rate": 0.00032010928961748636,
      "loss": 0.0016,
      "step": 118200
    },
    {
      "epoch": 34.0,
      "eval_loss": 0.010019629262387753,
      "eval_runtime": 17.4388,
      "eval_samples_per_second": 2736.368,
      "eval_steps_per_second": 42.778,
      "step": 118218
    },
    {
      "epoch": 34.009203336209374,
      "grad_norm": 0.00015600539336446673,
      "learning_rate": 0.00031982168536094337,
      "loss": 0.0021,
      "step": 118250
    },
    {
      "epoch": 34.02358354903652,
      "grad_norm": 3.3202741178683937e-05,
      "learning_rate": 0.00031953408110440033,
      "loss": 0.002,
      "step": 118300
    },
    {
      "epoch": 34.03796376186367,
      "grad_norm": 0.006496276706457138,
      "learning_rate": 0.00031924647684785734,
      "loss": 0.0042,
      "step": 118350
    },
    {
      "epoch": 34.05234397469083,
      "grad_norm": 2.6861633159569465e-05,
      "learning_rate": 0.00031895887259131435,
      "loss": 0.0044,
      "step": 118400
    },
    {
      "epoch": 34.06672418751798,
      "grad_norm": 0.0004917770274914801,
      "learning_rate": 0.0003186712683347713,
      "loss": 0.0043,
      "step": 118450
    },
    {
      "epoch": 34.081104400345126,
      "grad_norm": 0.00015094115224201232,
      "learning_rate": 0.0003183836640782284,
      "loss": 0.0008,
      "step": 118500
    },
    {
      "epoch": 34.095484613172275,
      "grad_norm": 0.005881798919290304,
      "learning_rate": 0.0003180960598216854,
      "loss": 0.0008,
      "step": 118550
    },
    {
      "epoch": 34.10986482599942,
      "grad_norm": 0.0012041886802762747,
      "learning_rate": 0.00031780845556514235,
      "loss": 0.0033,
      "step": 118600
    },
    {
      "epoch": 34.12424503882657,
      "grad_norm": 1.8859644114854746e-05,
      "learning_rate": 0.00031752085130859936,
      "loss": 0.0029,
      "step": 118650
    },
    {
      "epoch": 34.13862525165372,
      "grad_norm": 0.018493469804525375,
      "learning_rate": 0.0003172332470520564,
      "loss": 0.0041,
      "step": 118700
    },
    {
      "epoch": 34.15300546448088,
      "grad_norm": 0.0017154262168332934,
      "learning_rate": 0.00031694564279551333,
      "loss": 0.0011,
      "step": 118750
    },
    {
      "epoch": 34.167385677308026,
      "grad_norm": 4.628560782293789e-05,
      "learning_rate": 0.0003166580385389704,
      "loss": 0.0003,
      "step": 118800
    },
    {
      "epoch": 34.181765890135175,
      "grad_norm": 0.0042369673028588295,
      "learning_rate": 0.0003163704342824274,
      "loss": 0.0018,
      "step": 118850
    },
    {
      "epoch": 34.196146102962324,
      "grad_norm": 0.003588867373764515,
      "learning_rate": 0.00031608283002588437,
      "loss": 0.0024,
      "step": 118900
    },
    {
      "epoch": 34.21052631578947,
      "grad_norm": 0.0003710825403686613,
      "learning_rate": 0.0003157952257693414,
      "loss": 0.0029,
      "step": 118950
    },
    {
      "epoch": 34.22490652861662,
      "grad_norm": 0.0009742276160977781,
      "learning_rate": 0.0003155076215127984,
      "loss": 0.0028,
      "step": 119000
    },
    {
      "epoch": 34.23928674144377,
      "grad_norm": 0.016608990728855133,
      "learning_rate": 0.00031522001725625535,
      "loss": 0.0026,
      "step": 119050
    },
    {
      "epoch": 34.253666954270926,
      "grad_norm": 0.00017752294661477208,
      "learning_rate": 0.0003149324129997124,
      "loss": 0.0006,
      "step": 119100
    },
    {
      "epoch": 34.268047167098075,
      "grad_norm": 7.556333002867177e-05,
      "learning_rate": 0.00031464480874316943,
      "loss": 0.0008,
      "step": 119150
    },
    {
      "epoch": 34.282427379925224,
      "grad_norm": 0.0002655253338161856,
      "learning_rate": 0.0003143572044866264,
      "loss": 0.0032,
      "step": 119200
    },
    {
      "epoch": 34.29680759275237,
      "grad_norm": 0.007772710639983416,
      "learning_rate": 0.0003140696002300834,
      "loss": 0.004,
      "step": 119250
    },
    {
      "epoch": 34.31118780557952,
      "grad_norm": 0.00017792099970392883,
      "learning_rate": 0.0003137819959735404,
      "loss": 0.0015,
      "step": 119300
    },
    {
      "epoch": 34.32556801840667,
      "grad_norm": 0.006395839154720306,
      "learning_rate": 0.00031349439171699737,
      "loss": 0.0004,
      "step": 119350
    },
    {
      "epoch": 34.33994823123382,
      "grad_norm": 1.6498772311024368e-05,
      "learning_rate": 0.00031320678746045444,
      "loss": 0.0036,
      "step": 119400
    },
    {
      "epoch": 34.354328444060975,
      "grad_norm": 7.458268373738974e-05,
      "learning_rate": 0.00031291918320391145,
      "loss": 0.0024,
      "step": 119450
    },
    {
      "epoch": 34.368708656888124,
      "grad_norm": 0.0011557280085980892,
      "learning_rate": 0.0003126315789473684,
      "loss": 0.0027,
      "step": 119500
    },
    {
      "epoch": 34.38308886971527,
      "grad_norm": 3.252317401347682e-05,
      "learning_rate": 0.0003123439746908254,
      "loss": 0.0045,
      "step": 119550
    },
    {
      "epoch": 34.39746908254242,
      "grad_norm": 0.00011776189057854936,
      "learning_rate": 0.00031205637043428243,
      "loss": 0.004,
      "step": 119600
    },
    {
      "epoch": 34.41184929536957,
      "grad_norm": 0.0061063882894814014,
      "learning_rate": 0.0003117687661777394,
      "loss": 0.0037,
      "step": 119650
    },
    {
      "epoch": 34.42622950819672,
      "grad_norm": 0.008431966416537762,
      "learning_rate": 0.00031148116192119646,
      "loss": 0.003,
      "step": 119700
    },
    {
      "epoch": 34.44060972102387,
      "grad_norm": 4.5040393160888925e-05,
      "learning_rate": 0.00031119355766465347,
      "loss": 0.0003,
      "step": 119750
    },
    {
      "epoch": 34.454989933851024,
      "grad_norm": 0.00031826330814510584,
      "learning_rate": 0.00031090595340811043,
      "loss": 0.0042,
      "step": 119800
    },
    {
      "epoch": 34.46937014667817,
      "grad_norm": 2.26099891733611e-05,
      "learning_rate": 0.00031061834915156744,
      "loss": 0.0025,
      "step": 119850
    },
    {
      "epoch": 34.48375035950532,
      "grad_norm": 1.9620814782683738e-05,
      "learning_rate": 0.00031033074489502445,
      "loss": 0.0042,
      "step": 119900
    },
    {
      "epoch": 34.49813057233247,
      "grad_norm": 0.00017538621614221483,
      "learning_rate": 0.0003100431406384814,
      "loss": 0.0004,
      "step": 119950
    },
    {
      "epoch": 34.51251078515962,
      "grad_norm": 0.0033687290269881487,
      "learning_rate": 0.0003097555363819385,
      "loss": 0.0012,
      "step": 120000
    },
    {
      "epoch": 34.52689099798677,
      "grad_norm": 3.353803185746074e-05,
      "learning_rate": 0.0003094679321253955,
      "loss": 0.0019,
      "step": 120050
    },
    {
      "epoch": 34.54127121081392,
      "grad_norm": 0.004668533802032471,
      "learning_rate": 0.00030918032786885245,
      "loss": 0.0028,
      "step": 120100
    },
    {
      "epoch": 34.555651423641066,
      "grad_norm": 0.003467249684035778,
      "learning_rate": 0.00030889272361230946,
      "loss": 0.0013,
      "step": 120150
    },
    {
      "epoch": 34.57003163646822,
      "grad_norm": 3.479982842691243e-05,
      "learning_rate": 0.00030860511935576647,
      "loss": 0.0027,
      "step": 120200
    },
    {
      "epoch": 34.58441184929537,
      "grad_norm": 1.7825081158662215e-05,
      "learning_rate": 0.00030831751509922343,
      "loss": 0.0015,
      "step": 120250
    },
    {
      "epoch": 34.59879206212252,
      "grad_norm": 9.846608008956537e-05,
      "learning_rate": 0.0003080299108426805,
      "loss": 0.0044,
      "step": 120300
    },
    {
      "epoch": 34.61317227494967,
      "grad_norm": 0.0004741092270705849,
      "learning_rate": 0.0003077423065861375,
      "loss": 0.0004,
      "step": 120350
    },
    {
      "epoch": 34.62755248777682,
      "grad_norm": 3.338783062645234e-05,
      "learning_rate": 0.00030745470232959447,
      "loss": 0.002,
      "step": 120400
    },
    {
      "epoch": 34.64193270060397,
      "grad_norm": 0.00012204675294924527,
      "learning_rate": 0.0003071670980730515,
      "loss": 0.0023,
      "step": 120450
    },
    {
      "epoch": 34.656312913431115,
      "grad_norm": 0.0007938542403280735,
      "learning_rate": 0.0003068794938165085,
      "loss": 0.0025,
      "step": 120500
    },
    {
      "epoch": 34.67069312625827,
      "grad_norm": 3.3372140023857355e-05,
      "learning_rate": 0.00030659188955996545,
      "loss": 0.0037,
      "step": 120550
    },
    {
      "epoch": 34.68507333908542,
      "grad_norm": 0.0006062801694497466,
      "learning_rate": 0.00030630428530342246,
      "loss": 0.0024,
      "step": 120600
    },
    {
      "epoch": 34.69945355191257,
      "grad_norm": 0.004610678181052208,
      "learning_rate": 0.00030601668104687953,
      "loss": 0.0006,
      "step": 120650
    },
    {
      "epoch": 34.71383376473972,
      "grad_norm": 0.0060535031370818615,
      "learning_rate": 0.00030572907679033654,
      "loss": 0.002,
      "step": 120700
    },
    {
      "epoch": 34.72821397756687,
      "grad_norm": 0.0003290455788373947,
      "learning_rate": 0.0003054414725337935,
      "loss": 0.0027,
      "step": 120750
    },
    {
      "epoch": 34.742594190394016,
      "grad_norm": 2.948295332316775e-05,
      "learning_rate": 0.0003051538682772505,
      "loss": 0.004,
      "step": 120800
    },
    {
      "epoch": 34.756974403221164,
      "grad_norm": 2.6405050448374823e-05,
      "learning_rate": 0.00030486626402070747,
      "loss": 0.001,
      "step": 120850
    },
    {
      "epoch": 34.77135461604832,
      "grad_norm": 4.3883235775865614e-05,
      "learning_rate": 0.0003045786597641645,
      "loss": 0.0021,
      "step": 120900
    },
    {
      "epoch": 34.78573482887547,
      "grad_norm": 2.898603088397067e-05,
      "learning_rate": 0.00030429105550762155,
      "loss": 0.0019,
      "step": 120950
    },
    {
      "epoch": 34.80011504170262,
      "grad_norm": 0.00011962093412876129,
      "learning_rate": 0.00030400345125107856,
      "loss": 0.0013,
      "step": 121000
    },
    {
      "epoch": 34.81449525452977,
      "grad_norm": 0.0010239453986287117,
      "learning_rate": 0.0003037158469945355,
      "loss": 0.002,
      "step": 121050
    },
    {
      "epoch": 34.828875467356916,
      "grad_norm": 0.0018782928818836808,
      "learning_rate": 0.00030342824273799253,
      "loss": 0.0032,
      "step": 121100
    },
    {
      "epoch": 34.843255680184065,
      "grad_norm": 4.5287557441042736e-05,
      "learning_rate": 0.00030314063848144954,
      "loss": 0.0013,
      "step": 121150
    },
    {
      "epoch": 34.857635893011214,
      "grad_norm": 0.040911294519901276,
      "learning_rate": 0.0003028530342249065,
      "loss": 0.0023,
      "step": 121200
    },
    {
      "epoch": 34.87201610583837,
      "grad_norm": 8.837477798806503e-05,
      "learning_rate": 0.00030256542996836357,
      "loss": 0.0027,
      "step": 121250
    },
    {
      "epoch": 34.88639631866552,
      "grad_norm": 0.008460155688226223,
      "learning_rate": 0.0003022778257118206,
      "loss": 0.0015,
      "step": 121300
    },
    {
      "epoch": 34.90077653149267,
      "grad_norm": 0.04547093063592911,
      "learning_rate": 0.00030199022145527754,
      "loss": 0.0034,
      "step": 121350
    },
    {
      "epoch": 34.915156744319816,
      "grad_norm": 0.0005712880520150065,
      "learning_rate": 0.00030170261719873455,
      "loss": 0.0031,
      "step": 121400
    },
    {
      "epoch": 34.929536957146965,
      "grad_norm": 0.031291279941797256,
      "learning_rate": 0.00030141501294219156,
      "loss": 0.002,
      "step": 121450
    },
    {
      "epoch": 34.943917169974114,
      "grad_norm": 0.0007493879529647529,
      "learning_rate": 0.0003011274086856485,
      "loss": 0.0009,
      "step": 121500
    },
    {
      "epoch": 34.95829738280126,
      "grad_norm": 4.779851951752789e-05,
      "learning_rate": 0.0003008398044291056,
      "loss": 0.0014,
      "step": 121550
    },
    {
      "epoch": 34.97267759562842,
      "grad_norm": 0.004144000355154276,
      "learning_rate": 0.0003005522001725626,
      "loss": 0.002,
      "step": 121600
    },
    {
      "epoch": 34.98705780845557,
      "grad_norm": 0.00015947777137625962,
      "learning_rate": 0.00030026459591601956,
      "loss": 0.0013,
      "step": 121650
    },
    {
      "epoch": 35.0,
      "eval_loss": 0.009931255131959915,
      "eval_runtime": 16.9929,
      "eval_samples_per_second": 2808.17,
      "eval_steps_per_second": 43.901,
      "step": 121695
    },
    {
      "epoch": 35.001438021282716,
      "grad_norm": 0.0051031941547989845,
      "learning_rate": 0.00029997699165947657,
      "loss": 0.0033,
      "step": 121700
    },
    {
      "epoch": 35.015818234109865,
      "grad_norm": 0.00013148027937859297,
      "learning_rate": 0.0002996893874029336,
      "loss": 0.003,
      "step": 121750
    },
    {
      "epoch": 35.030198446937014,
      "grad_norm": 0.00015839669504202902,
      "learning_rate": 0.00029940178314639054,
      "loss": 0.0032,
      "step": 121800
    },
    {
      "epoch": 35.04457865976416,
      "grad_norm": 0.01466852705925703,
      "learning_rate": 0.0002991141788898476,
      "loss": 0.0015,
      "step": 121850
    },
    {
      "epoch": 35.05895887259131,
      "grad_norm": 0.00017304258653894067,
      "learning_rate": 0.0002988265746333046,
      "loss": 0.003,
      "step": 121900
    },
    {
      "epoch": 35.07333908541847,
      "grad_norm": 5.8839319535763934e-05,
      "learning_rate": 0.0002985389703767616,
      "loss": 0.0017,
      "step": 121950
    },
    {
      "epoch": 35.08771929824562,
      "grad_norm": 0.0015249717980623245,
      "learning_rate": 0.0002982513661202186,
      "loss": 0.0025,
      "step": 122000
    },
    {
      "epoch": 35.102099511072765,
      "grad_norm": 0.0007643038989044726,
      "learning_rate": 0.0002979637618636756,
      "loss": 0.0014,
      "step": 122050
    },
    {
      "epoch": 35.116479723899914,
      "grad_norm": 0.00030334890470840037,
      "learning_rate": 0.00029767615760713256,
      "loss": 0.0017,
      "step": 122100
    },
    {
      "epoch": 35.13085993672706,
      "grad_norm": 0.00016090877761598676,
      "learning_rate": 0.0002973885533505896,
      "loss": 0.0006,
      "step": 122150
    },
    {
      "epoch": 35.14524014955421,
      "grad_norm": 0.0004508419951889664,
      "learning_rate": 0.00029710094909404664,
      "loss": 0.0022,
      "step": 122200
    },
    {
      "epoch": 35.15962036238136,
      "grad_norm": 6.599177868338302e-05,
      "learning_rate": 0.0002968133448375036,
      "loss": 0.0007,
      "step": 122250
    },
    {
      "epoch": 35.17400057520851,
      "grad_norm": 0.0036830087192356586,
      "learning_rate": 0.0002965257405809606,
      "loss": 0.0016,
      "step": 122300
    },
    {
      "epoch": 35.188380788035666,
      "grad_norm": 0.013212512247264385,
      "learning_rate": 0.0002962381363244176,
      "loss": 0.0031,
      "step": 122350
    },
    {
      "epoch": 35.202761000862814,
      "grad_norm": 9.526015492156148e-05,
      "learning_rate": 0.0002959505320678746,
      "loss": 0.0014,
      "step": 122400
    },
    {
      "epoch": 35.21714121368996,
      "grad_norm": 6.283378752414137e-05,
      "learning_rate": 0.00029566292781133165,
      "loss": 0.0018,
      "step": 122450
    },
    {
      "epoch": 35.23152142651711,
      "grad_norm": 0.016750222072005272,
      "learning_rate": 0.00029537532355478866,
      "loss": 0.0019,
      "step": 122500
    },
    {
      "epoch": 35.24590163934426,
      "grad_norm": 0.01692856475710869,
      "learning_rate": 0.0002950877192982456,
      "loss": 0.0005,
      "step": 122550
    },
    {
      "epoch": 35.26028185217141,
      "grad_norm": 0.0007007142412476242,
      "learning_rate": 0.00029480011504170263,
      "loss": 0.0022,
      "step": 122600
    },
    {
      "epoch": 35.27466206499856,
      "grad_norm": 4.185269517620327e-06,
      "learning_rate": 0.00029451251078515964,
      "loss": 0.0023,
      "step": 122650
    },
    {
      "epoch": 35.289042277825715,
      "grad_norm": 3.9733873563818634e-05,
      "learning_rate": 0.0002942249065286166,
      "loss": 0.003,
      "step": 122700
    },
    {
      "epoch": 35.30342249065286,
      "grad_norm": 0.0005574231618084013,
      "learning_rate": 0.00029393730227207366,
      "loss": 0.0029,
      "step": 122750
    },
    {
      "epoch": 35.31780270348001,
      "grad_norm": 0.00012440394493751228,
      "learning_rate": 0.0002936496980155307,
      "loss": 0.0024,
      "step": 122800
    },
    {
      "epoch": 35.33218291630716,
      "grad_norm": 3.083078991039656e-05,
      "learning_rate": 0.00029336209375898764,
      "loss": 0.0018,
      "step": 122850
    },
    {
      "epoch": 35.34656312913431,
      "grad_norm": 0.003386635100468993,
      "learning_rate": 0.00029307448950244465,
      "loss": 0.0005,
      "step": 122900
    },
    {
      "epoch": 35.36094334196146,
      "grad_norm": 0.00014735398872289807,
      "learning_rate": 0.00029278688524590166,
      "loss": 0.0025,
      "step": 122950
    },
    {
      "epoch": 35.37532355478861,
      "grad_norm": 0.0007561519742012024,
      "learning_rate": 0.0002924992809893586,
      "loss": 0.0006,
      "step": 123000
    },
    {
      "epoch": 35.389703767615764,
      "grad_norm": 0.0012161631602793932,
      "learning_rate": 0.0002922116767328157,
      "loss": 0.0007,
      "step": 123050
    },
    {
      "epoch": 35.40408398044291,
      "grad_norm": 1.7790302081266418e-05,
      "learning_rate": 0.0002919240724762727,
      "loss": 0.0026,
      "step": 123100
    },
    {
      "epoch": 35.41846419327006,
      "grad_norm": 2.3873100872151554e-05,
      "learning_rate": 0.00029163646821972965,
      "loss": 0.0044,
      "step": 123150
    },
    {
      "epoch": 35.43284440609721,
      "grad_norm": 8.688779780641198e-05,
      "learning_rate": 0.00029134886396318667,
      "loss": 0.0052,
      "step": 123200
    },
    {
      "epoch": 35.44722461892436,
      "grad_norm": 0.00023454987967852503,
      "learning_rate": 0.0002910612597066437,
      "loss": 0.0017,
      "step": 123250
    },
    {
      "epoch": 35.46160483175151,
      "grad_norm": 0.002680644392967224,
      "learning_rate": 0.00029077365545010064,
      "loss": 0.0015,
      "step": 123300
    },
    {
      "epoch": 35.47598504457866,
      "grad_norm": 8.189763320842758e-05,
      "learning_rate": 0.00029048605119355765,
      "loss": 0.0037,
      "step": 123350
    },
    {
      "epoch": 35.49036525740581,
      "grad_norm": 1.669384255365003e-05,
      "learning_rate": 0.0002901984469370147,
      "loss": 0.0018,
      "step": 123400
    },
    {
      "epoch": 35.50474547023296,
      "grad_norm": 5.632117245113477e-05,
      "learning_rate": 0.0002899108426804717,
      "loss": 0.0026,
      "step": 123450
    },
    {
      "epoch": 35.51912568306011,
      "grad_norm": 0.00016354673425666988,
      "learning_rate": 0.0002896232384239287,
      "loss": 0.0052,
      "step": 123500
    },
    {
      "epoch": 35.53350589588726,
      "grad_norm": 0.0006489105871878564,
      "learning_rate": 0.0002893356341673857,
      "loss": 0.0027,
      "step": 123550
    },
    {
      "epoch": 35.54788610871441,
      "grad_norm": 0.0216213446110487,
      "learning_rate": 0.00028904802991084266,
      "loss": 0.0032,
      "step": 123600
    },
    {
      "epoch": 35.56226632154156,
      "grad_norm": 0.003777591045945883,
      "learning_rate": 0.00028876042565429967,
      "loss": 0.0013,
      "step": 123650
    },
    {
      "epoch": 35.576646534368706,
      "grad_norm": 0.004916054662317038,
      "learning_rate": 0.00028847282139775674,
      "loss": 0.0039,
      "step": 123700
    },
    {
      "epoch": 35.59102674719586,
      "grad_norm": 0.0002706732484512031,
      "learning_rate": 0.0002881852171412137,
      "loss": 0.003,
      "step": 123750
    },
    {
      "epoch": 35.60540696002301,
      "grad_norm": 0.00012543133925646544,
      "learning_rate": 0.0002878976128846707,
      "loss": 0.0027,
      "step": 123800
    },
    {
      "epoch": 35.61978717285016,
      "grad_norm": 0.0025109436828643084,
      "learning_rate": 0.0002876100086281277,
      "loss": 0.0023,
      "step": 123850
    },
    {
      "epoch": 35.63416738567731,
      "grad_norm": 0.0064692129381000996,
      "learning_rate": 0.0002873224043715847,
      "loss": 0.0012,
      "step": 123900
    },
    {
      "epoch": 35.64854759850446,
      "grad_norm": 0.0002130572684109211,
      "learning_rate": 0.0002870348001150417,
      "loss": 0.0019,
      "step": 123950
    },
    {
      "epoch": 35.662927811331606,
      "grad_norm": 3.110661782557145e-05,
      "learning_rate": 0.00028674719585849876,
      "loss": 0.0061,
      "step": 124000
    },
    {
      "epoch": 35.677308024158755,
      "grad_norm": 0.002569864969700575,
      "learning_rate": 0.0002864595916019557,
      "loss": 0.0025,
      "step": 124050
    },
    {
      "epoch": 35.69168823698591,
      "grad_norm": 0.004353528842329979,
      "learning_rate": 0.0002861719873454127,
      "loss": 0.0021,
      "step": 124100
    },
    {
      "epoch": 35.70606844981306,
      "grad_norm": 5.007422078051604e-05,
      "learning_rate": 0.00028588438308886974,
      "loss": 0.0044,
      "step": 124150
    },
    {
      "epoch": 35.72044866264021,
      "grad_norm": 1.1585202628339175e-05,
      "learning_rate": 0.0002855967788323267,
      "loss": 0.0002,
      "step": 124200
    },
    {
      "epoch": 35.73482887546736,
      "grad_norm": 1.0123618267243728e-05,
      "learning_rate": 0.0002853091745757837,
      "loss": 0.001,
      "step": 124250
    },
    {
      "epoch": 35.749209088294506,
      "grad_norm": 0.006787749007344246,
      "learning_rate": 0.0002850215703192408,
      "loss": 0.0015,
      "step": 124300
    },
    {
      "epoch": 35.763589301121655,
      "grad_norm": 0.00350998155772686,
      "learning_rate": 0.00028473396606269773,
      "loss": 0.0016,
      "step": 124350
    },
    {
      "epoch": 35.777969513948804,
      "grad_norm": 0.00026731615071184933,
      "learning_rate": 0.00028444636180615475,
      "loss": 0.0011,
      "step": 124400
    },
    {
      "epoch": 35.79234972677595,
      "grad_norm": 9.770347969606519e-05,
      "learning_rate": 0.00028415875754961176,
      "loss": 0.0028,
      "step": 124450
    },
    {
      "epoch": 35.80672993960311,
      "grad_norm": 0.01603265479207039,
      "learning_rate": 0.0002838711532930687,
      "loss": 0.0022,
      "step": 124500
    },
    {
      "epoch": 35.82111015243026,
      "grad_norm": 6.557247252203524e-05,
      "learning_rate": 0.00028358354903652573,
      "loss": 0.002,
      "step": 124550
    },
    {
      "epoch": 35.83549036525741,
      "grad_norm": 9.325226710643619e-05,
      "learning_rate": 0.0002832959447799828,
      "loss": 0.0019,
      "step": 124600
    },
    {
      "epoch": 35.849870578084555,
      "grad_norm": 7.101455412339419e-05,
      "learning_rate": 0.00028300834052343975,
      "loss": 0.0057,
      "step": 124650
    },
    {
      "epoch": 35.864250790911704,
      "grad_norm": 0.00010868177923839539,
      "learning_rate": 0.00028272073626689676,
      "loss": 0.0048,
      "step": 124700
    },
    {
      "epoch": 35.87863100373885,
      "grad_norm": 2.4802227926556952e-05,
      "learning_rate": 0.0002824331320103538,
      "loss": 0.0026,
      "step": 124750
    },
    {
      "epoch": 35.893011216566,
      "grad_norm": 0.0022262863349169493,
      "learning_rate": 0.00028214552775381074,
      "loss": 0.0019,
      "step": 124800
    },
    {
      "epoch": 35.90739142939316,
      "grad_norm": 2.1336318241083063e-05,
      "learning_rate": 0.00028185792349726775,
      "loss": 0.0014,
      "step": 124850
    },
    {
      "epoch": 35.92177164222031,
      "grad_norm": 0.046828679740428925,
      "learning_rate": 0.0002815703192407248,
      "loss": 0.0012,
      "step": 124900
    },
    {
      "epoch": 35.936151855047456,
      "grad_norm": 4.1340153984492645e-05,
      "learning_rate": 0.00028128271498418177,
      "loss": 0.0043,
      "step": 124950
    },
    {
      "epoch": 35.950532067874605,
      "grad_norm": 0.0008152928785420954,
      "learning_rate": 0.0002809951107276388,
      "loss": 0.0065,
      "step": 125000
    },
    {
      "epoch": 35.96491228070175,
      "grad_norm": 2.5474620997556485e-05,
      "learning_rate": 0.0002807075064710958,
      "loss": 0.0006,
      "step": 125050
    },
    {
      "epoch": 35.9792924935289,
      "grad_norm": 2.2422200345317833e-05,
      "learning_rate": 0.00028041990221455275,
      "loss": 0.0017,
      "step": 125100
    },
    {
      "epoch": 35.99367270635605,
      "grad_norm": 0.0007456218590959907,
      "learning_rate": 0.00028013229795800977,
      "loss": 0.0003,
      "step": 125150
    },
    {
      "epoch": 36.0,
      "eval_loss": 0.010276448912918568,
      "eval_runtime": 18.7134,
      "eval_samples_per_second": 2549.985,
      "eval_steps_per_second": 39.864,
      "step": 125172
    },
    {
      "epoch": 36.00805291918321,
      "grad_norm": 0.018788067623972893,
      "learning_rate": 0.00027984469370146683,
      "loss": 0.0018,
      "step": 125200
    },
    {
      "epoch": 36.022433132010356,
      "grad_norm": 0.004934555850923061,
      "learning_rate": 0.0002795570894449238,
      "loss": 0.0029,
      "step": 125250
    },
    {
      "epoch": 36.036813344837505,
      "grad_norm": 0.015896502882242203,
      "learning_rate": 0.0002792694851883808,
      "loss": 0.0009,
      "step": 125300
    },
    {
      "epoch": 36.051193557664654,
      "grad_norm": 2.0688952645286918e-05,
      "learning_rate": 0.0002789818809318378,
      "loss": 0.0004,
      "step": 125350
    },
    {
      "epoch": 36.0655737704918,
      "grad_norm": 0.00013945033424533904,
      "learning_rate": 0.0002786942766752948,
      "loss": 0.0018,
      "step": 125400
    },
    {
      "epoch": 36.07995398331895,
      "grad_norm": 0.003320864634588361,
      "learning_rate": 0.0002784066724187518,
      "loss": 0.0031,
      "step": 125450
    },
    {
      "epoch": 36.0943341961461,
      "grad_norm": 0.0008342656074091792,
      "learning_rate": 0.00027811906816220885,
      "loss": 0.0002,
      "step": 125500
    },
    {
      "epoch": 36.108714408973256,
      "grad_norm": 0.0016430601244792342,
      "learning_rate": 0.0002778314639056658,
      "loss": 0.0027,
      "step": 125550
    },
    {
      "epoch": 36.123094621800405,
      "grad_norm": 7.608169107697904e-05,
      "learning_rate": 0.0002775438596491228,
      "loss": 0.0044,
      "step": 125600
    },
    {
      "epoch": 36.137474834627554,
      "grad_norm": 0.00030470930505543947,
      "learning_rate": 0.00027725625539257984,
      "loss": 0.0018,
      "step": 125650
    },
    {
      "epoch": 36.1518550474547,
      "grad_norm": 5.127754047862254e-05,
      "learning_rate": 0.0002769686511360368,
      "loss": 0.0038,
      "step": 125700
    },
    {
      "epoch": 36.16623526028185,
      "grad_norm": 0.00040819693822413683,
      "learning_rate": 0.0002766810468794938,
      "loss": 0.0006,
      "step": 125750
    },
    {
      "epoch": 36.180615473109,
      "grad_norm": 9.07893045223318e-05,
      "learning_rate": 0.00027639344262295087,
      "loss": 0.0024,
      "step": 125800
    },
    {
      "epoch": 36.19499568593615,
      "grad_norm": 0.00012243076344020665,
      "learning_rate": 0.00027610583836640783,
      "loss": 0.0009,
      "step": 125850
    },
    {
      "epoch": 36.209375898763305,
      "grad_norm": 0.00020574354857672006,
      "learning_rate": 0.00027581823410986484,
      "loss": 0.0014,
      "step": 125900
    },
    {
      "epoch": 36.223756111590454,
      "grad_norm": 0.03761336952447891,
      "learning_rate": 0.00027553062985332186,
      "loss": 0.0021,
      "step": 125950
    },
    {
      "epoch": 36.2381363244176,
      "grad_norm": 4.862358400714584e-05,
      "learning_rate": 0.0002752430255967788,
      "loss": 0.0007,
      "step": 126000
    },
    {
      "epoch": 36.25251653724475,
      "grad_norm": 7.019154145382345e-05,
      "learning_rate": 0.0002749554213402358,
      "loss": 0.0021,
      "step": 126050
    },
    {
      "epoch": 36.2668967500719,
      "grad_norm": 0.0028460638131946325,
      "learning_rate": 0.00027466781708369284,
      "loss": 0.0008,
      "step": 126100
    },
    {
      "epoch": 36.28127696289905,
      "grad_norm": 5.824468462378718e-05,
      "learning_rate": 0.00027438021282714985,
      "loss": 0.001,
      "step": 126150
    },
    {
      "epoch": 36.2956571757262,
      "grad_norm": 0.0007218990358524024,
      "learning_rate": 0.00027409260857060686,
      "loss": 0.005,
      "step": 126200
    },
    {
      "epoch": 36.31003738855335,
      "grad_norm": 0.0008293955470435321,
      "learning_rate": 0.0002738050043140639,
      "loss": 0.0013,
      "step": 126250
    },
    {
      "epoch": 36.3244176013805,
      "grad_norm": 0.000516210391651839,
      "learning_rate": 0.00027351740005752083,
      "loss": 0.0014,
      "step": 126300
    },
    {
      "epoch": 36.33879781420765,
      "grad_norm": 9.07544672372751e-05,
      "learning_rate": 0.00027322979580097785,
      "loss": 0.0032,
      "step": 126350
    },
    {
      "epoch": 36.3531780270348,
      "grad_norm": 0.001242464524693787,
      "learning_rate": 0.00027294219154443486,
      "loss": 0.0044,
      "step": 126400
    },
    {
      "epoch": 36.36755823986195,
      "grad_norm": 0.00020604683959390968,
      "learning_rate": 0.00027265458728789187,
      "loss": 0.0022,
      "step": 126450
    },
    {
      "epoch": 36.3819384526891,
      "grad_norm": 0.0004450163687579334,
      "learning_rate": 0.0002723669830313489,
      "loss": 0.0019,
      "step": 126500
    },
    {
      "epoch": 36.39631866551625,
      "grad_norm": 0.0006278729415498674,
      "learning_rate": 0.0002720793787748059,
      "loss": 0.0056,
      "step": 126550
    },
    {
      "epoch": 36.410698878343396,
      "grad_norm": 0.0018097148276865482,
      "learning_rate": 0.00027179177451826285,
      "loss": 0.0006,
      "step": 126600
    },
    {
      "epoch": 36.42507909117055,
      "grad_norm": 5.7412791647948325e-05,
      "learning_rate": 0.00027150417026171986,
      "loss": 0.0029,
      "step": 126650
    },
    {
      "epoch": 36.4394593039977,
      "grad_norm": 0.011599740944802761,
      "learning_rate": 0.0002712165660051769,
      "loss": 0.0023,
      "step": 126700
    },
    {
      "epoch": 36.45383951682485,
      "grad_norm": 0.00034676562063395977,
      "learning_rate": 0.0002709289617486339,
      "loss": 0.0019,
      "step": 126750
    },
    {
      "epoch": 36.468219729652,
      "grad_norm": 3.613053559092805e-05,
      "learning_rate": 0.0002706413574920909,
      "loss": 0.002,
      "step": 126800
    },
    {
      "epoch": 36.48259994247915,
      "grad_norm": 0.004508019424974918,
      "learning_rate": 0.0002703537532355479,
      "loss": 0.0032,
      "step": 126850
    },
    {
      "epoch": 36.4969801553063,
      "grad_norm": 0.005403522867709398,
      "learning_rate": 0.00027006614897900487,
      "loss": 0.0052,
      "step": 126900
    },
    {
      "epoch": 36.511360368133445,
      "grad_norm": 5.5408734624506906e-05,
      "learning_rate": 0.0002697785447224619,
      "loss": 0.0009,
      "step": 126950
    },
    {
      "epoch": 36.5257405809606,
      "grad_norm": 0.004915024619549513,
      "learning_rate": 0.0002694909404659189,
      "loss": 0.0043,
      "step": 127000
    },
    {
      "epoch": 36.54012079378775,
      "grad_norm": 0.00015772850019857287,
      "learning_rate": 0.0002692033362093759,
      "loss": 0.0014,
      "step": 127050
    },
    {
      "epoch": 36.5545010066149,
      "grad_norm": 0.001527237007394433,
      "learning_rate": 0.0002689157319528329,
      "loss": 0.0019,
      "step": 127100
    },
    {
      "epoch": 36.56888121944205,
      "grad_norm": 0.00026904582045972347,
      "learning_rate": 0.00026862812769628993,
      "loss": 0.0009,
      "step": 127150
    },
    {
      "epoch": 36.5832614322692,
      "grad_norm": 0.022715311497449875,
      "learning_rate": 0.0002683405234397469,
      "loss": 0.0014,
      "step": 127200
    },
    {
      "epoch": 36.597641645096346,
      "grad_norm": 0.0001654384541325271,
      "learning_rate": 0.0002680529191832039,
      "loss": 0.0027,
      "step": 127250
    },
    {
      "epoch": 36.612021857923494,
      "grad_norm": 1.7279500752920285e-05,
      "learning_rate": 0.0002677653149266609,
      "loss": 0.0013,
      "step": 127300
    },
    {
      "epoch": 36.62640207075065,
      "grad_norm": 0.00010890610428759828,
      "learning_rate": 0.00026747771067011793,
      "loss": 0.0025,
      "step": 127350
    },
    {
      "epoch": 36.6407822835778,
      "grad_norm": 0.000650026195216924,
      "learning_rate": 0.00026719010641357494,
      "loss": 0.0017,
      "step": 127400
    },
    {
      "epoch": 36.65516249640495,
      "grad_norm": 0.0004196713271085173,
      "learning_rate": 0.00026690250215703195,
      "loss": 0.0005,
      "step": 127450
    },
    {
      "epoch": 36.6695427092321,
      "grad_norm": 2.0955752916052006e-05,
      "learning_rate": 0.0002666148979004889,
      "loss": 0.0036,
      "step": 127500
    },
    {
      "epoch": 36.683922922059246,
      "grad_norm": 2.541869798733387e-05,
      "learning_rate": 0.0002663272936439459,
      "loss": 0.0058,
      "step": 127550
    },
    {
      "epoch": 36.698303134886395,
      "grad_norm": 0.00731913186609745,
      "learning_rate": 0.00026603968938740294,
      "loss": 0.0027,
      "step": 127600
    },
    {
      "epoch": 36.71268334771354,
      "grad_norm": 0.00011204682232346386,
      "learning_rate": 0.00026575208513085995,
      "loss": 0.0022,
      "step": 127650
    },
    {
      "epoch": 36.7270635605407,
      "grad_norm": 7.46063597034663e-05,
      "learning_rate": 0.00026546448087431696,
      "loss": 0.0019,
      "step": 127700
    },
    {
      "epoch": 36.74144377336785,
      "grad_norm": 2.1260822904878296e-05,
      "learning_rate": 0.00026517687661777397,
      "loss": 0.0023,
      "step": 127750
    },
    {
      "epoch": 36.755823986195,
      "grad_norm": 0.00021649982954841107,
      "learning_rate": 0.00026488927236123093,
      "loss": 0.0015,
      "step": 127800
    },
    {
      "epoch": 36.770204199022146,
      "grad_norm": 5.6559314543846995e-05,
      "learning_rate": 0.00026460166810468794,
      "loss": 0.0023,
      "step": 127850
    },
    {
      "epoch": 36.784584411849295,
      "grad_norm": 0.00012085628986824304,
      "learning_rate": 0.00026431406384814496,
      "loss": 0.0021,
      "step": 127900
    },
    {
      "epoch": 36.798964624676444,
      "grad_norm": 0.014559345319867134,
      "learning_rate": 0.00026402645959160197,
      "loss": 0.0018,
      "step": 127950
    },
    {
      "epoch": 36.81334483750359,
      "grad_norm": 0.00014216790441423655,
      "learning_rate": 0.000263738855335059,
      "loss": 0.0031,
      "step": 128000
    },
    {
      "epoch": 36.82772505033074,
      "grad_norm": 0.0013935170136392117,
      "learning_rate": 0.000263451251078516,
      "loss": 0.0037,
      "step": 128050
    },
    {
      "epoch": 36.8421052631579,
      "grad_norm": 0.02711549401283264,
      "learning_rate": 0.00026316364682197295,
      "loss": 0.001,
      "step": 128100
    },
    {
      "epoch": 36.856485475985046,
      "grad_norm": 0.012147256173193455,
      "learning_rate": 0.00026287604256542996,
      "loss": 0.0048,
      "step": 128150
    },
    {
      "epoch": 36.870865688812195,
      "grad_norm": 7.709376950515434e-05,
      "learning_rate": 0.000262588438308887,
      "loss": 0.0029,
      "step": 128200
    },
    {
      "epoch": 36.885245901639344,
      "grad_norm": 4.993202310288325e-05,
      "learning_rate": 0.000262300834052344,
      "loss": 0.0013,
      "step": 128250
    },
    {
      "epoch": 36.89962611446649,
      "grad_norm": 0.00029702376923523843,
      "learning_rate": 0.000262013229795801,
      "loss": 0.0038,
      "step": 128300
    },
    {
      "epoch": 36.91400632729364,
      "grad_norm": 0.0007304279133677483,
      "learning_rate": 0.000261725625539258,
      "loss": 0.0029,
      "step": 128350
    },
    {
      "epoch": 36.92838654012079,
      "grad_norm": 4.2739549826364964e-05,
      "learning_rate": 0.00026143802128271497,
      "loss": 0.0021,
      "step": 128400
    },
    {
      "epoch": 36.942766752947946,
      "grad_norm": 2.578004205133766e-05,
      "learning_rate": 0.000261150417026172,
      "loss": 0.0033,
      "step": 128450
    },
    {
      "epoch": 36.957146965775095,
      "grad_norm": 0.00010254623339278623,
      "learning_rate": 0.000260862812769629,
      "loss": 0.0049,
      "step": 128500
    },
    {
      "epoch": 36.971527178602244,
      "grad_norm": 3.01893160212785e-05,
      "learning_rate": 0.000260575208513086,
      "loss": 0.0009,
      "step": 128550
    },
    {
      "epoch": 36.98590739142939,
      "grad_norm": 0.0006320146494545043,
      "learning_rate": 0.000260287604256543,
      "loss": 0.0016,
      "step": 128600
    },
    {
      "epoch": 37.0,
      "eval_loss": 0.009937587194144726,
      "eval_runtime": 17.6188,
      "eval_samples_per_second": 2708.408,
      "eval_steps_per_second": 42.341,
      "step": 128649
    },
    {
      "epoch": 37.00028760425654,
      "grad_norm": 0.00015667628031224012,
      "learning_rate": 0.00026000000000000003,
      "loss": 0.0014,
      "step": 128650
    },
    {
      "epoch": 37.01466781708369,
      "grad_norm": 0.007475476711988449,
      "learning_rate": 0.000259712395743457,
      "loss": 0.0025,
      "step": 128700
    },
    {
      "epoch": 37.02904802991084,
      "grad_norm": 0.0013626512372866273,
      "learning_rate": 0.000259424791486914,
      "loss": 0.0026,
      "step": 128750
    },
    {
      "epoch": 37.043428242737996,
      "grad_norm": 0.006479068659245968,
      "learning_rate": 0.000259137187230371,
      "loss": 0.0006,
      "step": 128800
    },
    {
      "epoch": 37.057808455565144,
      "grad_norm": 7.140677917050198e-05,
      "learning_rate": 0.00025884958297382797,
      "loss": 0.0016,
      "step": 128850
    },
    {
      "epoch": 37.07218866839229,
      "grad_norm": 0.0013336439151316881,
      "learning_rate": 0.00025856197871728504,
      "loss": 0.0037,
      "step": 128900
    },
    {
      "epoch": 37.08656888121944,
      "grad_norm": 1.7680771634331904e-05,
      "learning_rate": 0.00025827437446074205,
      "loss": 0.001,
      "step": 128950
    },
    {
      "epoch": 37.10094909404659,
      "grad_norm": 0.0009876398835331202,
      "learning_rate": 0.000257986770204199,
      "loss": 0.0006,
      "step": 129000
    },
    {
      "epoch": 37.11532930687374,
      "grad_norm": 8.713217539479956e-06,
      "learning_rate": 0.000257699165947656,
      "loss": 0.0008,
      "step": 129050
    },
    {
      "epoch": 37.12970951970089,
      "grad_norm": 0.0006076954887248576,
      "learning_rate": 0.00025741156169111303,
      "loss": 0.003,
      "step": 129100
    },
    {
      "epoch": 37.144089732528045,
      "grad_norm": 0.0005608889041468501,
      "learning_rate": 0.00025712395743457,
      "loss": 0.0027,
      "step": 129150
    },
    {
      "epoch": 37.15846994535519,
      "grad_norm": 4.065655230078846e-05,
      "learning_rate": 0.00025683635317802706,
      "loss": 0.0022,
      "step": 129200
    },
    {
      "epoch": 37.17285015818234,
      "grad_norm": 0.0209468025714159,
      "learning_rate": 0.00025654874892148407,
      "loss": 0.0006,
      "step": 129250
    },
    {
      "epoch": 37.18723037100949,
      "grad_norm": 0.00021997865405865014,
      "learning_rate": 0.00025626114466494103,
      "loss": 0.0017,
      "step": 129300
    },
    {
      "epoch": 37.20161058383664,
      "grad_norm": 0.00038837885949760675,
      "learning_rate": 0.00025597354040839804,
      "loss": 0.0012,
      "step": 129350
    },
    {
      "epoch": 37.21599079666379,
      "grad_norm": 9.97681709122844e-05,
      "learning_rate": 0.00025568593615185505,
      "loss": 0.0031,
      "step": 129400
    },
    {
      "epoch": 37.23037100949094,
      "grad_norm": 0.005085702519863844,
      "learning_rate": 0.000255398331895312,
      "loss": 0.0017,
      "step": 129450
    },
    {
      "epoch": 37.244751222318094,
      "grad_norm": 0.0010190446628257632,
      "learning_rate": 0.0002551107276387691,
      "loss": 0.0004,
      "step": 129500
    },
    {
      "epoch": 37.25913143514524,
      "grad_norm": 0.0018172601703554392,
      "learning_rate": 0.0002548231233822261,
      "loss": 0.0053,
      "step": 129550
    },
    {
      "epoch": 37.27351164797239,
      "grad_norm": 3.234820542274974e-05,
      "learning_rate": 0.00025453551912568305,
      "loss": 0.0019,
      "step": 129600
    },
    {
      "epoch": 37.28789186079954,
      "grad_norm": 3.853384259855375e-05,
      "learning_rate": 0.00025424791486914006,
      "loss": 0.0022,
      "step": 129650
    },
    {
      "epoch": 37.30227207362669,
      "grad_norm": 8.545931450498756e-06,
      "learning_rate": 0.00025396031061259707,
      "loss": 0.0004,
      "step": 129700
    },
    {
      "epoch": 37.31665228645384,
      "grad_norm": 5.181080268812366e-05,
      "learning_rate": 0.00025367270635605403,
      "loss": 0.0019,
      "step": 129750
    },
    {
      "epoch": 37.33103249928099,
      "grad_norm": 0.00047742441529408097,
      "learning_rate": 0.0002533851020995111,
      "loss": 0.0031,
      "step": 129800
    },
    {
      "epoch": 37.34541271210814,
      "grad_norm": 0.000666799780447036,
      "learning_rate": 0.0002530974978429681,
      "loss": 0.0021,
      "step": 129850
    },
    {
      "epoch": 37.35979292493529,
      "grad_norm": 0.0002593339595478028,
      "learning_rate": 0.00025280989358642507,
      "loss": 0.0058,
      "step": 129900
    },
    {
      "epoch": 37.37417313776244,
      "grad_norm": 0.0003252735477872193,
      "learning_rate": 0.0002525222893298821,
      "loss": 0.0028,
      "step": 129950
    },
    {
      "epoch": 37.38855335058959,
      "grad_norm": 0.022916918620467186,
      "learning_rate": 0.0002522346850733391,
      "loss": 0.0027,
      "step": 130000
    },
    {
      "epoch": 37.40293356341674,
      "grad_norm": 0.004210031591355801,
      "learning_rate": 0.00025194708081679605,
      "loss": 0.0028,
      "step": 130050
    },
    {
      "epoch": 37.41731377624389,
      "grad_norm": 0.00017348647816106677,
      "learning_rate": 0.0002516594765602531,
      "loss": 0.006,
      "step": 130100
    },
    {
      "epoch": 37.431693989071036,
      "grad_norm": 0.0008423339459113777,
      "learning_rate": 0.00025137187230371013,
      "loss": 0.001,
      "step": 130150
    },
    {
      "epoch": 37.446074201898185,
      "grad_norm": 0.0021591237746179104,
      "learning_rate": 0.0002510842680471671,
      "loss": 0.0029,
      "step": 130200
    },
    {
      "epoch": 37.46045441472534,
      "grad_norm": 1.6020781913539395e-05,
      "learning_rate": 0.0002507966637906241,
      "loss": 0.0037,
      "step": 130250
    },
    {
      "epoch": 37.47483462755249,
      "grad_norm": 6.444000609917566e-05,
      "learning_rate": 0.0002505090595340811,
      "loss": 0.0025,
      "step": 130300
    },
    {
      "epoch": 37.48921484037964,
      "grad_norm": 0.00013433916319627315,
      "learning_rate": 0.00025022145527753807,
      "loss": 0.0031,
      "step": 130350
    },
    {
      "epoch": 37.50359505320679,
      "grad_norm": 7.020502380328253e-05,
      "learning_rate": 0.00024993385102099514,
      "loss": 0.0015,
      "step": 130400
    },
    {
      "epoch": 37.517975266033936,
      "grad_norm": 0.00014415534678846598,
      "learning_rate": 0.0002496462467644521,
      "loss": 0.0025,
      "step": 130450
    },
    {
      "epoch": 37.532355478861085,
      "grad_norm": 0.006981212180107832,
      "learning_rate": 0.0002493586425079091,
      "loss": 0.0041,
      "step": 130500
    },
    {
      "epoch": 37.546735691688234,
      "grad_norm": 0.00047950295265764,
      "learning_rate": 0.0002490710382513661,
      "loss": 0.002,
      "step": 130550
    },
    {
      "epoch": 37.56111590451539,
      "grad_norm": 0.000581106694880873,
      "learning_rate": 0.00024878343399482313,
      "loss": 0.0026,
      "step": 130600
    },
    {
      "epoch": 37.57549611734254,
      "grad_norm": 6.724089325871319e-05,
      "learning_rate": 0.00024849582973828014,
      "loss": 0.0011,
      "step": 130650
    },
    {
      "epoch": 37.58987633016969,
      "grad_norm": 0.014696085825562477,
      "learning_rate": 0.00024820822548173716,
      "loss": 0.0024,
      "step": 130700
    },
    {
      "epoch": 37.604256542996836,
      "grad_norm": 0.0001996323117054999,
      "learning_rate": 0.0002479206212251941,
      "loss": 0.0009,
      "step": 130750
    },
    {
      "epoch": 37.618636755823985,
      "grad_norm": 4.7795390855753794e-05,
      "learning_rate": 0.0002476330169686511,
      "loss": 0.0032,
      "step": 130800
    },
    {
      "epoch": 37.633016968651134,
      "grad_norm": 0.008880398236215115,
      "learning_rate": 0.00024734541271210814,
      "loss": 0.0046,
      "step": 130850
    },
    {
      "epoch": 37.64739718147828,
      "grad_norm": 0.0015873056836426258,
      "learning_rate": 0.00024705780845556515,
      "loss": 0.0019,
      "step": 130900
    },
    {
      "epoch": 37.66177739430544,
      "grad_norm": 0.00046739165554754436,
      "learning_rate": 0.00024677020419902216,
      "loss": 0.0004,
      "step": 130950
    },
    {
      "epoch": 37.67615760713259,
      "grad_norm": 0.0013048480032011867,
      "learning_rate": 0.0002464825999424792,
      "loss": 0.0042,
      "step": 131000
    },
    {
      "epoch": 37.69053781995974,
      "grad_norm": 0.0004817440058104694,
      "learning_rate": 0.00024619499568593613,
      "loss": 0.0029,
      "step": 131050
    },
    {
      "epoch": 37.704918032786885,
      "grad_norm": 0.014756406657397747,
      "learning_rate": 0.00024590739142939315,
      "loss": 0.0037,
      "step": 131100
    },
    {
      "epoch": 37.719298245614034,
      "grad_norm": 0.010738556273281574,
      "learning_rate": 0.00024561978717285016,
      "loss": 0.0019,
      "step": 131150
    },
    {
      "epoch": 37.73367845844118,
      "grad_norm": 0.0003153798752464354,
      "learning_rate": 0.00024533218291630717,
      "loss": 0.006,
      "step": 131200
    },
    {
      "epoch": 37.74805867126833,
      "grad_norm": 0.009966404177248478,
      "learning_rate": 0.0002450445786597642,
      "loss": 0.001,
      "step": 131250
    },
    {
      "epoch": 37.76243888409549,
      "grad_norm": 0.00034960126504302025,
      "learning_rate": 0.0002447569744032212,
      "loss": 0.002,
      "step": 131300
    },
    {
      "epoch": 37.77681909692264,
      "grad_norm": 0.003444711212068796,
      "learning_rate": 0.00024446937014667815,
      "loss": 0.0015,
      "step": 131350
    },
    {
      "epoch": 37.791199309749786,
      "grad_norm": 0.002505237702280283,
      "learning_rate": 0.00024418176589013517,
      "loss": 0.0014,
      "step": 131400
    },
    {
      "epoch": 37.805579522576934,
      "grad_norm": 0.00012844651064369828,
      "learning_rate": 0.00024389416163359218,
      "loss": 0.0021,
      "step": 131450
    },
    {
      "epoch": 37.81995973540408,
      "grad_norm": 2.1678641132893972e-05,
      "learning_rate": 0.00024360655737704916,
      "loss": 0.0046,
      "step": 131500
    },
    {
      "epoch": 37.83433994823123,
      "grad_norm": 0.018268568441271782,
      "learning_rate": 0.0002433189531205062,
      "loss": 0.0005,
      "step": 131550
    },
    {
      "epoch": 37.84872016105838,
      "grad_norm": 0.002524889074265957,
      "learning_rate": 0.0002430313488639632,
      "loss": 0.0008,
      "step": 131600
    },
    {
      "epoch": 37.86310037388554,
      "grad_norm": 1.8208040273748338e-05,
      "learning_rate": 0.00024274374460742017,
      "loss": 0.0002,
      "step": 131650
    },
    {
      "epoch": 37.877480586712686,
      "grad_norm": 0.00011285876098554581,
      "learning_rate": 0.0002424561403508772,
      "loss": 0.0012,
      "step": 131700
    },
    {
      "epoch": 37.891860799539835,
      "grad_norm": 0.000130039406940341,
      "learning_rate": 0.0002421685360943342,
      "loss": 0.0019,
      "step": 131750
    },
    {
      "epoch": 37.90624101236698,
      "grad_norm": 4.270849240128882e-05,
      "learning_rate": 0.00024188093183779118,
      "loss": 0.0005,
      "step": 131800
    },
    {
      "epoch": 37.92062122519413,
      "grad_norm": 0.004550733137875795,
      "learning_rate": 0.00024159332758124822,
      "loss": 0.0039,
      "step": 131850
    },
    {
      "epoch": 37.93500143802128,
      "grad_norm": 4.888759576715529e-05,
      "learning_rate": 0.0002413057233247052,
      "loss": 0.0002,
      "step": 131900
    },
    {
      "epoch": 37.94938165084843,
      "grad_norm": 4.3400887079769745e-05,
      "learning_rate": 0.00024101811906816222,
      "loss": 0.0026,
      "step": 131950
    },
    {
      "epoch": 37.96376186367558,
      "grad_norm": 0.003612831002101302,
      "learning_rate": 0.00024073051481161923,
      "loss": 0.0014,
      "step": 132000
    },
    {
      "epoch": 37.978142076502735,
      "grad_norm": 0.0030335441697388887,
      "learning_rate": 0.00024044291055507622,
      "loss": 0.0049,
      "step": 132050
    },
    {
      "epoch": 37.992522289329884,
      "grad_norm": 0.019994674250483513,
      "learning_rate": 0.00024015530629853323,
      "loss": 0.0035,
      "step": 132100
    },
    {
      "epoch": 38.0,
      "eval_loss": 0.009909008629620075,
      "eval_runtime": 18.1327,
      "eval_samples_per_second": 2631.648,
      "eval_steps_per_second": 41.141,
      "step": 132126
    },
    {
      "epoch": 38.00690250215703,
      "grad_norm": 0.0007030041888356209,
      "learning_rate": 0.00023986770204199024,
      "loss": 0.0009,
      "step": 132150
    },
    {
      "epoch": 38.02128271498418,
      "grad_norm": 5.3968498832546175e-05,
      "learning_rate": 0.00023958009778544723,
      "loss": 0.0003,
      "step": 132200
    },
    {
      "epoch": 38.03566292781133,
      "grad_norm": 0.00011378519411664456,
      "learning_rate": 0.00023929249352890424,
      "loss": 0.0022,
      "step": 132250
    },
    {
      "epoch": 38.05004314063848,
      "grad_norm": 3.438917701714672e-05,
      "learning_rate": 0.00023900488927236125,
      "loss": 0.0022,
      "step": 132300
    },
    {
      "epoch": 38.06442335346563,
      "grad_norm": 0.014900481328368187,
      "learning_rate": 0.00023871728501581824,
      "loss": 0.0013,
      "step": 132350
    },
    {
      "epoch": 38.078803566292784,
      "grad_norm": 0.01746317744255066,
      "learning_rate": 0.00023842968075927525,
      "loss": 0.0038,
      "step": 132400
    },
    {
      "epoch": 38.09318377911993,
      "grad_norm": 0.00010677750833565369,
      "learning_rate": 0.00023814207650273226,
      "loss": 0.0046,
      "step": 132450
    },
    {
      "epoch": 38.10756399194708,
      "grad_norm": 0.02370104379951954,
      "learning_rate": 0.00023785447224618925,
      "loss": 0.0021,
      "step": 132500
    },
    {
      "epoch": 38.12194420477423,
      "grad_norm": 0.00067092792596668,
      "learning_rate": 0.00023756686798964626,
      "loss": 0.0026,
      "step": 132550
    },
    {
      "epoch": 38.13632441760138,
      "grad_norm": 0.0006089909002184868,
      "learning_rate": 0.00023727926373310327,
      "loss": 0.0029,
      "step": 132600
    },
    {
      "epoch": 38.15070463042853,
      "grad_norm": 4.3467862269608304e-05,
      "learning_rate": 0.00023699165947656026,
      "loss": 0.0005,
      "step": 132650
    },
    {
      "epoch": 38.16508484325568,
      "grad_norm": 2.4961020244518295e-05,
      "learning_rate": 0.00023670405522001727,
      "loss": 0.0017,
      "step": 132700
    },
    {
      "epoch": 38.17946505608283,
      "grad_norm": 0.015405742451548576,
      "learning_rate": 0.00023641645096347428,
      "loss": 0.0034,
      "step": 132750
    },
    {
      "epoch": 38.19384526890998,
      "grad_norm": 0.00028148925048299134,
      "learning_rate": 0.00023612884670693127,
      "loss": 0.0024,
      "step": 132800
    },
    {
      "epoch": 38.20822548173713,
      "grad_norm": 0.002610490657389164,
      "learning_rate": 0.00023584124245038828,
      "loss": 0.0019,
      "step": 132850
    },
    {
      "epoch": 38.22260569456428,
      "grad_norm": 0.0006061269668862224,
      "learning_rate": 0.00023555363819384526,
      "loss": 0.0011,
      "step": 132900
    },
    {
      "epoch": 38.23698590739143,
      "grad_norm": 0.032254427671432495,
      "learning_rate": 0.00023526603393730228,
      "loss": 0.0014,
      "step": 132950
    },
    {
      "epoch": 38.25136612021858,
      "grad_norm": 7.445099618053064e-05,
      "learning_rate": 0.0002349784296807593,
      "loss": 0.002,
      "step": 133000
    },
    {
      "epoch": 38.265746333045726,
      "grad_norm": 4.0573839214630425e-05,
      "learning_rate": 0.00023469082542421627,
      "loss": 0.0015,
      "step": 133050
    },
    {
      "epoch": 38.28012654587288,
      "grad_norm": 5.286194573272951e-05,
      "learning_rate": 0.00023440322116767329,
      "loss": 0.0049,
      "step": 133100
    },
    {
      "epoch": 38.29450675870003,
      "grad_norm": 0.0015729715814813972,
      "learning_rate": 0.0002341156169111303,
      "loss": 0.0016,
      "step": 133150
    },
    {
      "epoch": 38.30888697152718,
      "grad_norm": 0.003139722626656294,
      "learning_rate": 0.00023382801265458728,
      "loss": 0.002,
      "step": 133200
    },
    {
      "epoch": 38.32326718435433,
      "grad_norm": 0.004278554115444422,
      "learning_rate": 0.0002335404083980443,
      "loss": 0.0014,
      "step": 133250
    },
    {
      "epoch": 38.33764739718148,
      "grad_norm": 0.021213799715042114,
      "learning_rate": 0.0002332528041415013,
      "loss": 0.0003,
      "step": 133300
    },
    {
      "epoch": 38.352027610008626,
      "grad_norm": 6.130531255621463e-05,
      "learning_rate": 0.0002329651998849583,
      "loss": 0.0038,
      "step": 133350
    },
    {
      "epoch": 38.366407822835775,
      "grad_norm": 3.102805931121111e-05,
      "learning_rate": 0.0002326775956284153,
      "loss": 0.0025,
      "step": 133400
    },
    {
      "epoch": 38.38078803566293,
      "grad_norm": 5.204035187489353e-05,
      "learning_rate": 0.00023238999137187232,
      "loss": 0.004,
      "step": 133450
    },
    {
      "epoch": 38.39516824849008,
      "grad_norm": 2.3856340703787282e-05,
      "learning_rate": 0.0002321023871153293,
      "loss": 0.0014,
      "step": 133500
    },
    {
      "epoch": 38.40954846131723,
      "grad_norm": 0.0003469152143225074,
      "learning_rate": 0.00023181478285878631,
      "loss": 0.0042,
      "step": 133550
    },
    {
      "epoch": 38.42392867414438,
      "grad_norm": 0.00016439998580608517,
      "learning_rate": 0.00023152717860224333,
      "loss": 0.0047,
      "step": 133600
    },
    {
      "epoch": 38.43830888697153,
      "grad_norm": 0.0008874674676917493,
      "learning_rate": 0.0002312395743457003,
      "loss": 0.0017,
      "step": 133650
    },
    {
      "epoch": 38.452689099798675,
      "grad_norm": 0.0003431575605645776,
      "learning_rate": 0.00023095197008915732,
      "loss": 0.0025,
      "step": 133700
    },
    {
      "epoch": 38.467069312625824,
      "grad_norm": 8.471911860397086e-05,
      "learning_rate": 0.00023066436583261434,
      "loss": 0.0031,
      "step": 133750
    },
    {
      "epoch": 38.48144952545297,
      "grad_norm": 0.0005511571071110666,
      "learning_rate": 0.00023037676157607132,
      "loss": 0.0027,
      "step": 133800
    },
    {
      "epoch": 38.49582973828013,
      "grad_norm": 0.001012897351756692,
      "learning_rate": 0.00023008915731952833,
      "loss": 0.0006,
      "step": 133850
    },
    {
      "epoch": 38.51020995110728,
      "grad_norm": 0.014263099059462547,
      "learning_rate": 0.00022980155306298535,
      "loss": 0.0006,
      "step": 133900
    },
    {
      "epoch": 38.52459016393443,
      "grad_norm": 9.714114275993779e-05,
      "learning_rate": 0.00022951394880644233,
      "loss": 0.0017,
      "step": 133950
    },
    {
      "epoch": 38.538970376761576,
      "grad_norm": 7.013347931206226e-05,
      "learning_rate": 0.00022922634454989934,
      "loss": 0.0013,
      "step": 134000
    },
    {
      "epoch": 38.553350589588725,
      "grad_norm": 0.0006150816916488111,
      "learning_rate": 0.00022893874029335636,
      "loss": 0.0024,
      "step": 134050
    },
    {
      "epoch": 38.56773080241587,
      "grad_norm": 0.006839487235993147,
      "learning_rate": 0.00022865113603681334,
      "loss": 0.0043,
      "step": 134100
    },
    {
      "epoch": 38.58211101524302,
      "grad_norm": 0.0006490452215075493,
      "learning_rate": 0.00022836353178027035,
      "loss": 0.0016,
      "step": 134150
    },
    {
      "epoch": 38.59649122807018,
      "grad_norm": 1.9865085050696507e-05,
      "learning_rate": 0.00022807592752372737,
      "loss": 0.0014,
      "step": 134200
    },
    {
      "epoch": 38.61087144089733,
      "grad_norm": 0.0036528336349874735,
      "learning_rate": 0.00022778832326718435,
      "loss": 0.0023,
      "step": 134250
    },
    {
      "epoch": 38.625251653724476,
      "grad_norm": 0.0007627465529367328,
      "learning_rate": 0.00022750071901064136,
      "loss": 0.0032,
      "step": 134300
    },
    {
      "epoch": 38.639631866551625,
      "grad_norm": 0.02047177031636238,
      "learning_rate": 0.00022721311475409838,
      "loss": 0.0035,
      "step": 134350
    },
    {
      "epoch": 38.654012079378774,
      "grad_norm": 0.003443443449214101,
      "learning_rate": 0.00022692551049755536,
      "loss": 0.0015,
      "step": 134400
    },
    {
      "epoch": 38.66839229220592,
      "grad_norm": 0.00017181802832055837,
      "learning_rate": 0.00022663790624101237,
      "loss": 0.0018,
      "step": 134450
    },
    {
      "epoch": 38.68277250503307,
      "grad_norm": 7.479578925995156e-05,
      "learning_rate": 0.00022635030198446939,
      "loss": 0.0032,
      "step": 134500
    },
    {
      "epoch": 38.69715271786023,
      "grad_norm": 0.00013006650260649621,
      "learning_rate": 0.00022606269772792637,
      "loss": 0.0008,
      "step": 134550
    },
    {
      "epoch": 38.711532930687376,
      "grad_norm": 0.00010163035040022805,
      "learning_rate": 0.00022577509347138338,
      "loss": 0.0008,
      "step": 134600
    },
    {
      "epoch": 38.725913143514525,
      "grad_norm": 4.3537747842492536e-05,
      "learning_rate": 0.0002254874892148404,
      "loss": 0.0029,
      "step": 134650
    },
    {
      "epoch": 38.740293356341674,
      "grad_norm": 5.055431756773032e-05,
      "learning_rate": 0.00022519988495829738,
      "loss": 0.0006,
      "step": 134700
    },
    {
      "epoch": 38.75467356916882,
      "grad_norm": 0.002425492275506258,
      "learning_rate": 0.0002249122807017544,
      "loss": 0.0052,
      "step": 134750
    },
    {
      "epoch": 38.76905378199597,
      "grad_norm": 2.1055655452073552e-05,
      "learning_rate": 0.0002246246764452114,
      "loss": 0.001,
      "step": 134800
    },
    {
      "epoch": 38.78343399482312,
      "grad_norm": 0.00027934741228818893,
      "learning_rate": 0.0002243370721886684,
      "loss": 0.0014,
      "step": 134850
    },
    {
      "epoch": 38.797814207650276,
      "grad_norm": 0.0022348950151354074,
      "learning_rate": 0.0002240494679321254,
      "loss": 0.0013,
      "step": 134900
    },
    {
      "epoch": 38.812194420477425,
      "grad_norm": 0.0008258603629656136,
      "learning_rate": 0.00022376186367558241,
      "loss": 0.0034,
      "step": 134950
    },
    {
      "epoch": 38.826574633304574,
      "grad_norm": 2.505990232748445e-05,
      "learning_rate": 0.0002234742594190394,
      "loss": 0.0048,
      "step": 135000
    },
    {
      "epoch": 38.84095484613172,
      "grad_norm": 2.379363104410004e-05,
      "learning_rate": 0.0002231866551624964,
      "loss": 0.0008,
      "step": 135050
    },
    {
      "epoch": 38.85533505895887,
      "grad_norm": 0.008354194462299347,
      "learning_rate": 0.00022289905090595342,
      "loss": 0.0057,
      "step": 135100
    },
    {
      "epoch": 38.86971527178602,
      "grad_norm": 0.0034304512664675713,
      "learning_rate": 0.0002226114466494104,
      "loss": 0.0024,
      "step": 135150
    },
    {
      "epoch": 38.88409548461317,
      "grad_norm": 0.0005506575107574463,
      "learning_rate": 0.00022232384239286742,
      "loss": 0.0035,
      "step": 135200
    },
    {
      "epoch": 38.898475697440325,
      "grad_norm": 0.0007649349281564355,
      "learning_rate": 0.00022203623813632443,
      "loss": 0.0014,
      "step": 135250
    },
    {
      "epoch": 38.912855910267474,
      "grad_norm": 0.00044208738836459816,
      "learning_rate": 0.00022174863387978142,
      "loss": 0.0027,
      "step": 135300
    },
    {
      "epoch": 38.92723612309462,
      "grad_norm": 9.46703294175677e-05,
      "learning_rate": 0.00022146102962323843,
      "loss": 0.0003,
      "step": 135350
    },
    {
      "epoch": 38.94161633592177,
      "grad_norm": 0.00037921732291579247,
      "learning_rate": 0.00022117342536669544,
      "loss": 0.0052,
      "step": 135400
    },
    {
      "epoch": 38.95599654874892,
      "grad_norm": 0.004655835218727589,
      "learning_rate": 0.00022088582111015243,
      "loss": 0.003,
      "step": 135450
    },
    {
      "epoch": 38.97037676157607,
      "grad_norm": 1.237941160070477e-05,
      "learning_rate": 0.00022059821685360944,
      "loss": 0.002,
      "step": 135500
    },
    {
      "epoch": 38.98475697440322,
      "grad_norm": 0.00020485308778006583,
      "learning_rate": 0.00022031061259706645,
      "loss": 0.0012,
      "step": 135550
    },
    {
      "epoch": 38.999137187230374,
      "grad_norm": 0.0004128518921788782,
      "learning_rate": 0.00022002300834052344,
      "loss": 0.0014,
      "step": 135600
    },
    {
      "epoch": 39.0,
      "eval_loss": 0.009936566464602947,
      "eval_runtime": 16.9148,
      "eval_samples_per_second": 2821.142,
      "eval_steps_per_second": 44.103,
      "step": 135603
    },
    {
      "epoch": 39.01351740005752,
      "grad_norm": 0.0036725786048918962,
      "learning_rate": 0.00021973540408398045,
      "loss": 0.0018,
      "step": 135650
    },
    {
      "epoch": 39.02789761288467,
      "grad_norm": 0.0005256964941509068,
      "learning_rate": 0.00021944779982743746,
      "loss": 0.0027,
      "step": 135700
    },
    {
      "epoch": 39.04227782571182,
      "grad_norm": 0.0064325761049985886,
      "learning_rate": 0.00021916019557089445,
      "loss": 0.0006,
      "step": 135750
    },
    {
      "epoch": 39.05665803853897,
      "grad_norm": 0.0001554340560687706,
      "learning_rate": 0.00021887259131435143,
      "loss": 0.0019,
      "step": 135800
    },
    {
      "epoch": 39.07103825136612,
      "grad_norm": 0.0001257020194316283,
      "learning_rate": 0.00021858498705780847,
      "loss": 0.001,
      "step": 135850
    },
    {
      "epoch": 39.08541846419327,
      "grad_norm": 0.00021338852820917964,
      "learning_rate": 0.00021829738280126546,
      "loss": 0.0016,
      "step": 135900
    },
    {
      "epoch": 39.09979867702042,
      "grad_norm": 0.048296332359313965,
      "learning_rate": 0.00021800977854472244,
      "loss": 0.001,
      "step": 135950
    },
    {
      "epoch": 39.11417888984757,
      "grad_norm": 1.7253625628654845e-05,
      "learning_rate": 0.00021772217428817948,
      "loss": 0.002,
      "step": 136000
    },
    {
      "epoch": 39.12855910267472,
      "grad_norm": 0.019332697615027428,
      "learning_rate": 0.00021743457003163647,
      "loss": 0.0003,
      "step": 136050
    },
    {
      "epoch": 39.14293931550187,
      "grad_norm": 4.247438846505247e-05,
      "learning_rate": 0.00021714696577509345,
      "loss": 0.002,
      "step": 136100
    },
    {
      "epoch": 39.15731952832902,
      "grad_norm": 0.0002725974190980196,
      "learning_rate": 0.0002168593615185505,
      "loss": 0.002,
      "step": 136150
    },
    {
      "epoch": 39.17169974115617,
      "grad_norm": 0.003165110480040312,
      "learning_rate": 0.00021657175726200748,
      "loss": 0.0021,
      "step": 136200
    },
    {
      "epoch": 39.18607995398332,
      "grad_norm": 6.140972982393578e-05,
      "learning_rate": 0.00021628415300546446,
      "loss": 0.0014,
      "step": 136250
    },
    {
      "epoch": 39.200460166810466,
      "grad_norm": 0.00025188172003254294,
      "learning_rate": 0.0002159965487489215,
      "loss": 0.0018,
      "step": 136300
    },
    {
      "epoch": 39.21484037963762,
      "grad_norm": 0.0002862958936020732,
      "learning_rate": 0.0002157089444923785,
      "loss": 0.0007,
      "step": 136350
    },
    {
      "epoch": 39.22922059246477,
      "grad_norm": 7.658843242097646e-05,
      "learning_rate": 0.00021542134023583547,
      "loss": 0.001,
      "step": 136400
    },
    {
      "epoch": 39.24360080529192,
      "grad_norm": 0.00020297666196711361,
      "learning_rate": 0.0002151337359792925,
      "loss": 0.0005,
      "step": 136450
    },
    {
      "epoch": 39.25798101811907,
      "grad_norm": 4.151980465394445e-05,
      "learning_rate": 0.0002148461317227495,
      "loss": 0.001,
      "step": 136500
    },
    {
      "epoch": 39.27236123094622,
      "grad_norm": 0.007477446924895048,
      "learning_rate": 0.00021455852746620648,
      "loss": 0.0032,
      "step": 136550
    },
    {
      "epoch": 39.286741443773366,
      "grad_norm": 0.0001561102835694328,
      "learning_rate": 0.00021427092320966352,
      "loss": 0.0033,
      "step": 136600
    },
    {
      "epoch": 39.301121656600515,
      "grad_norm": 0.0009020572761073709,
      "learning_rate": 0.0002139833189531205,
      "loss": 0.0036,
      "step": 136650
    },
    {
      "epoch": 39.31550186942767,
      "grad_norm": 0.00015182342031039298,
      "learning_rate": 0.0002136957146965775,
      "loss": 0.0013,
      "step": 136700
    },
    {
      "epoch": 39.32988208225482,
      "grad_norm": 0.005601319018751383,
      "learning_rate": 0.00021340811044003453,
      "loss": 0.0008,
      "step": 136750
    },
    {
      "epoch": 39.34426229508197,
      "grad_norm": 3.0064280508668162e-05,
      "learning_rate": 0.00021312050618349152,
      "loss": 0.0035,
      "step": 136800
    },
    {
      "epoch": 39.35864250790912,
      "grad_norm": 0.015390703454613686,
      "learning_rate": 0.0002128329019269485,
      "loss": 0.0043,
      "step": 136850
    },
    {
      "epoch": 39.373022720736266,
      "grad_norm": 0.00013012606359552592,
      "learning_rate": 0.00021254529767040554,
      "loss": 0.0034,
      "step": 136900
    },
    {
      "epoch": 39.387402933563415,
      "grad_norm": 0.00037656474160030484,
      "learning_rate": 0.00021225769341386253,
      "loss": 0.0023,
      "step": 136950
    },
    {
      "epoch": 39.401783146390564,
      "grad_norm": 0.018984148278832436,
      "learning_rate": 0.0002119700891573195,
      "loss": 0.0022,
      "step": 137000
    },
    {
      "epoch": 39.41616335921772,
      "grad_norm": 0.0006868650089018047,
      "learning_rate": 0.00021168248490077655,
      "loss": 0.002,
      "step": 137050
    },
    {
      "epoch": 39.43054357204487,
      "grad_norm": 0.00045024818973615766,
      "learning_rate": 0.00021139488064423354,
      "loss": 0.0041,
      "step": 137100
    },
    {
      "epoch": 39.44492378487202,
      "grad_norm": 0.004610081668943167,
      "learning_rate": 0.00021110727638769052,
      "loss": 0.0014,
      "step": 137150
    },
    {
      "epoch": 39.459303997699166,
      "grad_norm": 0.00012034688552375883,
      "learning_rate": 0.00021081967213114756,
      "loss": 0.0043,
      "step": 137200
    },
    {
      "epoch": 39.473684210526315,
      "grad_norm": 0.009286432527005672,
      "learning_rate": 0.00021053206787460455,
      "loss": 0.0007,
      "step": 137250
    },
    {
      "epoch": 39.488064423353464,
      "grad_norm": 0.0034079060424119234,
      "learning_rate": 0.00021024446361806153,
      "loss": 0.001,
      "step": 137300
    },
    {
      "epoch": 39.50244463618061,
      "grad_norm": 1.0015249245043378e-05,
      "learning_rate": 0.00020995685936151857,
      "loss": 0.0017,
      "step": 137350
    },
    {
      "epoch": 39.51682484900777,
      "grad_norm": 0.0006071061943657696,
      "learning_rate": 0.00020966925510497556,
      "loss": 0.0038,
      "step": 137400
    },
    {
      "epoch": 39.53120506183492,
      "grad_norm": 0.000592741125728935,
      "learning_rate": 0.00020938165084843254,
      "loss": 0.001,
      "step": 137450
    },
    {
      "epoch": 39.545585274662066,
      "grad_norm": 0.00040158184128813446,
      "learning_rate": 0.00020909404659188958,
      "loss": 0.004,
      "step": 137500
    },
    {
      "epoch": 39.559965487489215,
      "grad_norm": 0.0001717443810775876,
      "learning_rate": 0.00020880644233534657,
      "loss": 0.0029,
      "step": 137550
    },
    {
      "epoch": 39.574345700316364,
      "grad_norm": 0.0020713545382022858,
      "learning_rate": 0.00020851883807880355,
      "loss": 0.0008,
      "step": 137600
    },
    {
      "epoch": 39.58872591314351,
      "grad_norm": 0.0003298100200481713,
      "learning_rate": 0.0002082312338222606,
      "loss": 0.0012,
      "step": 137650
    },
    {
      "epoch": 39.60310612597066,
      "grad_norm": 0.021811911836266518,
      "learning_rate": 0.00020794362956571758,
      "loss": 0.0056,
      "step": 137700
    },
    {
      "epoch": 39.61748633879782,
      "grad_norm": 0.00017513132479507476,
      "learning_rate": 0.00020765602530917456,
      "loss": 0.0019,
      "step": 137750
    },
    {
      "epoch": 39.63186655162497,
      "grad_norm": 9.202112414641306e-05,
      "learning_rate": 0.0002073684210526316,
      "loss": 0.0004,
      "step": 137800
    },
    {
      "epoch": 39.646246764452115,
      "grad_norm": 1.8966593415825628e-05,
      "learning_rate": 0.00020708081679608859,
      "loss": 0.0041,
      "step": 137850
    },
    {
      "epoch": 39.660626977279264,
      "grad_norm": 0.0018568445229902864,
      "learning_rate": 0.00020679321253954557,
      "loss": 0.0037,
      "step": 137900
    },
    {
      "epoch": 39.67500719010641,
      "grad_norm": 0.007055237423628569,
      "learning_rate": 0.0002065056082830026,
      "loss": 0.0036,
      "step": 137950
    },
    {
      "epoch": 39.68938740293356,
      "grad_norm": 6.718623626511544e-05,
      "learning_rate": 0.0002062180040264596,
      "loss": 0.0038,
      "step": 138000
    },
    {
      "epoch": 39.70376761576071,
      "grad_norm": 4.42736636614427e-05,
      "learning_rate": 0.00020593039976991658,
      "loss": 0.0009,
      "step": 138050
    },
    {
      "epoch": 39.71814782858786,
      "grad_norm": 0.001126746879890561,
      "learning_rate": 0.00020564279551337362,
      "loss": 0.004,
      "step": 138100
    },
    {
      "epoch": 39.732528041415016,
      "grad_norm": 3.0299333957373165e-05,
      "learning_rate": 0.0002053551912568306,
      "loss": 0.0002,
      "step": 138150
    },
    {
      "epoch": 39.746908254242165,
      "grad_norm": 0.0003113356069661677,
      "learning_rate": 0.0002050675870002876,
      "loss": 0.0029,
      "step": 138200
    },
    {
      "epoch": 39.76128846706931,
      "grad_norm": 1.4646283489128109e-05,
      "learning_rate": 0.00020477998274374463,
      "loss": 0.004,
      "step": 138250
    },
    {
      "epoch": 39.77566867989646,
      "grad_norm": 0.00429009273648262,
      "learning_rate": 0.00020449237848720162,
      "loss": 0.0045,
      "step": 138300
    },
    {
      "epoch": 39.79004889272361,
      "grad_norm": 0.00046014098916202784,
      "learning_rate": 0.0002042047742306586,
      "loss": 0.0015,
      "step": 138350
    },
    {
      "epoch": 39.80442910555076,
      "grad_norm": 0.0001992797915590927,
      "learning_rate": 0.00020391716997411564,
      "loss": 0.0014,
      "step": 138400
    },
    {
      "epoch": 39.81880931837791,
      "grad_norm": 5.0314291001996025e-05,
      "learning_rate": 0.00020362956571757263,
      "loss": 0.0034,
      "step": 138450
    },
    {
      "epoch": 39.833189531205065,
      "grad_norm": 0.00024061136355157942,
      "learning_rate": 0.0002033419614610296,
      "loss": 0.0048,
      "step": 138500
    },
    {
      "epoch": 39.847569744032214,
      "grad_norm": 0.023947497829794884,
      "learning_rate": 0.00020305435720448662,
      "loss": 0.0033,
      "step": 138550
    },
    {
      "epoch": 39.86194995685936,
      "grad_norm": 0.0036451753694564104,
      "learning_rate": 0.00020276675294794363,
      "loss": 0.0021,
      "step": 138600
    },
    {
      "epoch": 39.87633016968651,
      "grad_norm": 2.6328096282668412e-05,
      "learning_rate": 0.00020247914869140062,
      "loss": 0.0011,
      "step": 138650
    },
    {
      "epoch": 39.89071038251366,
      "grad_norm": 0.000567046576179564,
      "learning_rate": 0.00020219154443485763,
      "loss": 0.002,
      "step": 138700
    },
    {
      "epoch": 39.90509059534081,
      "grad_norm": 9.879411663860083e-05,
      "learning_rate": 0.00020190394017831464,
      "loss": 0.001,
      "step": 138750
    },
    {
      "epoch": 39.91947080816796,
      "grad_norm": 0.00012150872498750687,
      "learning_rate": 0.00020161633592177163,
      "loss": 0.0053,
      "step": 138800
    },
    {
      "epoch": 39.933851020995114,
      "grad_norm": 0.011452345177531242,
      "learning_rate": 0.00020132873166522864,
      "loss": 0.0028,
      "step": 138850
    },
    {
      "epoch": 39.94823123382226,
      "grad_norm": 0.00032241642475128174,
      "learning_rate": 0.00020104112740868565,
      "loss": 0.0034,
      "step": 138900
    },
    {
      "epoch": 39.96261144664941,
      "grad_norm": 9.448347555007786e-05,
      "learning_rate": 0.00020075352315214264,
      "loss": 0.0012,
      "step": 138950
    },
    {
      "epoch": 39.97699165947656,
      "grad_norm": 0.00018898257985711098,
      "learning_rate": 0.00020046591889559965,
      "loss": 0.0026,
      "step": 139000
    },
    {
      "epoch": 39.99137187230371,
      "grad_norm": 0.0002941887069027871,
      "learning_rate": 0.00020017831463905666,
      "loss": 0.0015,
      "step": 139050
    },
    {
      "epoch": 40.0,
      "eval_loss": 0.009853038005530834,
      "eval_runtime": 17.509,
      "eval_samples_per_second": 2725.403,
      "eval_steps_per_second": 42.607,
      "step": 139080
    },
    {
      "epoch": 40.00575208513086,
      "grad_norm": 0.00020402419613674283,
      "learning_rate": 0.00019989071038251368,
      "loss": 0.0014,
      "step": 139100
    },
    {
      "epoch": 40.02013229795801,
      "grad_norm": 3.636696783360094e-05,
      "learning_rate": 0.00019960310612597066,
      "loss": 0.0013,
      "step": 139150
    },
    {
      "epoch": 40.03451251078516,
      "grad_norm": 8.52477751323022e-05,
      "learning_rate": 0.00019931550186942767,
      "loss": 0.0027,
      "step": 139200
    },
    {
      "epoch": 40.04889272361231,
      "grad_norm": 0.012877309694886208,
      "learning_rate": 0.00019902789761288469,
      "loss": 0.0009,
      "step": 139250
    },
    {
      "epoch": 40.06327293643946,
      "grad_norm": 0.0036161395255476236,
      "learning_rate": 0.00019874029335634167,
      "loss": 0.0001,
      "step": 139300
    },
    {
      "epoch": 40.07765314926661,
      "grad_norm": 2.8684389690170065e-05,
      "learning_rate": 0.00019845268909979868,
      "loss": 0.0031,
      "step": 139350
    },
    {
      "epoch": 40.09203336209376,
      "grad_norm": 5.5640241043874994e-05,
      "learning_rate": 0.0001981650848432557,
      "loss": 0.0036,
      "step": 139400
    },
    {
      "epoch": 40.10641357492091,
      "grad_norm": 0.0006356692174449563,
      "learning_rate": 0.00019787748058671268,
      "loss": 0.0042,
      "step": 139450
    },
    {
      "epoch": 40.120793787748056,
      "grad_norm": 0.00046344188740476966,
      "learning_rate": 0.0001975898763301697,
      "loss": 0.0003,
      "step": 139500
    },
    {
      "epoch": 40.13517400057521,
      "grad_norm": 0.0011457123328000307,
      "learning_rate": 0.0001973022720736267,
      "loss": 0.0015,
      "step": 139550
    },
    {
      "epoch": 40.14955421340236,
      "grad_norm": 5.702869748347439e-05,
      "learning_rate": 0.0001970146678170837,
      "loss": 0.0017,
      "step": 139600
    },
    {
      "epoch": 40.16393442622951,
      "grad_norm": 2.788494930427987e-05,
      "learning_rate": 0.0001967270635605407,
      "loss": 0.0027,
      "step": 139650
    },
    {
      "epoch": 40.17831463905666,
      "grad_norm": 0.0011201974702998996,
      "learning_rate": 0.00019643945930399772,
      "loss": 0.003,
      "step": 139700
    },
    {
      "epoch": 40.19269485188381,
      "grad_norm": 0.0004920107312500477,
      "learning_rate": 0.0001961518550474547,
      "loss": 0.0033,
      "step": 139750
    },
    {
      "epoch": 40.207075064710956,
      "grad_norm": 1.661018541199155e-05,
      "learning_rate": 0.0001958642507909117,
      "loss": 0.0034,
      "step": 139800
    },
    {
      "epoch": 40.221455277538105,
      "grad_norm": 0.0012319739907979965,
      "learning_rate": 0.00019557664653436873,
      "loss": 0.0039,
      "step": 139850
    },
    {
      "epoch": 40.235835490365254,
      "grad_norm": 0.00013745840988121927,
      "learning_rate": 0.0001952890422778257,
      "loss": 0.0015,
      "step": 139900
    },
    {
      "epoch": 40.25021570319241,
      "grad_norm": 0.0012588475365191698,
      "learning_rate": 0.00019500143802128272,
      "loss": 0.0026,
      "step": 139950
    },
    {
      "epoch": 40.26459591601956,
      "grad_norm": 0.000749625382013619,
      "learning_rate": 0.00019471383376473974,
      "loss": 0.0024,
      "step": 140000
    },
    {
      "epoch": 40.27897612884671,
      "grad_norm": 0.0001587835868122056,
      "learning_rate": 0.00019442622950819672,
      "loss": 0.0039,
      "step": 140050
    },
    {
      "epoch": 40.29335634167386,
      "grad_norm": 0.00013357948046177626,
      "learning_rate": 0.00019413862525165373,
      "loss": 0.0022,
      "step": 140100
    },
    {
      "epoch": 40.307736554501005,
      "grad_norm": 9.407513425685465e-05,
      "learning_rate": 0.00019385102099511074,
      "loss": 0.0012,
      "step": 140150
    },
    {
      "epoch": 40.322116767328154,
      "grad_norm": 0.003343238029628992,
      "learning_rate": 0.00019356341673856773,
      "loss": 0.0038,
      "step": 140200
    },
    {
      "epoch": 40.3364969801553,
      "grad_norm": 0.0002005965361604467,
      "learning_rate": 0.00019327581248202474,
      "loss": 0.0017,
      "step": 140250
    },
    {
      "epoch": 40.35087719298246,
      "grad_norm": 0.0005241524195298553,
      "learning_rate": 0.00019298820822548175,
      "loss": 0.002,
      "step": 140300
    },
    {
      "epoch": 40.36525740580961,
      "grad_norm": 0.012232924811542034,
      "learning_rate": 0.00019270060396893874,
      "loss": 0.0012,
      "step": 140350
    },
    {
      "epoch": 40.37963761863676,
      "grad_norm": 0.000984741491265595,
      "learning_rate": 0.00019241299971239575,
      "loss": 0.0026,
      "step": 140400
    },
    {
      "epoch": 40.394017831463906,
      "grad_norm": 0.0006647812551818788,
      "learning_rate": 0.00019212539545585276,
      "loss": 0.0025,
      "step": 140450
    },
    {
      "epoch": 40.408398044291054,
      "grad_norm": 2.0941475668223575e-05,
      "learning_rate": 0.00019183779119930975,
      "loss": 0.003,
      "step": 140500
    },
    {
      "epoch": 40.4227782571182,
      "grad_norm": 0.0001417338353348896,
      "learning_rate": 0.00019155018694276676,
      "loss": 0.0036,
      "step": 140550
    },
    {
      "epoch": 40.43715846994535,
      "grad_norm": 6.009054050082341e-05,
      "learning_rate": 0.00019126258268622377,
      "loss": 0.0036,
      "step": 140600
    },
    {
      "epoch": 40.45153868277251,
      "grad_norm": 0.002226256299763918,
      "learning_rate": 0.00019097497842968076,
      "loss": 0.0007,
      "step": 140650
    },
    {
      "epoch": 40.46591889559966,
      "grad_norm": 4.010799966636114e-05,
      "learning_rate": 0.00019068737417313777,
      "loss": 0.0015,
      "step": 140700
    },
    {
      "epoch": 40.480299108426806,
      "grad_norm": 0.00012335253995843232,
      "learning_rate": 0.00019039976991659478,
      "loss": 0.001,
      "step": 140750
    },
    {
      "epoch": 40.494679321253955,
      "grad_norm": 3.040726551262196e-05,
      "learning_rate": 0.00019011216566005177,
      "loss": 0.0019,
      "step": 140800
    },
    {
      "epoch": 40.5090595340811,
      "grad_norm": 2.691093504836317e-05,
      "learning_rate": 0.00018982456140350878,
      "loss": 0.003,
      "step": 140850
    },
    {
      "epoch": 40.52343974690825,
      "grad_norm": 4.515255932346918e-05,
      "learning_rate": 0.0001895369571469658,
      "loss": 0.0048,
      "step": 140900
    },
    {
      "epoch": 40.5378199597354,
      "grad_norm": 0.00233745900914073,
      "learning_rate": 0.00018924935289042278,
      "loss": 0.0018,
      "step": 140950
    },
    {
      "epoch": 40.55220017256256,
      "grad_norm": 0.008467515930533409,
      "learning_rate": 0.0001889617486338798,
      "loss": 0.0033,
      "step": 141000
    },
    {
      "epoch": 40.566580385389706,
      "grad_norm": 0.0063280886970460415,
      "learning_rate": 0.0001886741443773368,
      "loss": 0.003,
      "step": 141050
    },
    {
      "epoch": 40.580960598216855,
      "grad_norm": 4.249776611686684e-05,
      "learning_rate": 0.0001883865401207938,
      "loss": 0.0013,
      "step": 141100
    },
    {
      "epoch": 40.595340811044004,
      "grad_norm": 0.011569146998226643,
      "learning_rate": 0.0001880989358642508,
      "loss": 0.0009,
      "step": 141150
    },
    {
      "epoch": 40.60972102387115,
      "grad_norm": 4.485207318793982e-05,
      "learning_rate": 0.0001878113316077078,
      "loss": 0.003,
      "step": 141200
    },
    {
      "epoch": 40.6241012366983,
      "grad_norm": 1.4581456525775138e-05,
      "learning_rate": 0.0001875237273511648,
      "loss": 0.0016,
      "step": 141250
    },
    {
      "epoch": 40.63848144952545,
      "grad_norm": 0.00027193804271519184,
      "learning_rate": 0.00018723612309462178,
      "loss": 0.0011,
      "step": 141300
    },
    {
      "epoch": 40.652861662352606,
      "grad_norm": 0.00015336992510128766,
      "learning_rate": 0.00018694851883807882,
      "loss": 0.0002,
      "step": 141350
    },
    {
      "epoch": 40.667241875179755,
      "grad_norm": 0.00010634522186592221,
      "learning_rate": 0.0001866609145815358,
      "loss": 0.0019,
      "step": 141400
    },
    {
      "epoch": 40.681622088006904,
      "grad_norm": 0.00014111293421592563,
      "learning_rate": 0.0001863733103249928,
      "loss": 0.003,
      "step": 141450
    },
    {
      "epoch": 40.69600230083405,
      "grad_norm": 4.206107769277878e-05,
      "learning_rate": 0.00018608570606844983,
      "loss": 0.0015,
      "step": 141500
    },
    {
      "epoch": 40.7103825136612,
      "grad_norm": 4.5538316044257954e-05,
      "learning_rate": 0.00018579810181190682,
      "loss": 0.0039,
      "step": 141550
    },
    {
      "epoch": 40.72476272648835,
      "grad_norm": 3.318103335914202e-05,
      "learning_rate": 0.0001855104975553638,
      "loss": 0.002,
      "step": 141600
    },
    {
      "epoch": 40.7391429393155,
      "grad_norm": 7.73199790273793e-05,
      "learning_rate": 0.00018522289329882084,
      "loss": 0.0014,
      "step": 141650
    },
    {
      "epoch": 40.75352315214265,
      "grad_norm": 0.0030569846276193857,
      "learning_rate": 0.00018493528904227783,
      "loss": 0.0018,
      "step": 141700
    },
    {
      "epoch": 40.767903364969804,
      "grad_norm": 0.005473426077514887,
      "learning_rate": 0.0001846476847857348,
      "loss": 0.0021,
      "step": 141750
    },
    {
      "epoch": 40.78228357779695,
      "grad_norm": 7.182179251685739e-05,
      "learning_rate": 0.00018436008052919185,
      "loss": 0.0013,
      "step": 141800
    },
    {
      "epoch": 40.7966637906241,
      "grad_norm": 5.539561607292853e-05,
      "learning_rate": 0.00018407247627264884,
      "loss": 0.0016,
      "step": 141850
    },
    {
      "epoch": 40.81104400345125,
      "grad_norm": 7.011758862063289e-05,
      "learning_rate": 0.00018378487201610582,
      "loss": 0.0014,
      "step": 141900
    },
    {
      "epoch": 40.8254242162784,
      "grad_norm": 1.4263862794905435e-05,
      "learning_rate": 0.00018349726775956286,
      "loss": 0.0048,
      "step": 141950
    },
    {
      "epoch": 40.83980442910555,
      "grad_norm": 0.013181484304368496,
      "learning_rate": 0.00018320966350301985,
      "loss": 0.0025,
      "step": 142000
    },
    {
      "epoch": 40.8541846419327,
      "grad_norm": 0.00022356437693815678,
      "learning_rate": 0.00018292205924647683,
      "loss": 0.0024,
      "step": 142050
    },
    {
      "epoch": 40.86856485475985,
      "grad_norm": 0.003315096953883767,
      "learning_rate": 0.00018263445498993387,
      "loss": 0.003,
      "step": 142100
    },
    {
      "epoch": 40.882945067587,
      "grad_norm": 0.0088423490524292,
      "learning_rate": 0.00018234685073339086,
      "loss": 0.004,
      "step": 142150
    },
    {
      "epoch": 40.89732528041415,
      "grad_norm": 0.009459316730499268,
      "learning_rate": 0.00018205924647684784,
      "loss": 0.0044,
      "step": 142200
    },
    {
      "epoch": 40.9117054932413,
      "grad_norm": 7.512677257182077e-05,
      "learning_rate": 0.00018177164222030488,
      "loss": 0.0036,
      "step": 142250
    },
    {
      "epoch": 40.92608570606845,
      "grad_norm": 5.508561298483983e-05,
      "learning_rate": 0.00018148403796376187,
      "loss": 0.0017,
      "step": 142300
    },
    {
      "epoch": 40.9404659188956,
      "grad_norm": 5.8611316489987075e-05,
      "learning_rate": 0.00018119643370721885,
      "loss": 0.0012,
      "step": 142350
    },
    {
      "epoch": 40.954846131722746,
      "grad_norm": 0.001946968026459217,
      "learning_rate": 0.0001809088294506759,
      "loss": 0.0029,
      "step": 142400
    },
    {
      "epoch": 40.9692263445499,
      "grad_norm": 7.734305108897388e-05,
      "learning_rate": 0.00018062122519413288,
      "loss": 0.001,
      "step": 142450
    },
    {
      "epoch": 40.98360655737705,
      "grad_norm": 0.003351133316755295,
      "learning_rate": 0.00018033362093758986,
      "loss": 0.0035,
      "step": 142500
    },
    {
      "epoch": 40.9979867702042,
      "grad_norm": 0.0016404876951128244,
      "learning_rate": 0.0001800460166810469,
      "loss": 0.0002,
      "step": 142550
    },
    {
      "epoch": 41.0,
      "eval_loss": 0.009905646555125713,
      "eval_runtime": 16.9955,
      "eval_samples_per_second": 2807.742,
      "eval_steps_per_second": 43.894,
      "step": 142557
    },
    {
      "epoch": 41.01236698303135,
      "grad_norm": 7.361952884821221e-05,
      "learning_rate": 0.0001797584124245039,
      "loss": 0.005,
      "step": 142600
    },
    {
      "epoch": 41.0267471958585,
      "grad_norm": 3.084025956923142e-05,
      "learning_rate": 0.00017947080816796087,
      "loss": 0.0014,
      "step": 142650
    },
    {
      "epoch": 41.04112740868565,
      "grad_norm": 0.0001591756590642035,
      "learning_rate": 0.0001791832039114179,
      "loss": 0.0009,
      "step": 142700
    },
    {
      "epoch": 41.055507621512795,
      "grad_norm": 0.00010738434502854943,
      "learning_rate": 0.0001788955996548749,
      "loss": 0.0008,
      "step": 142750
    },
    {
      "epoch": 41.06988783433995,
      "grad_norm": 1.0509540516068228e-05,
      "learning_rate": 0.00017860799539833188,
      "loss": 0.0011,
      "step": 142800
    },
    {
      "epoch": 41.0842680471671,
      "grad_norm": 0.003508088644593954,
      "learning_rate": 0.00017832039114178892,
      "loss": 0.0032,
      "step": 142850
    },
    {
      "epoch": 41.09864825999425,
      "grad_norm": 0.00041159623651765287,
      "learning_rate": 0.0001780327868852459,
      "loss": 0.0029,
      "step": 142900
    },
    {
      "epoch": 41.1130284728214,
      "grad_norm": 0.0005144283641129732,
      "learning_rate": 0.0001777451826287029,
      "loss": 0.0003,
      "step": 142950
    },
    {
      "epoch": 41.12740868564855,
      "grad_norm": 0.0003660357615444809,
      "learning_rate": 0.00017745757837215993,
      "loss": 0.0038,
      "step": 143000
    },
    {
      "epoch": 41.141788898475696,
      "grad_norm": 4.57371752418112e-05,
      "learning_rate": 0.00017716997411561692,
      "loss": 0.0012,
      "step": 143050
    },
    {
      "epoch": 41.156169111302844,
      "grad_norm": 0.0013033425202593207,
      "learning_rate": 0.0001768823698590739,
      "loss": 0.0021,
      "step": 143100
    },
    {
      "epoch": 41.17054932413,
      "grad_norm": 0.0008252391126006842,
      "learning_rate": 0.00017659476560253094,
      "loss": 0.0043,
      "step": 143150
    },
    {
      "epoch": 41.18492953695715,
      "grad_norm": 2.6842113584280014e-05,
      "learning_rate": 0.00017630716134598793,
      "loss": 0.0006,
      "step": 143200
    },
    {
      "epoch": 41.1993097497843,
      "grad_norm": 0.000422532728407532,
      "learning_rate": 0.0001760195570894449,
      "loss": 0.0023,
      "step": 143250
    },
    {
      "epoch": 41.21368996261145,
      "grad_norm": 2.09087593248114e-05,
      "learning_rate": 0.00017573195283290195,
      "loss": 0.0022,
      "step": 143300
    },
    {
      "epoch": 41.228070175438596,
      "grad_norm": 0.0004957275232300162,
      "learning_rate": 0.00017544434857635894,
      "loss": 0.0004,
      "step": 143350
    },
    {
      "epoch": 41.242450388265745,
      "grad_norm": 0.0008804950630292296,
      "learning_rate": 0.00017515674431981592,
      "loss": 0.0019,
      "step": 143400
    },
    {
      "epoch": 41.256830601092894,
      "grad_norm": 0.00015065903426147997,
      "learning_rate": 0.00017486914006327296,
      "loss": 0.0038,
      "step": 143450
    },
    {
      "epoch": 41.27121081392005,
      "grad_norm": 0.0002202319010393694,
      "learning_rate": 0.00017458153580672995,
      "loss": 0.002,
      "step": 143500
    },
    {
      "epoch": 41.2855910267472,
      "grad_norm": 3.062733230763115e-05,
      "learning_rate": 0.00017429393155018693,
      "loss": 0.0046,
      "step": 143550
    },
    {
      "epoch": 41.29997123957435,
      "grad_norm": 2.2778720449423417e-05,
      "learning_rate": 0.00017400632729364397,
      "loss": 0.0029,
      "step": 143600
    },
    {
      "epoch": 41.314351452401496,
      "grad_norm": 0.00037839787546545267,
      "learning_rate": 0.00017371872303710095,
      "loss": 0.0037,
      "step": 143650
    },
    {
      "epoch": 41.328731665228645,
      "grad_norm": 0.00036146779893897474,
      "learning_rate": 0.00017343111878055794,
      "loss": 0.0009,
      "step": 143700
    },
    {
      "epoch": 41.343111878055794,
      "grad_norm": 0.00017577310791239142,
      "learning_rate": 0.00017314351452401498,
      "loss": 0.0028,
      "step": 143750
    },
    {
      "epoch": 41.35749209088294,
      "grad_norm": 0.009045066311955452,
      "learning_rate": 0.00017285591026747196,
      "loss": 0.0026,
      "step": 143800
    },
    {
      "epoch": 41.37187230371009,
      "grad_norm": 0.0293615460395813,
      "learning_rate": 0.00017256830601092895,
      "loss": 0.003,
      "step": 143850
    },
    {
      "epoch": 41.38625251653725,
      "grad_norm": 8.932313357945532e-05,
      "learning_rate": 0.000172280701754386,
      "loss": 0.0041,
      "step": 143900
    },
    {
      "epoch": 41.400632729364396,
      "grad_norm": 2.7185766157344915e-05,
      "learning_rate": 0.00017199309749784297,
      "loss": 0.0008,
      "step": 143950
    },
    {
      "epoch": 41.415012942191545,
      "grad_norm": 0.00017241899331565946,
      "learning_rate": 0.00017170549324129996,
      "loss": 0.0024,
      "step": 144000
    },
    {
      "epoch": 41.429393155018694,
      "grad_norm": 4.012266799691133e-05,
      "learning_rate": 0.00017141788898475697,
      "loss": 0.001,
      "step": 144050
    },
    {
      "epoch": 41.44377336784584,
      "grad_norm": 0.0007155906641855836,
      "learning_rate": 0.00017113028472821398,
      "loss": 0.0022,
      "step": 144100
    },
    {
      "epoch": 41.45815358067299,
      "grad_norm": 3.7491106922971085e-05,
      "learning_rate": 0.00017084268047167097,
      "loss": 0.0003,
      "step": 144150
    },
    {
      "epoch": 41.47253379350014,
      "grad_norm": 0.00020523156854324043,
      "learning_rate": 0.00017055507621512798,
      "loss": 0.0007,
      "step": 144200
    },
    {
      "epoch": 41.4869140063273,
      "grad_norm": 0.015390729531645775,
      "learning_rate": 0.000170267471958585,
      "loss": 0.0012,
      "step": 144250
    },
    {
      "epoch": 41.501294219154445,
      "grad_norm": 1.6668751413817517e-05,
      "learning_rate": 0.00016997986770204198,
      "loss": 0.0018,
      "step": 144300
    },
    {
      "epoch": 41.515674431981594,
      "grad_norm": 6.01664069108665e-05,
      "learning_rate": 0.000169692263445499,
      "loss": 0.0055,
      "step": 144350
    },
    {
      "epoch": 41.53005464480874,
      "grad_norm": 0.0005707521922886372,
      "learning_rate": 0.000169404659188956,
      "loss": 0.0032,
      "step": 144400
    },
    {
      "epoch": 41.54443485763589,
      "grad_norm": 7.436632586177438e-05,
      "learning_rate": 0.000169117054932413,
      "loss": 0.0013,
      "step": 144450
    },
    {
      "epoch": 41.55881507046304,
      "grad_norm": 0.00244943262077868,
      "learning_rate": 0.00016882945067587,
      "loss": 0.002,
      "step": 144500
    },
    {
      "epoch": 41.57319528329019,
      "grad_norm": 0.011607237160205841,
      "learning_rate": 0.00016854184641932701,
      "loss": 0.0041,
      "step": 144550
    },
    {
      "epoch": 41.587575496117346,
      "grad_norm": 0.0021183216013014317,
      "learning_rate": 0.000168254242162784,
      "loss": 0.0018,
      "step": 144600
    },
    {
      "epoch": 41.601955708944494,
      "grad_norm": 0.0001587508013471961,
      "learning_rate": 0.000167966637906241,
      "loss": 0.003,
      "step": 144650
    },
    {
      "epoch": 41.61633592177164,
      "grad_norm": 0.00013787936768494546,
      "learning_rate": 0.00016767903364969802,
      "loss": 0.0026,
      "step": 144700
    },
    {
      "epoch": 41.63071613459879,
      "grad_norm": 0.00696840975433588,
      "learning_rate": 0.000167391429393155,
      "loss": 0.0016,
      "step": 144750
    },
    {
      "epoch": 41.64509634742594,
      "grad_norm": 0.0003676579799503088,
      "learning_rate": 0.00016710382513661202,
      "loss": 0.0005,
      "step": 144800
    },
    {
      "epoch": 41.65947656025309,
      "grad_norm": 2.89824038191e-05,
      "learning_rate": 0.00016681622088006903,
      "loss": 0.0016,
      "step": 144850
    },
    {
      "epoch": 41.67385677308024,
      "grad_norm": 0.0010518409544602036,
      "learning_rate": 0.00016652861662352602,
      "loss": 0.0019,
      "step": 144900
    },
    {
      "epoch": 41.688236985907395,
      "grad_norm": 4.5912271161796525e-05,
      "learning_rate": 0.00016624101236698303,
      "loss": 0.0023,
      "step": 144950
    },
    {
      "epoch": 41.70261719873454,
      "grad_norm": 3.275239942013286e-05,
      "learning_rate": 0.00016595340811044004,
      "loss": 0.0041,
      "step": 145000
    },
    {
      "epoch": 41.71699741156169,
      "grad_norm": 3.964632560382597e-05,
      "learning_rate": 0.00016566580385389703,
      "loss": 0.0025,
      "step": 145050
    },
    {
      "epoch": 41.73137762438884,
      "grad_norm": 0.004806084558367729,
      "learning_rate": 0.00016537819959735404,
      "loss": 0.0047,
      "step": 145100
    },
    {
      "epoch": 41.74575783721599,
      "grad_norm": 1.8652539438335225e-05,
      "learning_rate": 0.00016509059534081105,
      "loss": 0.0038,
      "step": 145150
    },
    {
      "epoch": 41.76013805004314,
      "grad_norm": 0.0016454675933346152,
      "learning_rate": 0.00016480299108426804,
      "loss": 0.0023,
      "step": 145200
    },
    {
      "epoch": 41.77451826287029,
      "grad_norm": 0.0001893913868116215,
      "learning_rate": 0.00016451538682772505,
      "loss": 0.0002,
      "step": 145250
    },
    {
      "epoch": 41.788898475697444,
      "grad_norm": 2.6966443329001777e-05,
      "learning_rate": 0.00016422778257118206,
      "loss": 0.0011,
      "step": 145300
    },
    {
      "epoch": 41.80327868852459,
      "grad_norm": 0.00023667392088100314,
      "learning_rate": 0.00016394017831463905,
      "loss": 0.0011,
      "step": 145350
    },
    {
      "epoch": 41.81765890135174,
      "grad_norm": 9.255686745746061e-05,
      "learning_rate": 0.00016365257405809606,
      "loss": 0.0003,
      "step": 145400
    },
    {
      "epoch": 41.83203911417889,
      "grad_norm": 0.00014382551307789981,
      "learning_rate": 0.00016336496980155307,
      "loss": 0.0009,
      "step": 145450
    },
    {
      "epoch": 41.84641932700604,
      "grad_norm": 3.6547753552440554e-05,
      "learning_rate": 0.00016307736554501006,
      "loss": 0.0035,
      "step": 145500
    },
    {
      "epoch": 41.86079953983319,
      "grad_norm": 0.001134465797804296,
      "learning_rate": 0.00016278976128846707,
      "loss": 0.0037,
      "step": 145550
    },
    {
      "epoch": 41.87517975266034,
      "grad_norm": 0.0010018836474046111,
      "learning_rate": 0.00016250215703192408,
      "loss": 0.0021,
      "step": 145600
    },
    {
      "epoch": 41.889559965487486,
      "grad_norm": 0.0008744965307414532,
      "learning_rate": 0.00016221455277538107,
      "loss": 0.0028,
      "step": 145650
    },
    {
      "epoch": 41.90394017831464,
      "grad_norm": 4.568840085994452e-05,
      "learning_rate": 0.00016192694851883808,
      "loss": 0.0056,
      "step": 145700
    },
    {
      "epoch": 41.91832039114179,
      "grad_norm": 0.026171408593654633,
      "learning_rate": 0.0001616393442622951,
      "loss": 0.0016,
      "step": 145750
    },
    {
      "epoch": 41.93270060396894,
      "grad_norm": 0.001162127940915525,
      "learning_rate": 0.00016135174000575208,
      "loss": 0.0022,
      "step": 145800
    },
    {
      "epoch": 41.94708081679609,
      "grad_norm": 0.0004459392512217164,
      "learning_rate": 0.0001610641357492091,
      "loss": 0.0021,
      "step": 145850
    },
    {
      "epoch": 41.96146102962324,
      "grad_norm": 0.0002030534960795194,
      "learning_rate": 0.0001607765314926661,
      "loss": 0.0008,
      "step": 145900
    },
    {
      "epoch": 41.975841242450386,
      "grad_norm": 4.1982766560977325e-05,
      "learning_rate": 0.0001604889272361231,
      "loss": 0.0017,
      "step": 145950
    },
    {
      "epoch": 41.990221455277535,
      "grad_norm": 5.799391510663554e-05,
      "learning_rate": 0.0001602013229795801,
      "loss": 0.0042,
      "step": 146000
    },
    {
      "epoch": 42.0,
      "eval_loss": 0.009906381368637085,
      "eval_runtime": 16.7979,
      "eval_samples_per_second": 2840.773,
      "eval_steps_per_second": 44.41,
      "step": 146034
    },
    {
      "epoch": 42.00460166810469,
      "grad_norm": 0.0003150059492327273,
      "learning_rate": 0.0001599137187230371,
      "loss": 0.0011,
      "step": 146050
    },
    {
      "epoch": 42.01898188093184,
      "grad_norm": 2.055875665973872e-05,
      "learning_rate": 0.0001596261144664941,
      "loss": 0.0025,
      "step": 146100
    },
    {
      "epoch": 42.03336209375899,
      "grad_norm": 0.000147005426697433,
      "learning_rate": 0.0001593385102099511,
      "loss": 0.0026,
      "step": 146150
    },
    {
      "epoch": 42.04774230658614,
      "grad_norm": 6.933409895282239e-05,
      "learning_rate": 0.00015905090595340812,
      "loss": 0.0009,
      "step": 146200
    },
    {
      "epoch": 42.062122519413286,
      "grad_norm": 3.059316804865375e-05,
      "learning_rate": 0.00015876330169686513,
      "loss": 0.0008,
      "step": 146250
    },
    {
      "epoch": 42.076502732240435,
      "grad_norm": 0.00024729158030822873,
      "learning_rate": 0.00015847569744032212,
      "loss": 0.0018,
      "step": 146300
    },
    {
      "epoch": 42.090882945067584,
      "grad_norm": 0.0005813274765387177,
      "learning_rate": 0.00015818809318377913,
      "loss": 0.0011,
      "step": 146350
    },
    {
      "epoch": 42.10526315789474,
      "grad_norm": 1.3136741472408175e-05,
      "learning_rate": 0.00015790048892723614,
      "loss": 0.0012,
      "step": 146400
    },
    {
      "epoch": 42.11964337072189,
      "grad_norm": 0.010895976796746254,
      "learning_rate": 0.00015761288467069313,
      "loss": 0.0021,
      "step": 146450
    },
    {
      "epoch": 42.13402358354904,
      "grad_norm": 7.61929404688999e-05,
      "learning_rate": 0.00015732528041415014,
      "loss": 0.0037,
      "step": 146500
    },
    {
      "epoch": 42.148403796376186,
      "grad_norm": 0.00011516205995576456,
      "learning_rate": 0.00015703767615760715,
      "loss": 0.0013,
      "step": 146550
    },
    {
      "epoch": 42.162784009203335,
      "grad_norm": 2.5582847229088657e-05,
      "learning_rate": 0.00015675007190106414,
      "loss": 0.003,
      "step": 146600
    },
    {
      "epoch": 42.177164222030484,
      "grad_norm": 0.00027669197879731655,
      "learning_rate": 0.00015646246764452115,
      "loss": 0.003,
      "step": 146650
    },
    {
      "epoch": 42.19154443485763,
      "grad_norm": 0.0023863022215664387,
      "learning_rate": 0.00015617486338797816,
      "loss": 0.0017,
      "step": 146700
    },
    {
      "epoch": 42.20592464768479,
      "grad_norm": 3.7913974665571004e-05,
      "learning_rate": 0.00015588725913143515,
      "loss": 0.003,
      "step": 146750
    },
    {
      "epoch": 42.22030486051194,
      "grad_norm": 0.00024006946478039026,
      "learning_rate": 0.00015559965487489213,
      "loss": 0.0036,
      "step": 146800
    },
    {
      "epoch": 42.23468507333909,
      "grad_norm": 0.002277783118188381,
      "learning_rate": 0.00015531205061834917,
      "loss": 0.0009,
      "step": 146850
    },
    {
      "epoch": 42.249065286166235,
      "grad_norm": 8.098900434561074e-05,
      "learning_rate": 0.00015502444636180616,
      "loss": 0.0028,
      "step": 146900
    },
    {
      "epoch": 42.263445498993384,
      "grad_norm": 0.0002734950103331357,
      "learning_rate": 0.00015473684210526314,
      "loss": 0.0039,
      "step": 146950
    },
    {
      "epoch": 42.27782571182053,
      "grad_norm": 0.00013129026046954095,
      "learning_rate": 0.00015444923784872018,
      "loss": 0.002,
      "step": 147000
    },
    {
      "epoch": 42.29220592464768,
      "grad_norm": 2.711753586481791e-05,
      "learning_rate": 0.00015416163359217717,
      "loss": 0.0038,
      "step": 147050
    },
    {
      "epoch": 42.30658613747484,
      "grad_norm": 0.0002046821464318782,
      "learning_rate": 0.00015387402933563415,
      "loss": 0.0013,
      "step": 147100
    },
    {
      "epoch": 42.32096635030199,
      "grad_norm": 4.053214433952235e-05,
      "learning_rate": 0.0001535864250790912,
      "loss": 0.0026,
      "step": 147150
    },
    {
      "epoch": 42.335346563129136,
      "grad_norm": 0.0006953587871976197,
      "learning_rate": 0.00015329882082254818,
      "loss": 0.0018,
      "step": 147200
    },
    {
      "epoch": 42.349726775956285,
      "grad_norm": 3.271846435382031e-05,
      "learning_rate": 0.00015301121656600516,
      "loss": 0.0007,
      "step": 147250
    },
    {
      "epoch": 42.36410698878343,
      "grad_norm": 0.0001173524433397688,
      "learning_rate": 0.0001527236123094622,
      "loss": 0.0009,
      "step": 147300
    },
    {
      "epoch": 42.37848720161058,
      "grad_norm": 0.0005738004110753536,
      "learning_rate": 0.0001524360080529192,
      "loss": 0.0032,
      "step": 147350
    },
    {
      "epoch": 42.39286741443773,
      "grad_norm": 0.003535972209647298,
      "learning_rate": 0.00015214840379637617,
      "loss": 0.004,
      "step": 147400
    },
    {
      "epoch": 42.40724762726488,
      "grad_norm": 0.018392259255051613,
      "learning_rate": 0.0001518607995398332,
      "loss": 0.001,
      "step": 147450
    },
    {
      "epoch": 42.421627840092036,
      "grad_norm": 8.72693199198693e-05,
      "learning_rate": 0.0001515731952832902,
      "loss": 0.0015,
      "step": 147500
    },
    {
      "epoch": 42.436008052919185,
      "grad_norm": 0.004214899614453316,
      "learning_rate": 0.00015128559102674718,
      "loss": 0.0031,
      "step": 147550
    },
    {
      "epoch": 42.450388265746334,
      "grad_norm": 5.577161209657788e-05,
      "learning_rate": 0.00015099798677020422,
      "loss": 0.0017,
      "step": 147600
    },
    {
      "epoch": 42.46476847857348,
      "grad_norm": 0.0037679828237742186,
      "learning_rate": 0.0001507103825136612,
      "loss": 0.0025,
      "step": 147650
    },
    {
      "epoch": 42.47914869140063,
      "grad_norm": 5.969536505290307e-05,
      "learning_rate": 0.0001504227782571182,
      "loss": 0.0007,
      "step": 147700
    },
    {
      "epoch": 42.49352890422778,
      "grad_norm": 0.00015278978389687836,
      "learning_rate": 0.00015013517400057523,
      "loss": 0.0007,
      "step": 147750
    },
    {
      "epoch": 42.50790911705493,
      "grad_norm": 0.001919106813147664,
      "learning_rate": 0.00014984756974403222,
      "loss": 0.0032,
      "step": 147800
    },
    {
      "epoch": 42.522289329882085,
      "grad_norm": 2.9473960239556618e-05,
      "learning_rate": 0.0001495599654874892,
      "loss": 0.0043,
      "step": 147850
    },
    {
      "epoch": 42.536669542709234,
      "grad_norm": 0.0010537041816860437,
      "learning_rate": 0.00014927236123094624,
      "loss": 0.0024,
      "step": 147900
    },
    {
      "epoch": 42.55104975553638,
      "grad_norm": 0.0058550648391246796,
      "learning_rate": 0.00014898475697440323,
      "loss": 0.0004,
      "step": 147950
    },
    {
      "epoch": 42.56542996836353,
      "grad_norm": 6.275314080994576e-05,
      "learning_rate": 0.0001486971527178602,
      "loss": 0.0015,
      "step": 148000
    },
    {
      "epoch": 42.57981018119068,
      "grad_norm": 0.0030212465208023787,
      "learning_rate": 0.00014840954846131725,
      "loss": 0.0029,
      "step": 148050
    },
    {
      "epoch": 42.59419039401783,
      "grad_norm": 0.0001515063049737364,
      "learning_rate": 0.00014812194420477424,
      "loss": 0.0035,
      "step": 148100
    },
    {
      "epoch": 42.60857060684498,
      "grad_norm": 0.0003146773960907012,
      "learning_rate": 0.00014783433994823122,
      "loss": 0.0051,
      "step": 148150
    },
    {
      "epoch": 42.622950819672134,
      "grad_norm": 0.0002126709878211841,
      "learning_rate": 0.00014754673569168826,
      "loss": 0.0026,
      "step": 148200
    },
    {
      "epoch": 42.63733103249928,
      "grad_norm": 3.080608803429641e-05,
      "learning_rate": 0.00014725913143514525,
      "loss": 0.0008,
      "step": 148250
    },
    {
      "epoch": 42.65171124532643,
      "grad_norm": 0.014706876128911972,
      "learning_rate": 0.00014697152717860223,
      "loss": 0.0036,
      "step": 148300
    },
    {
      "epoch": 42.66609145815358,
      "grad_norm": 4.277184052625671e-05,
      "learning_rate": 0.00014668392292205927,
      "loss": 0.0022,
      "step": 148350
    },
    {
      "epoch": 42.68047167098073,
      "grad_norm": 0.0008140155114233494,
      "learning_rate": 0.00014639631866551626,
      "loss": 0.0012,
      "step": 148400
    },
    {
      "epoch": 42.69485188380788,
      "grad_norm": 3.126384035567753e-05,
      "learning_rate": 0.00014610871440897324,
      "loss": 0.001,
      "step": 148450
    },
    {
      "epoch": 42.70923209663503,
      "grad_norm": 0.005963400471955538,
      "learning_rate": 0.00014582111015243028,
      "loss": 0.0008,
      "step": 148500
    },
    {
      "epoch": 42.72361230946218,
      "grad_norm": 0.00010166849824599922,
      "learning_rate": 0.00014553350589588727,
      "loss": 0.0029,
      "step": 148550
    },
    {
      "epoch": 42.73799252228933,
      "grad_norm": 0.0002254178252769634,
      "learning_rate": 0.00014524590163934425,
      "loss": 0.003,
      "step": 148600
    },
    {
      "epoch": 42.75237273511648,
      "grad_norm": 0.00039771696901880205,
      "learning_rate": 0.0001449582973828013,
      "loss": 0.0006,
      "step": 148650
    },
    {
      "epoch": 42.76675294794363,
      "grad_norm": 0.002442826284095645,
      "learning_rate": 0.00014467069312625828,
      "loss": 0.0012,
      "step": 148700
    },
    {
      "epoch": 42.78113316077078,
      "grad_norm": 0.00027046955074183643,
      "learning_rate": 0.00014438308886971526,
      "loss": 0.0039,
      "step": 148750
    },
    {
      "epoch": 42.79551337359793,
      "grad_norm": 0.005948778707534075,
      "learning_rate": 0.0001440954846131723,
      "loss": 0.0032,
      "step": 148800
    },
    {
      "epoch": 42.809893586425076,
      "grad_norm": 0.00011243724293308333,
      "learning_rate": 0.00014380788035662928,
      "loss": 0.0023,
      "step": 148850
    },
    {
      "epoch": 42.82427379925223,
      "grad_norm": 0.000308065878925845,
      "learning_rate": 0.00014352027610008627,
      "loss": 0.004,
      "step": 148900
    },
    {
      "epoch": 42.83865401207938,
      "grad_norm": 4.256875399732962e-05,
      "learning_rate": 0.0001432326718435433,
      "loss": 0.001,
      "step": 148950
    },
    {
      "epoch": 42.85303422490653,
      "grad_norm": 0.01353488489985466,
      "learning_rate": 0.0001429450675870003,
      "loss": 0.0016,
      "step": 149000
    },
    {
      "epoch": 42.86741443773368,
      "grad_norm": 0.009096050634980202,
      "learning_rate": 0.00014265746333045728,
      "loss": 0.0056,
      "step": 149050
    },
    {
      "epoch": 42.88179465056083,
      "grad_norm": 0.004228767938911915,
      "learning_rate": 0.00014236985907391432,
      "loss": 0.0011,
      "step": 149100
    },
    {
      "epoch": 42.89617486338798,
      "grad_norm": 9.281042002839968e-05,
      "learning_rate": 0.0001420822548173713,
      "loss": 0.0028,
      "step": 149150
    },
    {
      "epoch": 42.910555076215125,
      "grad_norm": 0.006586581468582153,
      "learning_rate": 0.0001417946505608283,
      "loss": 0.0036,
      "step": 149200
    },
    {
      "epoch": 42.92493528904228,
      "grad_norm": 1.9995846741949208e-05,
      "learning_rate": 0.00014150704630428533,
      "loss": 0.0013,
      "step": 149250
    },
    {
      "epoch": 42.93931550186943,
      "grad_norm": 0.00015283581160474569,
      "learning_rate": 0.00014121944204774231,
      "loss": 0.0013,
      "step": 149300
    },
    {
      "epoch": 42.95369571469658,
      "grad_norm": 0.0001972535828826949,
      "learning_rate": 0.0001409318377911993,
      "loss": 0.0007,
      "step": 149350
    },
    {
      "epoch": 42.96807592752373,
      "grad_norm": 3.8400772609747946e-05,
      "learning_rate": 0.00014064423353465634,
      "loss": 0.0035,
      "step": 149400
    },
    {
      "epoch": 42.98245614035088,
      "grad_norm": 0.002333673182874918,
      "learning_rate": 0.00014035662927811332,
      "loss": 0.0043,
      "step": 149450
    },
    {
      "epoch": 42.996836353178026,
      "grad_norm": 3.4993383451364934e-05,
      "learning_rate": 0.0001400690250215703,
      "loss": 0.002,
      "step": 149500
    },
    {
      "epoch": 43.0,
      "eval_loss": 0.009998003952205181,
      "eval_runtime": 17.5856,
      "eval_samples_per_second": 2713.522,
      "eval_steps_per_second": 42.421,
      "step": 149511
    },
    {
      "epoch": 43.011216566005174,
      "grad_norm": 3.3072836231440306e-05,
      "learning_rate": 0.00013978142076502732,
      "loss": 0.0061,
      "step": 149550
    },
    {
      "epoch": 43.02559677883232,
      "grad_norm": 8.305369556182995e-05,
      "learning_rate": 0.00013949381650848433,
      "loss": 0.0018,
      "step": 149600
    },
    {
      "epoch": 43.03997699165948,
      "grad_norm": 0.0002582125598564744,
      "learning_rate": 0.00013920621225194132,
      "loss": 0.002,
      "step": 149650
    },
    {
      "epoch": 43.05435720448663,
      "grad_norm": 0.006003020331263542,
      "learning_rate": 0.00013891860799539833,
      "loss": 0.0039,
      "step": 149700
    },
    {
      "epoch": 43.06873741731378,
      "grad_norm": 0.006485089659690857,
      "learning_rate": 0.00013863100373885534,
      "loss": 0.0022,
      "step": 149750
    },
    {
      "epoch": 43.083117630140926,
      "grad_norm": 0.011562717147171497,
      "learning_rate": 0.00013834339948231233,
      "loss": 0.0057,
      "step": 149800
    },
    {
      "epoch": 43.097497842968075,
      "grad_norm": 0.0006582125788554549,
      "learning_rate": 0.00013805579522576934,
      "loss": 0.0039,
      "step": 149850
    },
    {
      "epoch": 43.11187805579522,
      "grad_norm": 0.0016953811282292008,
      "learning_rate": 0.00013776819096922635,
      "loss": 0.0028,
      "step": 149900
    },
    {
      "epoch": 43.12625826862237,
      "grad_norm": 3.969699901062995e-05,
      "learning_rate": 0.00013748058671268334,
      "loss": 0.0023,
      "step": 149950
    },
    {
      "epoch": 43.14063848144953,
      "grad_norm": 5.336298272595741e-05,
      "learning_rate": 0.00013719298245614035,
      "loss": 0.0019,
      "step": 150000
    },
    {
      "epoch": 43.15501869427668,
      "grad_norm": 4.6544038923457265e-05,
      "learning_rate": 0.00013690537819959736,
      "loss": 0.0021,
      "step": 150050
    },
    {
      "epoch": 43.169398907103826,
      "grad_norm": 5.585559119936079e-05,
      "learning_rate": 0.00013661777394305435,
      "loss": 0.0045,
      "step": 150100
    },
    {
      "epoch": 43.183779119930975,
      "grad_norm": 2.0269075321266428e-05,
      "learning_rate": 0.00013633016968651136,
      "loss": 0.0046,
      "step": 150150
    },
    {
      "epoch": 43.198159332758124,
      "grad_norm": 2.6884419639827684e-05,
      "learning_rate": 0.00013604256542996837,
      "loss": 0.0008,
      "step": 150200
    },
    {
      "epoch": 43.21253954558527,
      "grad_norm": 3.3309359423583373e-05,
      "learning_rate": 0.00013575496117342536,
      "loss": 0.0006,
      "step": 150250
    },
    {
      "epoch": 43.22691975841242,
      "grad_norm": 0.0008093108772300184,
      "learning_rate": 0.00013546735691688237,
      "loss": 0.003,
      "step": 150300
    },
    {
      "epoch": 43.24129997123958,
      "grad_norm": 3.980923429480754e-05,
      "learning_rate": 0.00013517975266033938,
      "loss": 0.0043,
      "step": 150350
    },
    {
      "epoch": 43.255680184066726,
      "grad_norm": 6.498901348095387e-05,
      "learning_rate": 0.00013489214840379637,
      "loss": 0.0019,
      "step": 150400
    },
    {
      "epoch": 43.270060396893875,
      "grad_norm": 3.747830123757012e-05,
      "learning_rate": 0.00013460454414725338,
      "loss": 0.0003,
      "step": 150450
    },
    {
      "epoch": 43.284440609721024,
      "grad_norm": 4.57393798569683e-05,
      "learning_rate": 0.0001343169398907104,
      "loss": 0.0019,
      "step": 150500
    },
    {
      "epoch": 43.29882082254817,
      "grad_norm": 0.00873920414596796,
      "learning_rate": 0.00013402933563416738,
      "loss": 0.0019,
      "step": 150550
    },
    {
      "epoch": 43.31320103537532,
      "grad_norm": 0.0005144534516148269,
      "learning_rate": 0.0001337417313776244,
      "loss": 0.0035,
      "step": 150600
    },
    {
      "epoch": 43.32758124820247,
      "grad_norm": 0.00015252611774485558,
      "learning_rate": 0.0001334541271210814,
      "loss": 0.0025,
      "step": 150650
    },
    {
      "epoch": 43.341961461029626,
      "grad_norm": 2.6926398277282715e-05,
      "learning_rate": 0.0001331665228645384,
      "loss": 0.0022,
      "step": 150700
    },
    {
      "epoch": 43.356341673856775,
      "grad_norm": 0.004762845579534769,
      "learning_rate": 0.0001328789186079954,
      "loss": 0.0013,
      "step": 150750
    },
    {
      "epoch": 43.370721886683924,
      "grad_norm": 1.1466028809081763e-05,
      "learning_rate": 0.0001325913143514524,
      "loss": 0.0027,
      "step": 150800
    },
    {
      "epoch": 43.38510209951107,
      "grad_norm": 0.0005288319662213326,
      "learning_rate": 0.0001323037100949094,
      "loss": 0.002,
      "step": 150850
    },
    {
      "epoch": 43.39948231233822,
      "grad_norm": 0.0021974598057568073,
      "learning_rate": 0.0001320161058383664,
      "loss": 0.0041,
      "step": 150900
    },
    {
      "epoch": 43.41386252516537,
      "grad_norm": 3.131615449092351e-05,
      "learning_rate": 0.00013172850158182342,
      "loss": 0.0018,
      "step": 150950
    },
    {
      "epoch": 43.42824273799252,
      "grad_norm": 1.819204044295475e-05,
      "learning_rate": 0.0001314408973252804,
      "loss": 0.0011,
      "step": 151000
    },
    {
      "epoch": 43.442622950819676,
      "grad_norm": 7.608610758325085e-05,
      "learning_rate": 0.00013115329306873742,
      "loss": 0.0008,
      "step": 151050
    },
    {
      "epoch": 43.457003163646824,
      "grad_norm": 0.00013230563490651548,
      "learning_rate": 0.00013086568881219443,
      "loss": 0.001,
      "step": 151100
    },
    {
      "epoch": 43.47138337647397,
      "grad_norm": 4.301429362385534e-05,
      "learning_rate": 0.00013057808455565142,
      "loss": 0.0048,
      "step": 151150
    },
    {
      "epoch": 43.48576358930112,
      "grad_norm": 0.00802964810281992,
      "learning_rate": 0.00013029048029910843,
      "loss": 0.0012,
      "step": 151200
    },
    {
      "epoch": 43.50014380212827,
      "grad_norm": 0.0027421817649155855,
      "learning_rate": 0.00013000287604256544,
      "loss": 0.0014,
      "step": 151250
    },
    {
      "epoch": 43.51452401495542,
      "grad_norm": 3.9226091757882386e-05,
      "learning_rate": 0.00012971527178602243,
      "loss": 0.0021,
      "step": 151300
    },
    {
      "epoch": 43.52890422778257,
      "grad_norm": 0.0007643108256161213,
      "learning_rate": 0.00012942766752947944,
      "loss": 0.0017,
      "step": 151350
    },
    {
      "epoch": 43.543284440609725,
      "grad_norm": 0.007010268047451973,
      "learning_rate": 0.00012914006327293645,
      "loss": 0.0011,
      "step": 151400
    },
    {
      "epoch": 43.55766465343687,
      "grad_norm": 0.009681510739028454,
      "learning_rate": 0.00012885245901639344,
      "loss": 0.0015,
      "step": 151450
    },
    {
      "epoch": 43.57204486626402,
      "grad_norm": 4.132222238695249e-05,
      "learning_rate": 0.00012856485475985045,
      "loss": 0.0014,
      "step": 151500
    },
    {
      "epoch": 43.58642507909117,
      "grad_norm": 0.004665418528020382,
      "learning_rate": 0.00012827725050330746,
      "loss": 0.0012,
      "step": 151550
    },
    {
      "epoch": 43.60080529191832,
      "grad_norm": 0.0001458742335671559,
      "learning_rate": 0.00012798964624676445,
      "loss": 0.0019,
      "step": 151600
    },
    {
      "epoch": 43.61518550474547,
      "grad_norm": 0.0034830314107239246,
      "learning_rate": 0.00012770204199022146,
      "loss": 0.0033,
      "step": 151650
    },
    {
      "epoch": 43.62956571757262,
      "grad_norm": 0.0001391981786582619,
      "learning_rate": 0.00012741443773367847,
      "loss": 0.0029,
      "step": 151700
    },
    {
      "epoch": 43.64394593039977,
      "grad_norm": 0.00025970814749598503,
      "learning_rate": 0.00012712683347713546,
      "loss": 0.0017,
      "step": 151750
    },
    {
      "epoch": 43.65832614322692,
      "grad_norm": 7.611401088070124e-05,
      "learning_rate": 0.00012683922922059247,
      "loss": 0.0037,
      "step": 151800
    },
    {
      "epoch": 43.67270635605407,
      "grad_norm": 0.009512673132121563,
      "learning_rate": 0.00012655162496404948,
      "loss": 0.0026,
      "step": 151850
    },
    {
      "epoch": 43.68708656888122,
      "grad_norm": 5.7164688769262284e-05,
      "learning_rate": 0.00012626402070750647,
      "loss": 0.0019,
      "step": 151900
    },
    {
      "epoch": 43.70146678170837,
      "grad_norm": 0.0006743557751178741,
      "learning_rate": 0.00012597641645096348,
      "loss": 0.001,
      "step": 151950
    },
    {
      "epoch": 43.71584699453552,
      "grad_norm": 0.011264908127486706,
      "learning_rate": 0.0001256888121944205,
      "loss": 0.0002,
      "step": 152000
    },
    {
      "epoch": 43.73022720736267,
      "grad_norm": 0.00044296172563917935,
      "learning_rate": 0.00012540120793787748,
      "loss": 0.0036,
      "step": 152050
    },
    {
      "epoch": 43.744607420189816,
      "grad_norm": 0.0064606210216879845,
      "learning_rate": 0.0001251136036813345,
      "loss": 0.001,
      "step": 152100
    },
    {
      "epoch": 43.75898763301697,
      "grad_norm": 2.478812893969007e-05,
      "learning_rate": 0.0001248259994247915,
      "loss": 0.0012,
      "step": 152150
    },
    {
      "epoch": 43.77336784584412,
      "grad_norm": 0.019450321793556213,
      "learning_rate": 0.00012453839516824849,
      "loss": 0.0034,
      "step": 152200
    },
    {
      "epoch": 43.78774805867127,
      "grad_norm": 0.006360392086207867,
      "learning_rate": 0.0001242507909117055,
      "loss": 0.0023,
      "step": 152250
    },
    {
      "epoch": 43.80212827149842,
      "grad_norm": 0.0032711613457649946,
      "learning_rate": 0.0001239631866551625,
      "loss": 0.0024,
      "step": 152300
    },
    {
      "epoch": 43.81650848432557,
      "grad_norm": 6.975058204261586e-05,
      "learning_rate": 0.0001236755823986195,
      "loss": 0.0028,
      "step": 152350
    },
    {
      "epoch": 43.830888697152716,
      "grad_norm": 0.0002923523134086281,
      "learning_rate": 0.0001233879781420765,
      "loss": 0.001,
      "step": 152400
    },
    {
      "epoch": 43.845268909979865,
      "grad_norm": 0.00030324829276651144,
      "learning_rate": 0.00012310037388553352,
      "loss": 0.0075,
      "step": 152450
    },
    {
      "epoch": 43.85964912280702,
      "grad_norm": 2.2592308596358635e-05,
      "learning_rate": 0.0001228127696289905,
      "loss": 0.0003,
      "step": 152500
    },
    {
      "epoch": 43.87402933563417,
      "grad_norm": 0.0002500346163287759,
      "learning_rate": 0.00012252516537244752,
      "loss": 0.0015,
      "step": 152550
    },
    {
      "epoch": 43.88840954846132,
      "grad_norm": 0.006053931079804897,
      "learning_rate": 0.00012223756111590453,
      "loss": 0.0004,
      "step": 152600
    },
    {
      "epoch": 43.90278976128847,
      "grad_norm": 0.00011798569903476164,
      "learning_rate": 0.00012194995685936153,
      "loss": 0.0026,
      "step": 152650
    },
    {
      "epoch": 43.917169974115616,
      "grad_norm": 0.00012693379539996386,
      "learning_rate": 0.00012166235260281853,
      "loss": 0.0021,
      "step": 152700
    },
    {
      "epoch": 43.931550186942765,
      "grad_norm": 0.0015411889180541039,
      "learning_rate": 0.00012137474834627553,
      "loss": 0.0016,
      "step": 152750
    },
    {
      "epoch": 43.945930399769914,
      "grad_norm": 7.474668382201344e-05,
      "learning_rate": 0.00012108714408973254,
      "loss": 0.0016,
      "step": 152800
    },
    {
      "epoch": 43.96031061259707,
      "grad_norm": 0.0004843112837988883,
      "learning_rate": 0.00012079953983318954,
      "loss": 0.0013,
      "step": 152850
    },
    {
      "epoch": 43.97469082542422,
      "grad_norm": 0.007731392048299313,
      "learning_rate": 0.00012051193557664654,
      "loss": 0.0017,
      "step": 152900
    },
    {
      "epoch": 43.98907103825137,
      "grad_norm": 0.0002135748218279332,
      "learning_rate": 0.00012022433132010353,
      "loss": 0.0017,
      "step": 152950
    },
    {
      "epoch": 44.0,
      "eval_loss": 0.010028226301074028,
      "eval_runtime": 17.4359,
      "eval_samples_per_second": 2736.821,
      "eval_steps_per_second": 42.785,
      "step": 152988
    },
    {
      "epoch": 44.003451251078516,
      "grad_norm": 0.0024549264926463366,
      "learning_rate": 0.00011993672706356055,
      "loss": 0.0049,
      "step": 153000
    },
    {
      "epoch": 44.017831463905665,
      "grad_norm": 0.0023363856598734856,
      "learning_rate": 0.00011964912280701755,
      "loss": 0.0011,
      "step": 153050
    },
    {
      "epoch": 44.032211676732814,
      "grad_norm": 0.0001933372113853693,
      "learning_rate": 0.00011936151855047454,
      "loss": 0.0013,
      "step": 153100
    },
    {
      "epoch": 44.04659188955996,
      "grad_norm": 3.283777550677769e-05,
      "learning_rate": 0.00011907391429393156,
      "loss": 0.0036,
      "step": 153150
    },
    {
      "epoch": 44.06097210238712,
      "grad_norm": 0.01938510686159134,
      "learning_rate": 0.00011878631003738855,
      "loss": 0.0034,
      "step": 153200
    },
    {
      "epoch": 44.07535231521427,
      "grad_norm": 9.636416507419199e-05,
      "learning_rate": 0.00011849870578084555,
      "loss": 0.0051,
      "step": 153250
    },
    {
      "epoch": 44.08973252804142,
      "grad_norm": 0.007015041541308165,
      "learning_rate": 0.00011821110152430257,
      "loss": 0.0006,
      "step": 153300
    },
    {
      "epoch": 44.104112740868565,
      "grad_norm": 0.00029859552159905434,
      "learning_rate": 0.00011792349726775956,
      "loss": 0.0012,
      "step": 153350
    },
    {
      "epoch": 44.118492953695714,
      "grad_norm": 0.007634460460394621,
      "learning_rate": 0.00011763589301121656,
      "loss": 0.0019,
      "step": 153400
    },
    {
      "epoch": 44.13287316652286,
      "grad_norm": 0.00868295505642891,
      "learning_rate": 0.00011734828875467358,
      "loss": 0.0002,
      "step": 153450
    },
    {
      "epoch": 44.14725337935001,
      "grad_norm": 0.00015479758440051228,
      "learning_rate": 0.00011706068449813057,
      "loss": 0.0012,
      "step": 153500
    },
    {
      "epoch": 44.16163359217716,
      "grad_norm": 0.005202509928494692,
      "learning_rate": 0.00011677308024158757,
      "loss": 0.0025,
      "step": 153550
    },
    {
      "epoch": 44.17601380500432,
      "grad_norm": 0.0060155196115374565,
      "learning_rate": 0.00011648547598504459,
      "loss": 0.0015,
      "step": 153600
    },
    {
      "epoch": 44.190394017831466,
      "grad_norm": 0.00015709955187048763,
      "learning_rate": 0.00011619787172850158,
      "loss": 0.0023,
      "step": 153650
    },
    {
      "epoch": 44.204774230658614,
      "grad_norm": 6.839858542662114e-05,
      "learning_rate": 0.00011591026747195858,
      "loss": 0.0019,
      "step": 153700
    },
    {
      "epoch": 44.21915444348576,
      "grad_norm": 0.005771240685135126,
      "learning_rate": 0.0001156226632154156,
      "loss": 0.0054,
      "step": 153750
    },
    {
      "epoch": 44.23353465631291,
      "grad_norm": 0.0029738019220530987,
      "learning_rate": 0.0001153350589588726,
      "loss": 0.0011,
      "step": 153800
    },
    {
      "epoch": 44.24791486914006,
      "grad_norm": 5.403047180152498e-05,
      "learning_rate": 0.00011504745470232959,
      "loss": 0.0025,
      "step": 153850
    },
    {
      "epoch": 44.26229508196721,
      "grad_norm": 2.735105408646632e-05,
      "learning_rate": 0.0001147598504457866,
      "loss": 0.0014,
      "step": 153900
    },
    {
      "epoch": 44.276675294794366,
      "grad_norm": 0.0008506913436576724,
      "learning_rate": 0.0001144722461892436,
      "loss": 0.0024,
      "step": 153950
    },
    {
      "epoch": 44.291055507621515,
      "grad_norm": 3.335428482387215e-05,
      "learning_rate": 0.0001141846419327006,
      "loss": 0.0052,
      "step": 154000
    },
    {
      "epoch": 44.30543572044866,
      "grad_norm": 9.441586735192686e-05,
      "learning_rate": 0.00011389703767615761,
      "loss": 0.0028,
      "step": 154050
    },
    {
      "epoch": 44.31981593327581,
      "grad_norm": 0.0017821327783167362,
      "learning_rate": 0.00011360943341961461,
      "loss": 0.0003,
      "step": 154100
    },
    {
      "epoch": 44.33419614610296,
      "grad_norm": 2.8410187951521948e-05,
      "learning_rate": 0.00011332182916307161,
      "loss": 0.0014,
      "step": 154150
    },
    {
      "epoch": 44.34857635893011,
      "grad_norm": 1.4682160326628946e-05,
      "learning_rate": 0.00011303422490652862,
      "loss": 0.0007,
      "step": 154200
    },
    {
      "epoch": 44.36295657175726,
      "grad_norm": 0.0003996843588538468,
      "learning_rate": 0.00011274662064998562,
      "loss": 0.0017,
      "step": 154250
    },
    {
      "epoch": 44.377336784584415,
      "grad_norm": 0.00011155441461596638,
      "learning_rate": 0.00011245901639344262,
      "loss": 0.0017,
      "step": 154300
    },
    {
      "epoch": 44.391716997411564,
      "grad_norm": 0.00011035272473236546,
      "learning_rate": 0.00011217141213689963,
      "loss": 0.001,
      "step": 154350
    },
    {
      "epoch": 44.40609721023871,
      "grad_norm": 5.7280758483102545e-05,
      "learning_rate": 0.00011188380788035662,
      "loss": 0.0036,
      "step": 154400
    },
    {
      "epoch": 44.42047742306586,
      "grad_norm": 0.000465086312033236,
      "learning_rate": 0.00011159620362381363,
      "loss": 0.0012,
      "step": 154450
    },
    {
      "epoch": 44.43485763589301,
      "grad_norm": 0.0005359170027077198,
      "learning_rate": 0.00011130859936727064,
      "loss": 0.0018,
      "step": 154500
    },
    {
      "epoch": 44.44923784872016,
      "grad_norm": 3.772758645936847e-05,
      "learning_rate": 0.00011102099511072763,
      "loss": 0.0018,
      "step": 154550
    },
    {
      "epoch": 44.46361806154731,
      "grad_norm": 3.397898399271071e-05,
      "learning_rate": 0.00011073339085418464,
      "loss": 0.0021,
      "step": 154600
    },
    {
      "epoch": 44.477998274374464,
      "grad_norm": 2.3459087969968095e-05,
      "learning_rate": 0.00011044578659764165,
      "loss": 0.0018,
      "step": 154650
    },
    {
      "epoch": 44.49237848720161,
      "grad_norm": 0.0036749604623764753,
      "learning_rate": 0.00011015818234109864,
      "loss": 0.0026,
      "step": 154700
    },
    {
      "epoch": 44.50675870002876,
      "grad_norm": 0.005302834324538708,
      "learning_rate": 0.00010987057808455565,
      "loss": 0.0018,
      "step": 154750
    },
    {
      "epoch": 44.52113891285591,
      "grad_norm": 1.5258499843184836e-05,
      "learning_rate": 0.00010958297382801266,
      "loss": 0.0016,
      "step": 154800
    },
    {
      "epoch": 44.53551912568306,
      "grad_norm": 0.00024631767882965505,
      "learning_rate": 0.00010929536957146965,
      "loss": 0.0002,
      "step": 154850
    },
    {
      "epoch": 44.54989933851021,
      "grad_norm": 0.00011837278725579381,
      "learning_rate": 0.00010900776531492666,
      "loss": 0.0023,
      "step": 154900
    },
    {
      "epoch": 44.56427955133736,
      "grad_norm": 0.0008362616645172238,
      "learning_rate": 0.00010872016105838367,
      "loss": 0.001,
      "step": 154950
    },
    {
      "epoch": 44.57865976416451,
      "grad_norm": 4.1692208469612524e-05,
      "learning_rate": 0.00010843255680184066,
      "loss": 0.0022,
      "step": 155000
    },
    {
      "epoch": 44.59303997699166,
      "grad_norm": 0.0035344066563993692,
      "learning_rate": 0.00010814495254529767,
      "loss": 0.0044,
      "step": 155050
    },
    {
      "epoch": 44.60742018981881,
      "grad_norm": 0.0005071128834970295,
      "learning_rate": 0.00010785734828875468,
      "loss": 0.0027,
      "step": 155100
    },
    {
      "epoch": 44.62180040264596,
      "grad_norm": 0.00011043775884900242,
      "learning_rate": 0.00010756974403221167,
      "loss": 0.0029,
      "step": 155150
    },
    {
      "epoch": 44.63618061547311,
      "grad_norm": 0.004118626471608877,
      "learning_rate": 0.00010728213977566868,
      "loss": 0.0021,
      "step": 155200
    },
    {
      "epoch": 44.65056082830026,
      "grad_norm": 0.0005523848230950534,
      "learning_rate": 0.00010699453551912569,
      "loss": 0.0055,
      "step": 155250
    },
    {
      "epoch": 44.664941041127406,
      "grad_norm": 0.0012792818015441298,
      "learning_rate": 0.00010670693126258268,
      "loss": 0.0063,
      "step": 155300
    },
    {
      "epoch": 44.679321253954555,
      "grad_norm": 0.003080657683312893,
      "learning_rate": 0.00010641932700603969,
      "loss": 0.0048,
      "step": 155350
    },
    {
      "epoch": 44.69370146678171,
      "grad_norm": 0.0008668583468534052,
      "learning_rate": 0.0001061317227494967,
      "loss": 0.0006,
      "step": 155400
    },
    {
      "epoch": 44.70808167960886,
      "grad_norm": 0.029699064791202545,
      "learning_rate": 0.0001058441184929537,
      "loss": 0.0046,
      "step": 155450
    },
    {
      "epoch": 44.72246189243601,
      "grad_norm": 0.0005474371719174087,
      "learning_rate": 0.0001055565142364107,
      "loss": 0.0049,
      "step": 155500
    },
    {
      "epoch": 44.73684210526316,
      "grad_norm": 1.6525933460798115e-05,
      "learning_rate": 0.00010526890997986771,
      "loss": 0.0017,
      "step": 155550
    },
    {
      "epoch": 44.751222318090306,
      "grad_norm": 0.015365700237452984,
      "learning_rate": 0.00010498130572332471,
      "loss": 0.0011,
      "step": 155600
    },
    {
      "epoch": 44.765602530917455,
      "grad_norm": 0.00034360031713731587,
      "learning_rate": 0.00010469370146678171,
      "loss": 0.0008,
      "step": 155650
    },
    {
      "epoch": 44.779982743744604,
      "grad_norm": 0.0002131859218934551,
      "learning_rate": 0.00010440609721023871,
      "loss": 0.0008,
      "step": 155700
    },
    {
      "epoch": 44.79436295657176,
      "grad_norm": 0.01480237115174532,
      "learning_rate": 0.00010411849295369572,
      "loss": 0.0022,
      "step": 155750
    },
    {
      "epoch": 44.80874316939891,
      "grad_norm": 9.072612556337845e-06,
      "learning_rate": 0.00010383088869715272,
      "loss": 0.0024,
      "step": 155800
    },
    {
      "epoch": 44.82312338222606,
      "grad_norm": 3.204356107744388e-05,
      "learning_rate": 0.00010354328444060972,
      "loss": 0.0016,
      "step": 155850
    },
    {
      "epoch": 44.83750359505321,
      "grad_norm": 0.009989993646740913,
      "learning_rate": 0.00010325568018406673,
      "loss": 0.0011,
      "step": 155900
    },
    {
      "epoch": 44.851883807880355,
      "grad_norm": 0.007120149675756693,
      "learning_rate": 0.00010296807592752373,
      "loss": 0.0022,
      "step": 155950
    },
    {
      "epoch": 44.866264020707504,
      "grad_norm": 5.529788904823363e-05,
      "learning_rate": 0.00010268047167098073,
      "loss": 0.001,
      "step": 156000
    },
    {
      "epoch": 44.88064423353465,
      "grad_norm": 0.0008242319454438984,
      "learning_rate": 0.00010239286741443774,
      "loss": 0.0025,
      "step": 156050
    },
    {
      "epoch": 44.89502444636181,
      "grad_norm": 0.00015604132204316556,
      "learning_rate": 0.00010210526315789474,
      "loss": 0.0027,
      "step": 156100
    },
    {
      "epoch": 44.90940465918896,
      "grad_norm": 0.0006439865683205426,
      "learning_rate": 0.00010181765890135174,
      "loss": 0.0029,
      "step": 156150
    },
    {
      "epoch": 44.92378487201611,
      "grad_norm": 0.0006993178394623101,
      "learning_rate": 0.00010153005464480875,
      "loss": 0.0003,
      "step": 156200
    },
    {
      "epoch": 44.938165084843256,
      "grad_norm": 0.00012500448792707175,
      "learning_rate": 0.00010124245038826575,
      "loss": 0.0031,
      "step": 156250
    },
    {
      "epoch": 44.952545297670405,
      "grad_norm": 0.00028431808459572494,
      "learning_rate": 0.00010095484613172275,
      "loss": 0.0027,
      "step": 156300
    },
    {
      "epoch": 44.96692551049755,
      "grad_norm": 0.003139313543215394,
      "learning_rate": 0.00010066724187517976,
      "loss": 0.0028,
      "step": 156350
    },
    {
      "epoch": 44.9813057233247,
      "grad_norm": 3.245544576202519e-05,
      "learning_rate": 0.00010037963761863676,
      "loss": 0.0025,
      "step": 156400
    },
    {
      "epoch": 44.99568593615186,
      "grad_norm": 0.0009311795001849532,
      "learning_rate": 0.00010009203336209376,
      "loss": 0.0015,
      "step": 156450
    },
    {
      "epoch": 45.0,
      "eval_loss": 0.009921487420797348,
      "eval_runtime": 16.9712,
      "eval_samples_per_second": 2811.757,
      "eval_steps_per_second": 43.957,
      "step": 156465
    },
    {
      "epoch": 45.01006614897901,
      "grad_norm": 1.61943808052456e-05,
      "learning_rate": 9.980442910555077e-05,
      "loss": 0.0019,
      "step": 156500
    },
    {
      "epoch": 45.024446361806156,
      "grad_norm": 4.5921351556899026e-05,
      "learning_rate": 9.951682484900777e-05,
      "loss": 0.0007,
      "step": 156550
    },
    {
      "epoch": 45.038826574633305,
      "grad_norm": 2.7099387807538733e-05,
      "learning_rate": 9.922922059246477e-05,
      "loss": 0.0011,
      "step": 156600
    },
    {
      "epoch": 45.053206787460454,
      "grad_norm": 7.782939064782113e-05,
      "learning_rate": 9.894161633592178e-05,
      "loss": 0.0016,
      "step": 156650
    },
    {
      "epoch": 45.0675870002876,
      "grad_norm": 0.0003127340169157833,
      "learning_rate": 9.865401207937878e-05,
      "loss": 0.0049,
      "step": 156700
    },
    {
      "epoch": 45.08196721311475,
      "grad_norm": 4.370867463876493e-05,
      "learning_rate": 9.836640782283578e-05,
      "loss": 0.0025,
      "step": 156750
    },
    {
      "epoch": 45.09634742594191,
      "grad_norm": 2.8613920221687295e-05,
      "learning_rate": 9.807880356629279e-05,
      "loss": 0.0044,
      "step": 156800
    },
    {
      "epoch": 45.110727638769056,
      "grad_norm": 0.00011298511526547372,
      "learning_rate": 9.779119930974979e-05,
      "loss": 0.003,
      "step": 156850
    },
    {
      "epoch": 45.125107851596205,
      "grad_norm": 0.0017008313443511724,
      "learning_rate": 9.750359505320679e-05,
      "loss": 0.0011,
      "step": 156900
    },
    {
      "epoch": 45.139488064423354,
      "grad_norm": 5.6704055168665946e-05,
      "learning_rate": 9.72159907966638e-05,
      "loss": 0.0012,
      "step": 156950
    },
    {
      "epoch": 45.1538682772505,
      "grad_norm": 6.504121120087802e-05,
      "learning_rate": 9.69283865401208e-05,
      "loss": 0.001,
      "step": 157000
    },
    {
      "epoch": 45.16824849007765,
      "grad_norm": 0.0006099561578594148,
      "learning_rate": 9.66407822835778e-05,
      "loss": 0.0014,
      "step": 157050
    },
    {
      "epoch": 45.1826287029048,
      "grad_norm": 0.00015330359747167677,
      "learning_rate": 9.635317802703481e-05,
      "loss": 0.0021,
      "step": 157100
    },
    {
      "epoch": 45.197008915731956,
      "grad_norm": 2.622696956677828e-05,
      "learning_rate": 9.60655737704918e-05,
      "loss": 0.0029,
      "step": 157150
    },
    {
      "epoch": 45.211389128559105,
      "grad_norm": 0.005700244568288326,
      "learning_rate": 9.57779695139488e-05,
      "loss": 0.0005,
      "step": 157200
    },
    {
      "epoch": 45.225769341386254,
      "grad_norm": 0.00146814389154315,
      "learning_rate": 9.549036525740582e-05,
      "loss": 0.0025,
      "step": 157250
    },
    {
      "epoch": 45.2401495542134,
      "grad_norm": 0.011496572755277157,
      "learning_rate": 9.52027610008628e-05,
      "loss": 0.0025,
      "step": 157300
    },
    {
      "epoch": 45.25452976704055,
      "grad_norm": 6.603360088774934e-05,
      "learning_rate": 9.491515674431982e-05,
      "loss": 0.0022,
      "step": 157350
    },
    {
      "epoch": 45.2689099798677,
      "grad_norm": 6.29855931038037e-05,
      "learning_rate": 9.462755248777683e-05,
      "loss": 0.003,
      "step": 157400
    },
    {
      "epoch": 45.28329019269485,
      "grad_norm": 0.0007096042390912771,
      "learning_rate": 9.433994823123381e-05,
      "loss": 0.002,
      "step": 157450
    },
    {
      "epoch": 45.297670405522,
      "grad_norm": 4.9498965381644666e-05,
      "learning_rate": 9.405234397469083e-05,
      "loss": 0.0053,
      "step": 157500
    },
    {
      "epoch": 45.312050618349154,
      "grad_norm": 0.0012710373848676682,
      "learning_rate": 9.376473971814784e-05,
      "loss": 0.0009,
      "step": 157550
    },
    {
      "epoch": 45.3264308311763,
      "grad_norm": 0.0027948329225182533,
      "learning_rate": 9.347713546160482e-05,
      "loss": 0.0008,
      "step": 157600
    },
    {
      "epoch": 45.34081104400345,
      "grad_norm": 0.00012607073585968465,
      "learning_rate": 9.318953120506184e-05,
      "loss": 0.0048,
      "step": 157650
    },
    {
      "epoch": 45.3551912568306,
      "grad_norm": 0.0001335299457423389,
      "learning_rate": 9.290192694851885e-05,
      "loss": 0.0014,
      "step": 157700
    },
    {
      "epoch": 45.36957146965775,
      "grad_norm": 2.8876842407044023e-05,
      "learning_rate": 9.261432269197583e-05,
      "loss": 0.0037,
      "step": 157750
    },
    {
      "epoch": 45.3839516824849,
      "grad_norm": 0.004326746799051762,
      "learning_rate": 9.232671843543285e-05,
      "loss": 0.0019,
      "step": 157800
    },
    {
      "epoch": 45.39833189531205,
      "grad_norm": 4.2501964344410226e-05,
      "learning_rate": 9.203911417888986e-05,
      "loss": 0.003,
      "step": 157850
    },
    {
      "epoch": 45.4127121081392,
      "grad_norm": 0.0011245416244491935,
      "learning_rate": 9.175150992234684e-05,
      "loss": 0.0015,
      "step": 157900
    },
    {
      "epoch": 45.42709232096635,
      "grad_norm": 0.0005452549667097628,
      "learning_rate": 9.146390566580386e-05,
      "loss": 0.004,
      "step": 157950
    },
    {
      "epoch": 45.4414725337935,
      "grad_norm": 5.034120476921089e-05,
      "learning_rate": 9.117630140926087e-05,
      "loss": 0.0018,
      "step": 158000
    },
    {
      "epoch": 45.45585274662065,
      "grad_norm": 0.0019134681206196547,
      "learning_rate": 9.088869715271785e-05,
      "loss": 0.0013,
      "step": 158050
    },
    {
      "epoch": 45.4702329594478,
      "grad_norm": 0.0001152773984358646,
      "learning_rate": 9.060109289617487e-05,
      "loss": 0.003,
      "step": 158100
    },
    {
      "epoch": 45.48461317227495,
      "grad_norm": 4.216516390442848e-05,
      "learning_rate": 9.031348863963188e-05,
      "loss": 0.0014,
      "step": 158150
    },
    {
      "epoch": 45.4989933851021,
      "grad_norm": 6.877419218653813e-05,
      "learning_rate": 9.002588438308886e-05,
      "loss": 0.0022,
      "step": 158200
    },
    {
      "epoch": 45.51337359792925,
      "grad_norm": 0.0009844836313277483,
      "learning_rate": 8.973828012654587e-05,
      "loss": 0.0024,
      "step": 158250
    },
    {
      "epoch": 45.5277538107564,
      "grad_norm": 0.005218570586293936,
      "learning_rate": 8.945067587000289e-05,
      "loss": 0.0033,
      "step": 158300
    },
    {
      "epoch": 45.54213402358355,
      "grad_norm": 0.008306465111672878,
      "learning_rate": 8.916307161345987e-05,
      "loss": 0.0026,
      "step": 158350
    },
    {
      "epoch": 45.5565142364107,
      "grad_norm": 0.005162177607417107,
      "learning_rate": 8.887546735691688e-05,
      "loss": 0.0005,
      "step": 158400
    },
    {
      "epoch": 45.57089444923785,
      "grad_norm": 0.004118732642382383,
      "learning_rate": 8.858786310037388e-05,
      "loss": 0.0022,
      "step": 158450
    },
    {
      "epoch": 45.585274662065,
      "grad_norm": 0.00010637569357641041,
      "learning_rate": 8.830025884383088e-05,
      "loss": 0.0026,
      "step": 158500
    },
    {
      "epoch": 45.599654874892146,
      "grad_norm": 0.00012833748769480735,
      "learning_rate": 8.80126545872879e-05,
      "loss": 0.0029,
      "step": 158550
    },
    {
      "epoch": 45.6140350877193,
      "grad_norm": 9.034860704559833e-05,
      "learning_rate": 8.77250503307449e-05,
      "loss": 0.0031,
      "step": 158600
    },
    {
      "epoch": 45.62841530054645,
      "grad_norm": 0.002491613617166877,
      "learning_rate": 8.743744607420189e-05,
      "loss": 0.0021,
      "step": 158650
    },
    {
      "epoch": 45.6427955133736,
      "grad_norm": 2.865349051717203e-05,
      "learning_rate": 8.71498418176589e-05,
      "loss": 0.0031,
      "step": 158700
    },
    {
      "epoch": 45.65717572620075,
      "grad_norm": 0.0012071012752130628,
      "learning_rate": 8.68622375611159e-05,
      "loss": 0.0048,
      "step": 158750
    },
    {
      "epoch": 45.6715559390279,
      "grad_norm": 3.563297286746092e-05,
      "learning_rate": 8.65746333045729e-05,
      "loss": 0.0028,
      "step": 158800
    },
    {
      "epoch": 45.685936151855046,
      "grad_norm": 0.00016554426110815257,
      "learning_rate": 8.628702904802991e-05,
      "loss": 0.0021,
      "step": 158850
    },
    {
      "epoch": 45.700316364682195,
      "grad_norm": 4.266575342626311e-05,
      "learning_rate": 8.599942479148691e-05,
      "loss": 0.0028,
      "step": 158900
    },
    {
      "epoch": 45.71469657750935,
      "grad_norm": 0.00023167736071627587,
      "learning_rate": 8.571182053494393e-05,
      "loss": 0.0003,
      "step": 158950
    },
    {
      "epoch": 45.7290767903365,
      "grad_norm": 6.433425733121112e-05,
      "learning_rate": 8.542421627840092e-05,
      "loss": 0.0024,
      "step": 159000
    },
    {
      "epoch": 45.74345700316365,
      "grad_norm": 6.347504677250981e-05,
      "learning_rate": 8.513661202185792e-05,
      "loss": 0.0019,
      "step": 159050
    },
    {
      "epoch": 45.7578372159908,
      "grad_norm": 6.118661985965446e-05,
      "learning_rate": 8.484900776531493e-05,
      "loss": 0.0016,
      "step": 159100
    },
    {
      "epoch": 45.772217428817946,
      "grad_norm": 0.0007382585317827761,
      "learning_rate": 8.456140350877193e-05,
      "loss": 0.0011,
      "step": 159150
    },
    {
      "epoch": 45.786597641645095,
      "grad_norm": 0.00030688996776007116,
      "learning_rate": 8.427379925222893e-05,
      "loss": 0.0015,
      "step": 159200
    },
    {
      "epoch": 45.800977854472244,
      "grad_norm": 0.0009288613800890744,
      "learning_rate": 8.398619499568594e-05,
      "loss": 0.0005,
      "step": 159250
    },
    {
      "epoch": 45.8153580672994,
      "grad_norm": 0.0004223795549478382,
      "learning_rate": 8.369859073914294e-05,
      "loss": 0.0045,
      "step": 159300
    },
    {
      "epoch": 45.82973828012655,
      "grad_norm": 0.00036578477011062205,
      "learning_rate": 8.341098648259994e-05,
      "loss": 0.0007,
      "step": 159350
    },
    {
      "epoch": 45.8441184929537,
      "grad_norm": 6.39930585748516e-05,
      "learning_rate": 8.312338222605695e-05,
      "loss": 0.0025,
      "step": 159400
    },
    {
      "epoch": 45.858498705780846,
      "grad_norm": 1.718945895845536e-05,
      "learning_rate": 8.283577796951395e-05,
      "loss": 0.0015,
      "step": 159450
    },
    {
      "epoch": 45.872878918607995,
      "grad_norm": 8.849666483001783e-05,
      "learning_rate": 8.254817371297095e-05,
      "loss": 0.0011,
      "step": 159500
    },
    {
      "epoch": 45.887259131435144,
      "grad_norm": 2.9371931304922327e-05,
      "learning_rate": 8.226056945642796e-05,
      "loss": 0.0038,
      "step": 159550
    },
    {
      "epoch": 45.90163934426229,
      "grad_norm": 0.0007810459937900305,
      "learning_rate": 8.197296519988496e-05,
      "loss": 0.0036,
      "step": 159600
    },
    {
      "epoch": 45.91601955708944,
      "grad_norm": 3.090575410169549e-05,
      "learning_rate": 8.168536094334196e-05,
      "loss": 0.001,
      "step": 159650
    },
    {
      "epoch": 45.9303997699166,
      "grad_norm": 0.0050279367715120316,
      "learning_rate": 8.139775668679897e-05,
      "loss": 0.0032,
      "step": 159700
    },
    {
      "epoch": 45.944779982743746,
      "grad_norm": 1.2541523574327584e-05,
      "learning_rate": 8.111015243025597e-05,
      "loss": 0.0024,
      "step": 159750
    },
    {
      "epoch": 45.959160195570895,
      "grad_norm": 0.00038718513678759336,
      "learning_rate": 8.082254817371297e-05,
      "loss": 0.0004,
      "step": 159800
    },
    {
      "epoch": 45.973540408398044,
      "grad_norm": 0.011885976418852806,
      "learning_rate": 8.053494391716998e-05,
      "loss": 0.0007,
      "step": 159850
    },
    {
      "epoch": 45.98792062122519,
      "grad_norm": 0.004548685159534216,
      "learning_rate": 8.024733966062697e-05,
      "loss": 0.0036,
      "step": 159900
    },
    {
      "epoch": 46.0,
      "eval_loss": 0.009894889779388905,
      "eval_runtime": 18.0352,
      "eval_samples_per_second": 2645.878,
      "eval_steps_per_second": 41.363,
      "step": 159942
    },
    {
      "epoch": 46.00230083405234,
      "grad_norm": 0.0019469584804028273,
      "learning_rate": 7.995973540408398e-05,
      "loss": 0.0019,
      "step": 159950
    },
    {
      "epoch": 46.01668104687949,
      "grad_norm": 0.0037656514905393124,
      "learning_rate": 7.9672131147541e-05,
      "loss": 0.0024,
      "step": 160000
    },
    {
      "epoch": 46.03106125970665,
      "grad_norm": 0.0002293573197675869,
      "learning_rate": 7.938452689099798e-05,
      "loss": 0.003,
      "step": 160050
    },
    {
      "epoch": 46.045441472533795,
      "grad_norm": 2.7341391614754684e-05,
      "learning_rate": 7.909692263445499e-05,
      "loss": 0.0023,
      "step": 160100
    },
    {
      "epoch": 46.059821685360944,
      "grad_norm": 0.000716520007699728,
      "learning_rate": 7.8809318377912e-05,
      "loss": 0.0006,
      "step": 160150
    },
    {
      "epoch": 46.07420189818809,
      "grad_norm": 0.0005630868836306036,
      "learning_rate": 7.852171412136899e-05,
      "loss": 0.0007,
      "step": 160200
    },
    {
      "epoch": 46.08858211101524,
      "grad_norm": 0.00018178891332354397,
      "learning_rate": 7.8234109864826e-05,
      "loss": 0.0017,
      "step": 160250
    },
    {
      "epoch": 46.10296232384239,
      "grad_norm": 8.777308539720252e-05,
      "learning_rate": 7.794650560828301e-05,
      "loss": 0.0042,
      "step": 160300
    },
    {
      "epoch": 46.11734253666954,
      "grad_norm": 0.0008034204947762191,
      "learning_rate": 7.765890135174e-05,
      "loss": 0.004,
      "step": 160350
    },
    {
      "epoch": 46.131722749496696,
      "grad_norm": 0.0014250192325562239,
      "learning_rate": 7.737129709519701e-05,
      "loss": 0.004,
      "step": 160400
    },
    {
      "epoch": 46.146102962323845,
      "grad_norm": 0.0010568572906777263,
      "learning_rate": 7.708369283865402e-05,
      "loss": 0.0073,
      "step": 160450
    },
    {
      "epoch": 46.16048317515099,
      "grad_norm": 6.603493238799274e-05,
      "learning_rate": 7.679608858211101e-05,
      "loss": 0.0002,
      "step": 160500
    },
    {
      "epoch": 46.17486338797814,
      "grad_norm": 6.11294963164255e-05,
      "learning_rate": 7.650848432556802e-05,
      "loss": 0.0037,
      "step": 160550
    },
    {
      "epoch": 46.18924360080529,
      "grad_norm": 0.0019425021018832922,
      "learning_rate": 7.622088006902503e-05,
      "loss": 0.0009,
      "step": 160600
    },
    {
      "epoch": 46.20362381363244,
      "grad_norm": 0.00017943531565833837,
      "learning_rate": 7.593327581248202e-05,
      "loss": 0.0023,
      "step": 160650
    },
    {
      "epoch": 46.21800402645959,
      "grad_norm": 0.0005290779517963529,
      "learning_rate": 7.564567155593903e-05,
      "loss": 0.0008,
      "step": 160700
    },
    {
      "epoch": 46.232384239286745,
      "grad_norm": 0.00011459458619356155,
      "learning_rate": 7.535806729939604e-05,
      "loss": 0.0002,
      "step": 160750
    },
    {
      "epoch": 46.246764452113894,
      "grad_norm": 0.00012844374577980489,
      "learning_rate": 7.507046304285303e-05,
      "loss": 0.0014,
      "step": 160800
    },
    {
      "epoch": 46.26114466494104,
      "grad_norm": 0.006204239092767239,
      "learning_rate": 7.478285878631004e-05,
      "loss": 0.005,
      "step": 160850
    },
    {
      "epoch": 46.27552487776819,
      "grad_norm": 0.0002168305218219757,
      "learning_rate": 7.449525452976705e-05,
      "loss": 0.0024,
      "step": 160900
    },
    {
      "epoch": 46.28990509059534,
      "grad_norm": 0.009296181611716747,
      "learning_rate": 7.420765027322404e-05,
      "loss": 0.0022,
      "step": 160950
    },
    {
      "epoch": 46.30428530342249,
      "grad_norm": 0.00019479777256492525,
      "learning_rate": 7.392004601668105e-05,
      "loss": 0.0053,
      "step": 161000
    },
    {
      "epoch": 46.31866551624964,
      "grad_norm": 0.00022351916413754225,
      "learning_rate": 7.363244176013806e-05,
      "loss": 0.0006,
      "step": 161050
    },
    {
      "epoch": 46.33304572907679,
      "grad_norm": 3.0277764381025918e-05,
      "learning_rate": 7.334483750359505e-05,
      "loss": 0.0033,
      "step": 161100
    },
    {
      "epoch": 46.34742594190394,
      "grad_norm": 5.621595119009726e-05,
      "learning_rate": 7.305723324705206e-05,
      "loss": 0.0014,
      "step": 161150
    },
    {
      "epoch": 46.36180615473109,
      "grad_norm": 0.006357599515467882,
      "learning_rate": 7.276962899050906e-05,
      "loss": 0.003,
      "step": 161200
    },
    {
      "epoch": 46.37618636755824,
      "grad_norm": 0.025693509727716446,
      "learning_rate": 7.248202473396606e-05,
      "loss": 0.0021,
      "step": 161250
    },
    {
      "epoch": 46.39056658038539,
      "grad_norm": 0.00022088896366767585,
      "learning_rate": 7.219442047742307e-05,
      "loss": 0.0014,
      "step": 161300
    },
    {
      "epoch": 46.40494679321254,
      "grad_norm": 0.0008447591681033373,
      "learning_rate": 7.190681622088007e-05,
      "loss": 0.0057,
      "step": 161350
    },
    {
      "epoch": 46.41932700603969,
      "grad_norm": 2.177148599002976e-05,
      "learning_rate": 7.161921196433707e-05,
      "loss": 0.0032,
      "step": 161400
    },
    {
      "epoch": 46.433707218866836,
      "grad_norm": 1.5203215298242867e-05,
      "learning_rate": 7.133160770779408e-05,
      "loss": 0.0009,
      "step": 161450
    },
    {
      "epoch": 46.44808743169399,
      "grad_norm": 0.0027052827645093203,
      "learning_rate": 7.104400345125108e-05,
      "loss": 0.0013,
      "step": 161500
    },
    {
      "epoch": 46.46246764452114,
      "grad_norm": 5.013651752960868e-05,
      "learning_rate": 7.075639919470808e-05,
      "loss": 0.0012,
      "step": 161550
    },
    {
      "epoch": 46.47684785734829,
      "grad_norm": 0.006892124190926552,
      "learning_rate": 7.046879493816509e-05,
      "loss": 0.002,
      "step": 161600
    },
    {
      "epoch": 46.49122807017544,
      "grad_norm": 0.00032401137286797166,
      "learning_rate": 7.018119068162209e-05,
      "loss": 0.0029,
      "step": 161650
    },
    {
      "epoch": 46.50560828300259,
      "grad_norm": 0.009013102389872074,
      "learning_rate": 6.989358642507909e-05,
      "loss": 0.0023,
      "step": 161700
    },
    {
      "epoch": 46.519988495829736,
      "grad_norm": 2.5709969122544862e-05,
      "learning_rate": 6.96059821685361e-05,
      "loss": 0.0016,
      "step": 161750
    },
    {
      "epoch": 46.534368708656885,
      "grad_norm": 0.0004531441954895854,
      "learning_rate": 6.93183779119931e-05,
      "loss": 0.0014,
      "step": 161800
    },
    {
      "epoch": 46.54874892148404,
      "grad_norm": 0.0018191483104601502,
      "learning_rate": 6.90307736554501e-05,
      "loss": 0.0017,
      "step": 161850
    },
    {
      "epoch": 46.56312913431119,
      "grad_norm": 0.0001125923081417568,
      "learning_rate": 6.874316939890711e-05,
      "loss": 0.0002,
      "step": 161900
    },
    {
      "epoch": 46.57750934713834,
      "grad_norm": 0.0005177536513656378,
      "learning_rate": 6.845556514236411e-05,
      "loss": 0.0032,
      "step": 161950
    },
    {
      "epoch": 46.59188955996549,
      "grad_norm": 0.00010930877033388242,
      "learning_rate": 6.81679608858211e-05,
      "loss": 0.0011,
      "step": 162000
    },
    {
      "epoch": 46.606269772792636,
      "grad_norm": 0.00026177632389590144,
      "learning_rate": 6.788035662927812e-05,
      "loss": 0.0043,
      "step": 162050
    },
    {
      "epoch": 46.620649985619785,
      "grad_norm": 0.00011746257951017469,
      "learning_rate": 6.759275237273512e-05,
      "loss": 0.0038,
      "step": 162100
    },
    {
      "epoch": 46.635030198446934,
      "grad_norm": 0.002728407271206379,
      "learning_rate": 6.730514811619212e-05,
      "loss": 0.0005,
      "step": 162150
    },
    {
      "epoch": 46.64941041127409,
      "grad_norm": 2.825511728588026e-05,
      "learning_rate": 6.701754385964913e-05,
      "loss": 0.0022,
      "step": 162200
    },
    {
      "epoch": 46.66379062410124,
      "grad_norm": 0.002590401330962777,
      "learning_rate": 6.672993960310613e-05,
      "loss": 0.0045,
      "step": 162250
    },
    {
      "epoch": 46.67817083692839,
      "grad_norm": 5.1173436077078804e-05,
      "learning_rate": 6.644233534656313e-05,
      "loss": 0.0024,
      "step": 162300
    },
    {
      "epoch": 46.69255104975554,
      "grad_norm": 0.002300892025232315,
      "learning_rate": 6.615473109002014e-05,
      "loss": 0.0037,
      "step": 162350
    },
    {
      "epoch": 46.706931262582685,
      "grad_norm": 0.008746683597564697,
      "learning_rate": 6.586712683347714e-05,
      "loss": 0.0025,
      "step": 162400
    },
    {
      "epoch": 46.721311475409834,
      "grad_norm": 0.0006218852940946817,
      "learning_rate": 6.557952257693415e-05,
      "loss": 0.0008,
      "step": 162450
    },
    {
      "epoch": 46.73569168823698,
      "grad_norm": 0.0002596068661659956,
      "learning_rate": 6.529191832039115e-05,
      "loss": 0.0023,
      "step": 162500
    },
    {
      "epoch": 46.75007190106414,
      "grad_norm": 8.896847430150956e-05,
      "learning_rate": 6.500431406384815e-05,
      "loss": 0.0029,
      "step": 162550
    },
    {
      "epoch": 46.76445211389129,
      "grad_norm": 0.001149137387983501,
      "learning_rate": 6.471670980730516e-05,
      "loss": 0.0011,
      "step": 162600
    },
    {
      "epoch": 46.77883232671844,
      "grad_norm": 0.0002654412528499961,
      "learning_rate": 6.442910555076214e-05,
      "loss": 0.0014,
      "step": 162650
    },
    {
      "epoch": 46.793212539545586,
      "grad_norm": 0.000623902480583638,
      "learning_rate": 6.414150129421916e-05,
      "loss": 0.0025,
      "step": 162700
    },
    {
      "epoch": 46.807592752372734,
      "grad_norm": 3.7838744901819155e-05,
      "learning_rate": 6.385389703767617e-05,
      "loss": 0.0072,
      "step": 162750
    },
    {
      "epoch": 46.82197296519988,
      "grad_norm": 4.221166454954073e-05,
      "learning_rate": 6.356629278113315e-05,
      "loss": 0.0027,
      "step": 162800
    },
    {
      "epoch": 46.83635317802703,
      "grad_norm": 9.435931860934943e-05,
      "learning_rate": 6.327868852459017e-05,
      "loss": 0.0025,
      "step": 162850
    },
    {
      "epoch": 46.85073339085419,
      "grad_norm": 0.000455645436886698,
      "learning_rate": 6.299108426804718e-05,
      "loss": 0.0006,
      "step": 162900
    },
    {
      "epoch": 46.86511360368134,
      "grad_norm": 5.1718241593334824e-05,
      "learning_rate": 6.270348001150416e-05,
      "loss": 0.0008,
      "step": 162950
    },
    {
      "epoch": 46.879493816508486,
      "grad_norm": 0.003534081857651472,
      "learning_rate": 6.241587575496118e-05,
      "loss": 0.0013,
      "step": 163000
    },
    {
      "epoch": 46.893874029335635,
      "grad_norm": 0.00010136419587070122,
      "learning_rate": 6.212827149841817e-05,
      "loss": 0.0017,
      "step": 163050
    },
    {
      "epoch": 46.90825424216278,
      "grad_norm": 6.236090121092275e-05,
      "learning_rate": 6.184066724187519e-05,
      "loss": 0.004,
      "step": 163100
    },
    {
      "epoch": 46.92263445498993,
      "grad_norm": 0.00011062825069529936,
      "learning_rate": 6.155306298533219e-05,
      "loss": 0.0013,
      "step": 163150
    },
    {
      "epoch": 46.93701466781708,
      "grad_norm": 4.0520426409784704e-05,
      "learning_rate": 6.126545872878918e-05,
      "loss": 0.0011,
      "step": 163200
    },
    {
      "epoch": 46.95139488064423,
      "grad_norm": 2.3202957891044207e-05,
      "learning_rate": 6.097785447224619e-05,
      "loss": 0.0009,
      "step": 163250
    },
    {
      "epoch": 46.965775093471386,
      "grad_norm": 0.0027009574696421623,
      "learning_rate": 6.0690250215703195e-05,
      "loss": 0.0014,
      "step": 163300
    },
    {
      "epoch": 46.980155306298535,
      "grad_norm": 0.010822856798768044,
      "learning_rate": 6.0402645959160194e-05,
      "loss": 0.0007,
      "step": 163350
    },
    {
      "epoch": 46.994535519125684,
      "grad_norm": 0.009240753948688507,
      "learning_rate": 6.0115041702617206e-05,
      "loss": 0.0005,
      "step": 163400
    },
    {
      "epoch": 47.0,
      "eval_loss": 0.009862087666988373,
      "eval_runtime": 16.7722,
      "eval_samples_per_second": 2845.132,
      "eval_steps_per_second": 44.478,
      "step": 163419
    },
    {
      "epoch": 47.00891573195283,
      "grad_norm": 0.0026577687822282314,
      "learning_rate": 5.9827437446074205e-05,
      "loss": 0.0016,
      "step": 163450
    },
    {
      "epoch": 47.02329594477998,
      "grad_norm": 0.0005507630994543433,
      "learning_rate": 5.9539833189531204e-05,
      "loss": 0.0025,
      "step": 163500
    },
    {
      "epoch": 47.03767615760713,
      "grad_norm": 0.005812689661979675,
      "learning_rate": 5.9252228932988216e-05,
      "loss": 0.0015,
      "step": 163550
    },
    {
      "epoch": 47.05205637043428,
      "grad_norm": 0.003159264801070094,
      "learning_rate": 5.8964624676445215e-05,
      "loss": 0.0027,
      "step": 163600
    },
    {
      "epoch": 47.066436583261435,
      "grad_norm": 0.00028707104502245784,
      "learning_rate": 5.867702041990221e-05,
      "loss": 0.0023,
      "step": 163650
    },
    {
      "epoch": 47.080816796088584,
      "grad_norm": 4.208854807075113e-05,
      "learning_rate": 5.838941616335922e-05,
      "loss": 0.0018,
      "step": 163700
    },
    {
      "epoch": 47.09519700891573,
      "grad_norm": 5.0688086048467085e-05,
      "learning_rate": 5.8101811906816224e-05,
      "loss": 0.0048,
      "step": 163750
    },
    {
      "epoch": 47.10957722174288,
      "grad_norm": 0.05030683055520058,
      "learning_rate": 5.781420765027322e-05,
      "loss": 0.0045,
      "step": 163800
    },
    {
      "epoch": 47.12395743457003,
      "grad_norm": 3.153737270622514e-05,
      "learning_rate": 5.752660339373023e-05,
      "loss": 0.0025,
      "step": 163850
    },
    {
      "epoch": 47.13833764739718,
      "grad_norm": 3.133044083369896e-05,
      "learning_rate": 5.7238999137187234e-05,
      "loss": 0.0009,
      "step": 163900
    },
    {
      "epoch": 47.15271786022433,
      "grad_norm": 4.262228321749717e-05,
      "learning_rate": 5.695139488064423e-05,
      "loss": 0.0011,
      "step": 163950
    },
    {
      "epoch": 47.167098073051484,
      "grad_norm": 0.003087070072069764,
      "learning_rate": 5.666379062410124e-05,
      "loss": 0.0033,
      "step": 164000
    },
    {
      "epoch": 47.18147828587863,
      "grad_norm": 0.0008926308946684003,
      "learning_rate": 5.6376186367558244e-05,
      "loss": 0.0011,
      "step": 164050
    },
    {
      "epoch": 47.19585849870578,
      "grad_norm": 3.8446971302619204e-05,
      "learning_rate": 5.608858211101524e-05,
      "loss": 0.004,
      "step": 164100
    },
    {
      "epoch": 47.21023871153293,
      "grad_norm": 2.8698610549326986e-05,
      "learning_rate": 5.580097785447225e-05,
      "loss": 0.0002,
      "step": 164150
    },
    {
      "epoch": 47.22461892436008,
      "grad_norm": 0.00015422819706145674,
      "learning_rate": 5.5513373597929254e-05,
      "loss": 0.0015,
      "step": 164200
    },
    {
      "epoch": 47.23899913718723,
      "grad_norm": 5.162482557352632e-05,
      "learning_rate": 5.522576934138625e-05,
      "loss": 0.0025,
      "step": 164250
    },
    {
      "epoch": 47.25337935001438,
      "grad_norm": 0.0007526390254497528,
      "learning_rate": 5.493816508484326e-05,
      "loss": 0.0029,
      "step": 164300
    },
    {
      "epoch": 47.26775956284153,
      "grad_norm": 0.00013055048475507647,
      "learning_rate": 5.465056082830026e-05,
      "loss": 0.0034,
      "step": 164350
    },
    {
      "epoch": 47.28213977566868,
      "grad_norm": 0.0024733711034059525,
      "learning_rate": 5.436295657175726e-05,
      "loss": 0.0008,
      "step": 164400
    },
    {
      "epoch": 47.29651998849583,
      "grad_norm": 0.00011030521272914484,
      "learning_rate": 5.407535231521427e-05,
      "loss": 0.0022,
      "step": 164450
    },
    {
      "epoch": 47.31090020132298,
      "grad_norm": 1.847177918534726e-05,
      "learning_rate": 5.3787748058671266e-05,
      "loss": 0.0015,
      "step": 164500
    },
    {
      "epoch": 47.32528041415013,
      "grad_norm": 0.00024816818768158555,
      "learning_rate": 5.350014380212827e-05,
      "loss": 0.0008,
      "step": 164550
    },
    {
      "epoch": 47.33966062697728,
      "grad_norm": 0.01657661236822605,
      "learning_rate": 5.321253954558528e-05,
      "loss": 0.0031,
      "step": 164600
    },
    {
      "epoch": 47.354040839804426,
      "grad_norm": 3.0360104574356228e-05,
      "learning_rate": 5.2924935289042276e-05,
      "loss": 0.003,
      "step": 164650
    },
    {
      "epoch": 47.36842105263158,
      "grad_norm": 0.000595352437812835,
      "learning_rate": 5.263733103249928e-05,
      "loss": 0.0028,
      "step": 164700
    },
    {
      "epoch": 47.38280126545873,
      "grad_norm": 7.979539805091918e-05,
      "learning_rate": 5.234972677595629e-05,
      "loss": 0.0023,
      "step": 164750
    },
    {
      "epoch": 47.39718147828588,
      "grad_norm": 3.8649890484521165e-05,
      "learning_rate": 5.2062122519413286e-05,
      "loss": 0.0028,
      "step": 164800
    },
    {
      "epoch": 47.41156169111303,
      "grad_norm": 0.0035671722143888474,
      "learning_rate": 5.177451826287029e-05,
      "loss": 0.0022,
      "step": 164850
    },
    {
      "epoch": 47.42594190394018,
      "grad_norm": 1.7639551515458152e-05,
      "learning_rate": 5.14869140063273e-05,
      "loss": 0.0008,
      "step": 164900
    },
    {
      "epoch": 47.44032211676733,
      "grad_norm": 0.004638840910047293,
      "learning_rate": 5.1199309749784296e-05,
      "loss": 0.0023,
      "step": 164950
    },
    {
      "epoch": 47.454702329594475,
      "grad_norm": 3.1037307053338736e-05,
      "learning_rate": 5.0911705493241295e-05,
      "loss": 0.0037,
      "step": 165000
    },
    {
      "epoch": 47.46908254242163,
      "grad_norm": 4.1488161514280364e-05,
      "learning_rate": 5.062410123669831e-05,
      "loss": 0.0044,
      "step": 165050
    },
    {
      "epoch": 47.48346275524878,
      "grad_norm": 0.0007030348060652614,
      "learning_rate": 5.0336496980155306e-05,
      "loss": 0.0036,
      "step": 165100
    },
    {
      "epoch": 47.49784296807593,
      "grad_norm": 0.00013179975212551653,
      "learning_rate": 5.0048892723612304e-05,
      "loss": 0.0023,
      "step": 165150
    },
    {
      "epoch": 47.51222318090308,
      "grad_norm": 0.0003727843868546188,
      "learning_rate": 4.976128846706932e-05,
      "loss": 0.001,
      "step": 165200
    },
    {
      "epoch": 47.52660339373023,
      "grad_norm": 3.2277988793794066e-05,
      "learning_rate": 4.9473684210526315e-05,
      "loss": 0.0011,
      "step": 165250
    },
    {
      "epoch": 47.540983606557376,
      "grad_norm": 3.772987474803813e-05,
      "learning_rate": 4.918607995398332e-05,
      "loss": 0.0011,
      "step": 165300
    },
    {
      "epoch": 47.555363819384525,
      "grad_norm": 0.00012649527343455702,
      "learning_rate": 4.8898475697440326e-05,
      "loss": 0.0034,
      "step": 165350
    },
    {
      "epoch": 47.56974403221167,
      "grad_norm": 0.0002146077313227579,
      "learning_rate": 4.8610871440897325e-05,
      "loss": 0.0014,
      "step": 165400
    },
    {
      "epoch": 47.58412424503883,
      "grad_norm": 6.466962076956406e-05,
      "learning_rate": 4.832326718435433e-05,
      "loss": 0.0022,
      "step": 165450
    },
    {
      "epoch": 47.59850445786598,
      "grad_norm": 3.4188939025625587e-05,
      "learning_rate": 4.8035662927811336e-05,
      "loss": 0.0023,
      "step": 165500
    },
    {
      "epoch": 47.61288467069313,
      "grad_norm": 0.0011186389019712806,
      "learning_rate": 4.7748058671268335e-05,
      "loss": 0.0009,
      "step": 165550
    },
    {
      "epoch": 47.627264883520276,
      "grad_norm": 3.923093754565343e-05,
      "learning_rate": 4.746045441472534e-05,
      "loss": 0.003,
      "step": 165600
    },
    {
      "epoch": 47.641645096347425,
      "grad_norm": 5.3039315389469266e-05,
      "learning_rate": 4.7172850158182346e-05,
      "loss": 0.0008,
      "step": 165650
    },
    {
      "epoch": 47.656025309174574,
      "grad_norm": 0.0006914009572938085,
      "learning_rate": 4.6885245901639345e-05,
      "loss": 0.0022,
      "step": 165700
    },
    {
      "epoch": 47.67040552200172,
      "grad_norm": 7.379402086371556e-05,
      "learning_rate": 4.659764164509635e-05,
      "loss": 0.0024,
      "step": 165750
    },
    {
      "epoch": 47.68478573482888,
      "grad_norm": 0.00012452590453904122,
      "learning_rate": 4.631003738855335e-05,
      "loss": 0.0005,
      "step": 165800
    },
    {
      "epoch": 47.69916594765603,
      "grad_norm": 0.00938637275248766,
      "learning_rate": 4.6022433132010354e-05,
      "loss": 0.0007,
      "step": 165850
    },
    {
      "epoch": 47.713546160483176,
      "grad_norm": 0.003273227484896779,
      "learning_rate": 4.573482887546736e-05,
      "loss": 0.0014,
      "step": 165900
    },
    {
      "epoch": 47.727926373310325,
      "grad_norm": 0.004195097833871841,
      "learning_rate": 4.544722461892436e-05,
      "loss": 0.0004,
      "step": 165950
    },
    {
      "epoch": 47.742306586137474,
      "grad_norm": 0.0032431429717689753,
      "learning_rate": 4.5159620362381364e-05,
      "loss": 0.0066,
      "step": 166000
    },
    {
      "epoch": 47.75668679896462,
      "grad_norm": 9.132046398008242e-05,
      "learning_rate": 4.487201610583837e-05,
      "loss": 0.0057,
      "step": 166050
    },
    {
      "epoch": 47.77106701179177,
      "grad_norm": 0.01140831969678402,
      "learning_rate": 4.458441184929537e-05,
      "loss": 0.003,
      "step": 166100
    },
    {
      "epoch": 47.78544722461893,
      "grad_norm": 0.0005469114985316992,
      "learning_rate": 4.4296807592752374e-05,
      "loss": 0.003,
      "step": 166150
    },
    {
      "epoch": 47.799827437446076,
      "grad_norm": 9.038399730343372e-05,
      "learning_rate": 4.400920333620938e-05,
      "loss": 0.0044,
      "step": 166200
    },
    {
      "epoch": 47.814207650273225,
      "grad_norm": 3.806532913586125e-05,
      "learning_rate": 4.372159907966638e-05,
      "loss": 0.0012,
      "step": 166250
    },
    {
      "epoch": 47.828587863100374,
      "grad_norm": 0.0034034326672554016,
      "learning_rate": 4.3433994823123384e-05,
      "loss": 0.0027,
      "step": 166300
    },
    {
      "epoch": 47.84296807592752,
      "grad_norm": 7.63990028644912e-05,
      "learning_rate": 4.314639056658039e-05,
      "loss": 0.0008,
      "step": 166350
    },
    {
      "epoch": 47.85734828875467,
      "grad_norm": 7.268873014254496e-05,
      "learning_rate": 4.285878631003739e-05,
      "loss": 0.0005,
      "step": 166400
    },
    {
      "epoch": 47.87172850158182,
      "grad_norm": 0.0005729033728130162,
      "learning_rate": 4.257118205349439e-05,
      "loss": 0.0005,
      "step": 166450
    },
    {
      "epoch": 47.88610871440898,
      "grad_norm": 0.0010246358579024673,
      "learning_rate": 4.22835777969514e-05,
      "loss": 0.0011,
      "step": 166500
    },
    {
      "epoch": 47.900488927236125,
      "grad_norm": 0.0001616110239410773,
      "learning_rate": 4.19959735404084e-05,
      "loss": 0.0042,
      "step": 166550
    },
    {
      "epoch": 47.914869140063274,
      "grad_norm": 0.00010138576908502728,
      "learning_rate": 4.1708369283865397e-05,
      "loss": 0.0021,
      "step": 166600
    },
    {
      "epoch": 47.92924935289042,
      "grad_norm": 0.0006916172569617629,
      "learning_rate": 4.142076502732241e-05,
      "loss": 0.0031,
      "step": 166650
    },
    {
      "epoch": 47.94362956571757,
      "grad_norm": 0.00027530855732038617,
      "learning_rate": 4.113316077077941e-05,
      "loss": 0.0028,
      "step": 166700
    },
    {
      "epoch": 47.95800977854472,
      "grad_norm": 0.0009404129814356565,
      "learning_rate": 4.0845556514236406e-05,
      "loss": 0.0014,
      "step": 166750
    },
    {
      "epoch": 47.97238999137187,
      "grad_norm": 0.0018724561668932438,
      "learning_rate": 4.055795225769342e-05,
      "loss": 0.0037,
      "step": 166800
    },
    {
      "epoch": 47.986770204199026,
      "grad_norm": 0.00014506025763694197,
      "learning_rate": 4.027034800115042e-05,
      "loss": 0.001,
      "step": 166850
    },
    {
      "epoch": 48.0,
      "eval_loss": 0.00992473866790533,
      "eval_runtime": 17.8139,
      "eval_samples_per_second": 2678.746,
      "eval_steps_per_second": 41.877,
      "step": 166896
    },
    {
      "epoch": 48.001150417026174,
      "grad_norm": 0.00550328753888607,
      "learning_rate": 3.9982743744607416e-05,
      "loss": 0.0021,
      "step": 166900
    },
    {
      "epoch": 48.01553062985332,
      "grad_norm": 1.0759861652331892e-05,
      "learning_rate": 3.969513948806443e-05,
      "loss": 0.0025,
      "step": 166950
    },
    {
      "epoch": 48.02991084268047,
      "grad_norm": 8.934796642279252e-05,
      "learning_rate": 3.940753523152143e-05,
      "loss": 0.0025,
      "step": 167000
    },
    {
      "epoch": 48.04429105550762,
      "grad_norm": 2.119452619808726e-05,
      "learning_rate": 3.911993097497843e-05,
      "loss": 0.0021,
      "step": 167050
    },
    {
      "epoch": 48.05867126833477,
      "grad_norm": 1.7640295482124202e-05,
      "learning_rate": 3.883232671843543e-05,
      "loss": 0.0026,
      "step": 167100
    },
    {
      "epoch": 48.07305148116192,
      "grad_norm": 3.578661926439963e-05,
      "learning_rate": 3.854472246189244e-05,
      "loss": 0.0006,
      "step": 167150
    },
    {
      "epoch": 48.08743169398907,
      "grad_norm": 0.000327468995237723,
      "learning_rate": 3.825711820534944e-05,
      "loss": 0.0029,
      "step": 167200
    },
    {
      "epoch": 48.10181190681622,
      "grad_norm": 0.004436238668859005,
      "learning_rate": 3.796951394880644e-05,
      "loss": 0.0018,
      "step": 167250
    },
    {
      "epoch": 48.11619211964337,
      "grad_norm": 0.006374571938067675,
      "learning_rate": 3.768190969226345e-05,
      "loss": 0.0027,
      "step": 167300
    },
    {
      "epoch": 48.13057233247052,
      "grad_norm": 0.019381830468773842,
      "learning_rate": 3.739430543572045e-05,
      "loss": 0.0007,
      "step": 167350
    },
    {
      "epoch": 48.14495254529767,
      "grad_norm": 0.0006427211337722838,
      "learning_rate": 3.710670117917745e-05,
      "loss": 0.0021,
      "step": 167400
    },
    {
      "epoch": 48.15933275812482,
      "grad_norm": 1.16026567411609e-05,
      "learning_rate": 3.6819096922634457e-05,
      "loss": 0.0012,
      "step": 167450
    },
    {
      "epoch": 48.17371297095197,
      "grad_norm": 8.498729584971443e-05,
      "learning_rate": 3.653149266609146e-05,
      "loss": 0.0009,
      "step": 167500
    },
    {
      "epoch": 48.18809318377912,
      "grad_norm": 0.0007468818221241236,
      "learning_rate": 3.624388840954846e-05,
      "loss": 0.0025,
      "step": 167550
    },
    {
      "epoch": 48.20247339660627,
      "grad_norm": 0.00013960737851448357,
      "learning_rate": 3.5956284153005466e-05,
      "loss": 0.0036,
      "step": 167600
    },
    {
      "epoch": 48.21685360943342,
      "grad_norm": 2.9991673727636226e-05,
      "learning_rate": 3.566867989646247e-05,
      "loss": 0.0022,
      "step": 167650
    },
    {
      "epoch": 48.23123382226057,
      "grad_norm": 5.325574420567136e-06,
      "learning_rate": 3.538107563991947e-05,
      "loss": 0.0036,
      "step": 167700
    },
    {
      "epoch": 48.24561403508772,
      "grad_norm": 0.007938609458506107,
      "learning_rate": 3.509347138337647e-05,
      "loss": 0.0025,
      "step": 167750
    },
    {
      "epoch": 48.25999424791487,
      "grad_norm": 7.622780685778707e-05,
      "learning_rate": 3.480586712683348e-05,
      "loss": 0.0034,
      "step": 167800
    },
    {
      "epoch": 48.27437446074202,
      "grad_norm": 0.009765107184648514,
      "learning_rate": 3.451826287029048e-05,
      "loss": 0.0031,
      "step": 167850
    },
    {
      "epoch": 48.288754673569166,
      "grad_norm": 3.753106284420937e-05,
      "learning_rate": 3.423065861374748e-05,
      "loss": 0.0025,
      "step": 167900
    },
    {
      "epoch": 48.30313488639632,
      "grad_norm": 5.171943485038355e-05,
      "learning_rate": 3.394305435720449e-05,
      "loss": 0.0015,
      "step": 167950
    },
    {
      "epoch": 48.31751509922347,
      "grad_norm": 2.957896504085511e-05,
      "learning_rate": 3.365545010066149e-05,
      "loss": 0.0023,
      "step": 168000
    },
    {
      "epoch": 48.33189531205062,
      "grad_norm": 0.00010975551413139328,
      "learning_rate": 3.336784584411849e-05,
      "loss": 0.0004,
      "step": 168050
    },
    {
      "epoch": 48.34627552487777,
      "grad_norm": 0.00017966835002880543,
      "learning_rate": 3.30802415875755e-05,
      "loss": 0.0031,
      "step": 168100
    },
    {
      "epoch": 48.36065573770492,
      "grad_norm": 0.007616202812641859,
      "learning_rate": 3.27926373310325e-05,
      "loss": 0.0044,
      "step": 168150
    },
    {
      "epoch": 48.375035950532066,
      "grad_norm": 0.0060389237478375435,
      "learning_rate": 3.25050330744895e-05,
      "loss": 0.0016,
      "step": 168200
    },
    {
      "epoch": 48.389416163359215,
      "grad_norm": 0.019030466675758362,
      "learning_rate": 3.221742881794651e-05,
      "loss": 0.0023,
      "step": 168250
    },
    {
      "epoch": 48.40379637618637,
      "grad_norm": 6.328146992018446e-05,
      "learning_rate": 3.192982456140351e-05,
      "loss": 0.001,
      "step": 168300
    },
    {
      "epoch": 48.41817658901352,
      "grad_norm": 3.9644633943680674e-05,
      "learning_rate": 3.164222030486051e-05,
      "loss": 0.0027,
      "step": 168350
    },
    {
      "epoch": 48.43255680184067,
      "grad_norm": 0.003058341797441244,
      "learning_rate": 3.135461604831752e-05,
      "loss": 0.002,
      "step": 168400
    },
    {
      "epoch": 48.44693701466782,
      "grad_norm": 4.7939294745447114e-05,
      "learning_rate": 3.106701179177452e-05,
      "loss": 0.0019,
      "step": 168450
    },
    {
      "epoch": 48.461317227494966,
      "grad_norm": 0.002541725756600499,
      "learning_rate": 3.077940753523152e-05,
      "loss": 0.0025,
      "step": 168500
    },
    {
      "epoch": 48.475697440322115,
      "grad_norm": 1.94336062122602e-05,
      "learning_rate": 3.0491803278688527e-05,
      "loss": 0.0017,
      "step": 168550
    },
    {
      "epoch": 48.490077653149264,
      "grad_norm": 0.002529675839468837,
      "learning_rate": 3.020419902214553e-05,
      "loss": 0.0043,
      "step": 168600
    },
    {
      "epoch": 48.50445786597642,
      "grad_norm": 2.6979183530784212e-05,
      "learning_rate": 2.991659476560253e-05,
      "loss": 0.004,
      "step": 168650
    },
    {
      "epoch": 48.51883807880357,
      "grad_norm": 0.0006658209022134542,
      "learning_rate": 2.9628990509059537e-05,
      "loss": 0.0032,
      "step": 168700
    },
    {
      "epoch": 48.53321829163072,
      "grad_norm": 8.038980013225228e-05,
      "learning_rate": 2.9341386252516536e-05,
      "loss": 0.0006,
      "step": 168750
    },
    {
      "epoch": 48.547598504457866,
      "grad_norm": 5.2736912039108574e-05,
      "learning_rate": 2.905378199597354e-05,
      "loss": 0.004,
      "step": 168800
    },
    {
      "epoch": 48.561978717285015,
      "grad_norm": 0.00016564014367759228,
      "learning_rate": 2.8766177739430547e-05,
      "loss": 0.0035,
      "step": 168850
    },
    {
      "epoch": 48.576358930112164,
      "grad_norm": 0.0020988713949918747,
      "learning_rate": 2.8478573482887545e-05,
      "loss": 0.0024,
      "step": 168900
    },
    {
      "epoch": 48.59073914293931,
      "grad_norm": 0.0003453914832789451,
      "learning_rate": 2.819096922634455e-05,
      "loss": 0.0019,
      "step": 168950
    },
    {
      "epoch": 48.60511935576646,
      "grad_norm": 0.00010111170558957383,
      "learning_rate": 2.7903364969801553e-05,
      "loss": 0.0022,
      "step": 169000
    },
    {
      "epoch": 48.61949956859362,
      "grad_norm": 0.0042457436211407185,
      "learning_rate": 2.7615760713258555e-05,
      "loss": 0.0039,
      "step": 169050
    },
    {
      "epoch": 48.63387978142077,
      "grad_norm": 0.014988655224442482,
      "learning_rate": 2.732815645671556e-05,
      "loss": 0.002,
      "step": 169100
    },
    {
      "epoch": 48.648259994247915,
      "grad_norm": 0.0005865705898031592,
      "learning_rate": 2.7040552200172563e-05,
      "loss": 0.0021,
      "step": 169150
    },
    {
      "epoch": 48.662640207075064,
      "grad_norm": 0.0024593553971499205,
      "learning_rate": 2.6752947943629565e-05,
      "loss": 0.0047,
      "step": 169200
    },
    {
      "epoch": 48.67702041990221,
      "grad_norm": 2.7971373128821142e-05,
      "learning_rate": 2.646534368708657e-05,
      "loss": 0.0019,
      "step": 169250
    },
    {
      "epoch": 48.69140063272936,
      "grad_norm": 7.638907118234783e-05,
      "learning_rate": 2.6177739430543573e-05,
      "loss": 0.0013,
      "step": 169300
    },
    {
      "epoch": 48.70578084555651,
      "grad_norm": 6.728625885443762e-05,
      "learning_rate": 2.5890135174000578e-05,
      "loss": 0.001,
      "step": 169350
    },
    {
      "epoch": 48.72016105838367,
      "grad_norm": 1.9345146938576363e-05,
      "learning_rate": 2.5602530917457577e-05,
      "loss": 0.0041,
      "step": 169400
    },
    {
      "epoch": 48.734541271210816,
      "grad_norm": 0.00024213010328821838,
      "learning_rate": 2.5314926660914582e-05,
      "loss": 0.0008,
      "step": 169450
    },
    {
      "epoch": 48.748921484037965,
      "grad_norm": 0.0007118489011190832,
      "learning_rate": 2.5027322404371588e-05,
      "loss": 0.0011,
      "step": 169500
    },
    {
      "epoch": 48.76330169686511,
      "grad_norm": 0.00025662334519438446,
      "learning_rate": 2.4739718147828587e-05,
      "loss": 0.0019,
      "step": 169550
    },
    {
      "epoch": 48.77768190969226,
      "grad_norm": 0.007332118693739176,
      "learning_rate": 2.4452113891285592e-05,
      "loss": 0.0032,
      "step": 169600
    },
    {
      "epoch": 48.79206212251941,
      "grad_norm": 3.435317557887174e-05,
      "learning_rate": 2.4164509634742594e-05,
      "loss": 0.0039,
      "step": 169650
    },
    {
      "epoch": 48.80644233534656,
      "grad_norm": 0.0045635998249053955,
      "learning_rate": 2.3876905378199596e-05,
      "loss": 0.0019,
      "step": 169700
    },
    {
      "epoch": 48.820822548173716,
      "grad_norm": 0.0006849297787994146,
      "learning_rate": 2.3589301121656602e-05,
      "loss": 0.0014,
      "step": 169750
    },
    {
      "epoch": 48.835202761000865,
      "grad_norm": 0.00012209573469590396,
      "learning_rate": 2.3301696865113604e-05,
      "loss": 0.0005,
      "step": 169800
    },
    {
      "epoch": 48.849582973828014,
      "grad_norm": 7.563168765045702e-05,
      "learning_rate": 2.3014092608570606e-05,
      "loss": 0.0013,
      "step": 169850
    },
    {
      "epoch": 48.86396318665516,
      "grad_norm": 0.0009209151612594724,
      "learning_rate": 2.272648835202761e-05,
      "loss": 0.0012,
      "step": 169900
    },
    {
      "epoch": 48.87834339948231,
      "grad_norm": 5.1632803661050275e-05,
      "learning_rate": 2.2438884095484614e-05,
      "loss": 0.0021,
      "step": 169950
    },
    {
      "epoch": 48.89272361230946,
      "grad_norm": 0.0001412262354278937,
      "learning_rate": 2.2151279838941616e-05,
      "loss": 0.0031,
      "step": 170000
    },
    {
      "epoch": 48.90710382513661,
      "grad_norm": 0.0014209043001756072,
      "learning_rate": 2.1863675582398618e-05,
      "loss": 0.0007,
      "step": 170050
    },
    {
      "epoch": 48.921484037963765,
      "grad_norm": 0.003228056011721492,
      "learning_rate": 2.1576071325855624e-05,
      "loss": 0.0005,
      "step": 170100
    },
    {
      "epoch": 48.935864250790914,
      "grad_norm": 8.672486728755757e-05,
      "learning_rate": 2.1288467069312626e-05,
      "loss": 0.0028,
      "step": 170150
    },
    {
      "epoch": 48.95024446361806,
      "grad_norm": 0.0010426067747175694,
      "learning_rate": 2.1000862812769628e-05,
      "loss": 0.0021,
      "step": 170200
    },
    {
      "epoch": 48.96462467644521,
      "grad_norm": 0.0007964252727106214,
      "learning_rate": 2.0713258556226633e-05,
      "loss": 0.0016,
      "step": 170250
    },
    {
      "epoch": 48.97900488927236,
      "grad_norm": 0.000883535947650671,
      "learning_rate": 2.042565429968364e-05,
      "loss": 0.0019,
      "step": 170300
    },
    {
      "epoch": 48.99338510209951,
      "grad_norm": 9.064165351446718e-05,
      "learning_rate": 2.0138050043140638e-05,
      "loss": 0.004,
      "step": 170350
    },
    {
      "epoch": 49.0,
      "eval_loss": 0.009867902845144272,
      "eval_runtime": 17.0577,
      "eval_samples_per_second": 2797.506,
      "eval_steps_per_second": 43.734,
      "step": 170373
    }
  ],
  "logging_steps": 50,
  "max_steps": 173850,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 10,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 9
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0798489139085312e+17,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
