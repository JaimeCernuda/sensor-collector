{
  "best_global_step": 146034,
  "best_metric": 0.009875989519059658,
  "best_model_checkpoint": "/content/sensor-collector/tick2/notebooks/output/03/granite_ttm_ft/combined/E2_mix10/combined/checkpoints/checkpoint-146034",
  "epoch": 48.0,
  "eval_steps": 500,
  "global_step": 166896,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014380212827149842,
      "grad_norm": 0.0004645278968382627,
      "learning_rate": 0.0009997181478285878,
      "loss": 0.005,
      "step": 50
    },
    {
      "epoch": 0.028760425654299683,
      "grad_norm": 0.00017443571414332837,
      "learning_rate": 0.000999430543572045,
      "loss": 0.0044,
      "step": 100
    },
    {
      "epoch": 0.04314063848144953,
      "grad_norm": 0.006598859094083309,
      "learning_rate": 0.0009991429393155019,
      "loss": 0.0015,
      "step": 150
    },
    {
      "epoch": 0.057520851308599366,
      "grad_norm": 0.00025801430456340313,
      "learning_rate": 0.000998855335058959,
      "loss": 0.0023,
      "step": 200
    },
    {
      "epoch": 0.0719010641357492,
      "grad_norm": 0.00015968471416272223,
      "learning_rate": 0.0009985677308024159,
      "loss": 0.0035,
      "step": 250
    },
    {
      "epoch": 0.08628127696289906,
      "grad_norm": 0.0004316717095207423,
      "learning_rate": 0.0009982801265458728,
      "loss": 0.0025,
      "step": 300
    },
    {
      "epoch": 0.1006614897900489,
      "grad_norm": 7.931546133477241e-05,
      "learning_rate": 0.00099799252228933,
      "loss": 0.0031,
      "step": 350
    },
    {
      "epoch": 0.11504170261719873,
      "grad_norm": 0.00031458737794309855,
      "learning_rate": 0.000997704918032787,
      "loss": 0.0049,
      "step": 400
    },
    {
      "epoch": 0.12942191544434858,
      "grad_norm": 6.881597801111639e-05,
      "learning_rate": 0.000997417313776244,
      "loss": 0.0015,
      "step": 450
    },
    {
      "epoch": 0.1438021282714984,
      "grad_norm": 0.00012128299567848444,
      "learning_rate": 0.000997129709519701,
      "loss": 0.0021,
      "step": 500
    },
    {
      "epoch": 0.15818234109864826,
      "grad_norm": 0.00025060473126359284,
      "learning_rate": 0.000996842105263158,
      "loss": 0.0039,
      "step": 550
    },
    {
      "epoch": 0.1725625539257981,
      "grad_norm": 0.010127431713044643,
      "learning_rate": 0.0009965545010066149,
      "loss": 0.0025,
      "step": 600
    },
    {
      "epoch": 0.18694276675294794,
      "grad_norm": 0.05218309536576271,
      "learning_rate": 0.000996266896750072,
      "loss": 0.0014,
      "step": 650
    },
    {
      "epoch": 0.2013229795800978,
      "grad_norm": 7.192770135588944e-05,
      "learning_rate": 0.0009959792924935289,
      "loss": 0.0021,
      "step": 700
    },
    {
      "epoch": 0.21570319240724764,
      "grad_norm": 0.0001761617895681411,
      "learning_rate": 0.000995691688236986,
      "loss": 0.0022,
      "step": 750
    },
    {
      "epoch": 0.23008340523439746,
      "grad_norm": 8.125503518385813e-05,
      "learning_rate": 0.000995404083980443,
      "loss": 0.0029,
      "step": 800
    },
    {
      "epoch": 0.24446361806154732,
      "grad_norm": 0.004570275079458952,
      "learning_rate": 0.0009951164797239,
      "loss": 0.0019,
      "step": 850
    },
    {
      "epoch": 0.25884383088869717,
      "grad_norm": 0.00015719204384367913,
      "learning_rate": 0.000994828875467357,
      "loss": 0.003,
      "step": 900
    },
    {
      "epoch": 0.273224043715847,
      "grad_norm": 0.000117039933684282,
      "learning_rate": 0.000994541271210814,
      "loss": 0.0048,
      "step": 950
    },
    {
      "epoch": 0.2876042565429968,
      "grad_norm": 0.0001585542195243761,
      "learning_rate": 0.000994253666954271,
      "loss": 0.0041,
      "step": 1000
    },
    {
      "epoch": 0.30198446937014667,
      "grad_norm": 0.00013749077334068716,
      "learning_rate": 0.0009939660626977279,
      "loss": 0.0014,
      "step": 1050
    },
    {
      "epoch": 0.3163646821972965,
      "grad_norm": 0.0002008722658501938,
      "learning_rate": 0.000993678458441185,
      "loss": 0.0058,
      "step": 1100
    },
    {
      "epoch": 0.3307448950244464,
      "grad_norm": 3.4783144656103104e-05,
      "learning_rate": 0.0009933908541846419,
      "loss": 0.0028,
      "step": 1150
    },
    {
      "epoch": 0.3451251078515962,
      "grad_norm": 0.09211280196905136,
      "learning_rate": 0.000993103249928099,
      "loss": 0.0064,
      "step": 1200
    },
    {
      "epoch": 0.359505320678746,
      "grad_norm": 0.004378116223961115,
      "learning_rate": 0.000992815645671556,
      "loss": 0.0027,
      "step": 1250
    },
    {
      "epoch": 0.3738855335058959,
      "grad_norm": 0.00012154143769294024,
      "learning_rate": 0.000992528041415013,
      "loss": 0.0044,
      "step": 1300
    },
    {
      "epoch": 0.3882657463330457,
      "grad_norm": 0.009976482018828392,
      "learning_rate": 0.00099224043715847,
      "loss": 0.0026,
      "step": 1350
    },
    {
      "epoch": 0.4026459591601956,
      "grad_norm": 3.967104930779897e-05,
      "learning_rate": 0.000991952832901927,
      "loss": 0.0045,
      "step": 1400
    },
    {
      "epoch": 0.41702617198734543,
      "grad_norm": 0.0014515718212351203,
      "learning_rate": 0.000991665228645384,
      "loss": 0.0068,
      "step": 1450
    },
    {
      "epoch": 0.4314063848144953,
      "grad_norm": 0.004946050699800253,
      "learning_rate": 0.0009913776243888409,
      "loss": 0.0017,
      "step": 1500
    },
    {
      "epoch": 0.4457865976416451,
      "grad_norm": 4.7031939175212756e-05,
      "learning_rate": 0.000991090020132298,
      "loss": 0.0048,
      "step": 1550
    },
    {
      "epoch": 0.46016681046879493,
      "grad_norm": 0.023464491590857506,
      "learning_rate": 0.000990802415875755,
      "loss": 0.0035,
      "step": 1600
    },
    {
      "epoch": 0.4745470232959448,
      "grad_norm": 5.5546759540447965e-05,
      "learning_rate": 0.000990514811619212,
      "loss": 0.0035,
      "step": 1650
    },
    {
      "epoch": 0.48892723612309463,
      "grad_norm": 0.00023582389985676855,
      "learning_rate": 0.0009902272073626691,
      "loss": 0.0017,
      "step": 1700
    },
    {
      "epoch": 0.5033074489502445,
      "grad_norm": 0.0001285228063352406,
      "learning_rate": 0.000989939603106126,
      "loss": 0.0048,
      "step": 1750
    },
    {
      "epoch": 0.5176876617773943,
      "grad_norm": 0.0001780017337296158,
      "learning_rate": 0.000989651998849583,
      "loss": 0.0024,
      "step": 1800
    },
    {
      "epoch": 0.5320678746045442,
      "grad_norm": 0.020831452682614326,
      "learning_rate": 0.00098936439459304,
      "loss": 0.0038,
      "step": 1850
    },
    {
      "epoch": 0.546448087431694,
      "grad_norm": 0.0005455532809719443,
      "learning_rate": 0.000989076790336497,
      "loss": 0.0027,
      "step": 1900
    },
    {
      "epoch": 0.5608283002588438,
      "grad_norm": 9.521095489617437e-05,
      "learning_rate": 0.000988789186079954,
      "loss": 0.0016,
      "step": 1950
    },
    {
      "epoch": 0.5752085130859936,
      "grad_norm": 0.00015995903231669217,
      "learning_rate": 0.000988501581823411,
      "loss": 0.004,
      "step": 2000
    },
    {
      "epoch": 0.5895887259131435,
      "grad_norm": 0.0003183459339197725,
      "learning_rate": 0.000988213977566868,
      "loss": 0.0011,
      "step": 2050
    },
    {
      "epoch": 0.6039689387402933,
      "grad_norm": 0.04875005781650543,
      "learning_rate": 0.000987926373310325,
      "loss": 0.0018,
      "step": 2100
    },
    {
      "epoch": 0.6183491515674432,
      "grad_norm": 0.0005918768001720309,
      "learning_rate": 0.0009876387690537821,
      "loss": 0.0045,
      "step": 2150
    },
    {
      "epoch": 0.632729364394593,
      "grad_norm": 0.028055893257260323,
      "learning_rate": 0.000987351164797239,
      "loss": 0.0023,
      "step": 2200
    },
    {
      "epoch": 0.6471095772217429,
      "grad_norm": 0.014095992781221867,
      "learning_rate": 0.000987063560540696,
      "loss": 0.0048,
      "step": 2250
    },
    {
      "epoch": 0.6614897900488927,
      "grad_norm": 0.0014221088495105505,
      "learning_rate": 0.000986775956284153,
      "loss": 0.0032,
      "step": 2300
    },
    {
      "epoch": 0.6758700028760426,
      "grad_norm": 0.005517377983778715,
      "learning_rate": 0.00098648835202761,
      "loss": 0.0039,
      "step": 2350
    },
    {
      "epoch": 0.6902502157031924,
      "grad_norm": 0.010765179991722107,
      "learning_rate": 0.000986200747771067,
      "loss": 0.0016,
      "step": 2400
    },
    {
      "epoch": 0.7046304285303423,
      "grad_norm": 0.00019922538194805384,
      "learning_rate": 0.000985913143514524,
      "loss": 0.0055,
      "step": 2450
    },
    {
      "epoch": 0.719010641357492,
      "grad_norm": 0.16866596043109894,
      "learning_rate": 0.000985625539257981,
      "loss": 0.0036,
      "step": 2500
    },
    {
      "epoch": 0.7333908541846419,
      "grad_norm": 0.657078742980957,
      "learning_rate": 0.000985337935001438,
      "loss": 0.0025,
      "step": 2550
    },
    {
      "epoch": 0.7477710670117917,
      "grad_norm": 0.0021171625703573227,
      "learning_rate": 0.0009850503307448951,
      "loss": 0.0033,
      "step": 2600
    },
    {
      "epoch": 0.7621512798389416,
      "grad_norm": 0.17026107013225555,
      "learning_rate": 0.000984762726488352,
      "loss": 0.003,
      "step": 2650
    },
    {
      "epoch": 0.7765314926660914,
      "grad_norm": 0.053517185151576996,
      "learning_rate": 0.000984475122231809,
      "loss": 0.0052,
      "step": 2700
    },
    {
      "epoch": 0.7909117054932413,
      "grad_norm": 0.15468184649944305,
      "learning_rate": 0.000984187517975266,
      "loss": 0.003,
      "step": 2750
    },
    {
      "epoch": 0.8052919183203912,
      "grad_norm": 0.013220653869211674,
      "learning_rate": 0.000983899913718723,
      "loss": 0.0015,
      "step": 2800
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 0.00021985414787195623,
      "learning_rate": 0.00098361230946218,
      "loss": 0.0037,
      "step": 2850
    },
    {
      "epoch": 0.8340523439746909,
      "grad_norm": 0.0003038847935386002,
      "learning_rate": 0.0009833247052056372,
      "loss": 0.006,
      "step": 2900
    },
    {
      "epoch": 0.8484325568018407,
      "grad_norm": 0.00044594579958356917,
      "learning_rate": 0.0009830371009490941,
      "loss": 0.0026,
      "step": 2950
    },
    {
      "epoch": 0.8628127696289906,
      "grad_norm": 0.04814935103058815,
      "learning_rate": 0.000982749496692551,
      "loss": 0.0031,
      "step": 3000
    },
    {
      "epoch": 0.8771929824561403,
      "grad_norm": 0.010973446071147919,
      "learning_rate": 0.0009824618924360081,
      "loss": 0.0016,
      "step": 3050
    },
    {
      "epoch": 0.8915731952832902,
      "grad_norm": 0.00012747017899528146,
      "learning_rate": 0.000982174288179465,
      "loss": 0.0025,
      "step": 3100
    },
    {
      "epoch": 0.90595340811044,
      "grad_norm": 0.0005981738795526326,
      "learning_rate": 0.0009818866839229222,
      "loss": 0.0013,
      "step": 3150
    },
    {
      "epoch": 0.9203336209375899,
      "grad_norm": 0.00862814113497734,
      "learning_rate": 0.000981599079666379,
      "loss": 0.0034,
      "step": 3200
    },
    {
      "epoch": 0.9347138337647397,
      "grad_norm": 6.409685011021793e-05,
      "learning_rate": 0.0009813114754098362,
      "loss": 0.0025,
      "step": 3250
    },
    {
      "epoch": 0.9490940465918896,
      "grad_norm": 0.01610773615539074,
      "learning_rate": 0.000981023871153293,
      "loss": 0.0024,
      "step": 3300
    },
    {
      "epoch": 0.9634742594190394,
      "grad_norm": 0.00011530835035955533,
      "learning_rate": 0.0009807362668967502,
      "loss": 0.0009,
      "step": 3350
    },
    {
      "epoch": 0.9778544722461893,
      "grad_norm": 0.0004547621065285057,
      "learning_rate": 0.0009804486626402071,
      "loss": 0.0056,
      "step": 3400
    },
    {
      "epoch": 0.9922346850733391,
      "grad_norm": 0.05197335407137871,
      "learning_rate": 0.000980161058383664,
      "loss": 0.003,
      "step": 3450
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.011443640105426311,
      "eval_runtime": 16.5989,
      "eval_samples_per_second": 2874.83,
      "eval_steps_per_second": 44.943,
      "step": 3477
    },
    {
      "epoch": 1.006614897900489,
      "grad_norm": 0.0069944304414093494,
      "learning_rate": 0.0009798734541271211,
      "loss": 0.0035,
      "step": 3500
    },
    {
      "epoch": 1.0209951107276387,
      "grad_norm": 0.23555466532707214,
      "learning_rate": 0.000979585849870578,
      "loss": 0.0025,
      "step": 3550
    },
    {
      "epoch": 1.0353753235547887,
      "grad_norm": 0.02587115950882435,
      "learning_rate": 0.0009792982456140352,
      "loss": 0.0069,
      "step": 3600
    },
    {
      "epoch": 1.0497555363819384,
      "grad_norm": 0.00013538607163354754,
      "learning_rate": 0.000979010641357492,
      "loss": 0.0025,
      "step": 3650
    },
    {
      "epoch": 1.0641357492090884,
      "grad_norm": 0.03679618239402771,
      "learning_rate": 0.0009787230371009492,
      "loss": 0.0022,
      "step": 3700
    },
    {
      "epoch": 1.0785159620362381,
      "grad_norm": 0.0016495944000780582,
      "learning_rate": 0.000978435432844406,
      "loss": 0.0034,
      "step": 3750
    },
    {
      "epoch": 1.092896174863388,
      "grad_norm": 0.00032777435262687504,
      "learning_rate": 0.0009781478285878632,
      "loss": 0.0048,
      "step": 3800
    },
    {
      "epoch": 1.1072763876905378,
      "grad_norm": 0.0001833887945394963,
      "learning_rate": 0.0009778602243313201,
      "loss": 0.0018,
      "step": 3850
    },
    {
      "epoch": 1.1216566005176876,
      "grad_norm": 0.024774154648184776,
      "learning_rate": 0.000977572620074777,
      "loss": 0.0041,
      "step": 3900
    },
    {
      "epoch": 1.1360368133448375,
      "grad_norm": 0.009209041483700275,
      "learning_rate": 0.0009772850158182341,
      "loss": 0.0042,
      "step": 3950
    },
    {
      "epoch": 1.1504170261719873,
      "grad_norm": 9.22381950658746e-05,
      "learning_rate": 0.000976997411561691,
      "loss": 0.0056,
      "step": 4000
    },
    {
      "epoch": 1.1647972389991372,
      "grad_norm": 6.670633592875674e-05,
      "learning_rate": 0.0009767098073051482,
      "loss": 0.0029,
      "step": 4050
    },
    {
      "epoch": 1.179177451826287,
      "grad_norm": 0.041408196091651917,
      "learning_rate": 0.0009764222030486052,
      "loss": 0.0014,
      "step": 4100
    },
    {
      "epoch": 1.193557664653437,
      "grad_norm": 0.00012231220898684114,
      "learning_rate": 0.0009761345987920621,
      "loss": 0.0022,
      "step": 4150
    },
    {
      "epoch": 1.2079378774805867,
      "grad_norm": 0.02373529225587845,
      "learning_rate": 0.0009758469945355192,
      "loss": 0.0023,
      "step": 4200
    },
    {
      "epoch": 1.2223180903077366,
      "grad_norm": 0.00010445462976349518,
      "learning_rate": 0.0009755593902789761,
      "loss": 0.0029,
      "step": 4250
    },
    {
      "epoch": 1.2366983031348864,
      "grad_norm": 0.03232591971755028,
      "learning_rate": 0.0009752717860224331,
      "loss": 0.005,
      "step": 4300
    },
    {
      "epoch": 1.2510785159620363,
      "grad_norm": 0.08373481780290604,
      "learning_rate": 0.0009749841817658902,
      "loss": 0.0011,
      "step": 4350
    },
    {
      "epoch": 1.265458728789186,
      "grad_norm": 0.06530682742595673,
      "learning_rate": 0.0009746965775093471,
      "loss": 0.002,
      "step": 4400
    },
    {
      "epoch": 1.2798389416163358,
      "grad_norm": 0.041549716144800186,
      "learning_rate": 0.0009744089732528042,
      "loss": 0.0048,
      "step": 4450
    },
    {
      "epoch": 1.2942191544434858,
      "grad_norm": 0.13707087934017181,
      "learning_rate": 0.0009741213689962612,
      "loss": 0.0052,
      "step": 4500
    },
    {
      "epoch": 1.3085993672706355,
      "grad_norm": 3.927255966118537e-05,
      "learning_rate": 0.0009738337647397182,
      "loss": 0.0042,
      "step": 4550
    },
    {
      "epoch": 1.3229795800977855,
      "grad_norm": 0.0011760826455429196,
      "learning_rate": 0.0009735461604831751,
      "loss": 0.0054,
      "step": 4600
    },
    {
      "epoch": 1.3373597929249352,
      "grad_norm": 0.14562298357486725,
      "learning_rate": 0.0009732585562266322,
      "loss": 0.0021,
      "step": 4650
    },
    {
      "epoch": 1.3517400057520852,
      "grad_norm": 0.011173006147146225,
      "learning_rate": 0.0009729709519700892,
      "loss": 0.0016,
      "step": 4700
    },
    {
      "epoch": 1.366120218579235,
      "grad_norm": 0.017169460654258728,
      "learning_rate": 0.0009726833477135461,
      "loss": 0.0029,
      "step": 4750
    },
    {
      "epoch": 1.380500431406385,
      "grad_norm": 0.04352440685033798,
      "learning_rate": 0.0009723957434570032,
      "loss": 0.0039,
      "step": 4800
    },
    {
      "epoch": 1.3948806442335346,
      "grad_norm": 0.02980763278901577,
      "learning_rate": 0.0009721081392004601,
      "loss": 0.0054,
      "step": 4850
    },
    {
      "epoch": 1.4092608570606844,
      "grad_norm": 0.0001104683760786429,
      "learning_rate": 0.0009718205349439172,
      "loss": 0.0044,
      "step": 4900
    },
    {
      "epoch": 1.4236410698878343,
      "grad_norm": 0.007215824909508228,
      "learning_rate": 0.0009715329306873742,
      "loss": 0.0038,
      "step": 4950
    },
    {
      "epoch": 1.4380212827149843,
      "grad_norm": 0.014527317136526108,
      "learning_rate": 0.0009712453264308312,
      "loss": 0.0012,
      "step": 5000
    },
    {
      "epoch": 1.452401495542134,
      "grad_norm": 0.00014968168397899717,
      "learning_rate": 0.0009709577221742882,
      "loss": 0.0018,
      "step": 5050
    },
    {
      "epoch": 1.4667817083692838,
      "grad_norm": 0.00010361991735408083,
      "learning_rate": 0.0009706701179177452,
      "loss": 0.0043,
      "step": 5100
    },
    {
      "epoch": 1.4811619211964338,
      "grad_norm": 0.0003430013020988554,
      "learning_rate": 0.0009703825136612022,
      "loss": 0.0014,
      "step": 5150
    },
    {
      "epoch": 1.4955421340235835,
      "grad_norm": 0.0003510715323500335,
      "learning_rate": 0.0009700949094046591,
      "loss": 0.0032,
      "step": 5200
    },
    {
      "epoch": 1.5099223468507335,
      "grad_norm": 0.00013603027036879212,
      "learning_rate": 0.0009698073051481162,
      "loss": 0.0036,
      "step": 5250
    },
    {
      "epoch": 1.5243025596778832,
      "grad_norm": 0.00022688094759359956,
      "learning_rate": 0.0009695197008915733,
      "loss": 0.0033,
      "step": 5300
    },
    {
      "epoch": 1.538682772505033,
      "grad_norm": 0.0003723703557625413,
      "learning_rate": 0.0009692320966350302,
      "loss": 0.0012,
      "step": 5350
    },
    {
      "epoch": 1.553062985332183,
      "grad_norm": 0.0001117868087021634,
      "learning_rate": 0.0009689444923784873,
      "loss": 0.0033,
      "step": 5400
    },
    {
      "epoch": 1.5674431981593329,
      "grad_norm": 0.07585125416517258,
      "learning_rate": 0.0009686568881219442,
      "loss": 0.0036,
      "step": 5450
    },
    {
      "epoch": 1.5818234109864826,
      "grad_norm": 0.0004453353467397392,
      "learning_rate": 0.0009683692838654012,
      "loss": 0.005,
      "step": 5500
    },
    {
      "epoch": 1.5962036238136323,
      "grad_norm": 0.0015707510756328702,
      "learning_rate": 0.0009680816796088582,
      "loss": 0.0038,
      "step": 5550
    },
    {
      "epoch": 1.6105838366407823,
      "grad_norm": 0.014351509511470795,
      "learning_rate": 0.0009677940753523152,
      "loss": 0.0049,
      "step": 5600
    },
    {
      "epoch": 1.6249640494679323,
      "grad_norm": 0.00014813485904596746,
      "learning_rate": 0.0009675064710957722,
      "loss": 0.0017,
      "step": 5650
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 0.1371903419494629,
      "learning_rate": 0.0009672188668392292,
      "loss": 0.0061,
      "step": 5700
    },
    {
      "epoch": 1.6537244751222318,
      "grad_norm": 0.00020283831690903753,
      "learning_rate": 0.0009669312625826863,
      "loss": 0.0055,
      "step": 5750
    },
    {
      "epoch": 1.6681046879493815,
      "grad_norm": 0.07899129390716553,
      "learning_rate": 0.0009666436583261432,
      "loss": 0.0022,
      "step": 5800
    },
    {
      "epoch": 1.6824849007765315,
      "grad_norm": 0.00037431871169246733,
      "learning_rate": 0.0009663560540696003,
      "loss": 0.0051,
      "step": 5850
    },
    {
      "epoch": 1.6968651136036814,
      "grad_norm": 0.029954858124256134,
      "learning_rate": 0.0009660684498130573,
      "loss": 0.0018,
      "step": 5900
    },
    {
      "epoch": 1.7112453264308312,
      "grad_norm": 0.0012775607174262404,
      "learning_rate": 0.0009657808455565142,
      "loss": 0.0017,
      "step": 5950
    },
    {
      "epoch": 1.725625539257981,
      "grad_norm": 0.00267683039419353,
      "learning_rate": 0.0009654932412999713,
      "loss": 0.0038,
      "step": 6000
    },
    {
      "epoch": 1.7400057520851309,
      "grad_norm": 0.0001695845858193934,
      "learning_rate": 0.0009652056370434282,
      "loss": 0.0018,
      "step": 6050
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 6.131719419499859e-05,
      "learning_rate": 0.0009649180327868852,
      "loss": 0.0045,
      "step": 6100
    },
    {
      "epoch": 1.7687661777394306,
      "grad_norm": 0.0013950859429314733,
      "learning_rate": 0.0009646304285303422,
      "loss": 0.0028,
      "step": 6150
    },
    {
      "epoch": 1.7831463905665803,
      "grad_norm": 0.00038072941242717206,
      "learning_rate": 0.0009643428242737993,
      "loss": 0.0016,
      "step": 6200
    },
    {
      "epoch": 1.7975266033937303,
      "grad_norm": 0.27889174222946167,
      "learning_rate": 0.0009640552200172563,
      "loss": 0.0028,
      "step": 6250
    },
    {
      "epoch": 1.8119068162208802,
      "grad_norm": 0.001225312240421772,
      "learning_rate": 0.0009637676157607133,
      "loss": 0.0039,
      "step": 6300
    },
    {
      "epoch": 1.82628702904803,
      "grad_norm": 8.709653047844768e-05,
      "learning_rate": 0.0009634800115041703,
      "loss": 0.0027,
      "step": 6350
    },
    {
      "epoch": 1.8406672418751797,
      "grad_norm": 0.019752517342567444,
      "learning_rate": 0.0009631924072476272,
      "loss": 0.0031,
      "step": 6400
    },
    {
      "epoch": 1.8550474547023295,
      "grad_norm": 0.006800493691116571,
      "learning_rate": 0.0009629048029910843,
      "loss": 0.0021,
      "step": 6450
    },
    {
      "epoch": 1.8694276675294794,
      "grad_norm": 0.04230169579386711,
      "learning_rate": 0.0009626171987345413,
      "loss": 0.0007,
      "step": 6500
    },
    {
      "epoch": 1.8838078803566294,
      "grad_norm": 0.013841642066836357,
      "learning_rate": 0.0009623295944779982,
      "loss": 0.0005,
      "step": 6550
    },
    {
      "epoch": 1.8981880931837791,
      "grad_norm": 0.000605404784437269,
      "learning_rate": 0.0009620419902214554,
      "loss": 0.0006,
      "step": 6600
    },
    {
      "epoch": 1.9125683060109289,
      "grad_norm": 0.002950328402221203,
      "learning_rate": 0.0009617543859649123,
      "loss": 0.0038,
      "step": 6650
    },
    {
      "epoch": 1.9269485188380788,
      "grad_norm": 0.0015918291173875332,
      "learning_rate": 0.0009614667817083693,
      "loss": 0.0028,
      "step": 6700
    },
    {
      "epoch": 1.9413287316652288,
      "grad_norm": 0.051727812737226486,
      "learning_rate": 0.0009611791774518263,
      "loss": 0.0024,
      "step": 6750
    },
    {
      "epoch": 1.9557089444923785,
      "grad_norm": 0.00033528642961755395,
      "learning_rate": 0.0009608915731952833,
      "loss": 0.0029,
      "step": 6800
    },
    {
      "epoch": 1.9700891573195283,
      "grad_norm": 0.00406833877786994,
      "learning_rate": 0.0009606039689387403,
      "loss": 0.0036,
      "step": 6850
    },
    {
      "epoch": 1.984469370146678,
      "grad_norm": 0.001074494211934507,
      "learning_rate": 0.0009603163646821973,
      "loss": 0.0012,
      "step": 6900
    },
    {
      "epoch": 1.998849582973828,
      "grad_norm": 0.009931838139891624,
      "learning_rate": 0.0009600287604256543,
      "loss": 0.0028,
      "step": 6950
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.010799800045788288,
      "eval_runtime": 17.1842,
      "eval_samples_per_second": 2776.91,
      "eval_steps_per_second": 43.412,
      "step": 6954
    },
    {
      "epoch": 2.013229795800978,
      "grad_norm": 0.06260219216346741,
      "learning_rate": 0.0009597411561691112,
      "loss": 0.0035,
      "step": 7000
    },
    {
      "epoch": 2.0276100086281277,
      "grad_norm": 0.033838897943496704,
      "learning_rate": 0.0009594535519125684,
      "loss": 0.003,
      "step": 7050
    },
    {
      "epoch": 2.0419902214552774,
      "grad_norm": 4.895063102594577e-05,
      "learning_rate": 0.0009591659476560254,
      "loss": 0.0027,
      "step": 7100
    },
    {
      "epoch": 2.056370434282427,
      "grad_norm": 0.12606029212474823,
      "learning_rate": 0.0009588783433994823,
      "loss": 0.0024,
      "step": 7150
    },
    {
      "epoch": 2.0707506471095773,
      "grad_norm": 0.0014436666388064623,
      "learning_rate": 0.0009585907391429394,
      "loss": 0.0056,
      "step": 7200
    },
    {
      "epoch": 2.085130859936727,
      "grad_norm": 0.05299567058682442,
      "learning_rate": 0.0009583031348863963,
      "loss": 0.0011,
      "step": 7250
    },
    {
      "epoch": 2.099511072763877,
      "grad_norm": 0.0006136605516076088,
      "learning_rate": 0.0009580155306298533,
      "loss": 0.0018,
      "step": 7300
    },
    {
      "epoch": 2.1138912855910266,
      "grad_norm": 0.00010500993084860966,
      "learning_rate": 0.0009577279263733103,
      "loss": 0.0055,
      "step": 7350
    },
    {
      "epoch": 2.1282714984181768,
      "grad_norm": 0.08492358773946762,
      "learning_rate": 0.0009574403221167673,
      "loss": 0.0056,
      "step": 7400
    },
    {
      "epoch": 2.1426517112453265,
      "grad_norm": 0.17475667595863342,
      "learning_rate": 0.0009571527178602243,
      "loss": 0.0053,
      "step": 7450
    },
    {
      "epoch": 2.1570319240724762,
      "grad_norm": 0.00019579271611291915,
      "learning_rate": 0.0009568651136036814,
      "loss": 0.0047,
      "step": 7500
    },
    {
      "epoch": 2.171412136899626,
      "grad_norm": 0.00014194456161931157,
      "learning_rate": 0.0009565775093471384,
      "loss": 0.0018,
      "step": 7550
    },
    {
      "epoch": 2.185792349726776,
      "grad_norm": 0.04067615047097206,
      "learning_rate": 0.0009562899050905953,
      "loss": 0.0035,
      "step": 7600
    },
    {
      "epoch": 2.200172562553926,
      "grad_norm": 0.048927031457424164,
      "learning_rate": 0.0009560023008340524,
      "loss": 0.0033,
      "step": 7650
    },
    {
      "epoch": 2.2145527753810756,
      "grad_norm": 0.03833392634987831,
      "learning_rate": 0.0009557146965775093,
      "loss": 0.0025,
      "step": 7700
    },
    {
      "epoch": 2.2289329882082254,
      "grad_norm": 0.03947765752673149,
      "learning_rate": 0.0009554270923209663,
      "loss": 0.0038,
      "step": 7750
    },
    {
      "epoch": 2.243313201035375,
      "grad_norm": 0.0008837644127197564,
      "learning_rate": 0.0009551394880644234,
      "loss": 0.0038,
      "step": 7800
    },
    {
      "epoch": 2.2576934138625253,
      "grad_norm": 0.004491950385272503,
      "learning_rate": 0.0009548518838078803,
      "loss": 0.0018,
      "step": 7850
    },
    {
      "epoch": 2.272073626689675,
      "grad_norm": 0.0009976743021979928,
      "learning_rate": 0.0009545642795513374,
      "loss": 0.0016,
      "step": 7900
    },
    {
      "epoch": 2.286453839516825,
      "grad_norm": 0.012204230763018131,
      "learning_rate": 0.0009542766752947944,
      "loss": 0.0018,
      "step": 7950
    },
    {
      "epoch": 2.3008340523439745,
      "grad_norm": 0.00020224798936396837,
      "learning_rate": 0.0009539890710382514,
      "loss": 0.003,
      "step": 8000
    },
    {
      "epoch": 2.3152142651711247,
      "grad_norm": 0.016835549846291542,
      "learning_rate": 0.0009537014667817084,
      "loss": 0.0032,
      "step": 8050
    },
    {
      "epoch": 2.3295944779982745,
      "grad_norm": 0.00010820917668752372,
      "learning_rate": 0.0009534138625251654,
      "loss": 0.0018,
      "step": 8100
    },
    {
      "epoch": 2.343974690825424,
      "grad_norm": 0.000477835739729926,
      "learning_rate": 0.0009531262582686224,
      "loss": 0.006,
      "step": 8150
    },
    {
      "epoch": 2.358354903652574,
      "grad_norm": 0.09002061188220978,
      "learning_rate": 0.0009528386540120793,
      "loss": 0.0023,
      "step": 8200
    },
    {
      "epoch": 2.372735116479724,
      "grad_norm": 0.0005718892207369208,
      "learning_rate": 0.0009525510497555364,
      "loss": 0.0021,
      "step": 8250
    },
    {
      "epoch": 2.387115329306874,
      "grad_norm": 0.0007481028442271054,
      "learning_rate": 0.0009522634454989933,
      "loss": 0.0029,
      "step": 8300
    },
    {
      "epoch": 2.4014955421340236,
      "grad_norm": 0.002257132437080145,
      "learning_rate": 0.0009519758412424504,
      "loss": 0.0024,
      "step": 8350
    },
    {
      "epoch": 2.4158757549611733,
      "grad_norm": 0.0006523870397359133,
      "learning_rate": 0.0009516882369859075,
      "loss": 0.0011,
      "step": 8400
    },
    {
      "epoch": 2.430255967788323,
      "grad_norm": 0.0007339167641475797,
      "learning_rate": 0.0009514006327293644,
      "loss": 0.0029,
      "step": 8450
    },
    {
      "epoch": 2.4446361806154733,
      "grad_norm": 0.00011649807856883854,
      "learning_rate": 0.0009511130284728214,
      "loss": 0.0031,
      "step": 8500
    },
    {
      "epoch": 2.459016393442623,
      "grad_norm": 0.0695861205458641,
      "learning_rate": 0.0009508254242162784,
      "loss": 0.002,
      "step": 8550
    },
    {
      "epoch": 2.4733966062697728,
      "grad_norm": 0.031065870076417923,
      "learning_rate": 0.0009505378199597354,
      "loss": 0.0015,
      "step": 8600
    },
    {
      "epoch": 2.4877768190969225,
      "grad_norm": 0.019455594941973686,
      "learning_rate": 0.0009502502157031924,
      "loss": 0.0056,
      "step": 8650
    },
    {
      "epoch": 2.5021570319240727,
      "grad_norm": 0.009388770908117294,
      "learning_rate": 0.0009499626114466494,
      "loss": 0.0027,
      "step": 8700
    },
    {
      "epoch": 2.5165372447512224,
      "grad_norm": 0.005809389054775238,
      "learning_rate": 0.0009496750071901065,
      "loss": 0.0038,
      "step": 8750
    },
    {
      "epoch": 2.530917457578372,
      "grad_norm": 0.05575360730290413,
      "learning_rate": 0.0009493874029335634,
      "loss": 0.0011,
      "step": 8800
    },
    {
      "epoch": 2.545297670405522,
      "grad_norm": 0.005523367784917355,
      "learning_rate": 0.0009490997986770205,
      "loss": 0.0016,
      "step": 8850
    },
    {
      "epoch": 2.5596778832326716,
      "grad_norm": 0.015713250264525414,
      "learning_rate": 0.0009488121944204774,
      "loss": 0.0032,
      "step": 8900
    },
    {
      "epoch": 2.574058096059822,
      "grad_norm": 0.00016669643810018897,
      "learning_rate": 0.0009485245901639344,
      "loss": 0.0031,
      "step": 8950
    },
    {
      "epoch": 2.5884383088869716,
      "grad_norm": 6.04705601290334e-05,
      "learning_rate": 0.0009482369859073915,
      "loss": 0.0024,
      "step": 9000
    },
    {
      "epoch": 2.6028185217141213,
      "grad_norm": 0.00017252432007808238,
      "learning_rate": 0.0009479493816508484,
      "loss": 0.0018,
      "step": 9050
    },
    {
      "epoch": 2.617198734541271,
      "grad_norm": 0.00013969800784252584,
      "learning_rate": 0.0009476617773943054,
      "loss": 0.006,
      "step": 9100
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 0.0036659117322415113,
      "learning_rate": 0.0009473741731377624,
      "loss": 0.0028,
      "step": 9150
    },
    {
      "epoch": 2.645959160195571,
      "grad_norm": 0.01591820828616619,
      "learning_rate": 0.0009470865688812195,
      "loss": 0.0038,
      "step": 9200
    },
    {
      "epoch": 2.6603393730227207,
      "grad_norm": 0.0005499161779880524,
      "learning_rate": 0.0009467989646246765,
      "loss": 0.0012,
      "step": 9250
    },
    {
      "epoch": 2.6747195858498705,
      "grad_norm": 0.00043730525067076087,
      "learning_rate": 0.0009465113603681335,
      "loss": 0.0051,
      "step": 9300
    },
    {
      "epoch": 2.68909979867702,
      "grad_norm": 0.0022358419373631477,
      "learning_rate": 0.0009462237561115905,
      "loss": 0.0014,
      "step": 9350
    },
    {
      "epoch": 2.7034800115041704,
      "grad_norm": 0.003501546336337924,
      "learning_rate": 0.0009459361518550474,
      "loss": 0.0075,
      "step": 9400
    },
    {
      "epoch": 2.71786022433132,
      "grad_norm": 0.0009609232656657696,
      "learning_rate": 0.0009456485475985045,
      "loss": 0.0017,
      "step": 9450
    },
    {
      "epoch": 2.73224043715847,
      "grad_norm": 0.0018361620604991913,
      "learning_rate": 0.0009453609433419614,
      "loss": 0.0031,
      "step": 9500
    },
    {
      "epoch": 2.74662064998562,
      "grad_norm": 8.265401265816763e-05,
      "learning_rate": 0.0009450733390854184,
      "loss": 0.0021,
      "step": 9550
    },
    {
      "epoch": 2.76100086281277,
      "grad_norm": 0.011295881122350693,
      "learning_rate": 0.0009447857348288756,
      "loss": 0.0058,
      "step": 9600
    },
    {
      "epoch": 2.7753810756399195,
      "grad_norm": 0.0030116275884211063,
      "learning_rate": 0.0009444981305723325,
      "loss": 0.0039,
      "step": 9650
    },
    {
      "epoch": 2.7897612884670693,
      "grad_norm": 0.0005857138312421739,
      "learning_rate": 0.0009442105263157895,
      "loss": 0.0042,
      "step": 9700
    },
    {
      "epoch": 2.804141501294219,
      "grad_norm": 0.0003303262637928128,
      "learning_rate": 0.0009439229220592465,
      "loss": 0.0021,
      "step": 9750
    },
    {
      "epoch": 2.8185217141213688,
      "grad_norm": 0.04614737257361412,
      "learning_rate": 0.0009436353178027035,
      "loss": 0.0059,
      "step": 9800
    },
    {
      "epoch": 2.832901926948519,
      "grad_norm": 0.009237417951226234,
      "learning_rate": 0.0009433477135461605,
      "loss": 0.001,
      "step": 9850
    },
    {
      "epoch": 2.8472821397756687,
      "grad_norm": 0.0007738402928225696,
      "learning_rate": 0.0009430601092896175,
      "loss": 0.0051,
      "step": 9900
    },
    {
      "epoch": 2.8616623526028184,
      "grad_norm": 6.133087299531326e-05,
      "learning_rate": 0.0009427725050330745,
      "loss": 0.0029,
      "step": 9950
    },
    {
      "epoch": 2.8760425654299686,
      "grad_norm": 0.00025495162117294967,
      "learning_rate": 0.0009424849007765314,
      "loss": 0.0042,
      "step": 10000
    },
    {
      "epoch": 2.8904227782571184,
      "grad_norm": 0.00041781715117394924,
      "learning_rate": 0.0009421972965199886,
      "loss": 0.0046,
      "step": 10050
    },
    {
      "epoch": 2.904802991084268,
      "grad_norm": 0.03007436729967594,
      "learning_rate": 0.0009419096922634455,
      "loss": 0.0024,
      "step": 10100
    },
    {
      "epoch": 2.919183203911418,
      "grad_norm": 0.015538269653916359,
      "learning_rate": 0.0009416220880069025,
      "loss": 0.0012,
      "step": 10150
    },
    {
      "epoch": 2.9335634167385676,
      "grad_norm": 6.21842555119656e-05,
      "learning_rate": 0.0009413344837503596,
      "loss": 0.0029,
      "step": 10200
    },
    {
      "epoch": 2.9479436295657173,
      "grad_norm": 0.006558276247233152,
      "learning_rate": 0.0009410468794938165,
      "loss": 0.0059,
      "step": 10250
    },
    {
      "epoch": 2.9623238423928675,
      "grad_norm": 0.03989921137690544,
      "learning_rate": 0.0009407592752372735,
      "loss": 0.0048,
      "step": 10300
    },
    {
      "epoch": 2.9767040552200172,
      "grad_norm": 0.00501037435606122,
      "learning_rate": 0.0009404716709807305,
      "loss": 0.0021,
      "step": 10350
    },
    {
      "epoch": 2.991084268047167,
      "grad_norm": 0.4413738548755646,
      "learning_rate": 0.0009401840667241875,
      "loss": 0.0019,
      "step": 10400
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.010431637056171894,
      "eval_runtime": 17.4279,
      "eval_samples_per_second": 2738.088,
      "eval_steps_per_second": 42.805,
      "step": 10431
    },
    {
      "epoch": 3.0054644808743167,
      "grad_norm": 0.0006940938765183091,
      "learning_rate": 0.0009398964624676444,
      "loss": 0.0049,
      "step": 10450
    },
    {
      "epoch": 3.019844693701467,
      "grad_norm": 0.0006508651422336698,
      "learning_rate": 0.0009396088582111016,
      "loss": 0.0032,
      "step": 10500
    },
    {
      "epoch": 3.0342249065286166,
      "grad_norm": 0.0010223997524008155,
      "learning_rate": 0.0009393212539545586,
      "loss": 0.0031,
      "step": 10550
    },
    {
      "epoch": 3.0486051193557664,
      "grad_norm": 0.0026707996148616076,
      "learning_rate": 0.0009390336496980155,
      "loss": 0.0043,
      "step": 10600
    },
    {
      "epoch": 3.062985332182916,
      "grad_norm": 0.0004393986891955137,
      "learning_rate": 0.0009387460454414726,
      "loss": 0.0042,
      "step": 10650
    },
    {
      "epoch": 3.0773655450100663,
      "grad_norm": 0.013381805270910263,
      "learning_rate": 0.0009384584411849295,
      "loss": 0.0025,
      "step": 10700
    },
    {
      "epoch": 3.091745757837216,
      "grad_norm": 0.003617726732045412,
      "learning_rate": 0.0009381708369283865,
      "loss": 0.0038,
      "step": 10750
    },
    {
      "epoch": 3.106125970664366,
      "grad_norm": 0.000364899227861315,
      "learning_rate": 0.0009378832326718436,
      "loss": 0.0022,
      "step": 10800
    },
    {
      "epoch": 3.1205061834915155,
      "grad_norm": 0.00036387777072377503,
      "learning_rate": 0.0009375956284153005,
      "loss": 0.003,
      "step": 10850
    },
    {
      "epoch": 3.1348863963186657,
      "grad_norm": 0.00027993362164124846,
      "learning_rate": 0.0009373080241587575,
      "loss": 0.001,
      "step": 10900
    },
    {
      "epoch": 3.1492666091458155,
      "grad_norm": 0.0008596086408942938,
      "learning_rate": 0.0009370204199022146,
      "loss": 0.0042,
      "step": 10950
    },
    {
      "epoch": 3.163646821972965,
      "grad_norm": 6.564747309312224e-05,
      "learning_rate": 0.0009367328156456716,
      "loss": 0.0039,
      "step": 11000
    },
    {
      "epoch": 3.178027034800115,
      "grad_norm": 0.0001888457773020491,
      "learning_rate": 0.0009364452113891285,
      "loss": 0.003,
      "step": 11050
    },
    {
      "epoch": 3.1924072476272647,
      "grad_norm": 0.0005057744565419853,
      "learning_rate": 0.0009361576071325856,
      "loss": 0.0014,
      "step": 11100
    },
    {
      "epoch": 3.206787460454415,
      "grad_norm": 0.01950443722307682,
      "learning_rate": 0.0009358700028760426,
      "loss": 0.0064,
      "step": 11150
    },
    {
      "epoch": 3.2211676732815646,
      "grad_norm": 0.005898532923310995,
      "learning_rate": 0.0009355823986194995,
      "loss": 0.0041,
      "step": 11200
    },
    {
      "epoch": 3.2355478861087144,
      "grad_norm": 0.001087710028514266,
      "learning_rate": 0.0009352947943629566,
      "loss": 0.0006,
      "step": 11250
    },
    {
      "epoch": 3.249928098935864,
      "grad_norm": 0.015545658767223358,
      "learning_rate": 0.0009350071901064135,
      "loss": 0.0034,
      "step": 11300
    },
    {
      "epoch": 3.2643083117630143,
      "grad_norm": 3.651237420854159e-05,
      "learning_rate": 0.0009347195858498705,
      "loss": 0.0026,
      "step": 11350
    },
    {
      "epoch": 3.278688524590164,
      "grad_norm": 0.00816213246434927,
      "learning_rate": 0.0009344319815933277,
      "loss": 0.001,
      "step": 11400
    },
    {
      "epoch": 3.2930687374173138,
      "grad_norm": 0.0648769736289978,
      "learning_rate": 0.0009341443773367846,
      "loss": 0.0023,
      "step": 11450
    },
    {
      "epoch": 3.3074489502444635,
      "grad_norm": 0.05676992982625961,
      "learning_rate": 0.0009338567730802416,
      "loss": 0.003,
      "step": 11500
    },
    {
      "epoch": 3.3218291630716132,
      "grad_norm": 0.002327820286154747,
      "learning_rate": 0.0009335691688236986,
      "loss": 0.0022,
      "step": 11550
    },
    {
      "epoch": 3.3362093758987634,
      "grad_norm": 0.002787820529192686,
      "learning_rate": 0.0009332815645671556,
      "loss": 0.0013,
      "step": 11600
    },
    {
      "epoch": 3.350589588725913,
      "grad_norm": 9.075220441445708e-05,
      "learning_rate": 0.0009329939603106125,
      "loss": 0.0023,
      "step": 11650
    },
    {
      "epoch": 3.364969801553063,
      "grad_norm": 0.0005701097543351352,
      "learning_rate": 0.0009327063560540696,
      "loss": 0.0011,
      "step": 11700
    },
    {
      "epoch": 3.3793500143802127,
      "grad_norm": 0.01291273906826973,
      "learning_rate": 0.0009324187517975266,
      "loss": 0.004,
      "step": 11750
    },
    {
      "epoch": 3.393730227207363,
      "grad_norm": 0.0006759212701581419,
      "learning_rate": 0.0009321311475409836,
      "loss": 0.0042,
      "step": 11800
    },
    {
      "epoch": 3.4081104400345126,
      "grad_norm": 0.0008773997542448342,
      "learning_rate": 0.0009318435432844407,
      "loss": 0.0014,
      "step": 11850
    },
    {
      "epoch": 3.4224906528616623,
      "grad_norm": 0.004313154146075249,
      "learning_rate": 0.0009315559390278976,
      "loss": 0.0023,
      "step": 11900
    },
    {
      "epoch": 3.436870865688812,
      "grad_norm": 0.00015693520253989846,
      "learning_rate": 0.0009312683347713546,
      "loss": 0.0022,
      "step": 11950
    },
    {
      "epoch": 3.451251078515962,
      "grad_norm": 0.020058536902070045,
      "learning_rate": 0.0009309807305148117,
      "loss": 0.0013,
      "step": 12000
    },
    {
      "epoch": 3.465631291343112,
      "grad_norm": 0.000356721633579582,
      "learning_rate": 0.0009306931262582686,
      "loss": 0.0019,
      "step": 12050
    },
    {
      "epoch": 3.4800115041702617,
      "grad_norm": 0.01069308165460825,
      "learning_rate": 0.0009304055220017257,
      "loss": 0.0029,
      "step": 12100
    },
    {
      "epoch": 3.4943917169974115,
      "grad_norm": 0.0005174070247448981,
      "learning_rate": 0.0009301179177451826,
      "loss": 0.0039,
      "step": 12150
    },
    {
      "epoch": 3.5087719298245617,
      "grad_norm": 0.00014269140956457704,
      "learning_rate": 0.0009298303134886396,
      "loss": 0.0033,
      "step": 12200
    },
    {
      "epoch": 3.5231521426517114,
      "grad_norm": 0.07351025938987732,
      "learning_rate": 0.0009295427092320966,
      "loss": 0.0053,
      "step": 12250
    },
    {
      "epoch": 3.537532355478861,
      "grad_norm": 9.275358024751768e-05,
      "learning_rate": 0.0009292551049755537,
      "loss": 0.002,
      "step": 12300
    },
    {
      "epoch": 3.551912568306011,
      "grad_norm": 0.00019239020184613764,
      "learning_rate": 0.0009289675007190107,
      "loss": 0.0031,
      "step": 12350
    },
    {
      "epoch": 3.5662927811331606,
      "grad_norm": 0.004282767418771982,
      "learning_rate": 0.0009286798964624676,
      "loss": 0.0048,
      "step": 12400
    },
    {
      "epoch": 3.5806729939603104,
      "grad_norm": 0.09463165700435638,
      "learning_rate": 0.0009283922922059247,
      "loss": 0.0039,
      "step": 12450
    },
    {
      "epoch": 3.5950532067874605,
      "grad_norm": 0.0009465513867326081,
      "learning_rate": 0.0009281046879493816,
      "loss": 0.0045,
      "step": 12500
    },
    {
      "epoch": 3.6094334196146103,
      "grad_norm": 0.00021874671801924706,
      "learning_rate": 0.0009278170836928387,
      "loss": 0.0039,
      "step": 12550
    },
    {
      "epoch": 3.62381363244176,
      "grad_norm": 0.037701938301324844,
      "learning_rate": 0.0009275294794362957,
      "loss": 0.0021,
      "step": 12600
    },
    {
      "epoch": 3.63819384526891,
      "grad_norm": 0.00011455336061771959,
      "learning_rate": 0.0009272418751797527,
      "loss": 0.0029,
      "step": 12650
    },
    {
      "epoch": 3.65257405809606,
      "grad_norm": 0.00143083231523633,
      "learning_rate": 0.0009269542709232098,
      "loss": 0.004,
      "step": 12700
    },
    {
      "epoch": 3.6669542709232097,
      "grad_norm": 2.20128167711664e-05,
      "learning_rate": 0.0009266666666666667,
      "loss": 0.0036,
      "step": 12750
    },
    {
      "epoch": 3.6813344837503594,
      "grad_norm": 0.04346714913845062,
      "learning_rate": 0.0009263790624101237,
      "loss": 0.0045,
      "step": 12800
    },
    {
      "epoch": 3.695714696577509,
      "grad_norm": 0.05871553346514702,
      "learning_rate": 0.0009260914581535806,
      "loss": 0.0026,
      "step": 12850
    },
    {
      "epoch": 3.710094909404659,
      "grad_norm": 0.04056289792060852,
      "learning_rate": 0.0009258038538970377,
      "loss": 0.0038,
      "step": 12900
    },
    {
      "epoch": 3.724475122231809,
      "grad_norm": 0.000628515612334013,
      "learning_rate": 0.0009255162496404947,
      "loss": 0.0018,
      "step": 12950
    },
    {
      "epoch": 3.738855335058959,
      "grad_norm": 0.0050051650032401085,
      "learning_rate": 0.0009252286453839517,
      "loss": 0.0046,
      "step": 13000
    },
    {
      "epoch": 3.7532355478861086,
      "grad_norm": 0.0005931504420004785,
      "learning_rate": 0.0009249410411274087,
      "loss": 0.0064,
      "step": 13050
    },
    {
      "epoch": 3.7676157607132588,
      "grad_norm": 0.00012028280616505072,
      "learning_rate": 0.0009246534368708657,
      "loss": 0.0023,
      "step": 13100
    },
    {
      "epoch": 3.7819959735404085,
      "grad_norm": 0.0006175875896587968,
      "learning_rate": 0.0009243658326143228,
      "loss": 0.0061,
      "step": 13150
    },
    {
      "epoch": 3.7963761863675582,
      "grad_norm": 0.00010063577792607248,
      "learning_rate": 0.0009240782283577797,
      "loss": 0.0043,
      "step": 13200
    },
    {
      "epoch": 3.810756399194708,
      "grad_norm": 0.0005797814228571951,
      "learning_rate": 0.0009237906241012367,
      "loss": 0.005,
      "step": 13250
    },
    {
      "epoch": 3.8251366120218577,
      "grad_norm": 0.012822269462049007,
      "learning_rate": 0.0009235030198446938,
      "loss": 0.0011,
      "step": 13300
    },
    {
      "epoch": 3.839516824849008,
      "grad_norm": 0.002528035081923008,
      "learning_rate": 0.0009232154155881507,
      "loss": 0.0027,
      "step": 13350
    },
    {
      "epoch": 3.8538970376761577,
      "grad_norm": 0.06353459507226944,
      "learning_rate": 0.0009229278113316077,
      "loss": 0.0014,
      "step": 13400
    },
    {
      "epoch": 3.8682772505033074,
      "grad_norm": 0.02640707977116108,
      "learning_rate": 0.0009226402070750647,
      "loss": 0.0028,
      "step": 13450
    },
    {
      "epoch": 3.882657463330457,
      "grad_norm": 0.00010784521145978943,
      "learning_rate": 0.0009223526028185218,
      "loss": 0.0066,
      "step": 13500
    },
    {
      "epoch": 3.8970376761576073,
      "grad_norm": 5.580957076745108e-05,
      "learning_rate": 0.0009220649985619788,
      "loss": 0.0043,
      "step": 13550
    },
    {
      "epoch": 3.911417888984757,
      "grad_norm": 0.000112038993393071,
      "learning_rate": 0.0009217773943054358,
      "loss": 0.0028,
      "step": 13600
    },
    {
      "epoch": 3.925798101811907,
      "grad_norm": 0.0005121253780089319,
      "learning_rate": 0.0009214897900488928,
      "loss": 0.0042,
      "step": 13650
    },
    {
      "epoch": 3.9401783146390565,
      "grad_norm": 5.3021554776933044e-05,
      "learning_rate": 0.0009212021857923497,
      "loss": 0.0042,
      "step": 13700
    },
    {
      "epoch": 3.9545585274662063,
      "grad_norm": 0.010679749771952629,
      "learning_rate": 0.0009209145815358068,
      "loss": 0.0029,
      "step": 13750
    },
    {
      "epoch": 3.9689387402933565,
      "grad_norm": 0.0014005936682224274,
      "learning_rate": 0.0009206269772792637,
      "loss": 0.0013,
      "step": 13800
    },
    {
      "epoch": 3.983318953120506,
      "grad_norm": 0.08281589299440384,
      "learning_rate": 0.0009203393730227207,
      "loss": 0.0046,
      "step": 13850
    },
    {
      "epoch": 3.997699165947656,
      "grad_norm": 0.00024075069813989103,
      "learning_rate": 0.0009200517687661779,
      "loss": 0.0024,
      "step": 13900
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.010415537282824516,
      "eval_runtime": 16.8786,
      "eval_samples_per_second": 2827.185,
      "eval_steps_per_second": 44.198,
      "step": 13908
    },
    {
      "epoch": 4.012079378774806,
      "grad_norm": 0.028206439688801765,
      "learning_rate": 0.0009197641645096348,
      "loss": 0.0022,
      "step": 13950
    },
    {
      "epoch": 4.026459591601956,
      "grad_norm": 0.0007886759121902287,
      "learning_rate": 0.0009194765602530918,
      "loss": 0.0069,
      "step": 14000
    },
    {
      "epoch": 4.040839804429106,
      "grad_norm": 0.0006359423277899623,
      "learning_rate": 0.0009191889559965488,
      "loss": 0.0048,
      "step": 14050
    },
    {
      "epoch": 4.055220017256255,
      "grad_norm": 0.01441052183508873,
      "learning_rate": 0.0009189013517400058,
      "loss": 0.004,
      "step": 14100
    },
    {
      "epoch": 4.069600230083405,
      "grad_norm": 0.00030902912840247154,
      "learning_rate": 0.0009186137474834628,
      "loss": 0.0053,
      "step": 14150
    },
    {
      "epoch": 4.083980442910555,
      "grad_norm": 0.043572112917900085,
      "learning_rate": 0.0009183261432269198,
      "loss": 0.0036,
      "step": 14200
    },
    {
      "epoch": 4.098360655737705,
      "grad_norm": 0.0015008345944806933,
      "learning_rate": 0.0009180385389703768,
      "loss": 0.0012,
      "step": 14250
    },
    {
      "epoch": 4.112740868564854,
      "grad_norm": 0.0001759688457241282,
      "learning_rate": 0.0009177509347138337,
      "loss": 0.0037,
      "step": 14300
    },
    {
      "epoch": 4.127121081392005,
      "grad_norm": 0.0736381933093071,
      "learning_rate": 0.0009174633304572909,
      "loss": 0.005,
      "step": 14350
    },
    {
      "epoch": 4.141501294219155,
      "grad_norm": 6.84416590956971e-05,
      "learning_rate": 0.0009171757262007478,
      "loss": 0.0026,
      "step": 14400
    },
    {
      "epoch": 4.155881507046304,
      "grad_norm": 0.0005751079879701138,
      "learning_rate": 0.0009168881219442048,
      "loss": 0.0027,
      "step": 14450
    },
    {
      "epoch": 4.170261719873454,
      "grad_norm": 6.838972331024706e-05,
      "learning_rate": 0.0009166005176876619,
      "loss": 0.0018,
      "step": 14500
    },
    {
      "epoch": 4.184641932700604,
      "grad_norm": 0.014036620035767555,
      "learning_rate": 0.0009163129134311188,
      "loss": 0.002,
      "step": 14550
    },
    {
      "epoch": 4.199022145527754,
      "grad_norm": 0.0001720777217997238,
      "learning_rate": 0.0009160253091745758,
      "loss": 0.0023,
      "step": 14600
    },
    {
      "epoch": 4.213402358354903,
      "grad_norm": 0.00043323804857209325,
      "learning_rate": 0.0009157377049180328,
      "loss": 0.0027,
      "step": 14650
    },
    {
      "epoch": 4.227782571182053,
      "grad_norm": 0.014240456745028496,
      "learning_rate": 0.0009154501006614898,
      "loss": 0.003,
      "step": 14700
    },
    {
      "epoch": 4.242162784009203,
      "grad_norm": 0.026190536096692085,
      "learning_rate": 0.0009151624964049468,
      "loss": 0.0025,
      "step": 14750
    },
    {
      "epoch": 4.2565429968363535,
      "grad_norm": 0.0002720929915085435,
      "learning_rate": 0.0009148748921484039,
      "loss": 0.0041,
      "step": 14800
    },
    {
      "epoch": 4.270923209663503,
      "grad_norm": 0.003462571417912841,
      "learning_rate": 0.0009145872878918609,
      "loss": 0.0017,
      "step": 14850
    },
    {
      "epoch": 4.285303422490653,
      "grad_norm": 0.006865582894533873,
      "learning_rate": 0.0009142996836353178,
      "loss": 0.0045,
      "step": 14900
    },
    {
      "epoch": 4.299683635317803,
      "grad_norm": 0.00043535814620554447,
      "learning_rate": 0.0009140120793787749,
      "loss": 0.001,
      "step": 14950
    },
    {
      "epoch": 4.3140638481449525,
      "grad_norm": 0.0003036123816855252,
      "learning_rate": 0.0009137244751222318,
      "loss": 0.002,
      "step": 15000
    },
    {
      "epoch": 4.328444060972102,
      "grad_norm": 0.0004381926846690476,
      "learning_rate": 0.0009134368708656888,
      "loss": 0.0024,
      "step": 15050
    },
    {
      "epoch": 4.342824273799252,
      "grad_norm": 0.00022446358343586326,
      "learning_rate": 0.0009131492666091459,
      "loss": 0.0054,
      "step": 15100
    },
    {
      "epoch": 4.357204486626402,
      "grad_norm": 0.0001280482392758131,
      "learning_rate": 0.0009128616623526028,
      "loss": 0.004,
      "step": 15150
    },
    {
      "epoch": 4.371584699453552,
      "grad_norm": 0.002870578086003661,
      "learning_rate": 0.0009125740580960598,
      "loss": 0.0031,
      "step": 15200
    },
    {
      "epoch": 4.385964912280702,
      "grad_norm": 0.0027956736739724874,
      "learning_rate": 0.0009122864538395169,
      "loss": 0.0022,
      "step": 15250
    },
    {
      "epoch": 4.400345125107852,
      "grad_norm": 0.0005551831563934684,
      "learning_rate": 0.0009119988495829739,
      "loss": 0.0021,
      "step": 15300
    },
    {
      "epoch": 4.4147253379350015,
      "grad_norm": 0.0007397743756882846,
      "learning_rate": 0.0009117112453264309,
      "loss": 0.0041,
      "step": 15350
    },
    {
      "epoch": 4.429105550762151,
      "grad_norm": 0.001867174170911312,
      "learning_rate": 0.0009114236410698879,
      "loss": 0.0083,
      "step": 15400
    },
    {
      "epoch": 4.443485763589301,
      "grad_norm": 0.0008372843731194735,
      "learning_rate": 0.0009111360368133449,
      "loss": 0.0029,
      "step": 15450
    },
    {
      "epoch": 4.457865976416451,
      "grad_norm": 0.061873968690633774,
      "learning_rate": 0.0009108484325568018,
      "loss": 0.0074,
      "step": 15500
    },
    {
      "epoch": 4.4722461892436005,
      "grad_norm": 0.00037278729723766446,
      "learning_rate": 0.0009105608283002589,
      "loss": 0.002,
      "step": 15550
    },
    {
      "epoch": 4.48662640207075,
      "grad_norm": 0.002023245906457305,
      "learning_rate": 0.0009102732240437158,
      "loss": 0.0033,
      "step": 15600
    },
    {
      "epoch": 4.5010066148979,
      "grad_norm": 0.00022730969067197293,
      "learning_rate": 0.0009099856197871728,
      "loss": 0.0028,
      "step": 15650
    },
    {
      "epoch": 4.515386827725051,
      "grad_norm": 6.092356125009246e-05,
      "learning_rate": 0.00090969801553063,
      "loss": 0.0015,
      "step": 15700
    },
    {
      "epoch": 4.5297670405522,
      "grad_norm": 0.02512837015092373,
      "learning_rate": 0.0009094104112740869,
      "loss": 0.0054,
      "step": 15750
    },
    {
      "epoch": 4.54414725337935,
      "grad_norm": 0.007336049806326628,
      "learning_rate": 0.0009091228070175439,
      "loss": 0.0015,
      "step": 15800
    },
    {
      "epoch": 4.5585274662065,
      "grad_norm": 5.059686736785807e-05,
      "learning_rate": 0.0009088352027610009,
      "loss": 0.0021,
      "step": 15850
    },
    {
      "epoch": 4.57290767903365,
      "grad_norm": 0.062165189534425735,
      "learning_rate": 0.0009085475985044579,
      "loss": 0.0044,
      "step": 15900
    },
    {
      "epoch": 4.587287891860799,
      "grad_norm": 0.004995767027139664,
      "learning_rate": 0.0009082599942479148,
      "loss": 0.0036,
      "step": 15950
    },
    {
      "epoch": 4.601668104687949,
      "grad_norm": 0.001403850270435214,
      "learning_rate": 0.0009079723899913719,
      "loss": 0.0039,
      "step": 16000
    },
    {
      "epoch": 4.6160483175151,
      "grad_norm": 0.00016179485828615725,
      "learning_rate": 0.0009076847857348289,
      "loss": 0.0022,
      "step": 16050
    },
    {
      "epoch": 4.630428530342249,
      "grad_norm": 0.0002494654036127031,
      "learning_rate": 0.0009073971814782858,
      "loss": 0.004,
      "step": 16100
    },
    {
      "epoch": 4.644808743169399,
      "grad_norm": 0.00015029539645183831,
      "learning_rate": 0.000907109577221743,
      "loss": 0.0032,
      "step": 16150
    },
    {
      "epoch": 4.659188955996549,
      "grad_norm": 0.03247823193669319,
      "learning_rate": 0.0009068219729651999,
      "loss": 0.0013,
      "step": 16200
    },
    {
      "epoch": 4.673569168823699,
      "grad_norm": 0.00030097924172878265,
      "learning_rate": 0.0009065343687086569,
      "loss": 0.0012,
      "step": 16250
    },
    {
      "epoch": 4.687949381650848,
      "grad_norm": 0.02051479183137417,
      "learning_rate": 0.000906246764452114,
      "loss": 0.0022,
      "step": 16300
    },
    {
      "epoch": 4.702329594477998,
      "grad_norm": 0.00011428412835812196,
      "learning_rate": 0.0009059591601955709,
      "loss": 0.0009,
      "step": 16350
    },
    {
      "epoch": 4.716709807305148,
      "grad_norm": 0.034743405878543854,
      "learning_rate": 0.0009056715559390279,
      "loss": 0.0017,
      "step": 16400
    },
    {
      "epoch": 4.731090020132298,
      "grad_norm": 0.00010177680087508634,
      "learning_rate": 0.0009053839516824849,
      "loss": 0.0026,
      "step": 16450
    },
    {
      "epoch": 4.745470232959448,
      "grad_norm": 0.000544907758012414,
      "learning_rate": 0.000905096347425942,
      "loss": 0.0009,
      "step": 16500
    },
    {
      "epoch": 4.759850445786598,
      "grad_norm": 0.014538804069161415,
      "learning_rate": 0.0009048087431693989,
      "loss": 0.0039,
      "step": 16550
    },
    {
      "epoch": 4.774230658613748,
      "grad_norm": 0.0009969077073037624,
      "learning_rate": 0.000904521138912856,
      "loss": 0.0051,
      "step": 16600
    },
    {
      "epoch": 4.7886108714408975,
      "grad_norm": 0.08057456463575363,
      "learning_rate": 0.000904233534656313,
      "loss": 0.0006,
      "step": 16650
    },
    {
      "epoch": 4.802991084268047,
      "grad_norm": 9.667516860645264e-05,
      "learning_rate": 0.0009039459303997699,
      "loss": 0.0025,
      "step": 16700
    },
    {
      "epoch": 4.817371297095197,
      "grad_norm": 8.772766886977479e-05,
      "learning_rate": 0.000903658326143227,
      "loss": 0.0025,
      "step": 16750
    },
    {
      "epoch": 4.831751509922347,
      "grad_norm": 0.06798885017633438,
      "learning_rate": 0.0009033707218866839,
      "loss": 0.0011,
      "step": 16800
    },
    {
      "epoch": 4.846131722749496,
      "grad_norm": 5.703072383766994e-05,
      "learning_rate": 0.0009030831176301409,
      "loss": 0.0025,
      "step": 16850
    },
    {
      "epoch": 4.860511935576646,
      "grad_norm": 0.007055347785353661,
      "learning_rate": 0.000902795513373598,
      "loss": 0.0045,
      "step": 16900
    },
    {
      "epoch": 4.874892148403797,
      "grad_norm": 0.030957767739892006,
      "learning_rate": 0.000902507909117055,
      "loss": 0.0032,
      "step": 16950
    },
    {
      "epoch": 4.8892723612309466,
      "grad_norm": 0.13464264571666718,
      "learning_rate": 0.000902220304860512,
      "loss": 0.0022,
      "step": 17000
    },
    {
      "epoch": 4.903652574058096,
      "grad_norm": 0.0008019868400879204,
      "learning_rate": 0.000901932700603969,
      "loss": 0.0033,
      "step": 17050
    },
    {
      "epoch": 4.918032786885246,
      "grad_norm": 0.0007063833763822913,
      "learning_rate": 0.000901645096347426,
      "loss": 0.0014,
      "step": 17100
    },
    {
      "epoch": 4.932412999712396,
      "grad_norm": 6.719944212818518e-05,
      "learning_rate": 0.0009013574920908829,
      "loss": 0.0021,
      "step": 17150
    },
    {
      "epoch": 4.9467932125395455,
      "grad_norm": 0.0010515863541513681,
      "learning_rate": 0.00090106988783434,
      "loss": 0.0022,
      "step": 17200
    },
    {
      "epoch": 4.961173425366695,
      "grad_norm": 4.1578125092200935e-05,
      "learning_rate": 0.000900782283577797,
      "loss": 0.0023,
      "step": 17250
    },
    {
      "epoch": 4.975553638193845,
      "grad_norm": 0.14036019146442413,
      "learning_rate": 0.0009004946793212539,
      "loss": 0.0035,
      "step": 17300
    },
    {
      "epoch": 4.989933851020995,
      "grad_norm": 6.166296952869743e-05,
      "learning_rate": 0.000900207075064711,
      "loss": 0.003,
      "step": 17350
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.011607984080910683,
      "eval_runtime": 16.811,
      "eval_samples_per_second": 2838.551,
      "eval_steps_per_second": 44.376,
      "step": 17385
    },
    {
      "epoch": 5.004314063848145,
      "grad_norm": 0.0002138883137376979,
      "learning_rate": 0.000899919470808168,
      "loss": 0.002,
      "step": 17400
    },
    {
      "epoch": 5.018694276675295,
      "grad_norm": 0.020089635625481606,
      "learning_rate": 0.000899631866551625,
      "loss": 0.0037,
      "step": 17450
    },
    {
      "epoch": 5.033074489502445,
      "grad_norm": 0.00012275369954295456,
      "learning_rate": 0.0008993442622950821,
      "loss": 0.0021,
      "step": 17500
    },
    {
      "epoch": 5.047454702329595,
      "grad_norm": 0.00016586604760959744,
      "learning_rate": 0.000899056658038539,
      "loss": 0.0039,
      "step": 17550
    },
    {
      "epoch": 5.061834915156744,
      "grad_norm": 0.024775628000497818,
      "learning_rate": 0.000898769053781996,
      "loss": 0.0026,
      "step": 17600
    },
    {
      "epoch": 5.076215127983894,
      "grad_norm": 0.026633331552147865,
      "learning_rate": 0.000898481449525453,
      "loss": 0.0012,
      "step": 17650
    },
    {
      "epoch": 5.090595340811044,
      "grad_norm": 0.00020347720419522375,
      "learning_rate": 0.00089819384526891,
      "loss": 0.0025,
      "step": 17700
    },
    {
      "epoch": 5.1049755536381936,
      "grad_norm": 7.219219696708024e-05,
      "learning_rate": 0.0008979062410123669,
      "loss": 0.0007,
      "step": 17750
    },
    {
      "epoch": 5.119355766465343,
      "grad_norm": 0.00331045500934124,
      "learning_rate": 0.000897618636755824,
      "loss": 0.002,
      "step": 17800
    },
    {
      "epoch": 5.133735979292494,
      "grad_norm": 0.00019808260549325496,
      "learning_rate": 0.0008973310324992811,
      "loss": 0.0018,
      "step": 17850
    },
    {
      "epoch": 5.148116192119644,
      "grad_norm": 0.0009097623988054693,
      "learning_rate": 0.000897043428242738,
      "loss": 0.0033,
      "step": 17900
    },
    {
      "epoch": 5.162496404946793,
      "grad_norm": 0.00018265486869495362,
      "learning_rate": 0.0008967558239861951,
      "loss": 0.0039,
      "step": 17950
    },
    {
      "epoch": 5.176876617773943,
      "grad_norm": 0.11904298514127731,
      "learning_rate": 0.000896468219729652,
      "loss": 0.0025,
      "step": 18000
    },
    {
      "epoch": 5.191256830601093,
      "grad_norm": 0.0007340721203945577,
      "learning_rate": 0.000896180615473109,
      "loss": 0.0041,
      "step": 18050
    },
    {
      "epoch": 5.205637043428243,
      "grad_norm": 0.00012458532000891864,
      "learning_rate": 0.0008958930112165661,
      "loss": 0.0037,
      "step": 18100
    },
    {
      "epoch": 5.220017256255392,
      "grad_norm": 0.10465632379055023,
      "learning_rate": 0.000895605406960023,
      "loss": 0.0013,
      "step": 18150
    },
    {
      "epoch": 5.234397469082542,
      "grad_norm": 5.1741106290137395e-05,
      "learning_rate": 0.00089531780270348,
      "loss": 0.0041,
      "step": 18200
    },
    {
      "epoch": 5.248777681909692,
      "grad_norm": 0.006042984314262867,
      "learning_rate": 0.000895030198446937,
      "loss": 0.0027,
      "step": 18250
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 0.08179987967014313,
      "learning_rate": 0.0008947425941903941,
      "loss": 0.0036,
      "step": 18300
    },
    {
      "epoch": 5.277538107563992,
      "grad_norm": 0.038281895220279694,
      "learning_rate": 0.000894454989933851,
      "loss": 0.0031,
      "step": 18350
    },
    {
      "epoch": 5.291918320391142,
      "grad_norm": 0.02460954710841179,
      "learning_rate": 0.0008941673856773081,
      "loss": 0.0035,
      "step": 18400
    },
    {
      "epoch": 5.306298533218292,
      "grad_norm": 0.0009244638495147228,
      "learning_rate": 0.0008938797814207651,
      "loss": 0.0038,
      "step": 18450
    },
    {
      "epoch": 5.320678746045441,
      "grad_norm": 0.011289647780358791,
      "learning_rate": 0.000893592177164222,
      "loss": 0.0015,
      "step": 18500
    },
    {
      "epoch": 5.335058958872591,
      "grad_norm": 0.00024376797955483198,
      "learning_rate": 0.0008933045729076791,
      "loss": 0.0039,
      "step": 18550
    },
    {
      "epoch": 5.349439171699741,
      "grad_norm": 0.0028557348996400833,
      "learning_rate": 0.000893016968651136,
      "loss": 0.0025,
      "step": 18600
    },
    {
      "epoch": 5.363819384526891,
      "grad_norm": 0.014725922606885433,
      "learning_rate": 0.000892729364394593,
      "loss": 0.0026,
      "step": 18650
    },
    {
      "epoch": 5.37819959735404,
      "grad_norm": 0.0002685131912585348,
      "learning_rate": 0.0008924417601380501,
      "loss": 0.0034,
      "step": 18700
    },
    {
      "epoch": 5.392579810181191,
      "grad_norm": 0.0020683431066572666,
      "learning_rate": 0.0008921541558815071,
      "loss": 0.0032,
      "step": 18750
    },
    {
      "epoch": 5.406960023008341,
      "grad_norm": 5.158097337698564e-05,
      "learning_rate": 0.0008918665516249641,
      "loss": 0.005,
      "step": 18800
    },
    {
      "epoch": 5.4213402358354905,
      "grad_norm": 0.0007151931640692055,
      "learning_rate": 0.0008915789473684211,
      "loss": 0.004,
      "step": 18850
    },
    {
      "epoch": 5.43572044866264,
      "grad_norm": 0.0045994785614311695,
      "learning_rate": 0.0008912913431118781,
      "loss": 0.0043,
      "step": 18900
    },
    {
      "epoch": 5.45010066148979,
      "grad_norm": 0.013003555126488209,
      "learning_rate": 0.000891003738855335,
      "loss": 0.0018,
      "step": 18950
    },
    {
      "epoch": 5.46448087431694,
      "grad_norm": 0.0006789312465116382,
      "learning_rate": 0.0008907161345987921,
      "loss": 0.001,
      "step": 19000
    },
    {
      "epoch": 5.4788610871440895,
      "grad_norm": 0.0002945245651062578,
      "learning_rate": 0.0008904285303422491,
      "loss": 0.0023,
      "step": 19050
    },
    {
      "epoch": 5.493241299971239,
      "grad_norm": 0.0005018890951760113,
      "learning_rate": 0.000890140926085706,
      "loss": 0.0021,
      "step": 19100
    },
    {
      "epoch": 5.507621512798389,
      "grad_norm": 0.0010024670045822859,
      "learning_rate": 0.0008898533218291632,
      "loss": 0.0014,
      "step": 19150
    },
    {
      "epoch": 5.52200172562554,
      "grad_norm": 0.0019243658753111959,
      "learning_rate": 0.0008895657175726201,
      "loss": 0.0024,
      "step": 19200
    },
    {
      "epoch": 5.536381938452689,
      "grad_norm": 6.237651541596279e-05,
      "learning_rate": 0.0008892781133160771,
      "loss": 0.0046,
      "step": 19250
    },
    {
      "epoch": 5.550762151279839,
      "grad_norm": 2.67415089183487e-05,
      "learning_rate": 0.0008889905090595341,
      "loss": 0.0019,
      "step": 19300
    },
    {
      "epoch": 5.565142364106989,
      "grad_norm": 0.0004747876082547009,
      "learning_rate": 0.0008887029048029911,
      "loss": 0.0038,
      "step": 19350
    },
    {
      "epoch": 5.5795225769341386,
      "grad_norm": 0.001727235154248774,
      "learning_rate": 0.0008884153005464481,
      "loss": 0.001,
      "step": 19400
    },
    {
      "epoch": 5.593902789761288,
      "grad_norm": 0.007210325915366411,
      "learning_rate": 0.0008881276962899051,
      "loss": 0.0043,
      "step": 19450
    },
    {
      "epoch": 5.608283002588438,
      "grad_norm": 0.0013765738112851977,
      "learning_rate": 0.0008878400920333621,
      "loss": 0.0023,
      "step": 19500
    },
    {
      "epoch": 5.622663215415588,
      "grad_norm": 0.0007723344024270773,
      "learning_rate": 0.000887552487776819,
      "loss": 0.0031,
      "step": 19550
    },
    {
      "epoch": 5.6370434282427375,
      "grad_norm": 0.018707333132624626,
      "learning_rate": 0.0008872648835202762,
      "loss": 0.0019,
      "step": 19600
    },
    {
      "epoch": 5.651423641069888,
      "grad_norm": 5.7458561059320346e-05,
      "learning_rate": 0.0008869772792637332,
      "loss": 0.0027,
      "step": 19650
    },
    {
      "epoch": 5.665803853897038,
      "grad_norm": 6.74361945129931e-05,
      "learning_rate": 0.0008866896750071901,
      "loss": 0.002,
      "step": 19700
    },
    {
      "epoch": 5.680184066724188,
      "grad_norm": 0.00016838786541484296,
      "learning_rate": 0.0008864020707506472,
      "loss": 0.0031,
      "step": 19750
    },
    {
      "epoch": 5.694564279551337,
      "grad_norm": 0.0012728540459647775,
      "learning_rate": 0.0008861144664941041,
      "loss": 0.0017,
      "step": 19800
    },
    {
      "epoch": 5.708944492378487,
      "grad_norm": 0.00042554273386485875,
      "learning_rate": 0.0008858268622375611,
      "loss": 0.0064,
      "step": 19850
    },
    {
      "epoch": 5.723324705205637,
      "grad_norm": 0.00011762934445869178,
      "learning_rate": 0.0008855392579810181,
      "loss": 0.0021,
      "step": 19900
    },
    {
      "epoch": 5.737704918032787,
      "grad_norm": 0.025097975507378578,
      "learning_rate": 0.0008852516537244751,
      "loss": 0.0016,
      "step": 19950
    },
    {
      "epoch": 5.752085130859937,
      "grad_norm": 0.030832724645733833,
      "learning_rate": 0.0008849640494679322,
      "loss": 0.0027,
      "step": 20000
    },
    {
      "epoch": 5.766465343687086,
      "grad_norm": 0.009710337966680527,
      "learning_rate": 0.0008846764452113892,
      "loss": 0.0039,
      "step": 20050
    },
    {
      "epoch": 5.780845556514237,
      "grad_norm": 0.00020493734336923808,
      "learning_rate": 0.0008843888409548462,
      "loss": 0.0053,
      "step": 20100
    },
    {
      "epoch": 5.7952257693413864,
      "grad_norm": 0.00021291484881658107,
      "learning_rate": 0.0008841012366983031,
      "loss": 0.0017,
      "step": 20150
    },
    {
      "epoch": 5.809605982168536,
      "grad_norm": 0.05505095794796944,
      "learning_rate": 0.0008838136324417602,
      "loss": 0.002,
      "step": 20200
    },
    {
      "epoch": 5.823986194995686,
      "grad_norm": 0.0058324942365288734,
      "learning_rate": 0.0008835260281852172,
      "loss": 0.002,
      "step": 20250
    },
    {
      "epoch": 5.838366407822836,
      "grad_norm": 0.0016463734209537506,
      "learning_rate": 0.0008832384239286741,
      "loss": 0.0037,
      "step": 20300
    },
    {
      "epoch": 5.852746620649985,
      "grad_norm": 9.038564166985452e-05,
      "learning_rate": 0.0008829508196721312,
      "loss": 0.0032,
      "step": 20350
    },
    {
      "epoch": 5.867126833477135,
      "grad_norm": 8.549717313144356e-05,
      "learning_rate": 0.0008826632154155881,
      "loss": 0.0021,
      "step": 20400
    },
    {
      "epoch": 5.881507046304286,
      "grad_norm": 0.03651881590485573,
      "learning_rate": 0.0008823756111590452,
      "loss": 0.005,
      "step": 20450
    },
    {
      "epoch": 5.8958872591314355,
      "grad_norm": 0.02426745556294918,
      "learning_rate": 0.0008820880069025022,
      "loss": 0.0008,
      "step": 20500
    },
    {
      "epoch": 5.910267471958585,
      "grad_norm": 0.04261685907840729,
      "learning_rate": 0.0008818004026459592,
      "loss": 0.0051,
      "step": 20550
    },
    {
      "epoch": 5.924647684785735,
      "grad_norm": 0.00011370260472176597,
      "learning_rate": 0.0008815127983894162,
      "loss": 0.0021,
      "step": 20600
    },
    {
      "epoch": 5.939027897612885,
      "grad_norm": 0.00011169584468007088,
      "learning_rate": 0.0008812251941328732,
      "loss": 0.0033,
      "step": 20650
    },
    {
      "epoch": 5.9534081104400345,
      "grad_norm": 6.322802801150829e-05,
      "learning_rate": 0.0008809375898763302,
      "loss": 0.0041,
      "step": 20700
    },
    {
      "epoch": 5.967788323267184,
      "grad_norm": 0.00011870089656440541,
      "learning_rate": 0.0008806499856197871,
      "loss": 0.0019,
      "step": 20750
    },
    {
      "epoch": 5.982168536094334,
      "grad_norm": 0.0002823813701979816,
      "learning_rate": 0.0008803623813632442,
      "loss": 0.0058,
      "step": 20800
    },
    {
      "epoch": 5.996548748921484,
      "grad_norm": 0.021601248532533646,
      "learning_rate": 0.0008800747771067013,
      "loss": 0.0025,
      "step": 20850
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.01089561264961958,
      "eval_runtime": 17.3661,
      "eval_samples_per_second": 2747.832,
      "eval_steps_per_second": 42.957,
      "step": 20862
    },
    {
      "epoch": 6.0109289617486334,
      "grad_norm": 7.880322664277628e-05,
      "learning_rate": 0.0008797871728501582,
      "loss": 0.0026,
      "step": 20900
    },
    {
      "epoch": 6.025309174575784,
      "grad_norm": 0.00011280813487246633,
      "learning_rate": 0.0008794995685936153,
      "loss": 0.0043,
      "step": 20950
    },
    {
      "epoch": 6.039689387402934,
      "grad_norm": 0.028969353064894676,
      "learning_rate": 0.0008792119643370722,
      "loss": 0.0026,
      "step": 21000
    },
    {
      "epoch": 6.054069600230084,
      "grad_norm": 0.00871734507381916,
      "learning_rate": 0.0008789243600805292,
      "loss": 0.0035,
      "step": 21050
    },
    {
      "epoch": 6.068449813057233,
      "grad_norm": 0.017978152260184288,
      "learning_rate": 0.0008786367558239862,
      "loss": 0.0016,
      "step": 21100
    },
    {
      "epoch": 6.082830025884383,
      "grad_norm": 0.03219947963953018,
      "learning_rate": 0.0008783491515674432,
      "loss": 0.0023,
      "step": 21150
    },
    {
      "epoch": 6.097210238711533,
      "grad_norm": 0.004955274984240532,
      "learning_rate": 0.0008780615473109002,
      "loss": 0.0018,
      "step": 21200
    },
    {
      "epoch": 6.1115904515386825,
      "grad_norm": 0.00012769071327056736,
      "learning_rate": 0.0008777739430543572,
      "loss": 0.0036,
      "step": 21250
    },
    {
      "epoch": 6.125970664365832,
      "grad_norm": 0.0006488055223599076,
      "learning_rate": 0.0008774863387978143,
      "loss": 0.0048,
      "step": 21300
    },
    {
      "epoch": 6.140350877192983,
      "grad_norm": 7.985693082446232e-05,
      "learning_rate": 0.0008771987345412712,
      "loss": 0.003,
      "step": 21350
    },
    {
      "epoch": 6.154731090020133,
      "grad_norm": 0.006480422802269459,
      "learning_rate": 0.0008769111302847283,
      "loss": 0.0028,
      "step": 21400
    },
    {
      "epoch": 6.169111302847282,
      "grad_norm": 0.00013043227954767644,
      "learning_rate": 0.0008766235260281852,
      "loss": 0.0007,
      "step": 21450
    },
    {
      "epoch": 6.183491515674432,
      "grad_norm": 0.11040814965963364,
      "learning_rate": 0.0008763359217716422,
      "loss": 0.0034,
      "step": 21500
    },
    {
      "epoch": 6.197871728501582,
      "grad_norm": 0.010367226786911488,
      "learning_rate": 0.0008760483175150993,
      "loss": 0.0042,
      "step": 21550
    },
    {
      "epoch": 6.212251941328732,
      "grad_norm": 9.626063547329977e-05,
      "learning_rate": 0.0008757607132585562,
      "loss": 0.001,
      "step": 21600
    },
    {
      "epoch": 6.226632154155881,
      "grad_norm": 0.003071113722398877,
      "learning_rate": 0.0008754731090020132,
      "loss": 0.0037,
      "step": 21650
    },
    {
      "epoch": 6.241012366983031,
      "grad_norm": 0.34374046325683594,
      "learning_rate": 0.0008751855047454703,
      "loss": 0.0029,
      "step": 21700
    },
    {
      "epoch": 6.255392579810181,
      "grad_norm": 0.03498578444123268,
      "learning_rate": 0.0008748979004889273,
      "loss": 0.003,
      "step": 21750
    },
    {
      "epoch": 6.2697727926373314,
      "grad_norm": 0.0019601776730269194,
      "learning_rate": 0.0008746102962323843,
      "loss": 0.0089,
      "step": 21800
    },
    {
      "epoch": 6.284153005464481,
      "grad_norm": 0.0007357635768130422,
      "learning_rate": 0.0008743226919758413,
      "loss": 0.0039,
      "step": 21850
    },
    {
      "epoch": 6.298533218291631,
      "grad_norm": 0.05718820542097092,
      "learning_rate": 0.0008740350877192983,
      "loss": 0.0027,
      "step": 21900
    },
    {
      "epoch": 6.312913431118781,
      "grad_norm": 0.051391251385211945,
      "learning_rate": 0.0008737474834627552,
      "loss": 0.0045,
      "step": 21950
    },
    {
      "epoch": 6.32729364394593,
      "grad_norm": 0.00026891339803114533,
      "learning_rate": 0.0008734598792062123,
      "loss": 0.0009,
      "step": 22000
    },
    {
      "epoch": 6.34167385677308,
      "grad_norm": 0.0002804180548992008,
      "learning_rate": 0.0008731722749496692,
      "loss": 0.0051,
      "step": 22050
    },
    {
      "epoch": 6.35605406960023,
      "grad_norm": 0.0013948758132755756,
      "learning_rate": 0.0008728846706931262,
      "loss": 0.0029,
      "step": 22100
    },
    {
      "epoch": 6.37043428242738,
      "grad_norm": 0.00031103496439754963,
      "learning_rate": 0.0008725970664365834,
      "loss": 0.0043,
      "step": 22150
    },
    {
      "epoch": 6.384814495254529,
      "grad_norm": 0.08285292237997055,
      "learning_rate": 0.0008723094621800403,
      "loss": 0.0025,
      "step": 22200
    },
    {
      "epoch": 6.39919470808168,
      "grad_norm": 0.0001796566939447075,
      "learning_rate": 0.0008720218579234973,
      "loss": 0.0027,
      "step": 22250
    },
    {
      "epoch": 6.41357492090883,
      "grad_norm": 0.037105657160282135,
      "learning_rate": 0.0008717342536669543,
      "loss": 0.0039,
      "step": 22300
    },
    {
      "epoch": 6.4279551337359795,
      "grad_norm": 0.00017051970644388348,
      "learning_rate": 0.0008714466494104113,
      "loss": 0.0006,
      "step": 22350
    },
    {
      "epoch": 6.442335346563129,
      "grad_norm": 4.41943557234481e-05,
      "learning_rate": 0.0008711590451538683,
      "loss": 0.003,
      "step": 22400
    },
    {
      "epoch": 6.456715559390279,
      "grad_norm": 0.0005413729231804609,
      "learning_rate": 0.0008708714408973253,
      "loss": 0.0034,
      "step": 22450
    },
    {
      "epoch": 6.471095772217429,
      "grad_norm": 0.012436717748641968,
      "learning_rate": 0.0008705838366407823,
      "loss": 0.0032,
      "step": 22500
    },
    {
      "epoch": 6.4854759850445785,
      "grad_norm": 0.029240807518363,
      "learning_rate": 0.0008702962323842392,
      "loss": 0.0029,
      "step": 22550
    },
    {
      "epoch": 6.499856197871728,
      "grad_norm": 0.012517004273831844,
      "learning_rate": 0.0008700086281276964,
      "loss": 0.0008,
      "step": 22600
    },
    {
      "epoch": 6.514236410698878,
      "grad_norm": 0.005616765934973955,
      "learning_rate": 0.0008697210238711533,
      "loss": 0.002,
      "step": 22650
    },
    {
      "epoch": 6.528616623526029,
      "grad_norm": 0.0009285314590670168,
      "learning_rate": 0.0008694334196146103,
      "loss": 0.0034,
      "step": 22700
    },
    {
      "epoch": 6.542996836353178,
      "grad_norm": 5.6211487390100956e-05,
      "learning_rate": 0.0008691458153580674,
      "loss": 0.001,
      "step": 22750
    },
    {
      "epoch": 6.557377049180328,
      "grad_norm": 0.10263612866401672,
      "learning_rate": 0.0008688582111015243,
      "loss": 0.0032,
      "step": 22800
    },
    {
      "epoch": 6.571757262007478,
      "grad_norm": 0.00025634781923145056,
      "learning_rate": 0.0008685706068449813,
      "loss": 0.0061,
      "step": 22850
    },
    {
      "epoch": 6.5861374748346275,
      "grad_norm": 0.0001773853728082031,
      "learning_rate": 0.0008682830025884383,
      "loss": 0.0037,
      "step": 22900
    },
    {
      "epoch": 6.600517687661777,
      "grad_norm": 4.7686673497082666e-05,
      "learning_rate": 0.0008679953983318953,
      "loss": 0.0019,
      "step": 22950
    },
    {
      "epoch": 6.614897900488927,
      "grad_norm": 0.008216370828449726,
      "learning_rate": 0.0008677077940753524,
      "loss": 0.0017,
      "step": 23000
    },
    {
      "epoch": 6.629278113316077,
      "grad_norm": 0.14419430494308472,
      "learning_rate": 0.0008674201898188094,
      "loss": 0.006,
      "step": 23050
    },
    {
      "epoch": 6.6436583261432265,
      "grad_norm": 7.603684935020283e-05,
      "learning_rate": 0.0008671325855622664,
      "loss": 0.003,
      "step": 23100
    },
    {
      "epoch": 6.658038538970377,
      "grad_norm": 0.020095882937312126,
      "learning_rate": 0.0008668449813057233,
      "loss": 0.0041,
      "step": 23150
    },
    {
      "epoch": 6.672418751797527,
      "grad_norm": 0.0001542700338177383,
      "learning_rate": 0.0008665573770491804,
      "loss": 0.0014,
      "step": 23200
    },
    {
      "epoch": 6.686798964624677,
      "grad_norm": 0.006171111017465591,
      "learning_rate": 0.0008662697727926373,
      "loss": 0.0019,
      "step": 23250
    },
    {
      "epoch": 6.701179177451826,
      "grad_norm": 0.02121472731232643,
      "learning_rate": 0.0008659821685360943,
      "loss": 0.0019,
      "step": 23300
    },
    {
      "epoch": 6.715559390278976,
      "grad_norm": 0.00013102914090268314,
      "learning_rate": 0.0008656945642795514,
      "loss": 0.0029,
      "step": 23350
    },
    {
      "epoch": 6.729939603106126,
      "grad_norm": 0.00033378074294887483,
      "learning_rate": 0.0008654069600230083,
      "loss": 0.0041,
      "step": 23400
    },
    {
      "epoch": 6.744319815933276,
      "grad_norm": 0.0002622544125188142,
      "learning_rate": 0.0008651193557664654,
      "loss": 0.0014,
      "step": 23450
    },
    {
      "epoch": 6.758700028760425,
      "grad_norm": 0.0010177389485761523,
      "learning_rate": 0.0008648317515099224,
      "loss": 0.0045,
      "step": 23500
    },
    {
      "epoch": 6.773080241587575,
      "grad_norm": 0.00018815291696228087,
      "learning_rate": 0.0008645441472533794,
      "loss": 0.0036,
      "step": 23550
    },
    {
      "epoch": 6.787460454414726,
      "grad_norm": 0.0015386937884613872,
      "learning_rate": 0.0008642565429968364,
      "loss": 0.0027,
      "step": 23600
    },
    {
      "epoch": 6.801840667241875,
      "grad_norm": 0.03796341270208359,
      "learning_rate": 0.0008639689387402934,
      "loss": 0.0031,
      "step": 23650
    },
    {
      "epoch": 6.816220880069025,
      "grad_norm": 0.0003490727103780955,
      "learning_rate": 0.0008636813344837504,
      "loss": 0.0033,
      "step": 23700
    },
    {
      "epoch": 6.830601092896175,
      "grad_norm": 0.00010744699102360755,
      "learning_rate": 0.0008633937302272073,
      "loss": 0.0061,
      "step": 23750
    },
    {
      "epoch": 6.844981305723325,
      "grad_norm": 0.00043509749229997396,
      "learning_rate": 0.0008631061259706644,
      "loss": 0.0016,
      "step": 23800
    },
    {
      "epoch": 6.859361518550474,
      "grad_norm": 0.0002727511164266616,
      "learning_rate": 0.0008628185217141213,
      "loss": 0.0014,
      "step": 23850
    },
    {
      "epoch": 6.873741731377624,
      "grad_norm": 0.01723557710647583,
      "learning_rate": 0.0008625309174575784,
      "loss": 0.0021,
      "step": 23900
    },
    {
      "epoch": 6.888121944204774,
      "grad_norm": 0.024265509098768234,
      "learning_rate": 0.0008622433132010355,
      "loss": 0.0066,
      "step": 23950
    },
    {
      "epoch": 6.902502157031924,
      "grad_norm": 0.0003635526809375733,
      "learning_rate": 0.0008619557089444924,
      "loss": 0.0009,
      "step": 24000
    },
    {
      "epoch": 6.916882369859074,
      "grad_norm": 0.00017165033204946667,
      "learning_rate": 0.0008616681046879494,
      "loss": 0.0033,
      "step": 24050
    },
    {
      "epoch": 6.931262582686224,
      "grad_norm": 0.004259780049324036,
      "learning_rate": 0.0008613805004314064,
      "loss": 0.0036,
      "step": 24100
    },
    {
      "epoch": 6.945642795513374,
      "grad_norm": 0.0047028991393744946,
      "learning_rate": 0.0008610928961748634,
      "loss": 0.0039,
      "step": 24150
    },
    {
      "epoch": 6.9600230083405235,
      "grad_norm": 0.021518420428037643,
      "learning_rate": 0.0008608052919183203,
      "loss": 0.0029,
      "step": 24200
    },
    {
      "epoch": 6.974403221167673,
      "grad_norm": 0.019764265045523643,
      "learning_rate": 0.0008605176876617774,
      "loss": 0.0029,
      "step": 24250
    },
    {
      "epoch": 6.988783433994823,
      "grad_norm": 0.0004288625787012279,
      "learning_rate": 0.0008602300834052345,
      "loss": 0.0025,
      "step": 24300
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.011976854875683784,
      "eval_runtime": 16.9844,
      "eval_samples_per_second": 2809.57,
      "eval_steps_per_second": 43.923,
      "step": 24339
    },
    {
      "epoch": 7.003163646821973,
      "grad_norm": 0.019994309172034264,
      "learning_rate": 0.0008599424791486914,
      "loss": 0.002,
      "step": 24350
    },
    {
      "epoch": 7.017543859649122,
      "grad_norm": 0.00031249996391125023,
      "learning_rate": 0.0008596548748921485,
      "loss": 0.0056,
      "step": 24400
    },
    {
      "epoch": 7.031924072476273,
      "grad_norm": 0.0007027011597529054,
      "learning_rate": 0.0008593672706356054,
      "loss": 0.0033,
      "step": 24450
    },
    {
      "epoch": 7.046304285303423,
      "grad_norm": 0.07292474806308746,
      "learning_rate": 0.0008590796663790624,
      "loss": 0.0041,
      "step": 24500
    },
    {
      "epoch": 7.0606844981305725,
      "grad_norm": 0.00032144307624548674,
      "learning_rate": 0.0008587920621225195,
      "loss": 0.0039,
      "step": 24550
    },
    {
      "epoch": 7.075064710957722,
      "grad_norm": 0.00015129554958548397,
      "learning_rate": 0.0008585044578659764,
      "loss": 0.003,
      "step": 24600
    },
    {
      "epoch": 7.089444923784872,
      "grad_norm": 0.00044275386608205736,
      "learning_rate": 0.0008582168536094334,
      "loss": 0.0029,
      "step": 24650
    },
    {
      "epoch": 7.103825136612022,
      "grad_norm": 3.284498961875215e-05,
      "learning_rate": 0.0008579292493528904,
      "loss": 0.0013,
      "step": 24700
    },
    {
      "epoch": 7.1182053494391715,
      "grad_norm": 0.009021379053592682,
      "learning_rate": 0.0008576416450963475,
      "loss": 0.0017,
      "step": 24750
    },
    {
      "epoch": 7.132585562266321,
      "grad_norm": 3.852946974802762e-05,
      "learning_rate": 0.0008573540408398044,
      "loss": 0.0013,
      "step": 24800
    },
    {
      "epoch": 7.146965775093471,
      "grad_norm": 0.0005320453201420605,
      "learning_rate": 0.0008570664365832615,
      "loss": 0.0015,
      "step": 24850
    },
    {
      "epoch": 7.161345987920622,
      "grad_norm": 0.00012119326856918633,
      "learning_rate": 0.0008567788323267185,
      "loss": 0.0021,
      "step": 24900
    },
    {
      "epoch": 7.175726200747771,
      "grad_norm": 7.463887595804408e-05,
      "learning_rate": 0.0008564912280701754,
      "loss": 0.0046,
      "step": 24950
    },
    {
      "epoch": 7.190106413574921,
      "grad_norm": 0.0007139949593693018,
      "learning_rate": 0.0008562036238136325,
      "loss": 0.003,
      "step": 25000
    },
    {
      "epoch": 7.204486626402071,
      "grad_norm": 0.000583405198995024,
      "learning_rate": 0.0008559160195570894,
      "loss": 0.0022,
      "step": 25050
    },
    {
      "epoch": 7.218866839229221,
      "grad_norm": 5.234239870333113e-05,
      "learning_rate": 0.0008556284153005464,
      "loss": 0.0015,
      "step": 25100
    },
    {
      "epoch": 7.23324705205637,
      "grad_norm": 0.018149951472878456,
      "learning_rate": 0.0008553408110440036,
      "loss": 0.003,
      "step": 25150
    },
    {
      "epoch": 7.24762726488352,
      "grad_norm": 0.0002598926948849112,
      "learning_rate": 0.0008550532067874605,
      "loss": 0.0018,
      "step": 25200
    },
    {
      "epoch": 7.26200747771067,
      "grad_norm": 0.0002562269219197333,
      "learning_rate": 0.0008547656025309175,
      "loss": 0.0038,
      "step": 25250
    },
    {
      "epoch": 7.27638769053782,
      "grad_norm": 7.359019946306944e-05,
      "learning_rate": 0.0008544779982743745,
      "loss": 0.0077,
      "step": 25300
    },
    {
      "epoch": 7.29076790336497,
      "grad_norm": 0.012902754358947277,
      "learning_rate": 0.0008541903940178315,
      "loss": 0.0068,
      "step": 25350
    },
    {
      "epoch": 7.30514811619212,
      "grad_norm": 0.00011054117931053042,
      "learning_rate": 0.0008539027897612884,
      "loss": 0.0059,
      "step": 25400
    },
    {
      "epoch": 7.31952832901927,
      "grad_norm": 3.229968933737837e-05,
      "learning_rate": 0.0008536151855047455,
      "loss": 0.0025,
      "step": 25450
    },
    {
      "epoch": 7.333908541846419,
      "grad_norm": 6.77504503983073e-05,
      "learning_rate": 0.0008533275812482025,
      "loss": 0.0046,
      "step": 25500
    },
    {
      "epoch": 7.348288754673569,
      "grad_norm": 0.0006085251807235181,
      "learning_rate": 0.0008530399769916594,
      "loss": 0.004,
      "step": 25550
    },
    {
      "epoch": 7.362668967500719,
      "grad_norm": 0.003436828264966607,
      "learning_rate": 0.0008527523727351166,
      "loss": 0.0036,
      "step": 25600
    },
    {
      "epoch": 7.377049180327869,
      "grad_norm": 0.5195486545562744,
      "learning_rate": 0.0008524647684785735,
      "loss": 0.0026,
      "step": 25650
    },
    {
      "epoch": 7.391429393155018,
      "grad_norm": 5.276547381072305e-05,
      "learning_rate": 0.0008521771642220305,
      "loss": 0.002,
      "step": 25700
    },
    {
      "epoch": 7.405809605982169,
      "grad_norm": 0.030460676178336143,
      "learning_rate": 0.0008518895599654876,
      "loss": 0.0026,
      "step": 25750
    },
    {
      "epoch": 7.420189818809319,
      "grad_norm": 0.0005592312081716955,
      "learning_rate": 0.0008516019557089445,
      "loss": 0.0051,
      "step": 25800
    },
    {
      "epoch": 7.4345700316364685,
      "grad_norm": 0.00010867279343074188,
      "learning_rate": 0.0008513143514524015,
      "loss": 0.0034,
      "step": 25850
    },
    {
      "epoch": 7.448950244463618,
      "grad_norm": 0.00014456571079790592,
      "learning_rate": 0.0008510267471958585,
      "loss": 0.0016,
      "step": 25900
    },
    {
      "epoch": 7.463330457290768,
      "grad_norm": 0.00035526143619790673,
      "learning_rate": 0.0008507391429393155,
      "loss": 0.0035,
      "step": 25950
    },
    {
      "epoch": 7.477710670117918,
      "grad_norm": 0.05046256259083748,
      "learning_rate": 0.0008504515386827724,
      "loss": 0.0014,
      "step": 26000
    },
    {
      "epoch": 7.492090882945067,
      "grad_norm": 8.183404133887962e-05,
      "learning_rate": 0.0008501639344262296,
      "loss": 0.0008,
      "step": 26050
    },
    {
      "epoch": 7.506471095772217,
      "grad_norm": 0.005483269225805998,
      "learning_rate": 0.0008498763301696866,
      "loss": 0.0021,
      "step": 26100
    },
    {
      "epoch": 7.520851308599367,
      "grad_norm": 0.00045724076335318387,
      "learning_rate": 0.0008495887259131435,
      "loss": 0.0019,
      "step": 26150
    },
    {
      "epoch": 7.5352315214265175,
      "grad_norm": 0.061376240104436874,
      "learning_rate": 0.0008493011216566006,
      "loss": 0.0022,
      "step": 26200
    },
    {
      "epoch": 7.549611734253667,
      "grad_norm": 0.11139991134405136,
      "learning_rate": 0.0008490135174000575,
      "loss": 0.0032,
      "step": 26250
    },
    {
      "epoch": 7.563991947080817,
      "grad_norm": 5.5324035201920196e-05,
      "learning_rate": 0.0008487259131435145,
      "loss": 0.0035,
      "step": 26300
    },
    {
      "epoch": 7.578372159907967,
      "grad_norm": 0.0322575718164444,
      "learning_rate": 0.0008484383088869716,
      "loss": 0.0018,
      "step": 26350
    },
    {
      "epoch": 7.5927523727351165,
      "grad_norm": 0.00011877848737640306,
      "learning_rate": 0.0008481507046304285,
      "loss": 0.0049,
      "step": 26400
    },
    {
      "epoch": 7.607132585562266,
      "grad_norm": 0.0009156683809123933,
      "learning_rate": 0.0008478631003738856,
      "loss": 0.003,
      "step": 26450
    },
    {
      "epoch": 7.621512798389416,
      "grad_norm": 0.016195151954889297,
      "learning_rate": 0.0008475754961173426,
      "loss": 0.0037,
      "step": 26500
    },
    {
      "epoch": 7.635893011216566,
      "grad_norm": 0.0008524140575900674,
      "learning_rate": 0.0008472878918607996,
      "loss": 0.0018,
      "step": 26550
    },
    {
      "epoch": 7.6502732240437155,
      "grad_norm": 0.000519667228218168,
      "learning_rate": 0.0008470002876042565,
      "loss": 0.0027,
      "step": 26600
    },
    {
      "epoch": 7.664653436870866,
      "grad_norm": 0.0061040944419801235,
      "learning_rate": 0.0008467126833477136,
      "loss": 0.0025,
      "step": 26650
    },
    {
      "epoch": 7.679033649698016,
      "grad_norm": 0.00031466915970668197,
      "learning_rate": 0.0008464250790911706,
      "loss": 0.0019,
      "step": 26700
    },
    {
      "epoch": 7.693413862525166,
      "grad_norm": 8.163817983586341e-05,
      "learning_rate": 0.0008461374748346275,
      "loss": 0.0043,
      "step": 26750
    },
    {
      "epoch": 7.707794075352315,
      "grad_norm": 0.00011522268323460594,
      "learning_rate": 0.0008458498705780846,
      "loss": 0.0027,
      "step": 26800
    },
    {
      "epoch": 7.722174288179465,
      "grad_norm": 0.0008860942325554788,
      "learning_rate": 0.0008455622663215415,
      "loss": 0.0015,
      "step": 26850
    },
    {
      "epoch": 7.736554501006615,
      "grad_norm": 0.0005962763680145144,
      "learning_rate": 0.0008452746620649986,
      "loss": 0.0014,
      "step": 26900
    },
    {
      "epoch": 7.7509347138337645,
      "grad_norm": 0.010959522798657417,
      "learning_rate": 0.0008449870578084556,
      "loss": 0.0037,
      "step": 26950
    },
    {
      "epoch": 7.765314926660914,
      "grad_norm": 0.0009220065549015999,
      "learning_rate": 0.0008446994535519126,
      "loss": 0.0044,
      "step": 27000
    },
    {
      "epoch": 7.779695139488064,
      "grad_norm": 0.008778112940490246,
      "learning_rate": 0.0008444118492953696,
      "loss": 0.0055,
      "step": 27050
    },
    {
      "epoch": 7.794075352315215,
      "grad_norm": 0.000526658957824111,
      "learning_rate": 0.0008441242450388266,
      "loss": 0.0012,
      "step": 27100
    },
    {
      "epoch": 7.808455565142364,
      "grad_norm": 0.0028191234450787306,
      "learning_rate": 0.0008438366407822836,
      "loss": 0.0012,
      "step": 27150
    },
    {
      "epoch": 7.822835777969514,
      "grad_norm": 0.004593738820403814,
      "learning_rate": 0.0008435490365257405,
      "loss": 0.0006,
      "step": 27200
    },
    {
      "epoch": 7.837215990796664,
      "grad_norm": 0.000921507366001606,
      "learning_rate": 0.0008432614322691976,
      "loss": 0.0032,
      "step": 27250
    },
    {
      "epoch": 7.851596203623814,
      "grad_norm": 0.00023347674869000912,
      "learning_rate": 0.0008429738280126547,
      "loss": 0.0018,
      "step": 27300
    },
    {
      "epoch": 7.865976416450963,
      "grad_norm": 0.00048379559302702546,
      "learning_rate": 0.0008426862237561116,
      "loss": 0.0025,
      "step": 27350
    },
    {
      "epoch": 7.880356629278113,
      "grad_norm": 0.037978194653987885,
      "learning_rate": 0.0008423986194995687,
      "loss": 0.0031,
      "step": 27400
    },
    {
      "epoch": 7.894736842105263,
      "grad_norm": 0.0004440689808689058,
      "learning_rate": 0.0008421110152430256,
      "loss": 0.0029,
      "step": 27450
    },
    {
      "epoch": 7.909117054932413,
      "grad_norm": 0.000156562338816002,
      "learning_rate": 0.0008418234109864826,
      "loss": 0.0015,
      "step": 27500
    },
    {
      "epoch": 7.923497267759563,
      "grad_norm": 0.00014464194828178734,
      "learning_rate": 0.0008415358067299396,
      "loss": 0.0019,
      "step": 27550
    },
    {
      "epoch": 7.937877480586713,
      "grad_norm": 0.00011815078323706985,
      "learning_rate": 0.0008412482024733966,
      "loss": 0.0016,
      "step": 27600
    },
    {
      "epoch": 7.952257693413863,
      "grad_norm": 0.009272935800254345,
      "learning_rate": 0.0008409605982168536,
      "loss": 0.002,
      "step": 27650
    },
    {
      "epoch": 7.966637906241012,
      "grad_norm": 0.0010663492139428854,
      "learning_rate": 0.0008406729939603106,
      "loss": 0.0047,
      "step": 27700
    },
    {
      "epoch": 7.981018119068162,
      "grad_norm": 0.00021554330305662006,
      "learning_rate": 0.0008403853897037677,
      "loss": 0.0024,
      "step": 27750
    },
    {
      "epoch": 7.995398331895312,
      "grad_norm": 0.021015876904129982,
      "learning_rate": 0.0008400977854472246,
      "loss": 0.002,
      "step": 27800
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.010976948775351048,
      "eval_runtime": 17.7041,
      "eval_samples_per_second": 2695.364,
      "eval_steps_per_second": 42.137,
      "step": 27816
    },
    {
      "epoch": 8.009778544722462,
      "grad_norm": 8.35153041407466e-05,
      "learning_rate": 0.0008398101811906817,
      "loss": 0.0056,
      "step": 27850
    },
    {
      "epoch": 8.024158757549612,
      "grad_norm": 0.00015479512512683868,
      "learning_rate": 0.0008395225769341387,
      "loss": 0.0043,
      "step": 27900
    },
    {
      "epoch": 8.038538970376761,
      "grad_norm": 7.18752562534064e-05,
      "learning_rate": 0.0008392349726775956,
      "loss": 0.0054,
      "step": 27950
    },
    {
      "epoch": 8.052919183203912,
      "grad_norm": 0.00015394222282338887,
      "learning_rate": 0.0008389473684210527,
      "loss": 0.003,
      "step": 28000
    },
    {
      "epoch": 8.06729939603106,
      "grad_norm": 0.018209723755717278,
      "learning_rate": 0.0008386597641645096,
      "loss": 0.0025,
      "step": 28050
    },
    {
      "epoch": 8.081679608858211,
      "grad_norm": 2.1856358216609806e-05,
      "learning_rate": 0.0008383721599079666,
      "loss": 0.0017,
      "step": 28100
    },
    {
      "epoch": 8.09605982168536,
      "grad_norm": 0.00048730961862020195,
      "learning_rate": 0.0008380845556514236,
      "loss": 0.0018,
      "step": 28150
    },
    {
      "epoch": 8.11044003451251,
      "grad_norm": 0.024044957011938095,
      "learning_rate": 0.0008377969513948807,
      "loss": 0.0019,
      "step": 28200
    },
    {
      "epoch": 8.124820247339661,
      "grad_norm": 0.00979000236839056,
      "learning_rate": 0.0008375093471383377,
      "loss": 0.0034,
      "step": 28250
    },
    {
      "epoch": 8.13920046016681,
      "grad_norm": 0.0003764293505810201,
      "learning_rate": 0.0008372217428817947,
      "loss": 0.002,
      "step": 28300
    },
    {
      "epoch": 8.15358067299396,
      "grad_norm": 0.0003721156681422144,
      "learning_rate": 0.0008369341386252517,
      "loss": 0.0017,
      "step": 28350
    },
    {
      "epoch": 8.16796088582111,
      "grad_norm": 0.00662772823125124,
      "learning_rate": 0.0008366465343687086,
      "loss": 0.0028,
      "step": 28400
    },
    {
      "epoch": 8.18234109864826,
      "grad_norm": 0.03422058746218681,
      "learning_rate": 0.0008363589301121657,
      "loss": 0.0024,
      "step": 28450
    },
    {
      "epoch": 8.19672131147541,
      "grad_norm": 9.983070776797831e-05,
      "learning_rate": 0.0008360713258556227,
      "loss": 0.0021,
      "step": 28500
    },
    {
      "epoch": 8.21110152430256,
      "grad_norm": 0.022383948788046837,
      "learning_rate": 0.0008357837215990796,
      "loss": 0.0009,
      "step": 28550
    },
    {
      "epoch": 8.225481737129709,
      "grad_norm": 0.00011156456457683817,
      "learning_rate": 0.0008354961173425368,
      "loss": 0.0016,
      "step": 28600
    },
    {
      "epoch": 8.23986194995686,
      "grad_norm": 0.00032371244742535055,
      "learning_rate": 0.0008352085130859937,
      "loss": 0.0011,
      "step": 28650
    },
    {
      "epoch": 8.25424216278401,
      "grad_norm": 0.08149660378694534,
      "learning_rate": 0.0008349209088294507,
      "loss": 0.0022,
      "step": 28700
    },
    {
      "epoch": 8.268622375611159,
      "grad_norm": 0.0040032630786299706,
      "learning_rate": 0.0008346333045729077,
      "loss": 0.002,
      "step": 28750
    },
    {
      "epoch": 8.28300258843831,
      "grad_norm": 7.510860450565815e-05,
      "learning_rate": 0.0008343457003163647,
      "loss": 0.0026,
      "step": 28800
    },
    {
      "epoch": 8.297382801265458,
      "grad_norm": 0.028139567002654076,
      "learning_rate": 0.0008340580960598217,
      "loss": 0.003,
      "step": 28850
    },
    {
      "epoch": 8.311763014092609,
      "grad_norm": 0.0017425476107746363,
      "learning_rate": 0.0008337704918032787,
      "loss": 0.0014,
      "step": 28900
    },
    {
      "epoch": 8.326143226919758,
      "grad_norm": 0.0001763973996276036,
      "learning_rate": 0.0008334828875467357,
      "loss": 0.0044,
      "step": 28950
    },
    {
      "epoch": 8.340523439746908,
      "grad_norm": 0.0008044889545999467,
      "learning_rate": 0.0008331952832901926,
      "loss": 0.0083,
      "step": 29000
    },
    {
      "epoch": 8.354903652574059,
      "grad_norm": 7.45380311855115e-05,
      "learning_rate": 0.0008329076790336498,
      "loss": 0.0021,
      "step": 29050
    },
    {
      "epoch": 8.369283865401208,
      "grad_norm": 0.00101820460986346,
      "learning_rate": 0.0008326200747771068,
      "loss": 0.0024,
      "step": 29100
    },
    {
      "epoch": 8.383664078228358,
      "grad_norm": 5.976569082122296e-05,
      "learning_rate": 0.0008323324705205637,
      "loss": 0.0017,
      "step": 29150
    },
    {
      "epoch": 8.398044291055507,
      "grad_norm": 0.006968632806092501,
      "learning_rate": 0.0008320448662640208,
      "loss": 0.003,
      "step": 29200
    },
    {
      "epoch": 8.412424503882658,
      "grad_norm": 6.171956192702055e-05,
      "learning_rate": 0.0008317572620074777,
      "loss": 0.0032,
      "step": 29250
    },
    {
      "epoch": 8.426804716709807,
      "grad_norm": 0.011776742525398731,
      "learning_rate": 0.0008314696577509347,
      "loss": 0.0043,
      "step": 29300
    },
    {
      "epoch": 8.441184929536957,
      "grad_norm": 0.0006017155828885734,
      "learning_rate": 0.0008311820534943917,
      "loss": 0.0017,
      "step": 29350
    },
    {
      "epoch": 8.455565142364106,
      "grad_norm": 5.154413156560622e-05,
      "learning_rate": 0.0008308944492378487,
      "loss": 0.0021,
      "step": 29400
    },
    {
      "epoch": 8.469945355191257,
      "grad_norm": 0.0003037218120880425,
      "learning_rate": 0.0008306068449813057,
      "loss": 0.0015,
      "step": 29450
    },
    {
      "epoch": 8.484325568018406,
      "grad_norm": 0.002856142120435834,
      "learning_rate": 0.0008303192407247628,
      "loss": 0.0034,
      "step": 29500
    },
    {
      "epoch": 8.498705780845556,
      "grad_norm": 0.22295381128787994,
      "learning_rate": 0.0008300316364682198,
      "loss": 0.002,
      "step": 29550
    },
    {
      "epoch": 8.513085993672707,
      "grad_norm": 6.146172381704673e-05,
      "learning_rate": 0.0008297440322116767,
      "loss": 0.0034,
      "step": 29600
    },
    {
      "epoch": 8.527466206499856,
      "grad_norm": 0.00019050142145715654,
      "learning_rate": 0.0008294564279551338,
      "loss": 0.001,
      "step": 29650
    },
    {
      "epoch": 8.541846419327007,
      "grad_norm": 6.310046592261642e-05,
      "learning_rate": 0.0008291688236985907,
      "loss": 0.0047,
      "step": 29700
    },
    {
      "epoch": 8.556226632154155,
      "grad_norm": 0.0001367239310638979,
      "learning_rate": 0.0008288812194420477,
      "loss": 0.0024,
      "step": 29750
    },
    {
      "epoch": 8.570606844981306,
      "grad_norm": 0.00034166668774560094,
      "learning_rate": 0.0008285936151855048,
      "loss": 0.0024,
      "step": 29800
    },
    {
      "epoch": 8.584987057808455,
      "grad_norm": 0.05053228512406349,
      "learning_rate": 0.0008283060109289617,
      "loss": 0.0011,
      "step": 29850
    },
    {
      "epoch": 8.599367270635605,
      "grad_norm": 0.008693603798747063,
      "learning_rate": 0.0008280184066724187,
      "loss": 0.0019,
      "step": 29900
    },
    {
      "epoch": 8.613747483462756,
      "grad_norm": 0.030302369967103004,
      "learning_rate": 0.0008277308024158758,
      "loss": 0.0026,
      "step": 29950
    },
    {
      "epoch": 8.628127696289905,
      "grad_norm": 0.0006792289786972106,
      "learning_rate": 0.0008274431981593328,
      "loss": 0.006,
      "step": 30000
    },
    {
      "epoch": 8.642507909117056,
      "grad_norm": 0.030277181416749954,
      "learning_rate": 0.0008271555939027898,
      "loss": 0.002,
      "step": 30050
    },
    {
      "epoch": 8.656888121944204,
      "grad_norm": 9.51669062487781e-05,
      "learning_rate": 0.0008268679896462468,
      "loss": 0.0042,
      "step": 30100
    },
    {
      "epoch": 8.671268334771355,
      "grad_norm": 0.02011244371533394,
      "learning_rate": 0.0008265803853897038,
      "loss": 0.0014,
      "step": 30150
    },
    {
      "epoch": 8.685648547598504,
      "grad_norm": 8.154286479111761e-05,
      "learning_rate": 0.0008262927811331607,
      "loss": 0.0037,
      "step": 30200
    },
    {
      "epoch": 8.700028760425655,
      "grad_norm": 0.01988256722688675,
      "learning_rate": 0.0008260051768766178,
      "loss": 0.0067,
      "step": 30250
    },
    {
      "epoch": 8.714408973252803,
      "grad_norm": 0.01635463908314705,
      "learning_rate": 0.0008257175726200747,
      "loss": 0.0011,
      "step": 30300
    },
    {
      "epoch": 8.728789186079954,
      "grad_norm": 0.02313128300011158,
      "learning_rate": 0.0008254299683635318,
      "loss": 0.0034,
      "step": 30350
    },
    {
      "epoch": 8.743169398907105,
      "grad_norm": 5.175778642296791e-05,
      "learning_rate": 0.0008251423641069889,
      "loss": 0.0025,
      "step": 30400
    },
    {
      "epoch": 8.757549611734254,
      "grad_norm": 0.02088138461112976,
      "learning_rate": 0.0008248547598504458,
      "loss": 0.0021,
      "step": 30450
    },
    {
      "epoch": 8.771929824561404,
      "grad_norm": 2.8257900339667685e-05,
      "learning_rate": 0.0008245671555939028,
      "loss": 0.0048,
      "step": 30500
    },
    {
      "epoch": 8.786310037388553,
      "grad_norm": 0.00018632167484611273,
      "learning_rate": 0.0008242795513373598,
      "loss": 0.0024,
      "step": 30550
    },
    {
      "epoch": 8.800690250215704,
      "grad_norm": 0.00011847449786728248,
      "learning_rate": 0.0008239919470808168,
      "loss": 0.0006,
      "step": 30600
    },
    {
      "epoch": 8.815070463042852,
      "grad_norm": 0.028768790885806084,
      "learning_rate": 0.0008237043428242738,
      "loss": 0.0039,
      "step": 30650
    },
    {
      "epoch": 8.829450675870003,
      "grad_norm": 0.03851643577218056,
      "learning_rate": 0.0008234167385677308,
      "loss": 0.0029,
      "step": 30700
    },
    {
      "epoch": 8.843830888697152,
      "grad_norm": 0.01548119354993105,
      "learning_rate": 0.0008231291343111878,
      "loss": 0.0026,
      "step": 30750
    },
    {
      "epoch": 8.858211101524303,
      "grad_norm": 0.00047011664719320834,
      "learning_rate": 0.0008228415300546448,
      "loss": 0.0016,
      "step": 30800
    },
    {
      "epoch": 8.872591314351453,
      "grad_norm": 0.00016558424977120012,
      "learning_rate": 0.0008225539257981019,
      "loss": 0.005,
      "step": 30850
    },
    {
      "epoch": 8.886971527178602,
      "grad_norm": 0.0008124941377900541,
      "learning_rate": 0.0008222663215415588,
      "loss": 0.0029,
      "step": 30900
    },
    {
      "epoch": 8.901351740005753,
      "grad_norm": 0.011883671395480633,
      "learning_rate": 0.0008219787172850158,
      "loss": 0.0016,
      "step": 30950
    },
    {
      "epoch": 8.915731952832902,
      "grad_norm": 0.0001715020916890353,
      "learning_rate": 0.0008216911130284729,
      "loss": 0.0044,
      "step": 31000
    },
    {
      "epoch": 8.930112165660052,
      "grad_norm": 0.003818338504061103,
      "learning_rate": 0.0008214035087719298,
      "loss": 0.0027,
      "step": 31050
    },
    {
      "epoch": 8.944492378487201,
      "grad_norm": 0.015482080169022083,
      "learning_rate": 0.0008211159045153868,
      "loss": 0.0031,
      "step": 31100
    },
    {
      "epoch": 8.958872591314352,
      "grad_norm": 5.915931615163572e-05,
      "learning_rate": 0.0008208283002588438,
      "loss": 0.0024,
      "step": 31150
    },
    {
      "epoch": 8.9732528041415,
      "grad_norm": 0.0022032950073480606,
      "learning_rate": 0.0008205406960023009,
      "loss": 0.0039,
      "step": 31200
    },
    {
      "epoch": 8.987633016968651,
      "grad_norm": 0.017888078466057777,
      "learning_rate": 0.0008202530917457579,
      "loss": 0.0039,
      "step": 31250
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.012971486896276474,
      "eval_runtime": 17.2851,
      "eval_samples_per_second": 2760.704,
      "eval_steps_per_second": 43.159,
      "step": 31293
    },
    {
      "epoch": 9.002013229795802,
      "grad_norm": 0.00018607084348332137,
      "learning_rate": 0.0008199654874892149,
      "loss": 0.0023,
      "step": 31300
    },
    {
      "epoch": 9.01639344262295,
      "grad_norm": 0.0001271576329600066,
      "learning_rate": 0.0008196778832326719,
      "loss": 0.0028,
      "step": 31350
    },
    {
      "epoch": 9.030773655450101,
      "grad_norm": 0.018971838057041168,
      "learning_rate": 0.0008193902789761288,
      "loss": 0.0038,
      "step": 31400
    },
    {
      "epoch": 9.04515386827725,
      "grad_norm": 0.0009444043971598148,
      "learning_rate": 0.0008191026747195859,
      "loss": 0.0022,
      "step": 31450
    },
    {
      "epoch": 9.0595340811044,
      "grad_norm": 0.04780792072415352,
      "learning_rate": 0.0008188150704630428,
      "loss": 0.0014,
      "step": 31500
    },
    {
      "epoch": 9.07391429393155,
      "grad_norm": 0.0005970408674329519,
      "learning_rate": 0.0008185274662064998,
      "loss": 0.0032,
      "step": 31550
    },
    {
      "epoch": 9.0882945067587,
      "grad_norm": 0.01692398637533188,
      "learning_rate": 0.000818239861949957,
      "loss": 0.0022,
      "step": 31600
    },
    {
      "epoch": 9.102674719585849,
      "grad_norm": 0.016289353370666504,
      "learning_rate": 0.0008179522576934139,
      "loss": 0.0011,
      "step": 31650
    },
    {
      "epoch": 9.117054932413,
      "grad_norm": 0.0001551360619487241,
      "learning_rate": 0.0008176646534368709,
      "loss": 0.0021,
      "step": 31700
    },
    {
      "epoch": 9.13143514524015,
      "grad_norm": 4.067311601829715e-05,
      "learning_rate": 0.0008173770491803279,
      "loss": 0.0027,
      "step": 31750
    },
    {
      "epoch": 9.1458153580673,
      "grad_norm": 0.03163857012987137,
      "learning_rate": 0.0008170894449237849,
      "loss": 0.0049,
      "step": 31800
    },
    {
      "epoch": 9.16019557089445,
      "grad_norm": 7.108077988959849e-05,
      "learning_rate": 0.0008168018406672419,
      "loss": 0.0011,
      "step": 31850
    },
    {
      "epoch": 9.174575783721599,
      "grad_norm": 6.130289693828672e-05,
      "learning_rate": 0.0008165142364106989,
      "loss": 0.0013,
      "step": 31900
    },
    {
      "epoch": 9.18895599654875,
      "grad_norm": 0.006141204386949539,
      "learning_rate": 0.0008162266321541559,
      "loss": 0.001,
      "step": 31950
    },
    {
      "epoch": 9.203336209375898,
      "grad_norm": 0.04900874197483063,
      "learning_rate": 0.0008159390278976128,
      "loss": 0.0033,
      "step": 32000
    },
    {
      "epoch": 9.217716422203049,
      "grad_norm": 0.00035101958201266825,
      "learning_rate": 0.00081565142364107,
      "loss": 0.0034,
      "step": 32050
    },
    {
      "epoch": 9.232096635030198,
      "grad_norm": 0.0031141650397330523,
      "learning_rate": 0.0008153638193845269,
      "loss": 0.0024,
      "step": 32100
    },
    {
      "epoch": 9.246476847857348,
      "grad_norm": 0.00010046597162727267,
      "learning_rate": 0.0008150762151279839,
      "loss": 0.0011,
      "step": 32150
    },
    {
      "epoch": 9.260857060684499,
      "grad_norm": 0.002954407362267375,
      "learning_rate": 0.000814788610871441,
      "loss": 0.0015,
      "step": 32200
    },
    {
      "epoch": 9.275237273511648,
      "grad_norm": 0.0005863014957867563,
      "learning_rate": 0.0008145010066148979,
      "loss": 0.0048,
      "step": 32250
    },
    {
      "epoch": 9.289617486338798,
      "grad_norm": 0.0006792147178202868,
      "learning_rate": 0.0008142134023583549,
      "loss": 0.0021,
      "step": 32300
    },
    {
      "epoch": 9.303997699165947,
      "grad_norm": 0.019895553588867188,
      "learning_rate": 0.0008139257981018119,
      "loss": 0.0028,
      "step": 32350
    },
    {
      "epoch": 9.318377911993098,
      "grad_norm": 0.0004015738668385893,
      "learning_rate": 0.0008136381938452689,
      "loss": 0.0038,
      "step": 32400
    },
    {
      "epoch": 9.332758124820247,
      "grad_norm": 0.0007121650851331651,
      "learning_rate": 0.0008133505895887258,
      "loss": 0.0029,
      "step": 32450
    },
    {
      "epoch": 9.347138337647397,
      "grad_norm": 0.06241796165704727,
      "learning_rate": 0.000813062985332183,
      "loss": 0.0025,
      "step": 32500
    },
    {
      "epoch": 9.361518550474546,
      "grad_norm": 0.019737567752599716,
      "learning_rate": 0.00081277538107564,
      "loss": 0.0021,
      "step": 32550
    },
    {
      "epoch": 9.375898763301697,
      "grad_norm": 0.0001479770289734006,
      "learning_rate": 0.0008124877768190969,
      "loss": 0.0015,
      "step": 32600
    },
    {
      "epoch": 9.390278976128847,
      "grad_norm": 0.010975961573421955,
      "learning_rate": 0.000812200172562554,
      "loss": 0.002,
      "step": 32650
    },
    {
      "epoch": 9.404659188955996,
      "grad_norm": 0.0013618379598483443,
      "learning_rate": 0.0008119125683060109,
      "loss": 0.0037,
      "step": 32700
    },
    {
      "epoch": 9.419039401783147,
      "grad_norm": 0.008794223889708519,
      "learning_rate": 0.0008116249640494679,
      "loss": 0.0021,
      "step": 32750
    },
    {
      "epoch": 9.433419614610296,
      "grad_norm": 0.005841098725795746,
      "learning_rate": 0.000811337359792925,
      "loss": 0.0036,
      "step": 32800
    },
    {
      "epoch": 9.447799827437446,
      "grad_norm": 6.411311187548563e-05,
      "learning_rate": 0.0008110497555363819,
      "loss": 0.0041,
      "step": 32850
    },
    {
      "epoch": 9.462180040264595,
      "grad_norm": 7.527096749981865e-05,
      "learning_rate": 0.0008107621512798389,
      "loss": 0.0017,
      "step": 32900
    },
    {
      "epoch": 9.476560253091746,
      "grad_norm": 0.003512938506901264,
      "learning_rate": 0.000810474547023296,
      "loss": 0.003,
      "step": 32950
    },
    {
      "epoch": 9.490940465918897,
      "grad_norm": 0.00024025175662245601,
      "learning_rate": 0.000810186942766753,
      "loss": 0.0027,
      "step": 33000
    },
    {
      "epoch": 9.505320678746045,
      "grad_norm": 0.0008253993582911789,
      "learning_rate": 0.0008098993385102099,
      "loss": 0.0035,
      "step": 33050
    },
    {
      "epoch": 9.519700891573196,
      "grad_norm": 0.006261817179620266,
      "learning_rate": 0.000809611734253667,
      "loss": 0.0022,
      "step": 33100
    },
    {
      "epoch": 9.534081104400345,
      "grad_norm": 0.017562856897711754,
      "learning_rate": 0.000809324129997124,
      "loss": 0.003,
      "step": 33150
    },
    {
      "epoch": 9.548461317227495,
      "grad_norm": 0.00038424882222898304,
      "learning_rate": 0.0008090365257405809,
      "loss": 0.0016,
      "step": 33200
    },
    {
      "epoch": 9.562841530054644,
      "grad_norm": 0.11723048985004425,
      "learning_rate": 0.000808748921484038,
      "loss": 0.0015,
      "step": 33250
    },
    {
      "epoch": 9.577221742881795,
      "grad_norm": 0.00021507503697648644,
      "learning_rate": 0.0008084613172274949,
      "loss": 0.0058,
      "step": 33300
    },
    {
      "epoch": 9.591601955708944,
      "grad_norm": 0.00025238865055143833,
      "learning_rate": 0.000808173712970952,
      "loss": 0.0031,
      "step": 33350
    },
    {
      "epoch": 9.605982168536094,
      "grad_norm": 0.016932886093854904,
      "learning_rate": 0.0008078861087144091,
      "loss": 0.0035,
      "step": 33400
    },
    {
      "epoch": 9.620362381363243,
      "grad_norm": 0.00041860350756905973,
      "learning_rate": 0.000807598504457866,
      "loss": 0.0055,
      "step": 33450
    },
    {
      "epoch": 9.634742594190394,
      "grad_norm": 0.0007534451433457434,
      "learning_rate": 0.000807310900201323,
      "loss": 0.0062,
      "step": 33500
    },
    {
      "epoch": 9.649122807017545,
      "grad_norm": 0.02168388105928898,
      "learning_rate": 0.00080702329594478,
      "loss": 0.0012,
      "step": 33550
    },
    {
      "epoch": 9.663503019844693,
      "grad_norm": 0.02802916429936886,
      "learning_rate": 0.000806735691688237,
      "loss": 0.0023,
      "step": 33600
    },
    {
      "epoch": 9.677883232671844,
      "grad_norm": 0.00019418710144236684,
      "learning_rate": 0.0008064480874316939,
      "loss": 0.0013,
      "step": 33650
    },
    {
      "epoch": 9.692263445498993,
      "grad_norm": 9.814115765038878e-05,
      "learning_rate": 0.000806160483175151,
      "loss": 0.0028,
      "step": 33700
    },
    {
      "epoch": 9.706643658326144,
      "grad_norm": 0.001657014014199376,
      "learning_rate": 0.000805872878918608,
      "loss": 0.0051,
      "step": 33750
    },
    {
      "epoch": 9.721023871153292,
      "grad_norm": 6.594223668798804e-05,
      "learning_rate": 0.000805585274662065,
      "loss": 0.0023,
      "step": 33800
    },
    {
      "epoch": 9.735404083980443,
      "grad_norm": 0.002128188032656908,
      "learning_rate": 0.0008052976704055221,
      "loss": 0.0034,
      "step": 33850
    },
    {
      "epoch": 9.749784296807594,
      "grad_norm": 5.375606269808486e-05,
      "learning_rate": 0.000805010066148979,
      "loss": 0.0039,
      "step": 33900
    },
    {
      "epoch": 9.764164509634742,
      "grad_norm": 5.1904738938901573e-05,
      "learning_rate": 0.000804722461892436,
      "loss": 0.0008,
      "step": 33950
    },
    {
      "epoch": 9.778544722461893,
      "grad_norm": 0.0007957872585393488,
      "learning_rate": 0.0008044348576358931,
      "loss": 0.003,
      "step": 34000
    },
    {
      "epoch": 9.792924935289042,
      "grad_norm": 6.557921733474359e-05,
      "learning_rate": 0.00080414725337935,
      "loss": 0.002,
      "step": 34050
    },
    {
      "epoch": 9.807305148116193,
      "grad_norm": 0.006842019967734814,
      "learning_rate": 0.000803859649122807,
      "loss": 0.0025,
      "step": 34100
    },
    {
      "epoch": 9.821685360943341,
      "grad_norm": 0.0222980547696352,
      "learning_rate": 0.000803572044866264,
      "loss": 0.0051,
      "step": 34150
    },
    {
      "epoch": 9.836065573770492,
      "grad_norm": 0.0017285766080021858,
      "learning_rate": 0.000803284440609721,
      "loss": 0.0039,
      "step": 34200
    },
    {
      "epoch": 9.850445786597641,
      "grad_norm": 0.005804034415632486,
      "learning_rate": 0.000802996836353178,
      "loss": 0.0026,
      "step": 34250
    },
    {
      "epoch": 9.864825999424792,
      "grad_norm": 0.0002941278216894716,
      "learning_rate": 0.0008027092320966351,
      "loss": 0.0009,
      "step": 34300
    },
    {
      "epoch": 9.87920621225194,
      "grad_norm": 0.0038271588273346424,
      "learning_rate": 0.0008024216278400921,
      "loss": 0.0028,
      "step": 34350
    },
    {
      "epoch": 9.893586425079091,
      "grad_norm": 0.05005478113889694,
      "learning_rate": 0.000802134023583549,
      "loss": 0.0034,
      "step": 34400
    },
    {
      "epoch": 9.907966637906242,
      "grad_norm": 0.0005644162301905453,
      "learning_rate": 0.0008018464193270061,
      "loss": 0.0044,
      "step": 34450
    },
    {
      "epoch": 9.92234685073339,
      "grad_norm": 4.623207132681273e-05,
      "learning_rate": 0.000801558815070463,
      "loss": 0.0011,
      "step": 34500
    },
    {
      "epoch": 9.936727063560541,
      "grad_norm": 0.0009217009646818042,
      "learning_rate": 0.00080127121081392,
      "loss": 0.002,
      "step": 34550
    },
    {
      "epoch": 9.95110727638769,
      "grad_norm": 0.03243415430188179,
      "learning_rate": 0.0008009836065573771,
      "loss": 0.0028,
      "step": 34600
    },
    {
      "epoch": 9.96548748921484,
      "grad_norm": 0.00026373850414529443,
      "learning_rate": 0.000800696002300834,
      "loss": 0.0023,
      "step": 34650
    },
    {
      "epoch": 9.97986770204199,
      "grad_norm": 0.00011506407463457435,
      "learning_rate": 0.0008004083980442911,
      "loss": 0.0035,
      "step": 34700
    },
    {
      "epoch": 9.99424791486914,
      "grad_norm": 0.059690210968256,
      "learning_rate": 0.0008001207937877481,
      "loss": 0.0057,
      "step": 34750
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.010495204478502274,
      "eval_runtime": 18.0282,
      "eval_samples_per_second": 2646.907,
      "eval_steps_per_second": 41.38,
      "step": 34770
    },
    {
      "epoch": 10.00862812769629,
      "grad_norm": 0.0004692882066592574,
      "learning_rate": 0.0007998331895312051,
      "loss": 0.0009,
      "step": 34800
    },
    {
      "epoch": 10.02300834052344,
      "grad_norm": 0.003617025911808014,
      "learning_rate": 0.000799545585274662,
      "loss": 0.0058,
      "step": 34850
    },
    {
      "epoch": 10.03738855335059,
      "grad_norm": 0.0003242371021769941,
      "learning_rate": 0.0007992579810181191,
      "loss": 0.001,
      "step": 34900
    },
    {
      "epoch": 10.051768766177739,
      "grad_norm": 8.336987957591191e-05,
      "learning_rate": 0.0007989703767615761,
      "loss": 0.0025,
      "step": 34950
    },
    {
      "epoch": 10.06614897900489,
      "grad_norm": 5.526003224076703e-05,
      "learning_rate": 0.000798682772505033,
      "loss": 0.0026,
      "step": 35000
    },
    {
      "epoch": 10.080529191832039,
      "grad_norm": 0.00012132126721553504,
      "learning_rate": 0.0007983951682484901,
      "loss": 0.0035,
      "step": 35050
    },
    {
      "epoch": 10.09490940465919,
      "grad_norm": 0.00016717879043426365,
      "learning_rate": 0.000798107563991947,
      "loss": 0.0048,
      "step": 35100
    },
    {
      "epoch": 10.109289617486338,
      "grad_norm": 0.0059640477411448956,
      "learning_rate": 0.0007978199597354041,
      "loss": 0.0027,
      "step": 35150
    },
    {
      "epoch": 10.123669830313489,
      "grad_norm": 9.376597154187039e-05,
      "learning_rate": 0.0007975323554788611,
      "loss": 0.004,
      "step": 35200
    },
    {
      "epoch": 10.13805004314064,
      "grad_norm": 0.1159246563911438,
      "learning_rate": 0.0007972447512223181,
      "loss": 0.004,
      "step": 35250
    },
    {
      "epoch": 10.152430255967788,
      "grad_norm": 0.001093849539756775,
      "learning_rate": 0.0007969571469657751,
      "loss": 0.0017,
      "step": 35300
    },
    {
      "epoch": 10.166810468794939,
      "grad_norm": 0.00015389634063467383,
      "learning_rate": 0.0007966695427092321,
      "loss": 0.004,
      "step": 35350
    },
    {
      "epoch": 10.181190681622088,
      "grad_norm": 0.00041150060133077204,
      "learning_rate": 0.0007963819384526891,
      "loss": 0.0032,
      "step": 35400
    },
    {
      "epoch": 10.195570894449238,
      "grad_norm": 0.0010540698422119021,
      "learning_rate": 0.000796094334196146,
      "loss": 0.0034,
      "step": 35450
    },
    {
      "epoch": 10.209951107276387,
      "grad_norm": 0.0001979668450076133,
      "learning_rate": 0.0007958067299396031,
      "loss": 0.0012,
      "step": 35500
    },
    {
      "epoch": 10.224331320103538,
      "grad_norm": 8.932663331506774e-05,
      "learning_rate": 0.0007955191256830602,
      "loss": 0.0035,
      "step": 35550
    },
    {
      "epoch": 10.238711532930687,
      "grad_norm": 0.006555730011314154,
      "learning_rate": 0.0007952315214265171,
      "loss": 0.0029,
      "step": 35600
    },
    {
      "epoch": 10.253091745757837,
      "grad_norm": 0.020924530923366547,
      "learning_rate": 0.0007949439171699742,
      "loss": 0.0008,
      "step": 35650
    },
    {
      "epoch": 10.267471958584988,
      "grad_norm": 0.00706205889582634,
      "learning_rate": 0.0007946563129134311,
      "loss": 0.0038,
      "step": 35700
    },
    {
      "epoch": 10.281852171412137,
      "grad_norm": 5.8881098084384575e-05,
      "learning_rate": 0.0007943687086568881,
      "loss": 0.0022,
      "step": 35750
    },
    {
      "epoch": 10.296232384239287,
      "grad_norm": 0.00029444615938700736,
      "learning_rate": 0.0007940811044003451,
      "loss": 0.0025,
      "step": 35800
    },
    {
      "epoch": 10.310612597066436,
      "grad_norm": 0.0003542223130352795,
      "learning_rate": 0.0007937935001438021,
      "loss": 0.0018,
      "step": 35850
    },
    {
      "epoch": 10.324992809893587,
      "grad_norm": 0.028562230989336967,
      "learning_rate": 0.0007935058958872591,
      "loss": 0.0016,
      "step": 35900
    },
    {
      "epoch": 10.339373022720736,
      "grad_norm": 0.055152684450149536,
      "learning_rate": 0.0007932182916307162,
      "loss": 0.0053,
      "step": 35950
    },
    {
      "epoch": 10.353753235547886,
      "grad_norm": 0.00015625178639311343,
      "learning_rate": 0.0007929306873741732,
      "loss": 0.0021,
      "step": 36000
    },
    {
      "epoch": 10.368133448375035,
      "grad_norm": 0.0027436011005192995,
      "learning_rate": 0.0007926430831176301,
      "loss": 0.0019,
      "step": 36050
    },
    {
      "epoch": 10.382513661202186,
      "grad_norm": 0.0002656913420651108,
      "learning_rate": 0.0007923554788610872,
      "loss": 0.0029,
      "step": 36100
    },
    {
      "epoch": 10.396893874029336,
      "grad_norm": 0.0717560276389122,
      "learning_rate": 0.0007920678746045442,
      "loss": 0.0026,
      "step": 36150
    },
    {
      "epoch": 10.411274086856485,
      "grad_norm": 0.0005539743578992784,
      "learning_rate": 0.0007917802703480011,
      "loss": 0.0013,
      "step": 36200
    },
    {
      "epoch": 10.425654299683636,
      "grad_norm": 7.933242159197107e-05,
      "learning_rate": 0.0007914926660914582,
      "loss": 0.0007,
      "step": 36250
    },
    {
      "epoch": 10.440034512510785,
      "grad_norm": 0.0006635677418671548,
      "learning_rate": 0.0007912050618349151,
      "loss": 0.0034,
      "step": 36300
    },
    {
      "epoch": 10.454414725337935,
      "grad_norm": 0.0015802244888618588,
      "learning_rate": 0.0007909174575783721,
      "loss": 0.0007,
      "step": 36350
    },
    {
      "epoch": 10.468794938165084,
      "grad_norm": 0.026232661679387093,
      "learning_rate": 0.0007906298533218292,
      "loss": 0.0033,
      "step": 36400
    },
    {
      "epoch": 10.483175150992235,
      "grad_norm": 0.055304501205682755,
      "learning_rate": 0.0007903422490652862,
      "loss": 0.0034,
      "step": 36450
    },
    {
      "epoch": 10.497555363819384,
      "grad_norm": 0.0007433854043483734,
      "learning_rate": 0.0007900546448087432,
      "loss": 0.0018,
      "step": 36500
    },
    {
      "epoch": 10.511935576646534,
      "grad_norm": 0.0001834023423725739,
      "learning_rate": 0.0007897670405522002,
      "loss": 0.002,
      "step": 36550
    },
    {
      "epoch": 10.526315789473685,
      "grad_norm": 0.00023557728854939342,
      "learning_rate": 0.0007894794362956572,
      "loss": 0.0048,
      "step": 36600
    },
    {
      "epoch": 10.540696002300834,
      "grad_norm": 0.022551100701093674,
      "learning_rate": 0.0007891918320391141,
      "loss": 0.0043,
      "step": 36650
    },
    {
      "epoch": 10.555076215127984,
      "grad_norm": 5.268117456580512e-05,
      "learning_rate": 0.0007889042277825712,
      "loss": 0.0021,
      "step": 36700
    },
    {
      "epoch": 10.569456427955133,
      "grad_norm": 0.0007580372039228678,
      "learning_rate": 0.0007886166235260282,
      "loss": 0.002,
      "step": 36750
    },
    {
      "epoch": 10.583836640782284,
      "grad_norm": 3.752885095309466e-05,
      "learning_rate": 0.0007883290192694851,
      "loss": 0.0028,
      "step": 36800
    },
    {
      "epoch": 10.598216853609433,
      "grad_norm": 0.0006316355429589748,
      "learning_rate": 0.0007880414150129423,
      "loss": 0.0019,
      "step": 36850
    },
    {
      "epoch": 10.612597066436583,
      "grad_norm": 0.0028071689885109663,
      "learning_rate": 0.0007877538107563992,
      "loss": 0.0031,
      "step": 36900
    },
    {
      "epoch": 10.626977279263734,
      "grad_norm": 0.0005076063680462539,
      "learning_rate": 0.0007874662064998562,
      "loss": 0.0045,
      "step": 36950
    },
    {
      "epoch": 10.641357492090883,
      "grad_norm": 0.005302977282553911,
      "learning_rate": 0.0007871786022433132,
      "loss": 0.0009,
      "step": 37000
    },
    {
      "epoch": 10.655737704918034,
      "grad_norm": 3.707844007294625e-05,
      "learning_rate": 0.0007868909979867702,
      "loss": 0.0016,
      "step": 37050
    },
    {
      "epoch": 10.670117917745182,
      "grad_norm": 0.009051471017301083,
      "learning_rate": 0.0007866033937302272,
      "loss": 0.0022,
      "step": 37100
    },
    {
      "epoch": 10.684498130572333,
      "grad_norm": 4.4592907215701416e-05,
      "learning_rate": 0.0007863157894736842,
      "loss": 0.0027,
      "step": 37150
    },
    {
      "epoch": 10.698878343399482,
      "grad_norm": 2.6037489078589715e-05,
      "learning_rate": 0.0007860281852171412,
      "loss": 0.0044,
      "step": 37200
    },
    {
      "epoch": 10.713258556226632,
      "grad_norm": 8.016951323952526e-05,
      "learning_rate": 0.0007857405809605981,
      "loss": 0.0029,
      "step": 37250
    },
    {
      "epoch": 10.727638769053781,
      "grad_norm": 0.04531490430235863,
      "learning_rate": 0.0007854529767040553,
      "loss": 0.0031,
      "step": 37300
    },
    {
      "epoch": 10.742018981880932,
      "grad_norm": 0.0004139431403018534,
      "learning_rate": 0.0007851653724475123,
      "loss": 0.0036,
      "step": 37350
    },
    {
      "epoch": 10.75639919470808,
      "grad_norm": 9.696809138404205e-05,
      "learning_rate": 0.0007848777681909692,
      "loss": 0.0018,
      "step": 37400
    },
    {
      "epoch": 10.770779407535231,
      "grad_norm": 0.016354460269212723,
      "learning_rate": 0.0007845901639344263,
      "loss": 0.0024,
      "step": 37450
    },
    {
      "epoch": 10.785159620362382,
      "grad_norm": 0.0005019423551857471,
      "learning_rate": 0.0007843025596778832,
      "loss": 0.0027,
      "step": 37500
    },
    {
      "epoch": 10.799539833189531,
      "grad_norm": 0.015613127499818802,
      "learning_rate": 0.0007840149554213402,
      "loss": 0.0025,
      "step": 37550
    },
    {
      "epoch": 10.813920046016682,
      "grad_norm": 0.0005211144452914596,
      "learning_rate": 0.0007837273511647972,
      "loss": 0.0042,
      "step": 37600
    },
    {
      "epoch": 10.82830025884383,
      "grad_norm": 0.01787102408707142,
      "learning_rate": 0.0007834397469082542,
      "loss": 0.0024,
      "step": 37650
    },
    {
      "epoch": 10.842680471670981,
      "grad_norm": 0.0024179976899176836,
      "learning_rate": 0.0007831521426517113,
      "loss": 0.0012,
      "step": 37700
    },
    {
      "epoch": 10.85706068449813,
      "grad_norm": 0.020713763311505318,
      "learning_rate": 0.0007828645383951683,
      "loss": 0.0005,
      "step": 37750
    },
    {
      "epoch": 10.87144089732528,
      "grad_norm": 0.00019548478303477168,
      "learning_rate": 0.0007825769341386253,
      "loss": 0.0029,
      "step": 37800
    },
    {
      "epoch": 10.885821110152431,
      "grad_norm": 5.62038658245001e-05,
      "learning_rate": 0.0007822893298820822,
      "loss": 0.0031,
      "step": 37850
    },
    {
      "epoch": 10.90020132297958,
      "grad_norm": 0.0001018479306367226,
      "learning_rate": 0.0007820017256255393,
      "loss": 0.0015,
      "step": 37900
    },
    {
      "epoch": 10.91458153580673,
      "grad_norm": 5.9023288486059755e-05,
      "learning_rate": 0.0007817141213689962,
      "loss": 0.0049,
      "step": 37950
    },
    {
      "epoch": 10.92896174863388,
      "grad_norm": 0.001448633149266243,
      "learning_rate": 0.0007814265171124532,
      "loss": 0.004,
      "step": 38000
    },
    {
      "epoch": 10.94334196146103,
      "grad_norm": 0.0003177683101966977,
      "learning_rate": 0.0007811389128559103,
      "loss": 0.0047,
      "step": 38050
    },
    {
      "epoch": 10.957722174288179,
      "grad_norm": 0.0002989346394315362,
      "learning_rate": 0.0007808513085993672,
      "loss": 0.0055,
      "step": 38100
    },
    {
      "epoch": 10.97210238711533,
      "grad_norm": 0.0006199441268108785,
      "learning_rate": 0.0007805637043428243,
      "loss": 0.002,
      "step": 38150
    },
    {
      "epoch": 10.986482599942478,
      "grad_norm": 0.00010680118430173025,
      "learning_rate": 0.0007802761000862813,
      "loss": 0.0038,
      "step": 38200
    },
    {
      "epoch": 11.0,
      "eval_loss": 0.011731307953596115,
      "eval_runtime": 18.953,
      "eval_samples_per_second": 2517.752,
      "eval_steps_per_second": 39.36,
      "step": 38247
    },
    {
      "epoch": 11.000862812769629,
      "grad_norm": 0.0021045939065515995,
      "learning_rate": 0.0007799884958297383,
      "loss": 0.0018,
      "step": 38250
    },
    {
      "epoch": 11.01524302559678,
      "grad_norm": 0.0015929789515212178,
      "learning_rate": 0.0007797008915731953,
      "loss": 0.0033,
      "step": 38300
    },
    {
      "epoch": 11.029623238423929,
      "grad_norm": 0.0010796802816912532,
      "learning_rate": 0.0007794132873166523,
      "loss": 0.0043,
      "step": 38350
    },
    {
      "epoch": 11.04400345125108,
      "grad_norm": 0.009745807386934757,
      "learning_rate": 0.0007791256830601093,
      "loss": 0.0014,
      "step": 38400
    },
    {
      "epoch": 11.058383664078228,
      "grad_norm": 0.0008372451993636787,
      "learning_rate": 0.0007788380788035662,
      "loss": 0.0019,
      "step": 38450
    },
    {
      "epoch": 11.072763876905379,
      "grad_norm": 0.0038223001174628735,
      "learning_rate": 0.0007785504745470233,
      "loss": 0.0032,
      "step": 38500
    },
    {
      "epoch": 11.087144089732528,
      "grad_norm": 2.20391848415602e-05,
      "learning_rate": 0.0007782628702904802,
      "loss": 0.0026,
      "step": 38550
    },
    {
      "epoch": 11.101524302559678,
      "grad_norm": 3.488610309432261e-05,
      "learning_rate": 0.0007779752660339373,
      "loss": 0.004,
      "step": 38600
    },
    {
      "epoch": 11.115904515386827,
      "grad_norm": 0.00018009069026447833,
      "learning_rate": 0.0007776876617773944,
      "loss": 0.0027,
      "step": 38650
    },
    {
      "epoch": 11.130284728213978,
      "grad_norm": 0.0001769311202224344,
      "learning_rate": 0.0007774000575208513,
      "loss": 0.0026,
      "step": 38700
    },
    {
      "epoch": 11.144664941041128,
      "grad_norm": 0.008685593493282795,
      "learning_rate": 0.0007771124532643083,
      "loss": 0.0005,
      "step": 38750
    },
    {
      "epoch": 11.159045153868277,
      "grad_norm": 0.000707207596860826,
      "learning_rate": 0.0007768248490077653,
      "loss": 0.0025,
      "step": 38800
    },
    {
      "epoch": 11.173425366695428,
      "grad_norm": 0.0002516829699743539,
      "learning_rate": 0.0007765372447512223,
      "loss": 0.0041,
      "step": 38850
    },
    {
      "epoch": 11.187805579522577,
      "grad_norm": 0.0001667434407863766,
      "learning_rate": 0.0007762496404946793,
      "loss": 0.0014,
      "step": 38900
    },
    {
      "epoch": 11.202185792349727,
      "grad_norm": 0.00018619732873048633,
      "learning_rate": 0.0007759620362381363,
      "loss": 0.0012,
      "step": 38950
    },
    {
      "epoch": 11.216566005176876,
      "grad_norm": 0.0002948576002381742,
      "learning_rate": 0.0007756744319815934,
      "loss": 0.0032,
      "step": 39000
    },
    {
      "epoch": 11.230946218004027,
      "grad_norm": 0.0032184687443077564,
      "learning_rate": 0.0007753868277250503,
      "loss": 0.0042,
      "step": 39050
    },
    {
      "epoch": 11.245326430831176,
      "grad_norm": 0.0006350735202431679,
      "learning_rate": 0.0007750992234685074,
      "loss": 0.0035,
      "step": 39100
    },
    {
      "epoch": 11.259706643658326,
      "grad_norm": 0.0013873649295419455,
      "learning_rate": 0.0007748116192119643,
      "loss": 0.0015,
      "step": 39150
    },
    {
      "epoch": 11.274086856485477,
      "grad_norm": 4.758461000164971e-05,
      "learning_rate": 0.0007745240149554213,
      "loss": 0.0024,
      "step": 39200
    },
    {
      "epoch": 11.288467069312626,
      "grad_norm": 0.00023524316202383488,
      "learning_rate": 0.0007742364106988784,
      "loss": 0.0051,
      "step": 39250
    },
    {
      "epoch": 11.302847282139776,
      "grad_norm": 0.0005866699502803385,
      "learning_rate": 0.0007739488064423353,
      "loss": 0.0063,
      "step": 39300
    },
    {
      "epoch": 11.317227494966925,
      "grad_norm": 0.00027837883681058884,
      "learning_rate": 0.0007736612021857923,
      "loss": 0.0053,
      "step": 39350
    },
    {
      "epoch": 11.331607707794076,
      "grad_norm": 0.04976774379611015,
      "learning_rate": 0.0007733735979292493,
      "loss": 0.0023,
      "step": 39400
    },
    {
      "epoch": 11.345987920621225,
      "grad_norm": 0.005795445758849382,
      "learning_rate": 0.0007730859936727064,
      "loss": 0.0056,
      "step": 39450
    },
    {
      "epoch": 11.360368133448375,
      "grad_norm": 7.25450663594529e-05,
      "learning_rate": 0.0007727983894161634,
      "loss": 0.004,
      "step": 39500
    },
    {
      "epoch": 11.374748346275524,
      "grad_norm": 0.0009099388844333589,
      "learning_rate": 0.0007725107851596204,
      "loss": 0.0021,
      "step": 39550
    },
    {
      "epoch": 11.389128559102675,
      "grad_norm": 0.0004329268413130194,
      "learning_rate": 0.0007722231809030774,
      "loss": 0.0018,
      "step": 39600
    },
    {
      "epoch": 11.403508771929825,
      "grad_norm": 0.005519762635231018,
      "learning_rate": 0.0007719355766465343,
      "loss": 0.0014,
      "step": 39650
    },
    {
      "epoch": 11.417888984756974,
      "grad_norm": 0.003141044406220317,
      "learning_rate": 0.0007716479723899914,
      "loss": 0.0017,
      "step": 39700
    },
    {
      "epoch": 11.432269197584125,
      "grad_norm": 0.00020277872681617737,
      "learning_rate": 0.0007713603681334483,
      "loss": 0.001,
      "step": 39750
    },
    {
      "epoch": 11.446649410411274,
      "grad_norm": 0.00021545276104006916,
      "learning_rate": 0.0007710727638769053,
      "loss": 0.0018,
      "step": 39800
    },
    {
      "epoch": 11.461029623238424,
      "grad_norm": 0.0029788275714963675,
      "learning_rate": 0.0007707851596203625,
      "loss": 0.0024,
      "step": 39850
    },
    {
      "epoch": 11.475409836065573,
      "grad_norm": 0.022239312529563904,
      "learning_rate": 0.0007704975553638194,
      "loss": 0.0052,
      "step": 39900
    },
    {
      "epoch": 11.489790048892724,
      "grad_norm": 0.0002302161301486194,
      "learning_rate": 0.0007702099511072764,
      "loss": 0.0021,
      "step": 39950
    },
    {
      "epoch": 11.504170261719873,
      "grad_norm": 0.029422925785183907,
      "learning_rate": 0.0007699223468507334,
      "loss": 0.0053,
      "step": 40000
    },
    {
      "epoch": 11.518550474547023,
      "grad_norm": 0.00011006927525158972,
      "learning_rate": 0.0007696347425941904,
      "loss": 0.0029,
      "step": 40050
    },
    {
      "epoch": 11.532930687374174,
      "grad_norm": 0.00039856848889030516,
      "learning_rate": 0.0007693471383376474,
      "loss": 0.0025,
      "step": 40100
    },
    {
      "epoch": 11.547310900201323,
      "grad_norm": 0.02211921103298664,
      "learning_rate": 0.0007690595340811044,
      "loss": 0.0039,
      "step": 40150
    },
    {
      "epoch": 11.561691113028473,
      "grad_norm": 0.022904975339770317,
      "learning_rate": 0.0007687719298245614,
      "loss": 0.0032,
      "step": 40200
    },
    {
      "epoch": 11.576071325855622,
      "grad_norm": 0.0007499877829104662,
      "learning_rate": 0.0007684843255680183,
      "loss": 0.0035,
      "step": 40250
    },
    {
      "epoch": 11.590451538682773,
      "grad_norm": 0.04335460811853409,
      "learning_rate": 0.0007681967213114755,
      "loss": 0.0022,
      "step": 40300
    },
    {
      "epoch": 11.604831751509922,
      "grad_norm": 0.0006419673445634544,
      "learning_rate": 0.0007679091170549324,
      "loss": 0.0041,
      "step": 40350
    },
    {
      "epoch": 11.619211964337072,
      "grad_norm": 0.00041512661846354604,
      "learning_rate": 0.0007676215127983894,
      "loss": 0.0034,
      "step": 40400
    },
    {
      "epoch": 11.633592177164221,
      "grad_norm": 0.02475244551897049,
      "learning_rate": 0.0007673339085418465,
      "loss": 0.0017,
      "step": 40450
    },
    {
      "epoch": 11.647972389991372,
      "grad_norm": 0.00042095553362742066,
      "learning_rate": 0.0007670463042853034,
      "loss": 0.0013,
      "step": 40500
    },
    {
      "epoch": 11.662352602818522,
      "grad_norm": 9.206294635077938e-05,
      "learning_rate": 0.0007667587000287604,
      "loss": 0.0056,
      "step": 40550
    },
    {
      "epoch": 11.676732815645671,
      "grad_norm": 0.0017990334890782833,
      "learning_rate": 0.0007664710957722174,
      "loss": 0.0011,
      "step": 40600
    },
    {
      "epoch": 11.691113028472822,
      "grad_norm": 3.202297375537455e-05,
      "learning_rate": 0.0007661834915156744,
      "loss": 0.0023,
      "step": 40650
    },
    {
      "epoch": 11.70549324129997,
      "grad_norm": 0.000958550488576293,
      "learning_rate": 0.0007658958872591313,
      "loss": 0.0007,
      "step": 40700
    },
    {
      "epoch": 11.719873454127121,
      "grad_norm": 0.018898261711001396,
      "learning_rate": 0.0007656082830025885,
      "loss": 0.0018,
      "step": 40750
    },
    {
      "epoch": 11.73425366695427,
      "grad_norm": 0.00028557420591823757,
      "learning_rate": 0.0007653206787460455,
      "loss": 0.0015,
      "step": 40800
    },
    {
      "epoch": 11.748633879781421,
      "grad_norm": 5.031173714087345e-05,
      "learning_rate": 0.0007650330744895024,
      "loss": 0.0031,
      "step": 40850
    },
    {
      "epoch": 11.76301409260857,
      "grad_norm": 0.0001846188388299197,
      "learning_rate": 0.0007647454702329595,
      "loss": 0.0026,
      "step": 40900
    },
    {
      "epoch": 11.77739430543572,
      "grad_norm": 0.0008304773364216089,
      "learning_rate": 0.0007644578659764164,
      "loss": 0.0011,
      "step": 40950
    },
    {
      "epoch": 11.791774518262871,
      "grad_norm": 0.00027993766707368195,
      "learning_rate": 0.0007641702617198734,
      "loss": 0.0014,
      "step": 41000
    },
    {
      "epoch": 11.80615473109002,
      "grad_norm": 0.00024221536295954138,
      "learning_rate": 0.0007638826574633305,
      "loss": 0.0015,
      "step": 41050
    },
    {
      "epoch": 11.82053494391717,
      "grad_norm": 0.013856269419193268,
      "learning_rate": 0.0007635950532067874,
      "loss": 0.0057,
      "step": 41100
    },
    {
      "epoch": 11.83491515674432,
      "grad_norm": 0.0009484011097811162,
      "learning_rate": 0.0007633074489502446,
      "loss": 0.0018,
      "step": 41150
    },
    {
      "epoch": 11.84929536957147,
      "grad_norm": 0.06107458099722862,
      "learning_rate": 0.0007630198446937015,
      "loss": 0.0038,
      "step": 41200
    },
    {
      "epoch": 11.863675582398619,
      "grad_norm": 0.0020394225139170885,
      "learning_rate": 0.0007627322404371585,
      "loss": 0.0056,
      "step": 41250
    },
    {
      "epoch": 11.87805579522577,
      "grad_norm": 0.12624022364616394,
      "learning_rate": 0.0007624446361806154,
      "loss": 0.0013,
      "step": 41300
    },
    {
      "epoch": 11.892436008052918,
      "grad_norm": 0.0001250871573574841,
      "learning_rate": 0.0007621570319240725,
      "loss": 0.001,
      "step": 41350
    },
    {
      "epoch": 11.906816220880069,
      "grad_norm": 0.004740688484162092,
      "learning_rate": 0.0007618694276675295,
      "loss": 0.0042,
      "step": 41400
    },
    {
      "epoch": 11.92119643370722,
      "grad_norm": 0.0006127468659542501,
      "learning_rate": 0.0007615818234109864,
      "loss": 0.001,
      "step": 41450
    },
    {
      "epoch": 11.935576646534368,
      "grad_norm": 0.0005482205888256431,
      "learning_rate": 0.0007612942191544435,
      "loss": 0.003,
      "step": 41500
    },
    {
      "epoch": 11.949956859361519,
      "grad_norm": 0.014421948231756687,
      "learning_rate": 0.0007610066148979004,
      "loss": 0.0031,
      "step": 41550
    },
    {
      "epoch": 11.964337072188668,
      "grad_norm": 0.009544769302010536,
      "learning_rate": 0.0007607190106413576,
      "loss": 0.0038,
      "step": 41600
    },
    {
      "epoch": 11.978717285015819,
      "grad_norm": 0.0009011426009237766,
      "learning_rate": 0.0007604314063848146,
      "loss": 0.0028,
      "step": 41650
    },
    {
      "epoch": 11.993097497842967,
      "grad_norm": 0.00010334244143450633,
      "learning_rate": 0.0007601438021282715,
      "loss": 0.0013,
      "step": 41700
    },
    {
      "epoch": 12.0,
      "eval_loss": 0.010425788350403309,
      "eval_runtime": 18.1229,
      "eval_samples_per_second": 2633.072,
      "eval_steps_per_second": 41.163,
      "step": 41724
    },
    {
      "epoch": 12.007477710670118,
      "grad_norm": 0.0015269310679286718,
      "learning_rate": 0.0007598561978717286,
      "loss": 0.0011,
      "step": 41750
    },
    {
      "epoch": 12.021857923497267,
      "grad_norm": 0.06768899410963058,
      "learning_rate": 0.0007595685936151855,
      "loss": 0.0006,
      "step": 41800
    },
    {
      "epoch": 12.036238136324418,
      "grad_norm": 0.001518681412562728,
      "learning_rate": 0.0007592809893586425,
      "loss": 0.0026,
      "step": 41850
    },
    {
      "epoch": 12.050618349151568,
      "grad_norm": 0.08199676126241684,
      "learning_rate": 0.0007589933851020994,
      "loss": 0.0022,
      "step": 41900
    },
    {
      "epoch": 12.064998561978717,
      "grad_norm": 0.05205341428518295,
      "learning_rate": 0.0007587057808455565,
      "loss": 0.0005,
      "step": 41950
    },
    {
      "epoch": 12.079378774805868,
      "grad_norm": 0.06151073798537254,
      "learning_rate": 0.0007584181765890136,
      "loss": 0.0015,
      "step": 42000
    },
    {
      "epoch": 12.093758987633016,
      "grad_norm": 0.00015169357357081026,
      "learning_rate": 0.0007581305723324706,
      "loss": 0.0035,
      "step": 42050
    },
    {
      "epoch": 12.108139200460167,
      "grad_norm": 0.0009221735526807606,
      "learning_rate": 0.0007578429680759276,
      "loss": 0.0039,
      "step": 42100
    },
    {
      "epoch": 12.122519413287316,
      "grad_norm": 0.059007447212934494,
      "learning_rate": 0.0007575553638193845,
      "loss": 0.002,
      "step": 42150
    },
    {
      "epoch": 12.136899626114467,
      "grad_norm": 0.00015262597298715264,
      "learning_rate": 0.0007572677595628416,
      "loss": 0.0039,
      "step": 42200
    },
    {
      "epoch": 12.151279838941615,
      "grad_norm": 0.00028149961144663393,
      "learning_rate": 0.0007569801553062986,
      "loss": 0.0031,
      "step": 42250
    },
    {
      "epoch": 12.165660051768766,
      "grad_norm": 0.001169951050542295,
      "learning_rate": 0.0007566925510497555,
      "loss": 0.0032,
      "step": 42300
    },
    {
      "epoch": 12.180040264595917,
      "grad_norm": 0.002189280930906534,
      "learning_rate": 0.0007564049467932126,
      "loss": 0.0024,
      "step": 42350
    },
    {
      "epoch": 12.194420477423066,
      "grad_norm": 9.218377817887813e-05,
      "learning_rate": 0.0007561173425366695,
      "loss": 0.0024,
      "step": 42400
    },
    {
      "epoch": 12.208800690250216,
      "grad_norm": 0.0004009146068710834,
      "learning_rate": 0.0007558297382801266,
      "loss": 0.0013,
      "step": 42450
    },
    {
      "epoch": 12.223180903077365,
      "grad_norm": 0.00033371170866303146,
      "learning_rate": 0.0007555421340235836,
      "loss": 0.0022,
      "step": 42500
    },
    {
      "epoch": 12.237561115904516,
      "grad_norm": 0.05633486062288284,
      "learning_rate": 0.0007552545297670406,
      "loss": 0.0047,
      "step": 42550
    },
    {
      "epoch": 12.251941328731665,
      "grad_norm": 7.626674050698057e-05,
      "learning_rate": 0.0007549669255104976,
      "loss": 0.0033,
      "step": 42600
    },
    {
      "epoch": 12.266321541558815,
      "grad_norm": 8.06788302725181e-05,
      "learning_rate": 0.0007546793212539546,
      "loss": 0.001,
      "step": 42650
    },
    {
      "epoch": 12.280701754385966,
      "grad_norm": 0.0004992801696062088,
      "learning_rate": 0.0007543917169974116,
      "loss": 0.0037,
      "step": 42700
    },
    {
      "epoch": 12.295081967213115,
      "grad_norm": 0.00012243425589986145,
      "learning_rate": 0.0007541041127408685,
      "loss": 0.0014,
      "step": 42750
    },
    {
      "epoch": 12.309462180040265,
      "grad_norm": 5.643385520670563e-05,
      "learning_rate": 0.0007538165084843256,
      "loss": 0.003,
      "step": 42800
    },
    {
      "epoch": 12.323842392867414,
      "grad_norm": 9.176264575216919e-05,
      "learning_rate": 0.0007535289042277827,
      "loss": 0.0014,
      "step": 42850
    },
    {
      "epoch": 12.338222605694565,
      "grad_norm": 0.00030934912501834333,
      "learning_rate": 0.0007532412999712396,
      "loss": 0.0036,
      "step": 42900
    },
    {
      "epoch": 12.352602818521714,
      "grad_norm": 0.03142719343304634,
      "learning_rate": 0.0007529536957146967,
      "loss": 0.0008,
      "step": 42950
    },
    {
      "epoch": 12.366983031348864,
      "grad_norm": 0.0006225583492778242,
      "learning_rate": 0.0007526660914581536,
      "loss": 0.003,
      "step": 43000
    },
    {
      "epoch": 12.381363244176013,
      "grad_norm": 5.562663500313647e-05,
      "learning_rate": 0.0007523784872016106,
      "loss": 0.0025,
      "step": 43050
    },
    {
      "epoch": 12.395743457003164,
      "grad_norm": 0.0008760354248806834,
      "learning_rate": 0.0007520908829450676,
      "loss": 0.0014,
      "step": 43100
    },
    {
      "epoch": 12.410123669830314,
      "grad_norm": 3.466102862148546e-05,
      "learning_rate": 0.0007518032786885246,
      "loss": 0.0013,
      "step": 43150
    },
    {
      "epoch": 12.424503882657463,
      "grad_norm": 0.0013553760945796967,
      "learning_rate": 0.0007515156744319816,
      "loss": 0.0018,
      "step": 43200
    },
    {
      "epoch": 12.438884095484614,
      "grad_norm": 0.04662720486521721,
      "learning_rate": 0.0007512280701754386,
      "loss": 0.0059,
      "step": 43250
    },
    {
      "epoch": 12.453264308311763,
      "grad_norm": 8.201222954085097e-05,
      "learning_rate": 0.0007509404659188957,
      "loss": 0.0032,
      "step": 43300
    },
    {
      "epoch": 12.467644521138913,
      "grad_norm": 0.00018643119256012142,
      "learning_rate": 0.0007506528616623526,
      "loss": 0.004,
      "step": 43350
    },
    {
      "epoch": 12.482024733966062,
      "grad_norm": 6.0611058870563284e-05,
      "learning_rate": 0.0007503652574058097,
      "loss": 0.0022,
      "step": 43400
    },
    {
      "epoch": 12.496404946793213,
      "grad_norm": 0.0009976139990612864,
      "learning_rate": 0.0007500776531492666,
      "loss": 0.0037,
      "step": 43450
    },
    {
      "epoch": 12.510785159620362,
      "grad_norm": 0.003996517974883318,
      "learning_rate": 0.0007497900488927236,
      "loss": 0.0015,
      "step": 43500
    },
    {
      "epoch": 12.525165372447512,
      "grad_norm": 0.0006507303332909942,
      "learning_rate": 0.0007495024446361807,
      "loss": 0.0054,
      "step": 43550
    },
    {
      "epoch": 12.539545585274663,
      "grad_norm": 0.014651374891400337,
      "learning_rate": 0.0007492148403796376,
      "loss": 0.0031,
      "step": 43600
    },
    {
      "epoch": 12.553925798101812,
      "grad_norm": 0.0007688751793466508,
      "learning_rate": 0.0007489272361230946,
      "loss": 0.002,
      "step": 43650
    },
    {
      "epoch": 12.568306010928962,
      "grad_norm": 0.00015659068594686687,
      "learning_rate": 0.0007486396318665516,
      "loss": 0.0027,
      "step": 43700
    },
    {
      "epoch": 12.582686223756111,
      "grad_norm": 0.0009402933646924794,
      "learning_rate": 0.0007483520276100087,
      "loss": 0.0011,
      "step": 43750
    },
    {
      "epoch": 12.597066436583262,
      "grad_norm": 8.248433005064726e-05,
      "learning_rate": 0.0007480644233534657,
      "loss": 0.0011,
      "step": 43800
    },
    {
      "epoch": 12.61144664941041,
      "grad_norm": 0.0001533685135655105,
      "learning_rate": 0.0007477768190969227,
      "loss": 0.0049,
      "step": 43850
    },
    {
      "epoch": 12.625826862237561,
      "grad_norm": 0.028042668476700783,
      "learning_rate": 0.0007474892148403797,
      "loss": 0.0027,
      "step": 43900
    },
    {
      "epoch": 12.64020707506471,
      "grad_norm": 2.5212159016518854e-05,
      "learning_rate": 0.0007472016105838366,
      "loss": 0.0014,
      "step": 43950
    },
    {
      "epoch": 12.65458728789186,
      "grad_norm": 0.0022430631797760725,
      "learning_rate": 0.0007469140063272937,
      "loss": 0.0041,
      "step": 44000
    },
    {
      "epoch": 12.668967500719011,
      "grad_norm": 0.0009991752449423075,
      "learning_rate": 0.0007466264020707506,
      "loss": 0.0009,
      "step": 44050
    },
    {
      "epoch": 12.68334771354616,
      "grad_norm": 0.0001293253735639155,
      "learning_rate": 0.0007463387978142076,
      "loss": 0.0009,
      "step": 44100
    },
    {
      "epoch": 12.697727926373311,
      "grad_norm": 5.193962715566158e-05,
      "learning_rate": 0.0007460511935576648,
      "loss": 0.005,
      "step": 44150
    },
    {
      "epoch": 12.71210813920046,
      "grad_norm": 0.014855164103209972,
      "learning_rate": 0.0007457635893011217,
      "loss": 0.0039,
      "step": 44200
    },
    {
      "epoch": 12.72648835202761,
      "grad_norm": 0.003485526889562607,
      "learning_rate": 0.0007454759850445787,
      "loss": 0.0035,
      "step": 44250
    },
    {
      "epoch": 12.74086856485476,
      "grad_norm": 0.0007537157507613301,
      "learning_rate": 0.0007451883807880357,
      "loss": 0.0023,
      "step": 44300
    },
    {
      "epoch": 12.75524877768191,
      "grad_norm": 0.009672158397734165,
      "learning_rate": 0.0007449007765314927,
      "loss": 0.0044,
      "step": 44350
    },
    {
      "epoch": 12.769628990509059,
      "grad_norm": 0.00039498572004958987,
      "learning_rate": 0.0007446131722749497,
      "loss": 0.0034,
      "step": 44400
    },
    {
      "epoch": 12.78400920333621,
      "grad_norm": 0.03778428956866264,
      "learning_rate": 0.0007443255680184067,
      "loss": 0.004,
      "step": 44450
    },
    {
      "epoch": 12.79838941616336,
      "grad_norm": 0.13981275260448456,
      "learning_rate": 0.0007440379637618637,
      "loss": 0.0025,
      "step": 44500
    },
    {
      "epoch": 12.812769628990509,
      "grad_norm": 5.9434598369989544e-05,
      "learning_rate": 0.0007437503595053206,
      "loss": 0.0046,
      "step": 44550
    },
    {
      "epoch": 12.82714984181766,
      "grad_norm": 0.001055214088410139,
      "learning_rate": 0.0007434627552487778,
      "loss": 0.0048,
      "step": 44600
    },
    {
      "epoch": 12.841530054644808,
      "grad_norm": 0.010743401944637299,
      "learning_rate": 0.0007431751509922347,
      "loss": 0.0028,
      "step": 44650
    },
    {
      "epoch": 12.855910267471959,
      "grad_norm": 0.07843548059463501,
      "learning_rate": 0.0007428875467356917,
      "loss": 0.0045,
      "step": 44700
    },
    {
      "epoch": 12.870290480299108,
      "grad_norm": 0.00015421072021126747,
      "learning_rate": 0.0007425999424791488,
      "loss": 0.0031,
      "step": 44750
    },
    {
      "epoch": 12.884670693126258,
      "grad_norm": 0.0006320643587969244,
      "learning_rate": 0.0007423123382226057,
      "loss": 0.001,
      "step": 44800
    },
    {
      "epoch": 12.899050905953407,
      "grad_norm": 0.00028576338081620634,
      "learning_rate": 0.0007420247339660627,
      "loss": 0.0013,
      "step": 44850
    },
    {
      "epoch": 12.913431118780558,
      "grad_norm": 0.00017702307377476245,
      "learning_rate": 0.0007417371297095197,
      "loss": 0.0037,
      "step": 44900
    },
    {
      "epoch": 12.927811331607709,
      "grad_norm": 0.0005465315189212561,
      "learning_rate": 0.0007414495254529767,
      "loss": 0.0006,
      "step": 44950
    },
    {
      "epoch": 12.942191544434857,
      "grad_norm": 0.014520407654345036,
      "learning_rate": 0.0007411619211964338,
      "loss": 0.0007,
      "step": 45000
    },
    {
      "epoch": 12.956571757262008,
      "grad_norm": 0.00017693400150164962,
      "learning_rate": 0.0007408743169398908,
      "loss": 0.003,
      "step": 45050
    },
    {
      "epoch": 12.970951970089157,
      "grad_norm": 6.934615521458909e-05,
      "learning_rate": 0.0007405867126833478,
      "loss": 0.0058,
      "step": 45100
    },
    {
      "epoch": 12.985332182916308,
      "grad_norm": 0.00018602352065499872,
      "learning_rate": 0.0007402991084268047,
      "loss": 0.0009,
      "step": 45150
    },
    {
      "epoch": 12.999712395743456,
      "grad_norm": 4.9102894990937784e-05,
      "learning_rate": 0.0007400115041702618,
      "loss": 0.0025,
      "step": 45200
    },
    {
      "epoch": 13.0,
      "eval_loss": 0.010226191952824593,
      "eval_runtime": 17.3897,
      "eval_samples_per_second": 2744.103,
      "eval_steps_per_second": 42.899,
      "step": 45201
    },
    {
      "epoch": 13.014092608570607,
      "grad_norm": 9.639304334996268e-05,
      "learning_rate": 0.0007397238999137187,
      "loss": 0.0039,
      "step": 45250
    },
    {
      "epoch": 13.028472821397756,
      "grad_norm": 3.2900439691729844e-05,
      "learning_rate": 0.0007394362956571757,
      "loss": 0.0025,
      "step": 45300
    },
    {
      "epoch": 13.042853034224906,
      "grad_norm": 0.0002483176940586418,
      "learning_rate": 0.0007391486914006328,
      "loss": 0.0014,
      "step": 45350
    },
    {
      "epoch": 13.057233247052057,
      "grad_norm": 0.00014204472245182842,
      "learning_rate": 0.0007388610871440897,
      "loss": 0.0029,
      "step": 45400
    },
    {
      "epoch": 13.071613459879206,
      "grad_norm": 0.03935496136546135,
      "learning_rate": 0.0007385734828875468,
      "loss": 0.0053,
      "step": 45450
    },
    {
      "epoch": 13.085993672706357,
      "grad_norm": 0.0006481740856543183,
      "learning_rate": 0.0007382858786310038,
      "loss": 0.0018,
      "step": 45500
    },
    {
      "epoch": 13.100373885533505,
      "grad_norm": 3.0959188734414056e-05,
      "learning_rate": 0.0007379982743744608,
      "loss": 0.0035,
      "step": 45550
    },
    {
      "epoch": 13.114754098360656,
      "grad_norm": 8.638958388473839e-05,
      "learning_rate": 0.0007377106701179178,
      "loss": 0.0015,
      "step": 45600
    },
    {
      "epoch": 13.129134311187805,
      "grad_norm": 0.00022077337780501693,
      "learning_rate": 0.0007374230658613748,
      "loss": 0.0019,
      "step": 45650
    },
    {
      "epoch": 13.143514524014956,
      "grad_norm": 7.334253314184025e-05,
      "learning_rate": 0.0007371354616048318,
      "loss": 0.0058,
      "step": 45700
    },
    {
      "epoch": 13.157894736842104,
      "grad_norm": 0.025708870962262154,
      "learning_rate": 0.0007368478573482887,
      "loss": 0.0018,
      "step": 45750
    },
    {
      "epoch": 13.172274949669255,
      "grad_norm": 0.0026716599240899086,
      "learning_rate": 0.0007365602530917458,
      "loss": 0.0013,
      "step": 45800
    },
    {
      "epoch": 13.186655162496406,
      "grad_norm": 0.000406433071475476,
      "learning_rate": 0.0007362726488352027,
      "loss": 0.0019,
      "step": 45850
    },
    {
      "epoch": 13.201035375323555,
      "grad_norm": 0.18987375497817993,
      "learning_rate": 0.0007359850445786598,
      "loss": 0.0022,
      "step": 45900
    },
    {
      "epoch": 13.215415588150705,
      "grad_norm": 0.0002238889574073255,
      "learning_rate": 0.0007356974403221169,
      "loss": 0.0042,
      "step": 45950
    },
    {
      "epoch": 13.229795800977854,
      "grad_norm": 0.00014340488996822387,
      "learning_rate": 0.0007354098360655738,
      "loss": 0.0026,
      "step": 46000
    },
    {
      "epoch": 13.244176013805005,
      "grad_norm": 0.0014969289768487215,
      "learning_rate": 0.0007351222318090308,
      "loss": 0.0052,
      "step": 46050
    },
    {
      "epoch": 13.258556226632153,
      "grad_norm": 0.00037778893602080643,
      "learning_rate": 0.0007348346275524878,
      "loss": 0.0015,
      "step": 46100
    },
    {
      "epoch": 13.272936439459304,
      "grad_norm": 0.01841055229306221,
      "learning_rate": 0.0007345470232959448,
      "loss": 0.0028,
      "step": 46150
    },
    {
      "epoch": 13.287316652286453,
      "grad_norm": 0.09669206291437149,
      "learning_rate": 0.0007342594190394017,
      "loss": 0.0021,
      "step": 46200
    },
    {
      "epoch": 13.301696865113604,
      "grad_norm": 0.036694567650556564,
      "learning_rate": 0.0007339718147828588,
      "loss": 0.0033,
      "step": 46250
    },
    {
      "epoch": 13.316077077940754,
      "grad_norm": 0.0019943341612815857,
      "learning_rate": 0.0007336842105263159,
      "loss": 0.0017,
      "step": 46300
    },
    {
      "epoch": 13.330457290767903,
      "grad_norm": 0.01066504418849945,
      "learning_rate": 0.0007333966062697728,
      "loss": 0.0016,
      "step": 46350
    },
    {
      "epoch": 13.344837503595054,
      "grad_norm": 0.001750638009980321,
      "learning_rate": 0.0007331090020132299,
      "loss": 0.0022,
      "step": 46400
    },
    {
      "epoch": 13.359217716422203,
      "grad_norm": 0.0467824824154377,
      "learning_rate": 0.0007328213977566868,
      "loss": 0.0041,
      "step": 46450
    },
    {
      "epoch": 13.373597929249353,
      "grad_norm": 0.00028481328627094626,
      "learning_rate": 0.0007325337935001438,
      "loss": 0.0019,
      "step": 46500
    },
    {
      "epoch": 13.387978142076502,
      "grad_norm": 0.0004164533456787467,
      "learning_rate": 0.0007322461892436009,
      "loss": 0.0034,
      "step": 46550
    },
    {
      "epoch": 13.402358354903653,
      "grad_norm": 0.008362921886146069,
      "learning_rate": 0.0007319585849870578,
      "loss": 0.0049,
      "step": 46600
    },
    {
      "epoch": 13.416738567730803,
      "grad_norm": 6.964465137571096e-05,
      "learning_rate": 0.0007316709807305148,
      "loss": 0.0018,
      "step": 46650
    },
    {
      "epoch": 13.431118780557952,
      "grad_norm": 0.00012138853344367817,
      "learning_rate": 0.0007313833764739718,
      "loss": 0.0022,
      "step": 46700
    },
    {
      "epoch": 13.445498993385103,
      "grad_norm": 0.00019299134146422148,
      "learning_rate": 0.0007310957722174289,
      "loss": 0.0044,
      "step": 46750
    },
    {
      "epoch": 13.459879206212252,
      "grad_norm": 0.011120081879198551,
      "learning_rate": 0.0007308081679608858,
      "loss": 0.0018,
      "step": 46800
    },
    {
      "epoch": 13.474259419039402,
      "grad_norm": 0.004872577730566263,
      "learning_rate": 0.0007305205637043429,
      "loss": 0.0018,
      "step": 46850
    },
    {
      "epoch": 13.488639631866551,
      "grad_norm": 0.0013014301657676697,
      "learning_rate": 0.0007302329594477999,
      "loss": 0.0024,
      "step": 46900
    },
    {
      "epoch": 13.503019844693702,
      "grad_norm": 0.000799150497186929,
      "learning_rate": 0.0007299453551912568,
      "loss": 0.0034,
      "step": 46950
    },
    {
      "epoch": 13.51740005752085,
      "grad_norm": 4.481289943214506e-05,
      "learning_rate": 0.0007296577509347139,
      "loss": 0.0049,
      "step": 47000
    },
    {
      "epoch": 13.531780270348001,
      "grad_norm": 0.015724068507552147,
      "learning_rate": 0.0007293701466781708,
      "loss": 0.001,
      "step": 47050
    },
    {
      "epoch": 13.54616048317515,
      "grad_norm": 5.577799674938433e-05,
      "learning_rate": 0.0007290825424216278,
      "loss": 0.0047,
      "step": 47100
    },
    {
      "epoch": 13.5605406960023,
      "grad_norm": 6.103132909629494e-05,
      "learning_rate": 0.000728794938165085,
      "loss": 0.0009,
      "step": 47150
    },
    {
      "epoch": 13.574920908829451,
      "grad_norm": 0.0005394424078986049,
      "learning_rate": 0.0007285073339085419,
      "loss": 0.001,
      "step": 47200
    },
    {
      "epoch": 13.5893011216566,
      "grad_norm": 0.001007634331472218,
      "learning_rate": 0.0007282197296519989,
      "loss": 0.0025,
      "step": 47250
    },
    {
      "epoch": 13.60368133448375,
      "grad_norm": 8.317189349327236e-05,
      "learning_rate": 0.0007279321253954559,
      "loss": 0.0026,
      "step": 47300
    },
    {
      "epoch": 13.6180615473109,
      "grad_norm": 0.0006022124434821308,
      "learning_rate": 0.0007276445211389129,
      "loss": 0.0044,
      "step": 47350
    },
    {
      "epoch": 13.63244176013805,
      "grad_norm": 0.026906177401542664,
      "learning_rate": 0.0007273569168823698,
      "loss": 0.004,
      "step": 47400
    },
    {
      "epoch": 13.6468219729652,
      "grad_norm": 8.289411925943568e-05,
      "learning_rate": 0.0007270693126258269,
      "loss": 0.0033,
      "step": 47450
    },
    {
      "epoch": 13.66120218579235,
      "grad_norm": 0.02056179568171501,
      "learning_rate": 0.0007267817083692839,
      "loss": 0.0025,
      "step": 47500
    },
    {
      "epoch": 13.6755823986195,
      "grad_norm": 0.002818226581439376,
      "learning_rate": 0.0007264941041127408,
      "loss": 0.0025,
      "step": 47550
    },
    {
      "epoch": 13.68996261144665,
      "grad_norm": 8.457126386929303e-05,
      "learning_rate": 0.000726206499856198,
      "loss": 0.0037,
      "step": 47600
    },
    {
      "epoch": 13.7043428242738,
      "grad_norm": 8.290199912153184e-05,
      "learning_rate": 0.0007259188955996549,
      "loss": 0.0005,
      "step": 47650
    },
    {
      "epoch": 13.718723037100949,
      "grad_norm": 0.01615973562002182,
      "learning_rate": 0.0007256312913431119,
      "loss": 0.0011,
      "step": 47700
    },
    {
      "epoch": 13.7331032499281,
      "grad_norm": 0.030187131837010384,
      "learning_rate": 0.000725343687086569,
      "loss": 0.0038,
      "step": 47750
    },
    {
      "epoch": 13.747483462755248,
      "grad_norm": 0.057125527411699295,
      "learning_rate": 0.0007250560828300259,
      "loss": 0.0041,
      "step": 47800
    },
    {
      "epoch": 13.761863675582399,
      "grad_norm": 0.00012456112017389387,
      "learning_rate": 0.0007247684785734829,
      "loss": 0.0023,
      "step": 47850
    },
    {
      "epoch": 13.776243888409548,
      "grad_norm": 0.0001530157751403749,
      "learning_rate": 0.0007244808743169399,
      "loss": 0.0013,
      "step": 47900
    },
    {
      "epoch": 13.790624101236698,
      "grad_norm": 0.02339080534875393,
      "learning_rate": 0.0007241932700603969,
      "loss": 0.0018,
      "step": 47950
    },
    {
      "epoch": 13.805004314063847,
      "grad_norm": 0.0008844183757901192,
      "learning_rate": 0.0007239056658038538,
      "loss": 0.0008,
      "step": 48000
    },
    {
      "epoch": 13.819384526890998,
      "grad_norm": 0.004630669951438904,
      "learning_rate": 0.000723618061547311,
      "loss": 0.0029,
      "step": 48050
    },
    {
      "epoch": 13.833764739718148,
      "grad_norm": 0.0006673886091448367,
      "learning_rate": 0.000723330457290768,
      "loss": 0.0058,
      "step": 48100
    },
    {
      "epoch": 13.848144952545297,
      "grad_norm": 0.00018761304090730846,
      "learning_rate": 0.0007230428530342249,
      "loss": 0.0036,
      "step": 48150
    },
    {
      "epoch": 13.862525165372448,
      "grad_norm": 0.00021209596889093518,
      "learning_rate": 0.000722755248777682,
      "loss": 0.0009,
      "step": 48200
    },
    {
      "epoch": 13.876905378199597,
      "grad_norm": 0.00010818996088346466,
      "learning_rate": 0.0007224676445211389,
      "loss": 0.0023,
      "step": 48250
    },
    {
      "epoch": 13.891285591026747,
      "grad_norm": 0.020234404131770134,
      "learning_rate": 0.0007221800402645959,
      "loss": 0.0022,
      "step": 48300
    },
    {
      "epoch": 13.905665803853896,
      "grad_norm": 0.00018716782506089658,
      "learning_rate": 0.000721892436008053,
      "loss": 0.0025,
      "step": 48350
    },
    {
      "epoch": 13.920046016681047,
      "grad_norm": 0.02022225223481655,
      "learning_rate": 0.0007216048317515099,
      "loss": 0.0022,
      "step": 48400
    },
    {
      "epoch": 13.934426229508198,
      "grad_norm": 0.002458815462887287,
      "learning_rate": 0.000721317227494967,
      "loss": 0.0045,
      "step": 48450
    },
    {
      "epoch": 13.948806442335346,
      "grad_norm": 7.075788744259626e-05,
      "learning_rate": 0.000721029623238424,
      "loss": 0.0038,
      "step": 48500
    },
    {
      "epoch": 13.963186655162497,
      "grad_norm": 0.010267172940075397,
      "learning_rate": 0.000720742018981881,
      "loss": 0.0008,
      "step": 48550
    },
    {
      "epoch": 13.977566867989646,
      "grad_norm": 0.01025161799043417,
      "learning_rate": 0.0007204544147253379,
      "loss": 0.0025,
      "step": 48600
    },
    {
      "epoch": 13.991947080816797,
      "grad_norm": 2.1459740310092457e-05,
      "learning_rate": 0.000720166810468795,
      "loss": 0.0023,
      "step": 48650
    },
    {
      "epoch": 14.0,
      "eval_loss": 0.010492500849068165,
      "eval_runtime": 17.6581,
      "eval_samples_per_second": 2702.393,
      "eval_steps_per_second": 42.247,
      "step": 48678
    },
    {
      "epoch": 14.006327293643945,
      "grad_norm": 4.452247958397493e-05,
      "learning_rate": 0.000719879206212252,
      "loss": 0.004,
      "step": 48700
    },
    {
      "epoch": 14.020707506471096,
      "grad_norm": 0.02307010628283024,
      "learning_rate": 0.0007195916019557089,
      "loss": 0.0035,
      "step": 48750
    },
    {
      "epoch": 14.035087719298245,
      "grad_norm": 4.89098129037302e-05,
      "learning_rate": 0.000719303997699166,
      "loss": 0.0026,
      "step": 48800
    },
    {
      "epoch": 14.049467932125395,
      "grad_norm": 6.8347726482898e-05,
      "learning_rate": 0.0007190163934426229,
      "loss": 0.0012,
      "step": 48850
    },
    {
      "epoch": 14.063848144952546,
      "grad_norm": 0.0007992761675268412,
      "learning_rate": 0.00071872878918608,
      "loss": 0.0024,
      "step": 48900
    },
    {
      "epoch": 14.078228357779695,
      "grad_norm": 0.019858071580529213,
      "learning_rate": 0.000718441184929537,
      "loss": 0.0038,
      "step": 48950
    },
    {
      "epoch": 14.092608570606846,
      "grad_norm": 4.5566586777567863e-05,
      "learning_rate": 0.000718153580672994,
      "loss": 0.0022,
      "step": 49000
    },
    {
      "epoch": 14.106988783433994,
      "grad_norm": 0.0005888307350687683,
      "learning_rate": 0.000717865976416451,
      "loss": 0.0031,
      "step": 49050
    },
    {
      "epoch": 14.121368996261145,
      "grad_norm": 0.0001314816763624549,
      "learning_rate": 0.000717578372159908,
      "loss": 0.0021,
      "step": 49100
    },
    {
      "epoch": 14.135749209088294,
      "grad_norm": 0.00022860565513838083,
      "learning_rate": 0.000717290767903365,
      "loss": 0.0018,
      "step": 49150
    },
    {
      "epoch": 14.150129421915445,
      "grad_norm": 0.0007899448391981423,
      "learning_rate": 0.0007170031636468219,
      "loss": 0.0054,
      "step": 49200
    },
    {
      "epoch": 14.164509634742593,
      "grad_norm": 0.00012714587501250207,
      "learning_rate": 0.000716715559390279,
      "loss": 0.0025,
      "step": 49250
    },
    {
      "epoch": 14.178889847569744,
      "grad_norm": 6.910478987265378e-05,
      "learning_rate": 0.000716427955133736,
      "loss": 0.0063,
      "step": 49300
    },
    {
      "epoch": 14.193270060396895,
      "grad_norm": 6.337559898383915e-05,
      "learning_rate": 0.000716140350877193,
      "loss": 0.0017,
      "step": 49350
    },
    {
      "epoch": 14.207650273224044,
      "grad_norm": 0.00015778363740537316,
      "learning_rate": 0.0007158527466206501,
      "loss": 0.0033,
      "step": 49400
    },
    {
      "epoch": 14.222030486051194,
      "grad_norm": 0.018864687532186508,
      "learning_rate": 0.000715565142364107,
      "loss": 0.0021,
      "step": 49450
    },
    {
      "epoch": 14.236410698878343,
      "grad_norm": 0.0006556811276823282,
      "learning_rate": 0.000715277538107564,
      "loss": 0.0011,
      "step": 49500
    },
    {
      "epoch": 14.250790911705494,
      "grad_norm": 3.568820466171019e-05,
      "learning_rate": 0.000714989933851021,
      "loss": 0.0044,
      "step": 49550
    },
    {
      "epoch": 14.265171124532642,
      "grad_norm": 0.05329491198062897,
      "learning_rate": 0.000714702329594478,
      "loss": 0.0055,
      "step": 49600
    },
    {
      "epoch": 14.279551337359793,
      "grad_norm": 0.0002745168749243021,
      "learning_rate": 0.000714414725337935,
      "loss": 0.0032,
      "step": 49650
    },
    {
      "epoch": 14.293931550186942,
      "grad_norm": 0.004113811533898115,
      "learning_rate": 0.000714127121081392,
      "loss": 0.0012,
      "step": 49700
    },
    {
      "epoch": 14.308311763014093,
      "grad_norm": 0.0021908110938966274,
      "learning_rate": 0.000713839516824849,
      "loss": 0.0025,
      "step": 49750
    },
    {
      "epoch": 14.322691975841243,
      "grad_norm": 7.240792183438316e-05,
      "learning_rate": 0.000713551912568306,
      "loss": 0.0027,
      "step": 49800
    },
    {
      "epoch": 14.337072188668392,
      "grad_norm": 0.0006245407275855541,
      "learning_rate": 0.0007132643083117631,
      "loss": 0.0012,
      "step": 49850
    },
    {
      "epoch": 14.351452401495543,
      "grad_norm": 0.010104176588356495,
      "learning_rate": 0.0007129767040552201,
      "loss": 0.0008,
      "step": 49900
    },
    {
      "epoch": 14.365832614322692,
      "grad_norm": 0.00014735969307366759,
      "learning_rate": 0.000712689099798677,
      "loss": 0.0025,
      "step": 49950
    },
    {
      "epoch": 14.380212827149842,
      "grad_norm": 0.06781389564275742,
      "learning_rate": 0.0007124014955421341,
      "loss": 0.0022,
      "step": 50000
    },
    {
      "epoch": 14.394593039976991,
      "grad_norm": 4.6414672397077084e-05,
      "learning_rate": 0.000712113891285591,
      "loss": 0.0009,
      "step": 50050
    },
    {
      "epoch": 14.408973252804142,
      "grad_norm": 0.0006556398002430797,
      "learning_rate": 0.000711826287029048,
      "loss": 0.0018,
      "step": 50100
    },
    {
      "epoch": 14.42335346563129,
      "grad_norm": 0.0007367131183855236,
      "learning_rate": 0.000711538682772505,
      "loss": 0.0022,
      "step": 50150
    },
    {
      "epoch": 14.437733678458441,
      "grad_norm": 0.0019878915045410395,
      "learning_rate": 0.000711251078515962,
      "loss": 0.001,
      "step": 50200
    },
    {
      "epoch": 14.452113891285592,
      "grad_norm": 0.009422271512448788,
      "learning_rate": 0.0007109634742594191,
      "loss": 0.0027,
      "step": 50250
    },
    {
      "epoch": 14.46649410411274,
      "grad_norm": 0.00039193054544739425,
      "learning_rate": 0.0007106758700028761,
      "loss": 0.0036,
      "step": 50300
    },
    {
      "epoch": 14.480874316939891,
      "grad_norm": 0.00013071374269202352,
      "learning_rate": 0.0007103882657463331,
      "loss": 0.0027,
      "step": 50350
    },
    {
      "epoch": 14.49525452976704,
      "grad_norm": 0.00121747434604913,
      "learning_rate": 0.00071010066148979,
      "loss": 0.0012,
      "step": 50400
    },
    {
      "epoch": 14.50963474259419,
      "grad_norm": 8.491300832247362e-05,
      "learning_rate": 0.0007098130572332471,
      "loss": 0.0037,
      "step": 50450
    },
    {
      "epoch": 14.52401495542134,
      "grad_norm": 0.007173281162977219,
      "learning_rate": 0.0007095254529767041,
      "loss": 0.0044,
      "step": 50500
    },
    {
      "epoch": 14.53839516824849,
      "grad_norm": 0.001658682944253087,
      "learning_rate": 0.000709237848720161,
      "loss": 0.0012,
      "step": 50550
    },
    {
      "epoch": 14.55277538107564,
      "grad_norm": 0.00345292198471725,
      "learning_rate": 0.0007089502444636182,
      "loss": 0.0021,
      "step": 50600
    },
    {
      "epoch": 14.56715559390279,
      "grad_norm": 0.0006344389403238893,
      "learning_rate": 0.0007086626402070751,
      "loss": 0.0026,
      "step": 50650
    },
    {
      "epoch": 14.58153580672994,
      "grad_norm": 0.018153302371501923,
      "learning_rate": 0.0007083750359505321,
      "loss": 0.003,
      "step": 50700
    },
    {
      "epoch": 14.59591601955709,
      "grad_norm": 0.0003717153158504516,
      "learning_rate": 0.0007080874316939891,
      "loss": 0.0036,
      "step": 50750
    },
    {
      "epoch": 14.61029623238424,
      "grad_norm": 0.031028948724269867,
      "learning_rate": 0.0007077998274374461,
      "loss": 0.0048,
      "step": 50800
    },
    {
      "epoch": 14.624676445211389,
      "grad_norm": 0.009325527586042881,
      "learning_rate": 0.0007075122231809031,
      "loss": 0.0023,
      "step": 50850
    },
    {
      "epoch": 14.63905665803854,
      "grad_norm": 0.00025857280706986785,
      "learning_rate": 0.0007072246189243601,
      "loss": 0.0048,
      "step": 50900
    },
    {
      "epoch": 14.653436870865688,
      "grad_norm": 0.00013406635844148695,
      "learning_rate": 0.0007069370146678171,
      "loss": 0.003,
      "step": 50950
    },
    {
      "epoch": 14.667817083692839,
      "grad_norm": 0.00012271756713744253,
      "learning_rate": 0.000706649410411274,
      "loss": 0.0036,
      "step": 51000
    },
    {
      "epoch": 14.682197296519988,
      "grad_norm": 8.218524453695863e-05,
      "learning_rate": 0.0007063618061547312,
      "loss": 0.0012,
      "step": 51050
    },
    {
      "epoch": 14.696577509347138,
      "grad_norm": 0.013443940319120884,
      "learning_rate": 0.0007060742018981882,
      "loss": 0.0006,
      "step": 51100
    },
    {
      "epoch": 14.710957722174289,
      "grad_norm": 0.05899221450090408,
      "learning_rate": 0.0007057865976416451,
      "loss": 0.0013,
      "step": 51150
    },
    {
      "epoch": 14.725337935001438,
      "grad_norm": 0.00024015917733777314,
      "learning_rate": 0.0007054989933851022,
      "loss": 0.0008,
      "step": 51200
    },
    {
      "epoch": 14.739718147828588,
      "grad_norm": 0.0011391458101570606,
      "learning_rate": 0.0007052113891285591,
      "loss": 0.0032,
      "step": 51250
    },
    {
      "epoch": 14.754098360655737,
      "grad_norm": 0.005913897883147001,
      "learning_rate": 0.0007049237848720161,
      "loss": 0.0003,
      "step": 51300
    },
    {
      "epoch": 14.768478573482888,
      "grad_norm": 0.010832896456122398,
      "learning_rate": 0.0007046361806154731,
      "loss": 0.0024,
      "step": 51350
    },
    {
      "epoch": 14.782858786310037,
      "grad_norm": 0.05796346068382263,
      "learning_rate": 0.0007043485763589301,
      "loss": 0.0037,
      "step": 51400
    },
    {
      "epoch": 14.797238999137187,
      "grad_norm": 0.00046276493230834603,
      "learning_rate": 0.0007040609721023871,
      "loss": 0.0034,
      "step": 51450
    },
    {
      "epoch": 14.811619211964338,
      "grad_norm": 4.3250904127489775e-05,
      "learning_rate": 0.0007037733678458442,
      "loss": 0.0048,
      "step": 51500
    },
    {
      "epoch": 14.825999424791487,
      "grad_norm": 0.0105209369212389,
      "learning_rate": 0.0007034857635893012,
      "loss": 0.0016,
      "step": 51550
    },
    {
      "epoch": 14.840379637618637,
      "grad_norm": 0.03841434791684151,
      "learning_rate": 0.0007031981593327581,
      "loss": 0.0015,
      "step": 51600
    },
    {
      "epoch": 14.854759850445786,
      "grad_norm": 0.005731902550905943,
      "learning_rate": 0.0007029105550762152,
      "loss": 0.0017,
      "step": 51650
    },
    {
      "epoch": 14.869140063272937,
      "grad_norm": 0.0005423990660347044,
      "learning_rate": 0.0007026229508196721,
      "loss": 0.0061,
      "step": 51700
    },
    {
      "epoch": 14.883520276100086,
      "grad_norm": 3.0992050596978515e-05,
      "learning_rate": 0.0007023353465631291,
      "loss": 0.0019,
      "step": 51750
    },
    {
      "epoch": 14.897900488927236,
      "grad_norm": 3.4097236493835226e-05,
      "learning_rate": 0.0007020477423065862,
      "loss": 0.0016,
      "step": 51800
    },
    {
      "epoch": 14.912280701754385,
      "grad_norm": 0.0234732273966074,
      "learning_rate": 0.0007017601380500431,
      "loss": 0.0019,
      "step": 51850
    },
    {
      "epoch": 14.926660914581536,
      "grad_norm": 0.006629739422351122,
      "learning_rate": 0.0007014725337935001,
      "loss": 0.0036,
      "step": 51900
    },
    {
      "epoch": 14.941041127408685,
      "grad_norm": 0.04946921020746231,
      "learning_rate": 0.0007011849295369572,
      "loss": 0.0031,
      "step": 51950
    },
    {
      "epoch": 14.955421340235835,
      "grad_norm": 0.012670020572841167,
      "learning_rate": 0.0007008973252804142,
      "loss": 0.0025,
      "step": 52000
    },
    {
      "epoch": 14.969801553062986,
      "grad_norm": 9.785895235836506e-05,
      "learning_rate": 0.0007006097210238712,
      "loss": 0.0015,
      "step": 52050
    },
    {
      "epoch": 14.984181765890135,
      "grad_norm": 3.941915201721713e-05,
      "learning_rate": 0.0007003221167673282,
      "loss": 0.0034,
      "step": 52100
    },
    {
      "epoch": 14.998561978717285,
      "grad_norm": 0.0004695857933256775,
      "learning_rate": 0.0007000345125107852,
      "loss": 0.0017,
      "step": 52150
    },
    {
      "epoch": 15.0,
      "eval_loss": 0.012139685451984406,
      "eval_runtime": 17.7536,
      "eval_samples_per_second": 2687.843,
      "eval_steps_per_second": 42.02,
      "step": 52155
    },
    {
      "epoch": 15.012942191544434,
      "grad_norm": 0.0001890637940960005,
      "learning_rate": 0.0006997469082542421,
      "loss": 0.0032,
      "step": 52200
    },
    {
      "epoch": 15.027322404371585,
      "grad_norm": 0.0001638454123167321,
      "learning_rate": 0.0006994593039976992,
      "loss": 0.003,
      "step": 52250
    },
    {
      "epoch": 15.041702617198734,
      "grad_norm": 0.0002396169729763642,
      "learning_rate": 0.0006991716997411561,
      "loss": 0.002,
      "step": 52300
    },
    {
      "epoch": 15.056082830025884,
      "grad_norm": 9.652911103330553e-05,
      "learning_rate": 0.0006988840954846131,
      "loss": 0.0022,
      "step": 52350
    },
    {
      "epoch": 15.070463042853035,
      "grad_norm": 0.00044242461444810033,
      "learning_rate": 0.0006985964912280703,
      "loss": 0.0027,
      "step": 52400
    },
    {
      "epoch": 15.084843255680184,
      "grad_norm": 5.9111927839694545e-05,
      "learning_rate": 0.0006983088869715272,
      "loss": 0.0031,
      "step": 52450
    },
    {
      "epoch": 15.099223468507335,
      "grad_norm": 0.001663907547481358,
      "learning_rate": 0.0006980212827149842,
      "loss": 0.0018,
      "step": 52500
    },
    {
      "epoch": 15.113603681334483,
      "grad_norm": 1.0890560588450171e-05,
      "learning_rate": 0.0006977336784584412,
      "loss": 0.001,
      "step": 52550
    },
    {
      "epoch": 15.127983894161634,
      "grad_norm": 0.0007048657862469554,
      "learning_rate": 0.0006974460742018982,
      "loss": 0.0021,
      "step": 52600
    },
    {
      "epoch": 15.142364106988783,
      "grad_norm": 0.002881614025682211,
      "learning_rate": 0.0006971584699453552,
      "loss": 0.0014,
      "step": 52650
    },
    {
      "epoch": 15.156744319815934,
      "grad_norm": 0.00478047551587224,
      "learning_rate": 0.0006968708656888122,
      "loss": 0.0025,
      "step": 52700
    },
    {
      "epoch": 15.171124532643082,
      "grad_norm": 6.625384412473068e-05,
      "learning_rate": 0.0006965832614322692,
      "loss": 0.0035,
      "step": 52750
    },
    {
      "epoch": 15.185504745470233,
      "grad_norm": 0.03182610869407654,
      "learning_rate": 0.0006962956571757262,
      "loss": 0.002,
      "step": 52800
    },
    {
      "epoch": 15.199884958297384,
      "grad_norm": 0.00013341491285245866,
      "learning_rate": 0.0006960080529191833,
      "loss": 0.0012,
      "step": 52850
    },
    {
      "epoch": 15.214265171124532,
      "grad_norm": 5.754999801865779e-05,
      "learning_rate": 0.0006957204486626402,
      "loss": 0.0024,
      "step": 52900
    },
    {
      "epoch": 15.228645383951683,
      "grad_norm": 0.00016974798927549273,
      "learning_rate": 0.0006954328444060972,
      "loss": 0.0018,
      "step": 52950
    },
    {
      "epoch": 15.243025596778832,
      "grad_norm": 0.0011273104464635253,
      "learning_rate": 0.0006951452401495543,
      "loss": 0.0061,
      "step": 53000
    },
    {
      "epoch": 15.257405809605983,
      "grad_norm": 4.583820191328414e-05,
      "learning_rate": 0.0006948576358930112,
      "loss": 0.0053,
      "step": 53050
    },
    {
      "epoch": 15.271786022433131,
      "grad_norm": 0.03897909075021744,
      "learning_rate": 0.0006945700316364682,
      "loss": 0.0025,
      "step": 53100
    },
    {
      "epoch": 15.286166235260282,
      "grad_norm": 0.0679735392332077,
      "learning_rate": 0.0006942824273799252,
      "loss": 0.0017,
      "step": 53150
    },
    {
      "epoch": 15.300546448087431,
      "grad_norm": 7.758002902846783e-05,
      "learning_rate": 0.0006939948231233822,
      "loss": 0.0011,
      "step": 53200
    },
    {
      "epoch": 15.314926660914582,
      "grad_norm": 0.030596664175391197,
      "learning_rate": 0.0006937072188668393,
      "loss": 0.0021,
      "step": 53250
    },
    {
      "epoch": 15.329306873741732,
      "grad_norm": 0.0001317466958425939,
      "learning_rate": 0.0006934196146102963,
      "loss": 0.003,
      "step": 53300
    },
    {
      "epoch": 15.343687086568881,
      "grad_norm": 0.00017083493003156036,
      "learning_rate": 0.0006931320103537533,
      "loss": 0.0014,
      "step": 53350
    },
    {
      "epoch": 15.358067299396032,
      "grad_norm": 6.615682650590315e-05,
      "learning_rate": 0.0006928444060972102,
      "loss": 0.0025,
      "step": 53400
    },
    {
      "epoch": 15.37244751222318,
      "grad_norm": 0.008642621338367462,
      "learning_rate": 0.0006925568018406673,
      "loss": 0.0011,
      "step": 53450
    },
    {
      "epoch": 15.386827725050331,
      "grad_norm": 0.0005814533215016127,
      "learning_rate": 0.0006922691975841242,
      "loss": 0.001,
      "step": 53500
    },
    {
      "epoch": 15.40120793787748,
      "grad_norm": 0.00027500881697051227,
      "learning_rate": 0.0006919815933275812,
      "loss": 0.0054,
      "step": 53550
    },
    {
      "epoch": 15.41558815070463,
      "grad_norm": 0.0007280840072780848,
      "learning_rate": 0.0006916939890710383,
      "loss": 0.0031,
      "step": 53600
    },
    {
      "epoch": 15.42996836353178,
      "grad_norm": 0.04354097321629524,
      "learning_rate": 0.0006914063848144953,
      "loss": 0.0024,
      "step": 53650
    },
    {
      "epoch": 15.44434857635893,
      "grad_norm": 8.378663187613711e-05,
      "learning_rate": 0.0006911187805579523,
      "loss": 0.0026,
      "step": 53700
    },
    {
      "epoch": 15.45872878918608,
      "grad_norm": 6.933906843187287e-05,
      "learning_rate": 0.0006908311763014093,
      "loss": 0.002,
      "step": 53750
    },
    {
      "epoch": 15.47310900201323,
      "grad_norm": 0.06284023076295853,
      "learning_rate": 0.0006905435720448663,
      "loss": 0.0014,
      "step": 53800
    },
    {
      "epoch": 15.48748921484038,
      "grad_norm": 0.0004963590181432664,
      "learning_rate": 0.0006902559677883233,
      "loss": 0.0008,
      "step": 53850
    },
    {
      "epoch": 15.501869427667529,
      "grad_norm": 9.34851705096662e-05,
      "learning_rate": 0.0006899683635317803,
      "loss": 0.0026,
      "step": 53900
    },
    {
      "epoch": 15.51624964049468,
      "grad_norm": 0.009270530194044113,
      "learning_rate": 0.0006896807592752373,
      "loss": 0.0046,
      "step": 53950
    },
    {
      "epoch": 15.530629853321829,
      "grad_norm": 0.0005101118003949523,
      "learning_rate": 0.0006893931550186942,
      "loss": 0.003,
      "step": 54000
    },
    {
      "epoch": 15.54501006614898,
      "grad_norm": 0.011181861162185669,
      "learning_rate": 0.0006891055507621513,
      "loss": 0.0009,
      "step": 54050
    },
    {
      "epoch": 15.559390278976128,
      "grad_norm": 3.811046917689964e-05,
      "learning_rate": 0.0006888179465056083,
      "loss": 0.0029,
      "step": 54100
    },
    {
      "epoch": 15.573770491803279,
      "grad_norm": 7.282484148163348e-05,
      "learning_rate": 0.0006885303422490653,
      "loss": 0.0026,
      "step": 54150
    },
    {
      "epoch": 15.58815070463043,
      "grad_norm": 7.772986282361671e-05,
      "learning_rate": 0.0006882427379925224,
      "loss": 0.0013,
      "step": 54200
    },
    {
      "epoch": 15.602530917457578,
      "grad_norm": 0.02694587968289852,
      "learning_rate": 0.0006879551337359793,
      "loss": 0.0039,
      "step": 54250
    },
    {
      "epoch": 15.616911130284729,
      "grad_norm": 6.915975973242894e-05,
      "learning_rate": 0.0006876675294794363,
      "loss": 0.0024,
      "step": 54300
    },
    {
      "epoch": 15.631291343111878,
      "grad_norm": 0.005143384914845228,
      "learning_rate": 0.0006873799252228933,
      "loss": 0.0012,
      "step": 54350
    },
    {
      "epoch": 15.645671555939028,
      "grad_norm": 0.0008838904905132949,
      "learning_rate": 0.0006870923209663503,
      "loss": 0.0023,
      "step": 54400
    },
    {
      "epoch": 15.660051768766177,
      "grad_norm": 0.00039332848973572254,
      "learning_rate": 0.0006868047167098072,
      "loss": 0.0011,
      "step": 54450
    },
    {
      "epoch": 15.674431981593328,
      "grad_norm": 0.0094868503510952,
      "learning_rate": 0.0006865171124532644,
      "loss": 0.0019,
      "step": 54500
    },
    {
      "epoch": 15.688812194420478,
      "grad_norm": 0.00018367773736827075,
      "learning_rate": 0.0006862295081967214,
      "loss": 0.0037,
      "step": 54550
    },
    {
      "epoch": 15.703192407247627,
      "grad_norm": 0.03792606294155121,
      "learning_rate": 0.0006859419039401783,
      "loss": 0.0023,
      "step": 54600
    },
    {
      "epoch": 15.717572620074778,
      "grad_norm": 0.00022696865198668092,
      "learning_rate": 0.0006856542996836354,
      "loss": 0.0013,
      "step": 54650
    },
    {
      "epoch": 15.731952832901927,
      "grad_norm": 0.01275000162422657,
      "learning_rate": 0.0006853666954270923,
      "loss": 0.0027,
      "step": 54700
    },
    {
      "epoch": 15.746333045729077,
      "grad_norm": 4.094019459444098e-05,
      "learning_rate": 0.0006850790911705493,
      "loss": 0.0038,
      "step": 54750
    },
    {
      "epoch": 15.760713258556226,
      "grad_norm": 2.303117435076274e-05,
      "learning_rate": 0.0006847914869140064,
      "loss": 0.0024,
      "step": 54800
    },
    {
      "epoch": 15.775093471383377,
      "grad_norm": 0.01112802792340517,
      "learning_rate": 0.0006845038826574633,
      "loss": 0.0011,
      "step": 54850
    },
    {
      "epoch": 15.789473684210526,
      "grad_norm": 0.0005690615507774055,
      "learning_rate": 0.0006842162784009203,
      "loss": 0.0016,
      "step": 54900
    },
    {
      "epoch": 15.803853897037676,
      "grad_norm": 2.08443798328517e-05,
      "learning_rate": 0.0006839286741443774,
      "loss": 0.0031,
      "step": 54950
    },
    {
      "epoch": 15.818234109864825,
      "grad_norm": 0.10308244079351425,
      "learning_rate": 0.0006836410698878344,
      "loss": 0.0028,
      "step": 55000
    },
    {
      "epoch": 15.832614322691976,
      "grad_norm": 0.0006043298053555191,
      "learning_rate": 0.0006833534656312913,
      "loss": 0.0039,
      "step": 55050
    },
    {
      "epoch": 15.846994535519126,
      "grad_norm": 0.00021014623052906245,
      "learning_rate": 0.0006830658613747484,
      "loss": 0.0021,
      "step": 55100
    },
    {
      "epoch": 15.861374748346275,
      "grad_norm": 0.00047646317398175597,
      "learning_rate": 0.0006827782571182054,
      "loss": 0.0038,
      "step": 55150
    },
    {
      "epoch": 15.875754961173426,
      "grad_norm": 5.5222611990757287e-05,
      "learning_rate": 0.0006824906528616623,
      "loss": 0.0099,
      "step": 55200
    },
    {
      "epoch": 15.890135174000575,
      "grad_norm": 5.0124479457736015e-05,
      "learning_rate": 0.0006822030486051194,
      "loss": 0.0036,
      "step": 55250
    },
    {
      "epoch": 15.904515386827725,
      "grad_norm": 0.0007017050520516932,
      "learning_rate": 0.0006819154443485763,
      "loss": 0.0023,
      "step": 55300
    },
    {
      "epoch": 15.918895599654874,
      "grad_norm": 0.003650086931884289,
      "learning_rate": 0.0006816278400920333,
      "loss": 0.0007,
      "step": 55350
    },
    {
      "epoch": 15.933275812482025,
      "grad_norm": 7.392752740997821e-05,
      "learning_rate": 0.0006813402358354905,
      "loss": 0.0038,
      "step": 55400
    },
    {
      "epoch": 15.947656025309175,
      "grad_norm": 0.00011492080375319347,
      "learning_rate": 0.0006810526315789474,
      "loss": 0.0062,
      "step": 55450
    },
    {
      "epoch": 15.962036238136324,
      "grad_norm": 0.011798917315900326,
      "learning_rate": 0.0006807650273224044,
      "loss": 0.002,
      "step": 55500
    },
    {
      "epoch": 15.976416450963475,
      "grad_norm": 0.013008869253098965,
      "learning_rate": 0.0006804774230658614,
      "loss": 0.0011,
      "step": 55550
    },
    {
      "epoch": 15.990796663790624,
      "grad_norm": 0.0016159061342477798,
      "learning_rate": 0.0006801898188093184,
      "loss": 0.0053,
      "step": 55600
    },
    {
      "epoch": 16.0,
      "eval_loss": 0.010652391240000725,
      "eval_runtime": 17.0089,
      "eval_samples_per_second": 2805.537,
      "eval_steps_per_second": 43.859,
      "step": 55632
    },
    {
      "epoch": 16.005176876617774,
      "grad_norm": 0.03018197976052761,
      "learning_rate": 0.0006799022145527753,
      "loss": 0.0015,
      "step": 55650
    },
    {
      "epoch": 16.019557089444923,
      "grad_norm": 3.680122608784586e-05,
      "learning_rate": 0.0006796146102962324,
      "loss": 0.0013,
      "step": 55700
    },
    {
      "epoch": 16.033937302272072,
      "grad_norm": 8.288863318739459e-05,
      "learning_rate": 0.0006793270060396894,
      "loss": 0.0022,
      "step": 55750
    },
    {
      "epoch": 16.048317515099225,
      "grad_norm": 0.024105817079544067,
      "learning_rate": 0.0006790394017831463,
      "loss": 0.0024,
      "step": 55800
    },
    {
      "epoch": 16.062697727926373,
      "grad_norm": 0.00017800407658796757,
      "learning_rate": 0.0006787517975266035,
      "loss": 0.0034,
      "step": 55850
    },
    {
      "epoch": 16.077077940753522,
      "grad_norm": 0.005334653425961733,
      "learning_rate": 0.0006784641932700604,
      "loss": 0.0009,
      "step": 55900
    },
    {
      "epoch": 16.091458153580675,
      "grad_norm": 0.004239218775182962,
      "learning_rate": 0.0006781765890135174,
      "loss": 0.0022,
      "step": 55950
    },
    {
      "epoch": 16.105838366407824,
      "grad_norm": 0.03308514878153801,
      "learning_rate": 0.0006778889847569745,
      "loss": 0.0019,
      "step": 56000
    },
    {
      "epoch": 16.120218579234972,
      "grad_norm": 0.00016659540415275842,
      "learning_rate": 0.0006776013805004314,
      "loss": 0.0016,
      "step": 56050
    },
    {
      "epoch": 16.13459879206212,
      "grad_norm": 0.0010740364668890834,
      "learning_rate": 0.0006773137762438884,
      "loss": 0.0017,
      "step": 56100
    },
    {
      "epoch": 16.148979004889274,
      "grad_norm": 0.0002182591415476054,
      "learning_rate": 0.0006770261719873454,
      "loss": 0.0033,
      "step": 56150
    },
    {
      "epoch": 16.163359217716422,
      "grad_norm": 0.0004358271835371852,
      "learning_rate": 0.0006767385677308024,
      "loss": 0.0048,
      "step": 56200
    },
    {
      "epoch": 16.17773943054357,
      "grad_norm": 0.09108369052410126,
      "learning_rate": 0.0006764509634742593,
      "loss": 0.0027,
      "step": 56250
    },
    {
      "epoch": 16.19211964337072,
      "grad_norm": 0.00010933754674624652,
      "learning_rate": 0.0006761633592177165,
      "loss": 0.0015,
      "step": 56300
    },
    {
      "epoch": 16.206499856197873,
      "grad_norm": 0.015521947294473648,
      "learning_rate": 0.0006758757549611735,
      "loss": 0.0041,
      "step": 56350
    },
    {
      "epoch": 16.22088006902502,
      "grad_norm": 0.00014185780310072005,
      "learning_rate": 0.0006755881507046304,
      "loss": 0.0046,
      "step": 56400
    },
    {
      "epoch": 16.23526028185217,
      "grad_norm": 0.00014331078273244202,
      "learning_rate": 0.0006753005464480875,
      "loss": 0.0024,
      "step": 56450
    },
    {
      "epoch": 16.249640494679323,
      "grad_norm": 0.0005099304253235459,
      "learning_rate": 0.0006750129421915444,
      "loss": 0.0006,
      "step": 56500
    },
    {
      "epoch": 16.26402070750647,
      "grad_norm": 0.0038892386946827173,
      "learning_rate": 0.0006747253379350014,
      "loss": 0.0047,
      "step": 56550
    },
    {
      "epoch": 16.27840092033362,
      "grad_norm": 0.0024640813935548067,
      "learning_rate": 0.0006744377336784585,
      "loss": 0.0021,
      "step": 56600
    },
    {
      "epoch": 16.29278113316077,
      "grad_norm": 0.0002299719926668331,
      "learning_rate": 0.0006741501294219154,
      "loss": 0.0034,
      "step": 56650
    },
    {
      "epoch": 16.30716134598792,
      "grad_norm": 9.456897532800213e-05,
      "learning_rate": 0.0006738625251653725,
      "loss": 0.0014,
      "step": 56700
    },
    {
      "epoch": 16.32154155881507,
      "grad_norm": 0.022529810667037964,
      "learning_rate": 0.0006735749209088295,
      "loss": 0.001,
      "step": 56750
    },
    {
      "epoch": 16.33592177164222,
      "grad_norm": 0.005214132834225893,
      "learning_rate": 0.0006732873166522865,
      "loss": 0.0005,
      "step": 56800
    },
    {
      "epoch": 16.35030198446937,
      "grad_norm": 3.112238482572138e-05,
      "learning_rate": 0.0006729997123957434,
      "loss": 0.0031,
      "step": 56850
    },
    {
      "epoch": 16.36468219729652,
      "grad_norm": 0.0006759785464964807,
      "learning_rate": 0.0006727121081392005,
      "loss": 0.0012,
      "step": 56900
    },
    {
      "epoch": 16.37906241012367,
      "grad_norm": 0.03675074875354767,
      "learning_rate": 0.0006724245038826575,
      "loss": 0.0045,
      "step": 56950
    },
    {
      "epoch": 16.39344262295082,
      "grad_norm": 0.026121960952878,
      "learning_rate": 0.0006721368996261144,
      "loss": 0.0045,
      "step": 57000
    },
    {
      "epoch": 16.40782283577797,
      "grad_norm": 0.003141606692224741,
      "learning_rate": 0.0006718492953695715,
      "loss": 0.0058,
      "step": 57050
    },
    {
      "epoch": 16.42220304860512,
      "grad_norm": 8.648337825434282e-05,
      "learning_rate": 0.0006715616911130284,
      "loss": 0.0022,
      "step": 57100
    },
    {
      "epoch": 16.43658326143227,
      "grad_norm": 0.00011244706547586247,
      "learning_rate": 0.0006712740868564855,
      "loss": 0.0026,
      "step": 57150
    },
    {
      "epoch": 16.450963474259417,
      "grad_norm": 0.013472734950482845,
      "learning_rate": 0.0006709864825999425,
      "loss": 0.0064,
      "step": 57200
    },
    {
      "epoch": 16.46534368708657,
      "grad_norm": 3.841815487248823e-05,
      "learning_rate": 0.0006706988783433995,
      "loss": 0.0046,
      "step": 57250
    },
    {
      "epoch": 16.47972389991372,
      "grad_norm": 0.0012724373955279589,
      "learning_rate": 0.0006704112740868565,
      "loss": 0.0021,
      "step": 57300
    },
    {
      "epoch": 16.494104112740867,
      "grad_norm": 0.0024640141054987907,
      "learning_rate": 0.0006701236698303135,
      "loss": 0.0011,
      "step": 57350
    },
    {
      "epoch": 16.50848432556802,
      "grad_norm": 0.01662623882293701,
      "learning_rate": 0.0006698360655737705,
      "loss": 0.0035,
      "step": 57400
    },
    {
      "epoch": 16.52286453839517,
      "grad_norm": 0.00011609089415287599,
      "learning_rate": 0.0006695484613172274,
      "loss": 0.001,
      "step": 57450
    },
    {
      "epoch": 16.537244751222318,
      "grad_norm": 0.010520752519369125,
      "learning_rate": 0.0006692608570606845,
      "loss": 0.0012,
      "step": 57500
    },
    {
      "epoch": 16.551624964049466,
      "grad_norm": 0.00019991688895970583,
      "learning_rate": 0.0006689732528041416,
      "loss": 0.0006,
      "step": 57550
    },
    {
      "epoch": 16.56600517687662,
      "grad_norm": 3.293539339210838e-05,
      "learning_rate": 0.0006686856485475985,
      "loss": 0.0007,
      "step": 57600
    },
    {
      "epoch": 16.580385389703768,
      "grad_norm": 0.00404218677431345,
      "learning_rate": 0.0006683980442910556,
      "loss": 0.0018,
      "step": 57650
    },
    {
      "epoch": 16.594765602530916,
      "grad_norm": 0.00015357823576778173,
      "learning_rate": 0.0006681104400345125,
      "loss": 0.0017,
      "step": 57700
    },
    {
      "epoch": 16.60914581535807,
      "grad_norm": 0.00930252205580473,
      "learning_rate": 0.0006678228357779695,
      "loss": 0.0023,
      "step": 57750
    },
    {
      "epoch": 16.623526028185218,
      "grad_norm": 6.196748290676624e-05,
      "learning_rate": 0.0006675352315214265,
      "loss": 0.0027,
      "step": 57800
    },
    {
      "epoch": 16.637906241012367,
      "grad_norm": 0.06285210698843002,
      "learning_rate": 0.0006672476272648835,
      "loss": 0.0025,
      "step": 57850
    },
    {
      "epoch": 16.652286453839515,
      "grad_norm": 3.820834172074683e-05,
      "learning_rate": 0.0006669600230083405,
      "loss": 0.001,
      "step": 57900
    },
    {
      "epoch": 16.666666666666668,
      "grad_norm": 4.177723531029187e-05,
      "learning_rate": 0.0006666724187517975,
      "loss": 0.002,
      "step": 57950
    },
    {
      "epoch": 16.681046879493817,
      "grad_norm": 0.00021806514996569604,
      "learning_rate": 0.0006663848144952546,
      "loss": 0.0045,
      "step": 58000
    },
    {
      "epoch": 16.695427092320966,
      "grad_norm": 0.00015134525892790407,
      "learning_rate": 0.0006660972102387115,
      "loss": 0.0055,
      "step": 58050
    },
    {
      "epoch": 16.709807305148118,
      "grad_norm": 0.051587965339422226,
      "learning_rate": 0.0006658096059821686,
      "loss": 0.0039,
      "step": 58100
    },
    {
      "epoch": 16.724187517975267,
      "grad_norm": 2.477469934092369e-05,
      "learning_rate": 0.0006655220017256256,
      "loss": 0.0019,
      "step": 58150
    },
    {
      "epoch": 16.738567730802416,
      "grad_norm": 0.0007510234718210995,
      "learning_rate": 0.0006652343974690825,
      "loss": 0.0017,
      "step": 58200
    },
    {
      "epoch": 16.752947943629565,
      "grad_norm": 3.872556044370867e-05,
      "learning_rate": 0.0006649467932125396,
      "loss": 0.0029,
      "step": 58250
    },
    {
      "epoch": 16.767328156456717,
      "grad_norm": 0.035818737000226974,
      "learning_rate": 0.0006646591889559965,
      "loss": 0.0044,
      "step": 58300
    },
    {
      "epoch": 16.781708369283866,
      "grad_norm": 3.693638427648693e-05,
      "learning_rate": 0.0006643715846994535,
      "loss": 0.0044,
      "step": 58350
    },
    {
      "epoch": 16.796088582111015,
      "grad_norm": 0.003185918787494302,
      "learning_rate": 0.0006640839804429106,
      "loss": 0.0035,
      "step": 58400
    },
    {
      "epoch": 16.810468794938163,
      "grad_norm": 0.00025232991902157664,
      "learning_rate": 0.0006637963761863676,
      "loss": 0.0037,
      "step": 58450
    },
    {
      "epoch": 16.824849007765316,
      "grad_norm": 0.01916133053600788,
      "learning_rate": 0.0006635087719298246,
      "loss": 0.0024,
      "step": 58500
    },
    {
      "epoch": 16.839229220592465,
      "grad_norm": 0.00020050618331879377,
      "learning_rate": 0.0006632211676732816,
      "loss": 0.0011,
      "step": 58550
    },
    {
      "epoch": 16.853609433419614,
      "grad_norm": 9.278966172132641e-05,
      "learning_rate": 0.0006629335634167386,
      "loss": 0.0014,
      "step": 58600
    },
    {
      "epoch": 16.867989646246766,
      "grad_norm": 0.000819238368421793,
      "learning_rate": 0.0006626459591601955,
      "loss": 0.005,
      "step": 58650
    },
    {
      "epoch": 16.882369859073915,
      "grad_norm": 9.897322888718918e-05,
      "learning_rate": 0.0006623583549036526,
      "loss": 0.0028,
      "step": 58700
    },
    {
      "epoch": 16.896750071901064,
      "grad_norm": 0.015236437320709229,
      "learning_rate": 0.0006620707506471096,
      "loss": 0.001,
      "step": 58750
    },
    {
      "epoch": 16.911130284728213,
      "grad_norm": 0.0021927556954324245,
      "learning_rate": 0.0006617831463905665,
      "loss": 0.0019,
      "step": 58800
    },
    {
      "epoch": 16.925510497555365,
      "grad_norm": 0.0007674553198739886,
      "learning_rate": 0.0006614955421340237,
      "loss": 0.0039,
      "step": 58850
    },
    {
      "epoch": 16.939890710382514,
      "grad_norm": 0.0059791067615151405,
      "learning_rate": 0.0006612079378774806,
      "loss": 0.0009,
      "step": 58900
    },
    {
      "epoch": 16.954270923209663,
      "grad_norm": 0.00043916713912039995,
      "learning_rate": 0.0006609203336209376,
      "loss": 0.0024,
      "step": 58950
    },
    {
      "epoch": 16.96865113603681,
      "grad_norm": 5.35119506821502e-05,
      "learning_rate": 0.0006606327293643946,
      "loss": 0.002,
      "step": 59000
    },
    {
      "epoch": 16.983031348863964,
      "grad_norm": 0.002864475827664137,
      "learning_rate": 0.0006603451251078516,
      "loss": 0.0013,
      "step": 59050
    },
    {
      "epoch": 16.997411561691113,
      "grad_norm": 0.0010087263071909547,
      "learning_rate": 0.0006600575208513086,
      "loss": 0.0022,
      "step": 59100
    },
    {
      "epoch": 17.0,
      "eval_loss": 0.010867596603929996,
      "eval_runtime": 17.7053,
      "eval_samples_per_second": 2695.188,
      "eval_steps_per_second": 42.134,
      "step": 59109
    },
    {
      "epoch": 17.01179177451826,
      "grad_norm": 0.021854666993021965,
      "learning_rate": 0.0006597699165947656,
      "loss": 0.0028,
      "step": 59150
    },
    {
      "epoch": 17.026171987345414,
      "grad_norm": 0.01112610474228859,
      "learning_rate": 0.0006594823123382226,
      "loss": 0.0012,
      "step": 59200
    },
    {
      "epoch": 17.040552200172563,
      "grad_norm": 0.00017714366549625993,
      "learning_rate": 0.0006591947080816795,
      "loss": 0.0028,
      "step": 59250
    },
    {
      "epoch": 17.05493241299971,
      "grad_norm": 0.0018994471756741405,
      "learning_rate": 0.0006589071038251367,
      "loss": 0.0018,
      "step": 59300
    },
    {
      "epoch": 17.06931262582686,
      "grad_norm": 2.509495971025899e-05,
      "learning_rate": 0.0006586194995685937,
      "loss": 0.0033,
      "step": 59350
    },
    {
      "epoch": 17.083692838654013,
      "grad_norm": 5.712693746318109e-05,
      "learning_rate": 0.0006583318953120506,
      "loss": 0.0038,
      "step": 59400
    },
    {
      "epoch": 17.098073051481162,
      "grad_norm": 4.961950980941765e-05,
      "learning_rate": 0.0006580442910555077,
      "loss": 0.0011,
      "step": 59450
    },
    {
      "epoch": 17.11245326430831,
      "grad_norm": 0.07357189059257507,
      "learning_rate": 0.0006577566867989646,
      "loss": 0.0016,
      "step": 59500
    },
    {
      "epoch": 17.126833477135463,
      "grad_norm": 0.003073961241170764,
      "learning_rate": 0.0006574690825424216,
      "loss": 0.0011,
      "step": 59550
    },
    {
      "epoch": 17.141213689962612,
      "grad_norm": 0.00011514087964314967,
      "learning_rate": 0.0006571814782858786,
      "loss": 0.0006,
      "step": 59600
    },
    {
      "epoch": 17.15559390278976,
      "grad_norm": 0.0019811377860605717,
      "learning_rate": 0.0006568938740293356,
      "loss": 0.0036,
      "step": 59650
    },
    {
      "epoch": 17.16997411561691,
      "grad_norm": 0.0011238233419135213,
      "learning_rate": 0.0006566062697727927,
      "loss": 0.0046,
      "step": 59700
    },
    {
      "epoch": 17.184354328444062,
      "grad_norm": 0.02517153136432171,
      "learning_rate": 0.0006563186655162497,
      "loss": 0.0039,
      "step": 59750
    },
    {
      "epoch": 17.19873454127121,
      "grad_norm": 0.006188100203871727,
      "learning_rate": 0.0006560310612597067,
      "loss": 0.0015,
      "step": 59800
    },
    {
      "epoch": 17.21311475409836,
      "grad_norm": 0.00028868246590718627,
      "learning_rate": 0.0006557434570031636,
      "loss": 0.0014,
      "step": 59850
    },
    {
      "epoch": 17.227494966925512,
      "grad_norm": 0.061822958290576935,
      "learning_rate": 0.0006554558527466207,
      "loss": 0.0052,
      "step": 59900
    },
    {
      "epoch": 17.24187517975266,
      "grad_norm": 8.562193397665396e-05,
      "learning_rate": 0.0006551682484900776,
      "loss": 0.0012,
      "step": 59950
    },
    {
      "epoch": 17.25625539257981,
      "grad_norm": 0.0001245540624950081,
      "learning_rate": 0.0006548806442335346,
      "loss": 0.0018,
      "step": 60000
    },
    {
      "epoch": 17.27063560540696,
      "grad_norm": 0.0013157068751752377,
      "learning_rate": 0.0006545930399769917,
      "loss": 0.0018,
      "step": 60050
    },
    {
      "epoch": 17.28501581823411,
      "grad_norm": 0.007998106069862843,
      "learning_rate": 0.0006543054357204486,
      "loss": 0.0033,
      "step": 60100
    },
    {
      "epoch": 17.29939603106126,
      "grad_norm": 2.5793111490202136e-05,
      "learning_rate": 0.0006540178314639057,
      "loss": 0.0004,
      "step": 60150
    },
    {
      "epoch": 17.31377624388841,
      "grad_norm": 9.860921272775158e-05,
      "learning_rate": 0.0006537302272073627,
      "loss": 0.0012,
      "step": 60200
    },
    {
      "epoch": 17.328156456715558,
      "grad_norm": 2.3528875317424536e-05,
      "learning_rate": 0.0006534426229508197,
      "loss": 0.0031,
      "step": 60250
    },
    {
      "epoch": 17.34253666954271,
      "grad_norm": 0.007381124887615442,
      "learning_rate": 0.0006531550186942767,
      "loss": 0.0023,
      "step": 60300
    },
    {
      "epoch": 17.35691688236986,
      "grad_norm": 0.0005517130484804511,
      "learning_rate": 0.0006528674144377337,
      "loss": 0.0036,
      "step": 60350
    },
    {
      "epoch": 17.371297095197008,
      "grad_norm": 0.02428402751684189,
      "learning_rate": 0.0006525798101811907,
      "loss": 0.0017,
      "step": 60400
    },
    {
      "epoch": 17.38567730802416,
      "grad_norm": 0.0037523091305047274,
      "learning_rate": 0.0006522922059246476,
      "loss": 0.0017,
      "step": 60450
    },
    {
      "epoch": 17.40005752085131,
      "grad_norm": 0.02875569649040699,
      "learning_rate": 0.0006520046016681047,
      "loss": 0.0008,
      "step": 60500
    },
    {
      "epoch": 17.414437733678458,
      "grad_norm": 2.851673707482405e-05,
      "learning_rate": 0.0006517169974115616,
      "loss": 0.0017,
      "step": 60550
    },
    {
      "epoch": 17.428817946505607,
      "grad_norm": 0.00011829092545667663,
      "learning_rate": 0.0006514293931550187,
      "loss": 0.0016,
      "step": 60600
    },
    {
      "epoch": 17.44319815933276,
      "grad_norm": 0.00011970038030995056,
      "learning_rate": 0.0006511417888984758,
      "loss": 0.0021,
      "step": 60650
    },
    {
      "epoch": 17.457578372159908,
      "grad_norm": 0.0003766485024243593,
      "learning_rate": 0.0006508541846419327,
      "loss": 0.0015,
      "step": 60700
    },
    {
      "epoch": 17.471958584987057,
      "grad_norm": 0.046663422137498856,
      "learning_rate": 0.0006505665803853897,
      "loss": 0.0034,
      "step": 60750
    },
    {
      "epoch": 17.48633879781421,
      "grad_norm": 0.0036586730275303125,
      "learning_rate": 0.0006502789761288467,
      "loss": 0.0011,
      "step": 60800
    },
    {
      "epoch": 17.500719010641358,
      "grad_norm": 8.14560116850771e-05,
      "learning_rate": 0.0006499913718723037,
      "loss": 0.0041,
      "step": 60850
    },
    {
      "epoch": 17.515099223468507,
      "grad_norm": 0.008007295429706573,
      "learning_rate": 0.0006497037676157607,
      "loss": 0.0087,
      "step": 60900
    },
    {
      "epoch": 17.529479436295656,
      "grad_norm": 5.9958787460345775e-05,
      "learning_rate": 0.0006494161633592177,
      "loss": 0.0018,
      "step": 60950
    },
    {
      "epoch": 17.54385964912281,
      "grad_norm": 0.01705394871532917,
      "learning_rate": 0.0006491285591026748,
      "loss": 0.0033,
      "step": 61000
    },
    {
      "epoch": 17.558239861949957,
      "grad_norm": 0.0008362382650375366,
      "learning_rate": 0.0006488409548461317,
      "loss": 0.001,
      "step": 61050
    },
    {
      "epoch": 17.572620074777106,
      "grad_norm": 0.019493043422698975,
      "learning_rate": 0.0006485533505895888,
      "loss": 0.0033,
      "step": 61100
    },
    {
      "epoch": 17.587000287604255,
      "grad_norm": 0.00011630699736997485,
      "learning_rate": 0.0006482657463330457,
      "loss": 0.0026,
      "step": 61150
    },
    {
      "epoch": 17.601380500431407,
      "grad_norm": 0.0004710490466095507,
      "learning_rate": 0.0006479781420765027,
      "loss": 0.0012,
      "step": 61200
    },
    {
      "epoch": 17.615760713258556,
      "grad_norm": 0.007567288354039192,
      "learning_rate": 0.0006476905378199598,
      "loss": 0.0016,
      "step": 61250
    },
    {
      "epoch": 17.630140926085705,
      "grad_norm": 8.454301132587716e-05,
      "learning_rate": 0.0006474029335634167,
      "loss": 0.0031,
      "step": 61300
    },
    {
      "epoch": 17.644521138912857,
      "grad_norm": 0.0006586491363123059,
      "learning_rate": 0.0006471153293068737,
      "loss": 0.0042,
      "step": 61350
    },
    {
      "epoch": 17.658901351740006,
      "grad_norm": 0.00012178002361906692,
      "learning_rate": 0.0006468277250503307,
      "loss": 0.0043,
      "step": 61400
    },
    {
      "epoch": 17.673281564567155,
      "grad_norm": 0.0002359054342377931,
      "learning_rate": 0.0006465401207937878,
      "loss": 0.0009,
      "step": 61450
    },
    {
      "epoch": 17.687661777394304,
      "grad_norm": 0.007804383523762226,
      "learning_rate": 0.0006462525165372448,
      "loss": 0.0035,
      "step": 61500
    },
    {
      "epoch": 17.702041990221456,
      "grad_norm": 0.0709160640835762,
      "learning_rate": 0.0006459649122807018,
      "loss": 0.0029,
      "step": 61550
    },
    {
      "epoch": 17.716422203048605,
      "grad_norm": 0.009413079358637333,
      "learning_rate": 0.0006456773080241588,
      "loss": 0.0041,
      "step": 61600
    },
    {
      "epoch": 17.730802415875754,
      "grad_norm": 4.5483786379918456e-05,
      "learning_rate": 0.0006453897037676157,
      "loss": 0.0011,
      "step": 61650
    },
    {
      "epoch": 17.745182628702906,
      "grad_norm": 0.0003213629242964089,
      "learning_rate": 0.0006451020995110728,
      "loss": 0.0006,
      "step": 61700
    },
    {
      "epoch": 17.759562841530055,
      "grad_norm": 0.002034338191151619,
      "learning_rate": 0.0006448144952545297,
      "loss": 0.0023,
      "step": 61750
    },
    {
      "epoch": 17.773943054357204,
      "grad_norm": 0.021821612492203712,
      "learning_rate": 0.0006445268909979867,
      "loss": 0.0034,
      "step": 61800
    },
    {
      "epoch": 17.788323267184353,
      "grad_norm": 0.00683846790343523,
      "learning_rate": 0.0006442392867414439,
      "loss": 0.0026,
      "step": 61850
    },
    {
      "epoch": 17.802703480011505,
      "grad_norm": 6.136729643912986e-05,
      "learning_rate": 0.0006439516824849008,
      "loss": 0.0034,
      "step": 61900
    },
    {
      "epoch": 17.817083692838654,
      "grad_norm": 0.00017683730402495712,
      "learning_rate": 0.0006436640782283578,
      "loss": 0.0059,
      "step": 61950
    },
    {
      "epoch": 17.831463905665803,
      "grad_norm": 0.0017994216177612543,
      "learning_rate": 0.0006433764739718148,
      "loss": 0.0019,
      "step": 62000
    },
    {
      "epoch": 17.845844118492955,
      "grad_norm": 3.727102739503607e-05,
      "learning_rate": 0.0006430888697152718,
      "loss": 0.0024,
      "step": 62050
    },
    {
      "epoch": 17.860224331320104,
      "grad_norm": 0.00048214945127256215,
      "learning_rate": 0.0006428012654587288,
      "loss": 0.0029,
      "step": 62100
    },
    {
      "epoch": 17.874604544147253,
      "grad_norm": 0.00022338086273521185,
      "learning_rate": 0.0006425136612021858,
      "loss": 0.0016,
      "step": 62150
    },
    {
      "epoch": 17.888984756974402,
      "grad_norm": 0.011656892485916615,
      "learning_rate": 0.0006422260569456428,
      "loss": 0.0026,
      "step": 62200
    },
    {
      "epoch": 17.903364969801554,
      "grad_norm": 0.000147140453918837,
      "learning_rate": 0.0006419384526890997,
      "loss": 0.0009,
      "step": 62250
    },
    {
      "epoch": 17.917745182628703,
      "grad_norm": 0.00027219191542826593,
      "learning_rate": 0.0006416508484325569,
      "loss": 0.0011,
      "step": 62300
    },
    {
      "epoch": 17.932125395455852,
      "grad_norm": 0.0007122443057596684,
      "learning_rate": 0.0006413632441760138,
      "loss": 0.0005,
      "step": 62350
    },
    {
      "epoch": 17.946505608283,
      "grad_norm": 0.00022884656209498644,
      "learning_rate": 0.0006410756399194708,
      "loss": 0.0026,
      "step": 62400
    },
    {
      "epoch": 17.960885821110153,
      "grad_norm": 4.271283614798449e-05,
      "learning_rate": 0.0006407880356629279,
      "loss": 0.0052,
      "step": 62450
    },
    {
      "epoch": 17.975266033937302,
      "grad_norm": 0.0010976060293614864,
      "learning_rate": 0.0006405004314063848,
      "loss": 0.0059,
      "step": 62500
    },
    {
      "epoch": 17.98964624676445,
      "grad_norm": 0.0992562398314476,
      "learning_rate": 0.0006402128271498418,
      "loss": 0.0057,
      "step": 62550
    },
    {
      "epoch": 18.0,
      "eval_loss": 0.010147531516849995,
      "eval_runtime": 17.3331,
      "eval_samples_per_second": 2753.062,
      "eval_steps_per_second": 43.039,
      "step": 62586
    },
    {
      "epoch": 18.004026459591604,
      "grad_norm": 0.00038674045936204493,
      "learning_rate": 0.0006399252228932988,
      "loss": 0.0048,
      "step": 62600
    },
    {
      "epoch": 18.018406672418752,
      "grad_norm": 0.0024078399874269962,
      "learning_rate": 0.0006396376186367558,
      "loss": 0.0013,
      "step": 62650
    },
    {
      "epoch": 18.0327868852459,
      "grad_norm": 0.04929863661527634,
      "learning_rate": 0.0006393500143802127,
      "loss": 0.0027,
      "step": 62700
    },
    {
      "epoch": 18.04716709807305,
      "grad_norm": 4.85591335745994e-05,
      "learning_rate": 0.0006390624101236699,
      "loss": 0.0014,
      "step": 62750
    },
    {
      "epoch": 18.061547310900202,
      "grad_norm": 0.011114505119621754,
      "learning_rate": 0.0006387748058671269,
      "loss": 0.0017,
      "step": 62800
    },
    {
      "epoch": 18.07592752372735,
      "grad_norm": 0.0010296449763700366,
      "learning_rate": 0.0006384872016105838,
      "loss": 0.003,
      "step": 62850
    },
    {
      "epoch": 18.0903077365545,
      "grad_norm": 0.008481492288410664,
      "learning_rate": 0.0006381995973540409,
      "loss": 0.0013,
      "step": 62900
    },
    {
      "epoch": 18.104687949381653,
      "grad_norm": 4.075285323779099e-05,
      "learning_rate": 0.0006379119930974978,
      "loss": 0.0016,
      "step": 62950
    },
    {
      "epoch": 18.1190681622088,
      "grad_norm": 7.434091821778566e-05,
      "learning_rate": 0.0006376243888409548,
      "loss": 0.0021,
      "step": 63000
    },
    {
      "epoch": 18.13344837503595,
      "grad_norm": 0.0004329696239437908,
      "learning_rate": 0.0006373367845844119,
      "loss": 0.0047,
      "step": 63050
    },
    {
      "epoch": 18.1478285878631,
      "grad_norm": 4.823308699997142e-05,
      "learning_rate": 0.0006370491803278688,
      "loss": 0.0045,
      "step": 63100
    },
    {
      "epoch": 18.16220880069025,
      "grad_norm": 7.116382766980678e-05,
      "learning_rate": 0.0006367615760713259,
      "loss": 0.0003,
      "step": 63150
    },
    {
      "epoch": 18.1765890135174,
      "grad_norm": 0.00034472160041332245,
      "learning_rate": 0.0006364739718147829,
      "loss": 0.0008,
      "step": 63200
    },
    {
      "epoch": 18.19096922634455,
      "grad_norm": 0.0011134184896945953,
      "learning_rate": 0.0006361863675582399,
      "loss": 0.0038,
      "step": 63250
    },
    {
      "epoch": 18.205349439171698,
      "grad_norm": 3.5326334909768775e-05,
      "learning_rate": 0.0006358987633016968,
      "loss": 0.0012,
      "step": 63300
    },
    {
      "epoch": 18.21972965199885,
      "grad_norm": 0.010610763914883137,
      "learning_rate": 0.0006356111590451539,
      "loss": 0.0022,
      "step": 63350
    },
    {
      "epoch": 18.234109864826,
      "grad_norm": 0.00036816272768191993,
      "learning_rate": 0.0006353235547886109,
      "loss": 0.0028,
      "step": 63400
    },
    {
      "epoch": 18.24849007765315,
      "grad_norm": 0.010946718044579029,
      "learning_rate": 0.0006350359505320678,
      "loss": 0.003,
      "step": 63450
    },
    {
      "epoch": 18.2628702904803,
      "grad_norm": 0.00018318294314667583,
      "learning_rate": 0.0006347483462755249,
      "loss": 0.0034,
      "step": 63500
    },
    {
      "epoch": 18.27725050330745,
      "grad_norm": 0.000916915712878108,
      "learning_rate": 0.0006344607420189818,
      "loss": 0.0043,
      "step": 63550
    },
    {
      "epoch": 18.2916307161346,
      "grad_norm": 0.0008027240401133895,
      "learning_rate": 0.0006341731377624389,
      "loss": 0.0049,
      "step": 63600
    },
    {
      "epoch": 18.306010928961747,
      "grad_norm": 0.031174147501587868,
      "learning_rate": 0.000633885533505896,
      "loss": 0.002,
      "step": 63650
    },
    {
      "epoch": 18.3203911417889,
      "grad_norm": 0.00017326818488072604,
      "learning_rate": 0.0006335979292493529,
      "loss": 0.0023,
      "step": 63700
    },
    {
      "epoch": 18.33477135461605,
      "grad_norm": 0.0001453354925615713,
      "learning_rate": 0.0006333103249928099,
      "loss": 0.0011,
      "step": 63750
    },
    {
      "epoch": 18.349151567443197,
      "grad_norm": 0.0006772457272745669,
      "learning_rate": 0.0006330227207362669,
      "loss": 0.0029,
      "step": 63800
    },
    {
      "epoch": 18.36353178027035,
      "grad_norm": 0.007578670047223568,
      "learning_rate": 0.0006327351164797239,
      "loss": 0.0024,
      "step": 63850
    },
    {
      "epoch": 18.3779119930975,
      "grad_norm": 7.036868191789836e-05,
      "learning_rate": 0.0006324475122231808,
      "loss": 0.0035,
      "step": 63900
    },
    {
      "epoch": 18.392292205924647,
      "grad_norm": 0.0011838153004646301,
      "learning_rate": 0.0006321599079666379,
      "loss": 0.0024,
      "step": 63950
    },
    {
      "epoch": 18.406672418751796,
      "grad_norm": 6.771380867576227e-05,
      "learning_rate": 0.000631872303710095,
      "loss": 0.0006,
      "step": 64000
    },
    {
      "epoch": 18.42105263157895,
      "grad_norm": 5.593337118625641e-05,
      "learning_rate": 0.0006315846994535519,
      "loss": 0.0024,
      "step": 64050
    },
    {
      "epoch": 18.435432844406098,
      "grad_norm": 0.030018573626875877,
      "learning_rate": 0.000631297095197009,
      "loss": 0.0046,
      "step": 64100
    },
    {
      "epoch": 18.449813057233246,
      "grad_norm": 0.005925017409026623,
      "learning_rate": 0.0006310094909404659,
      "loss": 0.0036,
      "step": 64150
    },
    {
      "epoch": 18.464193270060395,
      "grad_norm": 0.0007193175260908902,
      "learning_rate": 0.0006307218866839229,
      "loss": 0.0027,
      "step": 64200
    },
    {
      "epoch": 18.478573482887548,
      "grad_norm": 0.0002465038269292563,
      "learning_rate": 0.00063043428242738,
      "loss": 0.0013,
      "step": 64250
    },
    {
      "epoch": 18.492953695714696,
      "grad_norm": 0.0003812669310718775,
      "learning_rate": 0.0006301466781708369,
      "loss": 0.0025,
      "step": 64300
    },
    {
      "epoch": 18.507333908541845,
      "grad_norm": 0.00021863252914045006,
      "learning_rate": 0.0006298590739142939,
      "loss": 0.0025,
      "step": 64350
    },
    {
      "epoch": 18.521714121368998,
      "grad_norm": 0.024850867688655853,
      "learning_rate": 0.0006295714696577509,
      "loss": 0.0022,
      "step": 64400
    },
    {
      "epoch": 18.536094334196147,
      "grad_norm": 0.0023516465444117785,
      "learning_rate": 0.000629283865401208,
      "loss": 0.0042,
      "step": 64450
    },
    {
      "epoch": 18.550474547023295,
      "grad_norm": 0.008817275054752827,
      "learning_rate": 0.0006289962611446649,
      "loss": 0.0019,
      "step": 64500
    },
    {
      "epoch": 18.564854759850444,
      "grad_norm": 0.001032656291499734,
      "learning_rate": 0.000628708656888122,
      "loss": 0.003,
      "step": 64550
    },
    {
      "epoch": 18.579234972677597,
      "grad_norm": 0.00018864589219447225,
      "learning_rate": 0.000628421052631579,
      "loss": 0.0003,
      "step": 64600
    },
    {
      "epoch": 18.593615185504746,
      "grad_norm": 0.04035460948944092,
      "learning_rate": 0.0006281334483750359,
      "loss": 0.0038,
      "step": 64650
    },
    {
      "epoch": 18.607995398331894,
      "grad_norm": 0.0002241528854938224,
      "learning_rate": 0.000627845844118493,
      "loss": 0.0054,
      "step": 64700
    },
    {
      "epoch": 18.622375611159047,
      "grad_norm": 0.0001383051712764427,
      "learning_rate": 0.0006275582398619499,
      "loss": 0.0017,
      "step": 64750
    },
    {
      "epoch": 18.636755823986196,
      "grad_norm": 0.0010434145806357265,
      "learning_rate": 0.0006272706356054069,
      "loss": 0.0028,
      "step": 64800
    },
    {
      "epoch": 18.651136036813345,
      "grad_norm": 0.00017266589566133916,
      "learning_rate": 0.000626983031348864,
      "loss": 0.0005,
      "step": 64850
    },
    {
      "epoch": 18.665516249640493,
      "grad_norm": 0.00011400059156585485,
      "learning_rate": 0.000626695427092321,
      "loss": 0.0025,
      "step": 64900
    },
    {
      "epoch": 18.679896462467646,
      "grad_norm": 0.0004522314702626318,
      "learning_rate": 0.000626407822835778,
      "loss": 0.0008,
      "step": 64950
    },
    {
      "epoch": 18.694276675294795,
      "grad_norm": 0.0001394177379552275,
      "learning_rate": 0.000626120218579235,
      "loss": 0.002,
      "step": 65000
    },
    {
      "epoch": 18.708656888121943,
      "grad_norm": 0.0005571864312514663,
      "learning_rate": 0.000625832614322692,
      "loss": 0.0007,
      "step": 65050
    },
    {
      "epoch": 18.723037100949092,
      "grad_norm": 0.006601890083402395,
      "learning_rate": 0.0006255450100661489,
      "loss": 0.0008,
      "step": 65100
    },
    {
      "epoch": 18.737417313776245,
      "grad_norm": 0.00021976963034830987,
      "learning_rate": 0.000625257405809606,
      "loss": 0.005,
      "step": 65150
    },
    {
      "epoch": 18.751797526603394,
      "grad_norm": 5.1308008551131934e-05,
      "learning_rate": 0.000624969801553063,
      "loss": 0.0038,
      "step": 65200
    },
    {
      "epoch": 18.766177739430542,
      "grad_norm": 5.689696263289079e-05,
      "learning_rate": 0.0006246821972965199,
      "loss": 0.0043,
      "step": 65250
    },
    {
      "epoch": 18.780557952257695,
      "grad_norm": 0.0018313556211069226,
      "learning_rate": 0.0006243945930399771,
      "loss": 0.0011,
      "step": 65300
    },
    {
      "epoch": 18.794938165084844,
      "grad_norm": 4.274633465684019e-05,
      "learning_rate": 0.000624106988783434,
      "loss": 0.0022,
      "step": 65350
    },
    {
      "epoch": 18.809318377911993,
      "grad_norm": 0.005594042129814625,
      "learning_rate": 0.000623819384526891,
      "loss": 0.0025,
      "step": 65400
    },
    {
      "epoch": 18.82369859073914,
      "grad_norm": 0.01986619643867016,
      "learning_rate": 0.000623531780270348,
      "loss": 0.004,
      "step": 65450
    },
    {
      "epoch": 18.838078803566294,
      "grad_norm": 0.00012569050886668265,
      "learning_rate": 0.000623244176013805,
      "loss": 0.0042,
      "step": 65500
    },
    {
      "epoch": 18.852459016393443,
      "grad_norm": 0.004321957938373089,
      "learning_rate": 0.000622956571757262,
      "loss": 0.0029,
      "step": 65550
    },
    {
      "epoch": 18.86683922922059,
      "grad_norm": 0.002576664322987199,
      "learning_rate": 0.000622668967500719,
      "loss": 0.0019,
      "step": 65600
    },
    {
      "epoch": 18.881219442047744,
      "grad_norm": 0.0014042326947674155,
      "learning_rate": 0.000622381363244176,
      "loss": 0.0009,
      "step": 65650
    },
    {
      "epoch": 18.895599654874893,
      "grad_norm": 0.00047605036525055766,
      "learning_rate": 0.0006220937589876329,
      "loss": 0.0016,
      "step": 65700
    },
    {
      "epoch": 18.90997986770204,
      "grad_norm": 0.04670507460832596,
      "learning_rate": 0.0006218061547310901,
      "loss": 0.0031,
      "step": 65750
    },
    {
      "epoch": 18.92436008052919,
      "grad_norm": 0.007868258282542229,
      "learning_rate": 0.0006215185504745471,
      "loss": 0.0023,
      "step": 65800
    },
    {
      "epoch": 18.938740293356343,
      "grad_norm": 0.003519738093018532,
      "learning_rate": 0.000621230946218004,
      "loss": 0.0043,
      "step": 65850
    },
    {
      "epoch": 18.95312050618349,
      "grad_norm": 1.464994147681864e-05,
      "learning_rate": 0.0006209433419614611,
      "loss": 0.0006,
      "step": 65900
    },
    {
      "epoch": 18.96750071901064,
      "grad_norm": 8.755461749387905e-05,
      "learning_rate": 0.000620655737704918,
      "loss": 0.0035,
      "step": 65950
    },
    {
      "epoch": 18.981880931837793,
      "grad_norm": 0.00011710707622114569,
      "learning_rate": 0.000620368133448375,
      "loss": 0.0022,
      "step": 66000
    },
    {
      "epoch": 18.996261144664942,
      "grad_norm": 0.00021360824757721275,
      "learning_rate": 0.000620080529191832,
      "loss": 0.0024,
      "step": 66050
    },
    {
      "epoch": 19.0,
      "eval_loss": 0.010397407226264477,
      "eval_runtime": 19.322,
      "eval_samples_per_second": 2469.669,
      "eval_steps_per_second": 38.609,
      "step": 66063
    },
    {
      "epoch": 19.01064135749209,
      "grad_norm": 0.0002752476721070707,
      "learning_rate": 0.000619792924935289,
      "loss": 0.0015,
      "step": 66100
    },
    {
      "epoch": 19.02502157031924,
      "grad_norm": 5.563418744713999e-05,
      "learning_rate": 0.000619505320678746,
      "loss": 0.0019,
      "step": 66150
    },
    {
      "epoch": 19.039401783146392,
      "grad_norm": 0.012175850570201874,
      "learning_rate": 0.0006192177164222031,
      "loss": 0.0015,
      "step": 66200
    },
    {
      "epoch": 19.05378199597354,
      "grad_norm": 0.0003009655047208071,
      "learning_rate": 0.0006189301121656601,
      "loss": 0.0011,
      "step": 66250
    },
    {
      "epoch": 19.06816220880069,
      "grad_norm": 0.00841076672077179,
      "learning_rate": 0.000618642507909117,
      "loss": 0.0029,
      "step": 66300
    },
    {
      "epoch": 19.08254242162784,
      "grad_norm": 0.008708245120942593,
      "learning_rate": 0.0006183549036525741,
      "loss": 0.003,
      "step": 66350
    },
    {
      "epoch": 19.09692263445499,
      "grad_norm": 9.660013893153518e-05,
      "learning_rate": 0.0006180672993960311,
      "loss": 0.0021,
      "step": 66400
    },
    {
      "epoch": 19.11130284728214,
      "grad_norm": 2.5521661882521585e-05,
      "learning_rate": 0.000617779695139488,
      "loss": 0.0032,
      "step": 66450
    },
    {
      "epoch": 19.12568306010929,
      "grad_norm": 0.00033945523318834603,
      "learning_rate": 0.0006174920908829451,
      "loss": 0.0022,
      "step": 66500
    },
    {
      "epoch": 19.14006327293644,
      "grad_norm": 8.21966168587096e-05,
      "learning_rate": 0.000617204486626402,
      "loss": 0.0066,
      "step": 66550
    },
    {
      "epoch": 19.15444348576359,
      "grad_norm": 0.001405504415743053,
      "learning_rate": 0.000616916882369859,
      "loss": 0.0007,
      "step": 66600
    },
    {
      "epoch": 19.16882369859074,
      "grad_norm": 0.00021147563529666513,
      "learning_rate": 0.0006166292781133161,
      "loss": 0.0024,
      "step": 66650
    },
    {
      "epoch": 19.183203911417888,
      "grad_norm": 7.854424620745704e-05,
      "learning_rate": 0.0006163416738567731,
      "loss": 0.001,
      "step": 66700
    },
    {
      "epoch": 19.19758412424504,
      "grad_norm": 4.210197948850691e-05,
      "learning_rate": 0.0006160540696002301,
      "loss": 0.0024,
      "step": 66750
    },
    {
      "epoch": 19.21196433707219,
      "grad_norm": 0.0008405806729570031,
      "learning_rate": 0.0006157664653436871,
      "loss": 0.0042,
      "step": 66800
    },
    {
      "epoch": 19.226344549899338,
      "grad_norm": 0.0002342531952308491,
      "learning_rate": 0.0006154788610871441,
      "loss": 0.0012,
      "step": 66850
    },
    {
      "epoch": 19.240724762726487,
      "grad_norm": 0.0005258575547486544,
      "learning_rate": 0.000615191256830601,
      "loss": 0.0037,
      "step": 66900
    },
    {
      "epoch": 19.25510497555364,
      "grad_norm": 0.0001433442230336368,
      "learning_rate": 0.0006149036525740581,
      "loss": 0.0035,
      "step": 66950
    },
    {
      "epoch": 19.269485188380788,
      "grad_norm": 5.700422843801789e-05,
      "learning_rate": 0.0006146160483175151,
      "loss": 0.0021,
      "step": 67000
    },
    {
      "epoch": 19.283865401207937,
      "grad_norm": 8.049734606174752e-05,
      "learning_rate": 0.000614328444060972,
      "loss": 0.0012,
      "step": 67050
    },
    {
      "epoch": 19.29824561403509,
      "grad_norm": 0.007109643891453743,
      "learning_rate": 0.0006140408398044292,
      "loss": 0.0026,
      "step": 67100
    },
    {
      "epoch": 19.312625826862238,
      "grad_norm": 4.34439534728881e-05,
      "learning_rate": 0.0006137532355478861,
      "loss": 0.0026,
      "step": 67150
    },
    {
      "epoch": 19.327006039689387,
      "grad_norm": 0.002414095215499401,
      "learning_rate": 0.0006134656312913431,
      "loss": 0.0037,
      "step": 67200
    },
    {
      "epoch": 19.341386252516536,
      "grad_norm": 0.0002844473929144442,
      "learning_rate": 0.0006131780270348001,
      "loss": 0.003,
      "step": 67250
    },
    {
      "epoch": 19.355766465343688,
      "grad_norm": 5.422771937446669e-05,
      "learning_rate": 0.0006128904227782571,
      "loss": 0.001,
      "step": 67300
    },
    {
      "epoch": 19.370146678170837,
      "grad_norm": 7.118689973140135e-05,
      "learning_rate": 0.0006126028185217141,
      "loss": 0.0003,
      "step": 67350
    },
    {
      "epoch": 19.384526890997986,
      "grad_norm": 0.004052785225212574,
      "learning_rate": 0.0006123152142651711,
      "loss": 0.0031,
      "step": 67400
    },
    {
      "epoch": 19.398907103825138,
      "grad_norm": 0.0005007053259760141,
      "learning_rate": 0.0006120276100086282,
      "loss": 0.0048,
      "step": 67450
    },
    {
      "epoch": 19.413287316652287,
      "grad_norm": 0.020983252674341202,
      "learning_rate": 0.0006117400057520851,
      "loss": 0.003,
      "step": 67500
    },
    {
      "epoch": 19.427667529479436,
      "grad_norm": 0.00040697664371691644,
      "learning_rate": 0.0006114524014955422,
      "loss": 0.0021,
      "step": 67550
    },
    {
      "epoch": 19.442047742306585,
      "grad_norm": 0.0014057100052013993,
      "learning_rate": 0.0006111647972389992,
      "loss": 0.0029,
      "step": 67600
    },
    {
      "epoch": 19.456427955133737,
      "grad_norm": 0.0007343242177739739,
      "learning_rate": 0.0006108771929824561,
      "loss": 0.0015,
      "step": 67650
    },
    {
      "epoch": 19.470808167960886,
      "grad_norm": 4.54415385320317e-05,
      "learning_rate": 0.0006105895887259132,
      "loss": 0.0015,
      "step": 67700
    },
    {
      "epoch": 19.485188380788035,
      "grad_norm": 0.0003755208745133132,
      "learning_rate": 0.0006103019844693701,
      "loss": 0.0014,
      "step": 67750
    },
    {
      "epoch": 19.499568593615187,
      "grad_norm": 2.9931345125078224e-05,
      "learning_rate": 0.0006100143802128271,
      "loss": 0.0054,
      "step": 67800
    },
    {
      "epoch": 19.513948806442336,
      "grad_norm": 0.0005658609443344176,
      "learning_rate": 0.0006097267759562841,
      "loss": 0.0037,
      "step": 67850
    },
    {
      "epoch": 19.528329019269485,
      "grad_norm": 0.0001736224803607911,
      "learning_rate": 0.0006094391716997412,
      "loss": 0.0021,
      "step": 67900
    },
    {
      "epoch": 19.542709232096634,
      "grad_norm": 0.0003715905186254531,
      "learning_rate": 0.0006091515674431982,
      "loss": 0.0011,
      "step": 67950
    },
    {
      "epoch": 19.557089444923786,
      "grad_norm": 5.216389035922475e-05,
      "learning_rate": 0.0006088639631866552,
      "loss": 0.0009,
      "step": 68000
    },
    {
      "epoch": 19.571469657750935,
      "grad_norm": 0.0005695061408914626,
      "learning_rate": 0.0006085763589301122,
      "loss": 0.0028,
      "step": 68050
    },
    {
      "epoch": 19.585849870578084,
      "grad_norm": 0.003060638438910246,
      "learning_rate": 0.0006082887546735691,
      "loss": 0.0028,
      "step": 68100
    },
    {
      "epoch": 19.600230083405233,
      "grad_norm": 0.001559975789859891,
      "learning_rate": 0.0006080011504170262,
      "loss": 0.0033,
      "step": 68150
    },
    {
      "epoch": 19.614610296232385,
      "grad_norm": 0.0009287187131121755,
      "learning_rate": 0.0006077135461604831,
      "loss": 0.003,
      "step": 68200
    },
    {
      "epoch": 19.628990509059534,
      "grad_norm": 0.0104253850877285,
      "learning_rate": 0.0006074259419039401,
      "loss": 0.0011,
      "step": 68250
    },
    {
      "epoch": 19.643370721886683,
      "grad_norm": 0.00020123209105804563,
      "learning_rate": 0.0006071383376473973,
      "loss": 0.0055,
      "step": 68300
    },
    {
      "epoch": 19.657750934713835,
      "grad_norm": 0.005249418783932924,
      "learning_rate": 0.0006068507333908542,
      "loss": 0.001,
      "step": 68350
    },
    {
      "epoch": 19.672131147540984,
      "grad_norm": 0.0002354858152102679,
      "learning_rate": 0.0006065631291343112,
      "loss": 0.0057,
      "step": 68400
    },
    {
      "epoch": 19.686511360368133,
      "grad_norm": 0.029397480189800262,
      "learning_rate": 0.0006062755248777682,
      "loss": 0.0015,
      "step": 68450
    },
    {
      "epoch": 19.700891573195282,
      "grad_norm": 0.00042989032226614654,
      "learning_rate": 0.0006059879206212252,
      "loss": 0.0011,
      "step": 68500
    },
    {
      "epoch": 19.715271786022434,
      "grad_norm": 2.9252894819364883e-05,
      "learning_rate": 0.0006057003163646822,
      "loss": 0.0026,
      "step": 68550
    },
    {
      "epoch": 19.729651998849583,
      "grad_norm": 0.021890727803111076,
      "learning_rate": 0.0006054127121081392,
      "loss": 0.0012,
      "step": 68600
    },
    {
      "epoch": 19.744032211676732,
      "grad_norm": 0.0004893044824711978,
      "learning_rate": 0.0006051251078515962,
      "loss": 0.0017,
      "step": 68650
    },
    {
      "epoch": 19.758412424503884,
      "grad_norm": 0.0002124074089806527,
      "learning_rate": 0.0006048375035950531,
      "loss": 0.0014,
      "step": 68700
    },
    {
      "epoch": 19.772792637331033,
      "grad_norm": 0.010958516970276833,
      "learning_rate": 0.0006045498993385103,
      "loss": 0.0028,
      "step": 68750
    },
    {
      "epoch": 19.787172850158182,
      "grad_norm": 0.013170517981052399,
      "learning_rate": 0.0006042622950819672,
      "loss": 0.0006,
      "step": 68800
    },
    {
      "epoch": 19.80155306298533,
      "grad_norm": 4.100200385437347e-05,
      "learning_rate": 0.0006039746908254242,
      "loss": 0.0027,
      "step": 68850
    },
    {
      "epoch": 19.815933275812483,
      "grad_norm": 5.633684850181453e-05,
      "learning_rate": 0.0006036870865688813,
      "loss": 0.0033,
      "step": 68900
    },
    {
      "epoch": 19.830313488639632,
      "grad_norm": 0.00132068304810673,
      "learning_rate": 0.0006033994823123382,
      "loss": 0.0012,
      "step": 68950
    },
    {
      "epoch": 19.84469370146678,
      "grad_norm": 2.4539762307540514e-05,
      "learning_rate": 0.0006031118780557952,
      "loss": 0.0021,
      "step": 69000
    },
    {
      "epoch": 19.85907391429393,
      "grad_norm": 3.4534663427621126e-05,
      "learning_rate": 0.0006028242737992522,
      "loss": 0.0057,
      "step": 69050
    },
    {
      "epoch": 19.873454127121082,
      "grad_norm": 8.6774438386783e-05,
      "learning_rate": 0.0006025366695427092,
      "loss": 0.001,
      "step": 69100
    },
    {
      "epoch": 19.88783433994823,
      "grad_norm": 0.036505963653326035,
      "learning_rate": 0.0006022490652861662,
      "loss": 0.005,
      "step": 69150
    },
    {
      "epoch": 19.90221455277538,
      "grad_norm": 0.0010220527183264494,
      "learning_rate": 0.0006019614610296233,
      "loss": 0.0016,
      "step": 69200
    },
    {
      "epoch": 19.916594765602532,
      "grad_norm": 0.0003423856105655432,
      "learning_rate": 0.0006016738567730803,
      "loss": 0.004,
      "step": 69250
    },
    {
      "epoch": 19.93097497842968,
      "grad_norm": 0.00021507227211259305,
      "learning_rate": 0.0006013862525165372,
      "loss": 0.002,
      "step": 69300
    },
    {
      "epoch": 19.94535519125683,
      "grad_norm": 0.0003739894018508494,
      "learning_rate": 0.0006010986482599943,
      "loss": 0.0035,
      "step": 69350
    },
    {
      "epoch": 19.95973540408398,
      "grad_norm": 0.02439461275935173,
      "learning_rate": 0.0006008110440034512,
      "loss": 0.0034,
      "step": 69400
    },
    {
      "epoch": 19.97411561691113,
      "grad_norm": 3.568709871615283e-05,
      "learning_rate": 0.0006005234397469082,
      "loss": 0.0031,
      "step": 69450
    },
    {
      "epoch": 19.98849582973828,
      "grad_norm": 0.008483580313622952,
      "learning_rate": 0.0006002358354903653,
      "loss": 0.0021,
      "step": 69500
    },
    {
      "epoch": 20.0,
      "eval_loss": 0.010281960479915142,
      "eval_runtime": 17.3715,
      "eval_samples_per_second": 2746.973,
      "eval_steps_per_second": 42.944,
      "step": 69540
    },
    {
      "epoch": 20.00287604256543,
      "grad_norm": 0.04455876350402832,
      "learning_rate": 0.0005999482312338222,
      "loss": 0.0036,
      "step": 69550
    },
    {
      "epoch": 20.01725625539258,
      "grad_norm": 0.03581111878156662,
      "learning_rate": 0.0005996606269772792,
      "loss": 0.0013,
      "step": 69600
    },
    {
      "epoch": 20.03163646821973,
      "grad_norm": 0.009365183301270008,
      "learning_rate": 0.0005993730227207363,
      "loss": 0.0028,
      "step": 69650
    },
    {
      "epoch": 20.04601668104688,
      "grad_norm": 0.004262000322341919,
      "learning_rate": 0.0005990854184641933,
      "loss": 0.0018,
      "step": 69700
    },
    {
      "epoch": 20.060396893874028,
      "grad_norm": 0.0002206095086876303,
      "learning_rate": 0.0005987978142076504,
      "loss": 0.0014,
      "step": 69750
    },
    {
      "epoch": 20.07477710670118,
      "grad_norm": 4.787922807736322e-05,
      "learning_rate": 0.0005985102099511073,
      "loss": 0.0026,
      "step": 69800
    },
    {
      "epoch": 20.08915731952833,
      "grad_norm": 0.00017157132970169187,
      "learning_rate": 0.0005982226056945643,
      "loss": 0.0042,
      "step": 69850
    },
    {
      "epoch": 20.103537532355478,
      "grad_norm": 0.000846623326651752,
      "learning_rate": 0.0005979350014380212,
      "loss": 0.0063,
      "step": 69900
    },
    {
      "epoch": 20.117917745182627,
      "grad_norm": 9.804605360841379e-05,
      "learning_rate": 0.0005976473971814783,
      "loss": 0.0029,
      "step": 69950
    },
    {
      "epoch": 20.13229795800978,
      "grad_norm": 0.021778345108032227,
      "learning_rate": 0.0005973597929249352,
      "loss": 0.0022,
      "step": 70000
    },
    {
      "epoch": 20.14667817083693,
      "grad_norm": 0.0025040041655302048,
      "learning_rate": 0.0005970721886683922,
      "loss": 0.0017,
      "step": 70050
    },
    {
      "epoch": 20.161058383664077,
      "grad_norm": 3.302331606391817e-05,
      "learning_rate": 0.0005967845844118494,
      "loss": 0.0014,
      "step": 70100
    },
    {
      "epoch": 20.17543859649123,
      "grad_norm": 0.017506491392850876,
      "learning_rate": 0.0005964969801553063,
      "loss": 0.0026,
      "step": 70150
    },
    {
      "epoch": 20.18981880931838,
      "grad_norm": 0.00015085068298503757,
      "learning_rate": 0.0005962093758987634,
      "loss": 0.0012,
      "step": 70200
    },
    {
      "epoch": 20.204199022145527,
      "grad_norm": 0.00020866296836175025,
      "learning_rate": 0.0005959217716422203,
      "loss": 0.0015,
      "step": 70250
    },
    {
      "epoch": 20.218579234972676,
      "grad_norm": 0.0001193233547382988,
      "learning_rate": 0.0005956341673856773,
      "loss": 0.0021,
      "step": 70300
    },
    {
      "epoch": 20.23295944779983,
      "grad_norm": 8.692558913026005e-05,
      "learning_rate": 0.0005953465631291344,
      "loss": 0.0038,
      "step": 70350
    },
    {
      "epoch": 20.247339660626977,
      "grad_norm": 0.0002291322307428345,
      "learning_rate": 0.0005950589588725913,
      "loss": 0.0017,
      "step": 70400
    },
    {
      "epoch": 20.261719873454126,
      "grad_norm": 0.00581369549036026,
      "learning_rate": 0.0005947713546160483,
      "loss": 0.0008,
      "step": 70450
    },
    {
      "epoch": 20.27610008628128,
      "grad_norm": 0.00281863403506577,
      "learning_rate": 0.0005944837503595053,
      "loss": 0.0038,
      "step": 70500
    },
    {
      "epoch": 20.290480299108427,
      "grad_norm": 0.0011343971127644181,
      "learning_rate": 0.0005941961461029624,
      "loss": 0.0026,
      "step": 70550
    },
    {
      "epoch": 20.304860511935576,
      "grad_norm": 7.655585068278015e-05,
      "learning_rate": 0.0005939085418464193,
      "loss": 0.0016,
      "step": 70600
    },
    {
      "epoch": 20.319240724762725,
      "grad_norm": 0.01422237791121006,
      "learning_rate": 0.0005936209375898764,
      "loss": 0.0033,
      "step": 70650
    },
    {
      "epoch": 20.333620937589878,
      "grad_norm": 0.0002981717116199434,
      "learning_rate": 0.0005933333333333334,
      "loss": 0.0055,
      "step": 70700
    },
    {
      "epoch": 20.348001150417026,
      "grad_norm": 6.542354094563052e-05,
      "learning_rate": 0.0005930457290767903,
      "loss": 0.0047,
      "step": 70750
    },
    {
      "epoch": 20.362381363244175,
      "grad_norm": 0.009175736457109451,
      "learning_rate": 0.0005927581248202474,
      "loss": 0.0024,
      "step": 70800
    },
    {
      "epoch": 20.376761576071324,
      "grad_norm": 0.016142934560775757,
      "learning_rate": 0.0005924705205637043,
      "loss": 0.0045,
      "step": 70850
    },
    {
      "epoch": 20.391141788898477,
      "grad_norm": 0.00016986156697385013,
      "learning_rate": 0.0005921829163071613,
      "loss": 0.0019,
      "step": 70900
    },
    {
      "epoch": 20.405522001725625,
      "grad_norm": 0.0011387456906959414,
      "learning_rate": 0.0005918953120506183,
      "loss": 0.0027,
      "step": 70950
    },
    {
      "epoch": 20.419902214552774,
      "grad_norm": 0.006369154434651136,
      "learning_rate": 0.0005916077077940754,
      "loss": 0.0017,
      "step": 71000
    },
    {
      "epoch": 20.434282427379927,
      "grad_norm": 0.00021833386563230306,
      "learning_rate": 0.0005913201035375324,
      "loss": 0.0031,
      "step": 71050
    },
    {
      "epoch": 20.448662640207075,
      "grad_norm": 2.8834716431447305e-05,
      "learning_rate": 0.0005910324992809894,
      "loss": 0.0006,
      "step": 71100
    },
    {
      "epoch": 20.463042853034224,
      "grad_norm": 0.008072283118963242,
      "learning_rate": 0.0005907448950244464,
      "loss": 0.0008,
      "step": 71150
    },
    {
      "epoch": 20.477423065861373,
      "grad_norm": 0.00047761210589669645,
      "learning_rate": 0.0005904572907679033,
      "loss": 0.0036,
      "step": 71200
    },
    {
      "epoch": 20.491803278688526,
      "grad_norm": 0.001139672240242362,
      "learning_rate": 0.0005901696865113604,
      "loss": 0.0029,
      "step": 71250
    },
    {
      "epoch": 20.506183491515674,
      "grad_norm": 6.760900578228757e-05,
      "learning_rate": 0.0005898820822548174,
      "loss": 0.0008,
      "step": 71300
    },
    {
      "epoch": 20.520563704342823,
      "grad_norm": 0.002015607664361596,
      "learning_rate": 0.0005895944779982744,
      "loss": 0.0018,
      "step": 71350
    },
    {
      "epoch": 20.534943917169976,
      "grad_norm": 0.02045493759214878,
      "learning_rate": 0.0005893068737417315,
      "loss": 0.0018,
      "step": 71400
    },
    {
      "epoch": 20.549324129997125,
      "grad_norm": 0.0006889700307510793,
      "learning_rate": 0.0005890192694851884,
      "loss": 0.0022,
      "step": 71450
    },
    {
      "epoch": 20.563704342824273,
      "grad_norm": 0.00011074940994149074,
      "learning_rate": 0.0005887316652286454,
      "loss": 0.0021,
      "step": 71500
    },
    {
      "epoch": 20.578084555651422,
      "grad_norm": 9.050768858287483e-05,
      "learning_rate": 0.0005884440609721024,
      "loss": 0.0017,
      "step": 71550
    },
    {
      "epoch": 20.592464768478575,
      "grad_norm": 0.0009899594588205218,
      "learning_rate": 0.0005881564567155594,
      "loss": 0.0017,
      "step": 71600
    },
    {
      "epoch": 20.606844981305724,
      "grad_norm": 3.42924686265178e-05,
      "learning_rate": 0.0005878688524590164,
      "loss": 0.0038,
      "step": 71650
    },
    {
      "epoch": 20.621225194132872,
      "grad_norm": 0.02778291516005993,
      "learning_rate": 0.0005875812482024734,
      "loss": 0.0041,
      "step": 71700
    },
    {
      "epoch": 20.635605406960025,
      "grad_norm": 0.00011477286898298189,
      "learning_rate": 0.0005872936439459304,
      "loss": 0.0015,
      "step": 71750
    },
    {
      "epoch": 20.649985619787174,
      "grad_norm": 0.00025313347578048706,
      "learning_rate": 0.0005870060396893874,
      "loss": 0.0043,
      "step": 71800
    },
    {
      "epoch": 20.664365832614322,
      "grad_norm": 7.638611714355648e-05,
      "learning_rate": 0.0005867184354328445,
      "loss": 0.0011,
      "step": 71850
    },
    {
      "epoch": 20.67874604544147,
      "grad_norm": 0.0023881259839981794,
      "learning_rate": 0.0005864308311763015,
      "loss": 0.0035,
      "step": 71900
    },
    {
      "epoch": 20.693126258268624,
      "grad_norm": 0.007246227934956551,
      "learning_rate": 0.0005861432269197584,
      "loss": 0.0008,
      "step": 71950
    },
    {
      "epoch": 20.707506471095773,
      "grad_norm": 7.3708375566639e-05,
      "learning_rate": 0.0005858556226632155,
      "loss": 0.0012,
      "step": 72000
    },
    {
      "epoch": 20.72188668392292,
      "grad_norm": 4.4727166823577136e-05,
      "learning_rate": 0.0005855680184066724,
      "loss": 0.0009,
      "step": 72050
    },
    {
      "epoch": 20.73626689675007,
      "grad_norm": 6.401044083759189e-05,
      "learning_rate": 0.0005852804141501294,
      "loss": 0.0033,
      "step": 72100
    },
    {
      "epoch": 20.750647109577223,
      "grad_norm": 0.0009572263807058334,
      "learning_rate": 0.0005849928098935864,
      "loss": 0.003,
      "step": 72150
    },
    {
      "epoch": 20.76502732240437,
      "grad_norm": 0.0005258327582851052,
      "learning_rate": 0.0005847052056370435,
      "loss": 0.0033,
      "step": 72200
    },
    {
      "epoch": 20.77940753523152,
      "grad_norm": 0.01470005139708519,
      "learning_rate": 0.0005844176013805005,
      "loss": 0.0009,
      "step": 72250
    },
    {
      "epoch": 20.793787748058673,
      "grad_norm": 0.0012936454731971025,
      "learning_rate": 0.0005841299971239575,
      "loss": 0.0067,
      "step": 72300
    },
    {
      "epoch": 20.80816796088582,
      "grad_norm": 0.000156394686200656,
      "learning_rate": 0.0005838423928674145,
      "loss": 0.0016,
      "step": 72350
    },
    {
      "epoch": 20.82254817371297,
      "grad_norm": 3.687245407490991e-05,
      "learning_rate": 0.0005835547886108714,
      "loss": 0.0025,
      "step": 72400
    },
    {
      "epoch": 20.83692838654012,
      "grad_norm": 3.729752643266693e-05,
      "learning_rate": 0.0005832671843543285,
      "loss": 0.0036,
      "step": 72450
    },
    {
      "epoch": 20.85130859936727,
      "grad_norm": 0.00022581049415748566,
      "learning_rate": 0.0005829795800977855,
      "loss": 0.0035,
      "step": 72500
    },
    {
      "epoch": 20.86568881219442,
      "grad_norm": 0.0008252380648627877,
      "learning_rate": 0.0005826919758412424,
      "loss": 0.0027,
      "step": 72550
    },
    {
      "epoch": 20.88006902502157,
      "grad_norm": 3.7337871617637575e-05,
      "learning_rate": 0.0005824043715846995,
      "loss": 0.0003,
      "step": 72600
    },
    {
      "epoch": 20.894449237848722,
      "grad_norm": 0.020693428814411163,
      "learning_rate": 0.0005821167673281565,
      "loss": 0.0013,
      "step": 72650
    },
    {
      "epoch": 20.90882945067587,
      "grad_norm": 7.637526141479611e-05,
      "learning_rate": 0.0005818291630716135,
      "loss": 0.0008,
      "step": 72700
    },
    {
      "epoch": 20.92320966350302,
      "grad_norm": 0.007807204499840736,
      "learning_rate": 0.0005815415588150705,
      "loss": 0.0031,
      "step": 72750
    },
    {
      "epoch": 20.93758987633017,
      "grad_norm": 0.0017163208685815334,
      "learning_rate": 0.0005812539545585275,
      "loss": 0.0012,
      "step": 72800
    },
    {
      "epoch": 20.95197008915732,
      "grad_norm": 0.0035916552878916264,
      "learning_rate": 0.0005809663503019845,
      "loss": 0.0049,
      "step": 72850
    },
    {
      "epoch": 20.96635030198447,
      "grad_norm": 0.010953754186630249,
      "learning_rate": 0.0005806787460454415,
      "loss": 0.0016,
      "step": 72900
    },
    {
      "epoch": 20.98073051481162,
      "grad_norm": 0.026304716244339943,
      "learning_rate": 0.0005803911417888985,
      "loss": 0.0071,
      "step": 72950
    },
    {
      "epoch": 20.995110727638767,
      "grad_norm": 0.003842579899355769,
      "learning_rate": 0.0005801035375323554,
      "loss": 0.0014,
      "step": 73000
    },
    {
      "epoch": 21.0,
      "eval_loss": 0.010005435906350613,
      "eval_runtime": 17.9635,
      "eval_samples_per_second": 2656.449,
      "eval_steps_per_second": 41.529,
      "step": 73017
    },
    {
      "epoch": 21.00949094046592,
      "grad_norm": 0.018334152176976204,
      "learning_rate": 0.0005798159332758126,
      "loss": 0.002,
      "step": 73050
    },
    {
      "epoch": 21.02387115329307,
      "grad_norm": 0.00010729525820352137,
      "learning_rate": 0.0005795283290192696,
      "loss": 0.0009,
      "step": 73100
    },
    {
      "epoch": 21.038251366120218,
      "grad_norm": 0.0009174704900942743,
      "learning_rate": 0.0005792407247627265,
      "loss": 0.0017,
      "step": 73150
    },
    {
      "epoch": 21.05263157894737,
      "grad_norm": 3.669822763185948e-05,
      "learning_rate": 0.0005789531205061836,
      "loss": 0.0056,
      "step": 73200
    },
    {
      "epoch": 21.06701179177452,
      "grad_norm": 0.006402301602065563,
      "learning_rate": 0.0005786655162496405,
      "loss": 0.002,
      "step": 73250
    },
    {
      "epoch": 21.081392004601668,
      "grad_norm": 0.029684556648135185,
      "learning_rate": 0.0005783779119930975,
      "loss": 0.002,
      "step": 73300
    },
    {
      "epoch": 21.095772217428816,
      "grad_norm": 4.2400093661854044e-05,
      "learning_rate": 0.0005780903077365545,
      "loss": 0.0027,
      "step": 73350
    },
    {
      "epoch": 21.11015243025597,
      "grad_norm": 0.0011934804497286677,
      "learning_rate": 0.0005778027034800115,
      "loss": 0.0034,
      "step": 73400
    },
    {
      "epoch": 21.124532643083118,
      "grad_norm": 0.004195625893771648,
      "learning_rate": 0.0005775150992234685,
      "loss": 0.0027,
      "step": 73450
    },
    {
      "epoch": 21.138912855910267,
      "grad_norm": 7.993839244591072e-05,
      "learning_rate": 0.0005772274949669256,
      "loss": 0.0022,
      "step": 73500
    },
    {
      "epoch": 21.15329306873742,
      "grad_norm": 0.008613640442490578,
      "learning_rate": 0.0005769398907103826,
      "loss": 0.0047,
      "step": 73550
    },
    {
      "epoch": 21.167673281564568,
      "grad_norm": 0.00010579932131804526,
      "learning_rate": 0.0005766522864538395,
      "loss": 0.0013,
      "step": 73600
    },
    {
      "epoch": 21.182053494391717,
      "grad_norm": 0.021047087386250496,
      "learning_rate": 0.0005763646821972966,
      "loss": 0.0013,
      "step": 73650
    },
    {
      "epoch": 21.196433707218866,
      "grad_norm": 0.007240384351462126,
      "learning_rate": 0.0005760770779407535,
      "loss": 0.0032,
      "step": 73700
    },
    {
      "epoch": 21.210813920046018,
      "grad_norm": 0.0005691211554221809,
      "learning_rate": 0.0005757894736842105,
      "loss": 0.0023,
      "step": 73750
    },
    {
      "epoch": 21.225194132873167,
      "grad_norm": 9.839569975156337e-05,
      "learning_rate": 0.0005755018694276676,
      "loss": 0.0053,
      "step": 73800
    },
    {
      "epoch": 21.239574345700316,
      "grad_norm": 9.309971210313961e-05,
      "learning_rate": 0.0005752142651711245,
      "loss": 0.0011,
      "step": 73850
    },
    {
      "epoch": 21.253954558527465,
      "grad_norm": 0.00016863667406141758,
      "learning_rate": 0.0005749266609145815,
      "loss": 0.0034,
      "step": 73900
    },
    {
      "epoch": 21.268334771354617,
      "grad_norm": 0.004875272046774626,
      "learning_rate": 0.0005746390566580386,
      "loss": 0.0013,
      "step": 73950
    },
    {
      "epoch": 21.282714984181766,
      "grad_norm": 8.080397674348205e-05,
      "learning_rate": 0.0005743514524014956,
      "loss": 0.001,
      "step": 74000
    },
    {
      "epoch": 21.297095197008915,
      "grad_norm": 0.0003016426635440439,
      "learning_rate": 0.0005740638481449526,
      "loss": 0.001,
      "step": 74050
    },
    {
      "epoch": 21.311475409836067,
      "grad_norm": 0.027908852323889732,
      "learning_rate": 0.0005737762438884096,
      "loss": 0.0029,
      "step": 74100
    },
    {
      "epoch": 21.325855622663216,
      "grad_norm": 8.388530113734305e-05,
      "learning_rate": 0.0005734886396318666,
      "loss": 0.002,
      "step": 74150
    },
    {
      "epoch": 21.340235835490365,
      "grad_norm": 0.00033878342946991324,
      "learning_rate": 0.0005732010353753235,
      "loss": 0.0035,
      "step": 74200
    },
    {
      "epoch": 21.354616048317514,
      "grad_norm": 3.423308226047084e-05,
      "learning_rate": 0.0005729134311187806,
      "loss": 0.0012,
      "step": 74250
    },
    {
      "epoch": 21.368996261144666,
      "grad_norm": 6.612028664676473e-05,
      "learning_rate": 0.0005726258268622375,
      "loss": 0.0008,
      "step": 74300
    },
    {
      "epoch": 21.383376473971815,
      "grad_norm": 0.0025082568172365427,
      "learning_rate": 0.0005723382226056945,
      "loss": 0.0016,
      "step": 74350
    },
    {
      "epoch": 21.397756686798964,
      "grad_norm": 0.0008510273182764649,
      "learning_rate": 0.0005720506183491517,
      "loss": 0.0047,
      "step": 74400
    },
    {
      "epoch": 21.412136899626116,
      "grad_norm": 0.0003253256145399064,
      "learning_rate": 0.0005717630140926086,
      "loss": 0.0027,
      "step": 74450
    },
    {
      "epoch": 21.426517112453265,
      "grad_norm": 0.00013945528189651668,
      "learning_rate": 0.0005714754098360656,
      "loss": 0.0003,
      "step": 74500
    },
    {
      "epoch": 21.440897325280414,
      "grad_norm": 9.953336120815948e-05,
      "learning_rate": 0.0005711878055795226,
      "loss": 0.0031,
      "step": 74550
    },
    {
      "epoch": 21.455277538107563,
      "grad_norm": 0.0001148418232332915,
      "learning_rate": 0.0005709002013229796,
      "loss": 0.0016,
      "step": 74600
    },
    {
      "epoch": 21.469657750934715,
      "grad_norm": 0.001291167689487338,
      "learning_rate": 0.0005706125970664366,
      "loss": 0.0021,
      "step": 74650
    },
    {
      "epoch": 21.484037963761864,
      "grad_norm": 6.954680429771543e-05,
      "learning_rate": 0.0005703249928098936,
      "loss": 0.0022,
      "step": 74700
    },
    {
      "epoch": 21.498418176589013,
      "grad_norm": 0.057190462946891785,
      "learning_rate": 0.0005700373885533506,
      "loss": 0.0018,
      "step": 74750
    },
    {
      "epoch": 21.51279838941616,
      "grad_norm": 2.7833641070174053e-05,
      "learning_rate": 0.0005697497842968075,
      "loss": 0.0043,
      "step": 74800
    },
    {
      "epoch": 21.527178602243314,
      "grad_norm": 7.64605138101615e-05,
      "learning_rate": 0.0005694621800402647,
      "loss": 0.0015,
      "step": 74850
    },
    {
      "epoch": 21.541558815070463,
      "grad_norm": 0.00014264749188441783,
      "learning_rate": 0.0005691745757837216,
      "loss": 0.0003,
      "step": 74900
    },
    {
      "epoch": 21.55593902789761,
      "grad_norm": 0.0009864144958555698,
      "learning_rate": 0.0005688869715271786,
      "loss": 0.0039,
      "step": 74950
    },
    {
      "epoch": 21.570319240724764,
      "grad_norm": 0.00891716219484806,
      "learning_rate": 0.0005685993672706357,
      "loss": 0.002,
      "step": 75000
    },
    {
      "epoch": 21.584699453551913,
      "grad_norm": 0.0005214146221987903,
      "learning_rate": 0.0005683117630140926,
      "loss": 0.0039,
      "step": 75050
    },
    {
      "epoch": 21.599079666379062,
      "grad_norm": 0.0006774301873520017,
      "learning_rate": 0.0005680241587575496,
      "loss": 0.0025,
      "step": 75100
    },
    {
      "epoch": 21.61345987920621,
      "grad_norm": 0.00010654486686689779,
      "learning_rate": 0.0005677365545010066,
      "loss": 0.0026,
      "step": 75150
    },
    {
      "epoch": 21.627840092033363,
      "grad_norm": 0.027404572814702988,
      "learning_rate": 0.0005674489502444636,
      "loss": 0.0007,
      "step": 75200
    },
    {
      "epoch": 21.642220304860512,
      "grad_norm": 0.0009052716777659953,
      "learning_rate": 0.0005671613459879207,
      "loss": 0.003,
      "step": 75250
    },
    {
      "epoch": 21.65660051768766,
      "grad_norm": 0.004408794455230236,
      "learning_rate": 0.0005668737417313777,
      "loss": 0.0014,
      "step": 75300
    },
    {
      "epoch": 21.670980730514813,
      "grad_norm": 0.00010935783211607486,
      "learning_rate": 0.0005665861374748347,
      "loss": 0.0035,
      "step": 75350
    },
    {
      "epoch": 21.685360943341962,
      "grad_norm": 0.00034645546111278236,
      "learning_rate": 0.0005662985332182916,
      "loss": 0.0016,
      "step": 75400
    },
    {
      "epoch": 21.69974115616911,
      "grad_norm": 3.2262265449389815e-05,
      "learning_rate": 0.0005660109289617487,
      "loss": 0.0016,
      "step": 75450
    },
    {
      "epoch": 21.71412136899626,
      "grad_norm": 0.0024062874726951122,
      "learning_rate": 0.0005657233247052056,
      "loss": 0.0044,
      "step": 75500
    },
    {
      "epoch": 21.728501581823412,
      "grad_norm": 3.809696136158891e-05,
      "learning_rate": 0.0005654357204486626,
      "loss": 0.0038,
      "step": 75550
    },
    {
      "epoch": 21.74288179465056,
      "grad_norm": 0.00018471514340490103,
      "learning_rate": 0.0005651481161921197,
      "loss": 0.0024,
      "step": 75600
    },
    {
      "epoch": 21.75726200747771,
      "grad_norm": 0.023457083851099014,
      "learning_rate": 0.0005648605119355766,
      "loss": 0.0033,
      "step": 75650
    },
    {
      "epoch": 21.771642220304862,
      "grad_norm": 0.0015550462994724512,
      "learning_rate": 0.0005645729076790337,
      "loss": 0.0011,
      "step": 75700
    },
    {
      "epoch": 21.78602243313201,
      "grad_norm": 0.005418234970420599,
      "learning_rate": 0.0005642853034224907,
      "loss": 0.0033,
      "step": 75750
    },
    {
      "epoch": 21.80040264595916,
      "grad_norm": 3.865965845761821e-05,
      "learning_rate": 0.0005639976991659477,
      "loss": 0.0025,
      "step": 75800
    },
    {
      "epoch": 21.81478285878631,
      "grad_norm": 0.016037244349718094,
      "learning_rate": 0.0005637100949094047,
      "loss": 0.005,
      "step": 75850
    },
    {
      "epoch": 21.82916307161346,
      "grad_norm": 0.012033669278025627,
      "learning_rate": 0.0005634224906528617,
      "loss": 0.0013,
      "step": 75900
    },
    {
      "epoch": 21.84354328444061,
      "grad_norm": 0.0009124952484853566,
      "learning_rate": 0.0005631348863963187,
      "loss": 0.0013,
      "step": 75950
    },
    {
      "epoch": 21.85792349726776,
      "grad_norm": 0.00017700369062367827,
      "learning_rate": 0.0005628472821397756,
      "loss": 0.0052,
      "step": 76000
    },
    {
      "epoch": 21.872303710094908,
      "grad_norm": 0.04134685546159744,
      "learning_rate": 0.0005625596778832327,
      "loss": 0.0037,
      "step": 76050
    },
    {
      "epoch": 21.88668392292206,
      "grad_norm": 3.5985245631309226e-05,
      "learning_rate": 0.0005622720736266897,
      "loss": 0.0013,
      "step": 76100
    },
    {
      "epoch": 21.90106413574921,
      "grad_norm": 6.409572961274534e-05,
      "learning_rate": 0.0005619844693701467,
      "loss": 0.0042,
      "step": 76150
    },
    {
      "epoch": 21.915444348576358,
      "grad_norm": 0.03820681571960449,
      "learning_rate": 0.0005616968651136038,
      "loss": 0.001,
      "step": 76200
    },
    {
      "epoch": 21.92982456140351,
      "grad_norm": 0.00021736184135079384,
      "learning_rate": 0.0005614092608570607,
      "loss": 0.0023,
      "step": 76250
    },
    {
      "epoch": 21.94420477423066,
      "grad_norm": 0.0005896613001823425,
      "learning_rate": 0.0005611216566005177,
      "loss": 0.004,
      "step": 76300
    },
    {
      "epoch": 21.958584987057808,
      "grad_norm": 0.0013028780231252313,
      "learning_rate": 0.0005608340523439747,
      "loss": 0.0027,
      "step": 76350
    },
    {
      "epoch": 21.972965199884957,
      "grad_norm": 0.005656491499394178,
      "learning_rate": 0.0005605464480874317,
      "loss": 0.0019,
      "step": 76400
    },
    {
      "epoch": 21.98734541271211,
      "grad_norm": 6.0410799051169306e-05,
      "learning_rate": 0.0005602588438308886,
      "loss": 0.0028,
      "step": 76450
    },
    {
      "epoch": 22.0,
      "eval_loss": 0.010172332637012005,
      "eval_runtime": 17.2809,
      "eval_samples_per_second": 2761.367,
      "eval_steps_per_second": 43.169,
      "step": 76494
    },
    {
      "epoch": 22.001725625539258,
      "grad_norm": 0.01772455871105194,
      "learning_rate": 0.0005599712395743457,
      "loss": 0.0026,
      "step": 76500
    },
    {
      "epoch": 22.016105838366407,
      "grad_norm": 8.598618478572462e-06,
      "learning_rate": 0.0005596836353178028,
      "loss": 0.0039,
      "step": 76550
    },
    {
      "epoch": 22.03048605119356,
      "grad_norm": 0.0012165982043370605,
      "learning_rate": 0.0005593960310612597,
      "loss": 0.0018,
      "step": 76600
    },
    {
      "epoch": 22.04486626402071,
      "grad_norm": 0.005933623295277357,
      "learning_rate": 0.0005591084268047168,
      "loss": 0.0042,
      "step": 76650
    },
    {
      "epoch": 22.059246476847857,
      "grad_norm": 0.037600673735141754,
      "learning_rate": 0.0005588208225481737,
      "loss": 0.0025,
      "step": 76700
    },
    {
      "epoch": 22.073626689675006,
      "grad_norm": 0.00011983307194896042,
      "learning_rate": 0.0005585332182916307,
      "loss": 0.0034,
      "step": 76750
    },
    {
      "epoch": 22.08800690250216,
      "grad_norm": 0.00559578649699688,
      "learning_rate": 0.0005582456140350878,
      "loss": 0.0031,
      "step": 76800
    },
    {
      "epoch": 22.102387115329307,
      "grad_norm": 0.0005503608845174313,
      "learning_rate": 0.0005579580097785447,
      "loss": 0.0029,
      "step": 76850
    },
    {
      "epoch": 22.116767328156456,
      "grad_norm": 0.03467953950166702,
      "learning_rate": 0.0005576704055220017,
      "loss": 0.0011,
      "step": 76900
    },
    {
      "epoch": 22.131147540983605,
      "grad_norm": 0.0003167135873809457,
      "learning_rate": 0.0005573828012654588,
      "loss": 0.0039,
      "step": 76950
    },
    {
      "epoch": 22.145527753810757,
      "grad_norm": 0.03602336719632149,
      "learning_rate": 0.0005570951970089158,
      "loss": 0.0028,
      "step": 77000
    },
    {
      "epoch": 22.159907966637906,
      "grad_norm": 0.005562890786677599,
      "learning_rate": 0.0005568075927523727,
      "loss": 0.0041,
      "step": 77050
    },
    {
      "epoch": 22.174288179465055,
      "grad_norm": 0.007685347460210323,
      "learning_rate": 0.0005565199884958298,
      "loss": 0.0007,
      "step": 77100
    },
    {
      "epoch": 22.188668392292207,
      "grad_norm": 0.003073743311688304,
      "learning_rate": 0.0005562323842392868,
      "loss": 0.0013,
      "step": 77150
    },
    {
      "epoch": 22.203048605119356,
      "grad_norm": 0.006042038090527058,
      "learning_rate": 0.0005559447799827437,
      "loss": 0.0004,
      "step": 77200
    },
    {
      "epoch": 22.217428817946505,
      "grad_norm": 0.0008328248513862491,
      "learning_rate": 0.0005556571757262008,
      "loss": 0.0044,
      "step": 77250
    },
    {
      "epoch": 22.231809030773654,
      "grad_norm": 0.00010073327575810254,
      "learning_rate": 0.0005553695714696577,
      "loss": 0.0029,
      "step": 77300
    },
    {
      "epoch": 22.246189243600806,
      "grad_norm": 0.00036159553565084934,
      "learning_rate": 0.0005550819672131147,
      "loss": 0.0042,
      "step": 77350
    },
    {
      "epoch": 22.260569456427955,
      "grad_norm": 0.005036879330873489,
      "learning_rate": 0.0005547943629565719,
      "loss": 0.0031,
      "step": 77400
    },
    {
      "epoch": 22.274949669255104,
      "grad_norm": 4.444631122169085e-05,
      "learning_rate": 0.0005545067587000288,
      "loss": 0.0032,
      "step": 77450
    },
    {
      "epoch": 22.289329882082257,
      "grad_norm": 0.00034301308915019035,
      "learning_rate": 0.0005542191544434858,
      "loss": 0.005,
      "step": 77500
    },
    {
      "epoch": 22.303710094909405,
      "grad_norm": 0.00022014760179445148,
      "learning_rate": 0.0005539315501869428,
      "loss": 0.0054,
      "step": 77550
    },
    {
      "epoch": 22.318090307736554,
      "grad_norm": 0.0003399712441023439,
      "learning_rate": 0.0005536439459303998,
      "loss": 0.0019,
      "step": 77600
    },
    {
      "epoch": 22.332470520563703,
      "grad_norm": 0.00015203660586848855,
      "learning_rate": 0.0005533563416738567,
      "loss": 0.0024,
      "step": 77650
    },
    {
      "epoch": 22.346850733390855,
      "grad_norm": 6.76726340316236e-05,
      "learning_rate": 0.0005530687374173138,
      "loss": 0.0021,
      "step": 77700
    },
    {
      "epoch": 22.361230946218004,
      "grad_norm": 8.617954881628975e-05,
      "learning_rate": 0.0005527811331607708,
      "loss": 0.0016,
      "step": 77750
    },
    {
      "epoch": 22.375611159045153,
      "grad_norm": 0.000323208310874179,
      "learning_rate": 0.0005524935289042277,
      "loss": 0.0018,
      "step": 77800
    },
    {
      "epoch": 22.389991371872302,
      "grad_norm": 0.0003190534480381757,
      "learning_rate": 0.0005522059246476849,
      "loss": 0.0047,
      "step": 77850
    },
    {
      "epoch": 22.404371584699454,
      "grad_norm": 0.00036178980371914804,
      "learning_rate": 0.0005519183203911418,
      "loss": 0.0011,
      "step": 77900
    },
    {
      "epoch": 22.418751797526603,
      "grad_norm": 0.01105678454041481,
      "learning_rate": 0.0005516307161345988,
      "loss": 0.0019,
      "step": 77950
    },
    {
      "epoch": 22.433132010353752,
      "grad_norm": 0.00031615261104889214,
      "learning_rate": 0.0005513431118780559,
      "loss": 0.0011,
      "step": 78000
    },
    {
      "epoch": 22.447512223180905,
      "grad_norm": 0.0013946520630270243,
      "learning_rate": 0.0005510555076215128,
      "loss": 0.0015,
      "step": 78050
    },
    {
      "epoch": 22.461892436008053,
      "grad_norm": 4.569971497403458e-05,
      "learning_rate": 0.0005507679033649698,
      "loss": 0.0021,
      "step": 78100
    },
    {
      "epoch": 22.476272648835202,
      "grad_norm": 6.47205815766938e-05,
      "learning_rate": 0.0005504802991084268,
      "loss": 0.0019,
      "step": 78150
    },
    {
      "epoch": 22.49065286166235,
      "grad_norm": 0.007861564867198467,
      "learning_rate": 0.0005501926948518838,
      "loss": 0.0026,
      "step": 78200
    },
    {
      "epoch": 22.505033074489504,
      "grad_norm": 8.195919508580118e-05,
      "learning_rate": 0.0005499050905953407,
      "loss": 0.0019,
      "step": 78250
    },
    {
      "epoch": 22.519413287316652,
      "grad_norm": 0.010730224661529064,
      "learning_rate": 0.0005496174863387979,
      "loss": 0.0028,
      "step": 78300
    },
    {
      "epoch": 22.5337935001438,
      "grad_norm": 6.485659832833335e-05,
      "learning_rate": 0.0005493298820822549,
      "loss": 0.0023,
      "step": 78350
    },
    {
      "epoch": 22.548173712970954,
      "grad_norm": 0.00839136727154255,
      "learning_rate": 0.0005490422778257118,
      "loss": 0.0007,
      "step": 78400
    },
    {
      "epoch": 22.562553925798102,
      "grad_norm": 0.044491108506917953,
      "learning_rate": 0.0005487546735691689,
      "loss": 0.0013,
      "step": 78450
    },
    {
      "epoch": 22.57693413862525,
      "grad_norm": 0.009992602281272411,
      "learning_rate": 0.0005484670693126258,
      "loss": 0.0018,
      "step": 78500
    },
    {
      "epoch": 22.5913143514524,
      "grad_norm": 8.00175330368802e-05,
      "learning_rate": 0.0005481794650560828,
      "loss": 0.0032,
      "step": 78550
    },
    {
      "epoch": 22.605694564279553,
      "grad_norm": 0.00033070240169763565,
      "learning_rate": 0.0005478918607995399,
      "loss": 0.0009,
      "step": 78600
    },
    {
      "epoch": 22.6200747771067,
      "grad_norm": 4.868024552706629e-05,
      "learning_rate": 0.0005476042565429968,
      "loss": 0.0024,
      "step": 78650
    },
    {
      "epoch": 22.63445498993385,
      "grad_norm": 0.0015093067195266485,
      "learning_rate": 0.0005473166522864539,
      "loss": 0.0057,
      "step": 78700
    },
    {
      "epoch": 22.648835202761,
      "grad_norm": 6.308556476142257e-05,
      "learning_rate": 0.0005470290480299109,
      "loss": 0.0021,
      "step": 78750
    },
    {
      "epoch": 22.66321541558815,
      "grad_norm": 2.8473310521803796e-05,
      "learning_rate": 0.0005467414437733679,
      "loss": 0.0029,
      "step": 78800
    },
    {
      "epoch": 22.6775956284153,
      "grad_norm": 6.0316895542200655e-05,
      "learning_rate": 0.0005464538395168248,
      "loss": 0.0047,
      "step": 78850
    },
    {
      "epoch": 22.69197584124245,
      "grad_norm": 0.0004975799820385873,
      "learning_rate": 0.0005461662352602819,
      "loss": 0.0046,
      "step": 78900
    },
    {
      "epoch": 22.7063560540696,
      "grad_norm": 0.0013575146440416574,
      "learning_rate": 0.0005458786310037389,
      "loss": 0.0029,
      "step": 78950
    },
    {
      "epoch": 22.72073626689675,
      "grad_norm": 0.00013572670286521316,
      "learning_rate": 0.0005455910267471958,
      "loss": 0.0008,
      "step": 79000
    },
    {
      "epoch": 22.7351164797239,
      "grad_norm": 0.0002025830326601863,
      "learning_rate": 0.0005453034224906529,
      "loss": 0.0026,
      "step": 79050
    },
    {
      "epoch": 22.74949669255105,
      "grad_norm": 0.00013709916674997658,
      "learning_rate": 0.0005450158182341098,
      "loss": 0.0009,
      "step": 79100
    },
    {
      "epoch": 22.7638769053782,
      "grad_norm": 0.03713760897517204,
      "learning_rate": 0.0005447282139775669,
      "loss": 0.0008,
      "step": 79150
    },
    {
      "epoch": 22.77825711820535,
      "grad_norm": 3.2658452255418524e-05,
      "learning_rate": 0.0005444406097210239,
      "loss": 0.0011,
      "step": 79200
    },
    {
      "epoch": 22.7926373310325,
      "grad_norm": 0.00010538424976402894,
      "learning_rate": 0.0005441530054644809,
      "loss": 0.0037,
      "step": 79250
    },
    {
      "epoch": 22.80701754385965,
      "grad_norm": 0.0007408619276247919,
      "learning_rate": 0.0005438654012079379,
      "loss": 0.0024,
      "step": 79300
    },
    {
      "epoch": 22.8213977566868,
      "grad_norm": 0.007570439949631691,
      "learning_rate": 0.0005435777969513949,
      "loss": 0.0015,
      "step": 79350
    },
    {
      "epoch": 22.83577796951395,
      "grad_norm": 0.0010886428644880652,
      "learning_rate": 0.0005432901926948519,
      "loss": 0.0006,
      "step": 79400
    },
    {
      "epoch": 22.850158182341097,
      "grad_norm": 0.0019092238508164883,
      "learning_rate": 0.0005430025884383088,
      "loss": 0.0033,
      "step": 79450
    },
    {
      "epoch": 22.86453839516825,
      "grad_norm": 3.946409924537875e-05,
      "learning_rate": 0.0005427149841817659,
      "loss": 0.0015,
      "step": 79500
    },
    {
      "epoch": 22.8789186079954,
      "grad_norm": 0.000130202024593018,
      "learning_rate": 0.000542427379925223,
      "loss": 0.0009,
      "step": 79550
    },
    {
      "epoch": 22.893298820822547,
      "grad_norm": 0.0004222599964123219,
      "learning_rate": 0.0005421397756686799,
      "loss": 0.0038,
      "step": 79600
    },
    {
      "epoch": 22.9076790336497,
      "grad_norm": 0.0007116901688277721,
      "learning_rate": 0.000541852171412137,
      "loss": 0.0021,
      "step": 79650
    },
    {
      "epoch": 22.92205924647685,
      "grad_norm": 0.0004875785089097917,
      "learning_rate": 0.0005415645671555939,
      "loss": 0.0006,
      "step": 79700
    },
    {
      "epoch": 22.936439459303998,
      "grad_norm": 0.004912890028208494,
      "learning_rate": 0.0005412769628990509,
      "loss": 0.0041,
      "step": 79750
    },
    {
      "epoch": 22.950819672131146,
      "grad_norm": 4.20761170971673e-05,
      "learning_rate": 0.0005409893586425079,
      "loss": 0.0021,
      "step": 79800
    },
    {
      "epoch": 22.9651998849583,
      "grad_norm": 0.007614548783749342,
      "learning_rate": 0.0005407017543859649,
      "loss": 0.0048,
      "step": 79850
    },
    {
      "epoch": 22.979580097785448,
      "grad_norm": 0.001105373608879745,
      "learning_rate": 0.0005404141501294219,
      "loss": 0.0018,
      "step": 79900
    },
    {
      "epoch": 22.993960310612596,
      "grad_norm": 0.011485590599477291,
      "learning_rate": 0.000540126545872879,
      "loss": 0.0017,
      "step": 79950
    },
    {
      "epoch": 23.0,
      "eval_loss": 0.010081496089696884,
      "eval_runtime": 17.9766,
      "eval_samples_per_second": 2654.509,
      "eval_steps_per_second": 41.498,
      "step": 79971
    },
    {
      "epoch": 23.008340523439745,
      "grad_norm": 0.0001810878311516717,
      "learning_rate": 0.000539838941616336,
      "loss": 0.0017,
      "step": 80000
    },
    {
      "epoch": 23.022720736266898,
      "grad_norm": 0.0005229676025919616,
      "learning_rate": 0.0005395513373597929,
      "loss": 0.0039,
      "step": 80050
    },
    {
      "epoch": 23.037100949094047,
      "grad_norm": 0.016234470531344414,
      "learning_rate": 0.00053926373310325,
      "loss": 0.0029,
      "step": 80100
    },
    {
      "epoch": 23.051481161921195,
      "grad_norm": 0.0012306957505643368,
      "learning_rate": 0.000538976128846707,
      "loss": 0.0016,
      "step": 80150
    },
    {
      "epoch": 23.065861374748348,
      "grad_norm": 0.0003510553215164691,
      "learning_rate": 0.0005386885245901639,
      "loss": 0.0009,
      "step": 80200
    },
    {
      "epoch": 23.080241587575497,
      "grad_norm": 0.020753836259245872,
      "learning_rate": 0.000538400920333621,
      "loss": 0.0023,
      "step": 80250
    },
    {
      "epoch": 23.094621800402646,
      "grad_norm": 9.43373961490579e-05,
      "learning_rate": 0.0005381133160770779,
      "loss": 0.006,
      "step": 80300
    },
    {
      "epoch": 23.109002013229794,
      "grad_norm": 0.024037064984440804,
      "learning_rate": 0.0005378257118205349,
      "loss": 0.0022,
      "step": 80350
    },
    {
      "epoch": 23.123382226056947,
      "grad_norm": 3.8072212191764265e-05,
      "learning_rate": 0.000537538107563992,
      "loss": 0.0016,
      "step": 80400
    },
    {
      "epoch": 23.137762438884096,
      "grad_norm": 0.04949980974197388,
      "learning_rate": 0.000537250503307449,
      "loss": 0.0022,
      "step": 80450
    },
    {
      "epoch": 23.152142651711245,
      "grad_norm": 1.4341029782372061e-05,
      "learning_rate": 0.000536962899050906,
      "loss": 0.0029,
      "step": 80500
    },
    {
      "epoch": 23.166522864538393,
      "grad_norm": 0.002472260734066367,
      "learning_rate": 0.000536675294794363,
      "loss": 0.0056,
      "step": 80550
    },
    {
      "epoch": 23.180903077365546,
      "grad_norm": 0.00013473995204549283,
      "learning_rate": 0.00053638769053782,
      "loss": 0.0048,
      "step": 80600
    },
    {
      "epoch": 23.195283290192695,
      "grad_norm": 0.00027990134549327195,
      "learning_rate": 0.0005361000862812769,
      "loss": 0.0041,
      "step": 80650
    },
    {
      "epoch": 23.209663503019843,
      "grad_norm": 0.009187526069581509,
      "learning_rate": 0.000535812482024734,
      "loss": 0.0026,
      "step": 80700
    },
    {
      "epoch": 23.224043715846996,
      "grad_norm": 2.0882975150016136e-05,
      "learning_rate": 0.000535524877768191,
      "loss": 0.0028,
      "step": 80750
    },
    {
      "epoch": 23.238423928674145,
      "grad_norm": 5.242101542535238e-05,
      "learning_rate": 0.0005352372735116479,
      "loss": 0.0045,
      "step": 80800
    },
    {
      "epoch": 23.252804141501294,
      "grad_norm": 2.8179367291158997e-05,
      "learning_rate": 0.0005349496692551051,
      "loss": 0.0029,
      "step": 80850
    },
    {
      "epoch": 23.267184354328442,
      "grad_norm": 0.02481173537671566,
      "learning_rate": 0.000534662064998562,
      "loss": 0.0007,
      "step": 80900
    },
    {
      "epoch": 23.281564567155595,
      "grad_norm": 0.02847713977098465,
      "learning_rate": 0.000534374460742019,
      "loss": 0.0051,
      "step": 80950
    },
    {
      "epoch": 23.295944779982744,
      "grad_norm": 6.239058711798862e-05,
      "learning_rate": 0.000534086856485476,
      "loss": 0.0013,
      "step": 81000
    },
    {
      "epoch": 23.310324992809893,
      "grad_norm": 7.813965930836275e-05,
      "learning_rate": 0.000533799252228933,
      "loss": 0.0008,
      "step": 81050
    },
    {
      "epoch": 23.324705205637045,
      "grad_norm": 0.04024282470345497,
      "learning_rate": 0.00053351164797239,
      "loss": 0.0031,
      "step": 81100
    },
    {
      "epoch": 23.339085418464194,
      "grad_norm": 6.948695954633877e-05,
      "learning_rate": 0.000533224043715847,
      "loss": 0.0021,
      "step": 81150
    },
    {
      "epoch": 23.353465631291343,
      "grad_norm": 0.008494772017002106,
      "learning_rate": 0.000532936439459304,
      "loss": 0.0023,
      "step": 81200
    },
    {
      "epoch": 23.36784584411849,
      "grad_norm": 0.0010198772652074695,
      "learning_rate": 0.0005326488352027609,
      "loss": 0.0023,
      "step": 81250
    },
    {
      "epoch": 23.382226056945644,
      "grad_norm": 0.00525978347286582,
      "learning_rate": 0.0005323612309462181,
      "loss": 0.0009,
      "step": 81300
    },
    {
      "epoch": 23.396606269772793,
      "grad_norm": 6.691758608212695e-05,
      "learning_rate": 0.0005320736266896751,
      "loss": 0.0046,
      "step": 81350
    },
    {
      "epoch": 23.41098648259994,
      "grad_norm": 0.07257671654224396,
      "learning_rate": 0.000531786022433132,
      "loss": 0.0025,
      "step": 81400
    },
    {
      "epoch": 23.425366695427094,
      "grad_norm": 0.009517892263829708,
      "learning_rate": 0.0005314984181765891,
      "loss": 0.0052,
      "step": 81450
    },
    {
      "epoch": 23.439746908254243,
      "grad_norm": 0.0006857537664473057,
      "learning_rate": 0.000531210813920046,
      "loss": 0.0018,
      "step": 81500
    },
    {
      "epoch": 23.45412712108139,
      "grad_norm": 0.0004282676090952009,
      "learning_rate": 0.000530923209663503,
      "loss": 0.0011,
      "step": 81550
    },
    {
      "epoch": 23.46850733390854,
      "grad_norm": 0.007864904589951038,
      "learning_rate": 0.00053063560540696,
      "loss": 0.0043,
      "step": 81600
    },
    {
      "epoch": 23.482887546735693,
      "grad_norm": 0.004944207612425089,
      "learning_rate": 0.000530348001150417,
      "loss": 0.0019,
      "step": 81650
    },
    {
      "epoch": 23.497267759562842,
      "grad_norm": 0.0012281443923711777,
      "learning_rate": 0.000530060396893874,
      "loss": 0.0003,
      "step": 81700
    },
    {
      "epoch": 23.51164797238999,
      "grad_norm": 0.002832509344443679,
      "learning_rate": 0.0005297727926373311,
      "loss": 0.0008,
      "step": 81750
    },
    {
      "epoch": 23.52602818521714,
      "grad_norm": 5.996329491608776e-05,
      "learning_rate": 0.0005294851883807881,
      "loss": 0.0045,
      "step": 81800
    },
    {
      "epoch": 23.540408398044292,
      "grad_norm": 0.03768380358815193,
      "learning_rate": 0.000529197584124245,
      "loss": 0.0004,
      "step": 81850
    },
    {
      "epoch": 23.55478861087144,
      "grad_norm": 0.000540585198905319,
      "learning_rate": 0.0005289099798677021,
      "loss": 0.0043,
      "step": 81900
    },
    {
      "epoch": 23.56916882369859,
      "grad_norm": 6.327670416794717e-05,
      "learning_rate": 0.000528622375611159,
      "loss": 0.0023,
      "step": 81950
    },
    {
      "epoch": 23.583549036525742,
      "grad_norm": 1.9940038328059018e-05,
      "learning_rate": 0.000528334771354616,
      "loss": 0.003,
      "step": 82000
    },
    {
      "epoch": 23.59792924935289,
      "grad_norm": 0.00017407616542186588,
      "learning_rate": 0.0005280471670980731,
      "loss": 0.0015,
      "step": 82050
    },
    {
      "epoch": 23.61230946218004,
      "grad_norm": 0.00016975461039692163,
      "learning_rate": 0.00052775956284153,
      "loss": 0.001,
      "step": 82100
    },
    {
      "epoch": 23.62668967500719,
      "grad_norm": 0.004032528027892113,
      "learning_rate": 0.0005274719585849871,
      "loss": 0.0008,
      "step": 82150
    },
    {
      "epoch": 23.64106988783434,
      "grad_norm": 0.001937085879035294,
      "learning_rate": 0.0005271843543284441,
      "loss": 0.0039,
      "step": 82200
    },
    {
      "epoch": 23.65545010066149,
      "grad_norm": 2.8924150683451444e-05,
      "learning_rate": 0.0005268967500719011,
      "loss": 0.0017,
      "step": 82250
    },
    {
      "epoch": 23.66983031348864,
      "grad_norm": 0.0591273158788681,
      "learning_rate": 0.0005266091458153581,
      "loss": 0.0032,
      "step": 82300
    },
    {
      "epoch": 23.68421052631579,
      "grad_norm": 0.0005567020853050053,
      "learning_rate": 0.0005263215415588151,
      "loss": 0.0017,
      "step": 82350
    },
    {
      "epoch": 23.69859073914294,
      "grad_norm": 0.00011102894495707005,
      "learning_rate": 0.0005260339373022721,
      "loss": 0.0029,
      "step": 82400
    },
    {
      "epoch": 23.71297095197009,
      "grad_norm": 5.219148806645535e-05,
      "learning_rate": 0.000525746333045729,
      "loss": 0.0002,
      "step": 82450
    },
    {
      "epoch": 23.727351164797238,
      "grad_norm": 0.0003943757910747081,
      "learning_rate": 0.0005254587287891861,
      "loss": 0.0015,
      "step": 82500
    },
    {
      "epoch": 23.74173137762439,
      "grad_norm": 0.0024963223841041327,
      "learning_rate": 0.000525171124532643,
      "loss": 0.0014,
      "step": 82550
    },
    {
      "epoch": 23.75611159045154,
      "grad_norm": 0.02287423238158226,
      "learning_rate": 0.0005248835202761001,
      "loss": 0.0029,
      "step": 82600
    },
    {
      "epoch": 23.770491803278688,
      "grad_norm": 0.00024363238480873406,
      "learning_rate": 0.0005245959160195572,
      "loss": 0.0018,
      "step": 82650
    },
    {
      "epoch": 23.784872016105837,
      "grad_norm": 9.874022362055257e-05,
      "learning_rate": 0.0005243083117630141,
      "loss": 0.0037,
      "step": 82700
    },
    {
      "epoch": 23.79925222893299,
      "grad_norm": 0.01683426834642887,
      "learning_rate": 0.0005240207075064711,
      "loss": 0.0024,
      "step": 82750
    },
    {
      "epoch": 23.813632441760138,
      "grad_norm": 0.0003457531856838614,
      "learning_rate": 0.0005237331032499281,
      "loss": 0.0012,
      "step": 82800
    },
    {
      "epoch": 23.828012654587287,
      "grad_norm": 4.830574835068546e-05,
      "learning_rate": 0.0005234454989933851,
      "loss": 0.0006,
      "step": 82850
    },
    {
      "epoch": 23.84239286741444,
      "grad_norm": 0.003869291627779603,
      "learning_rate": 0.0005231578947368421,
      "loss": 0.0026,
      "step": 82900
    },
    {
      "epoch": 23.856773080241588,
      "grad_norm": 0.016783056780695915,
      "learning_rate": 0.0005228702904802991,
      "loss": 0.0026,
      "step": 82950
    },
    {
      "epoch": 23.871153293068737,
      "grad_norm": 0.007669213693588972,
      "learning_rate": 0.0005225826862237562,
      "loss": 0.0041,
      "step": 83000
    },
    {
      "epoch": 23.885533505895886,
      "grad_norm": 0.006905234418809414,
      "learning_rate": 0.0005222950819672131,
      "loss": 0.002,
      "step": 83050
    },
    {
      "epoch": 23.899913718723038,
      "grad_norm": 0.017538297921419144,
      "learning_rate": 0.0005220074777106702,
      "loss": 0.003,
      "step": 83100
    },
    {
      "epoch": 23.914293931550187,
      "grad_norm": 0.00024165099603123963,
      "learning_rate": 0.0005217198734541271,
      "loss": 0.0015,
      "step": 83150
    },
    {
      "epoch": 23.928674144377336,
      "grad_norm": 0.006477431859821081,
      "learning_rate": 0.0005214322691975841,
      "loss": 0.0021,
      "step": 83200
    },
    {
      "epoch": 23.94305435720449,
      "grad_norm": 3.9502669096691534e-05,
      "learning_rate": 0.0005211446649410412,
      "loss": 0.0026,
      "step": 83250
    },
    {
      "epoch": 23.957434570031637,
      "grad_norm": 9.324568236479536e-05,
      "learning_rate": 0.0005208570606844981,
      "loss": 0.0027,
      "step": 83300
    },
    {
      "epoch": 23.971814782858786,
      "grad_norm": 7.720506255282089e-05,
      "learning_rate": 0.0005205694564279551,
      "loss": 0.0017,
      "step": 83350
    },
    {
      "epoch": 23.986194995685935,
      "grad_norm": 2.9130198527127504e-05,
      "learning_rate": 0.0005202818521714121,
      "loss": 0.0018,
      "step": 83400
    },
    {
      "epoch": 24.0,
      "eval_loss": 0.010434028692543507,
      "eval_runtime": 17.0735,
      "eval_samples_per_second": 2794.913,
      "eval_steps_per_second": 43.693,
      "step": 83448
    },
    {
      "epoch": 24.000575208513087,
      "grad_norm": 7.717220432823524e-05,
      "learning_rate": 0.0005199942479148692,
      "loss": 0.002,
      "step": 83450
    },
    {
      "epoch": 24.014955421340236,
      "grad_norm": 0.0073534827679395676,
      "learning_rate": 0.0005197066436583262,
      "loss": 0.0032,
      "step": 83500
    },
    {
      "epoch": 24.029335634167385,
      "grad_norm": 0.00041087885620072484,
      "learning_rate": 0.0005194190394017832,
      "loss": 0.0005,
      "step": 83550
    },
    {
      "epoch": 24.043715846994534,
      "grad_norm": 0.0003003010351676494,
      "learning_rate": 0.0005191314351452402,
      "loss": 0.0018,
      "step": 83600
    },
    {
      "epoch": 24.058096059821686,
      "grad_norm": 1.7782374925445765e-05,
      "learning_rate": 0.0005188438308886971,
      "loss": 0.0048,
      "step": 83650
    },
    {
      "epoch": 24.072476272648835,
      "grad_norm": 2.942958235507831e-05,
      "learning_rate": 0.0005185562266321542,
      "loss": 0.0024,
      "step": 83700
    },
    {
      "epoch": 24.086856485475984,
      "grad_norm": 0.0006273571634665132,
      "learning_rate": 0.0005182686223756111,
      "loss": 0.0039,
      "step": 83750
    },
    {
      "epoch": 24.101236698303136,
      "grad_norm": 0.07699861377477646,
      "learning_rate": 0.0005179810181190681,
      "loss": 0.0012,
      "step": 83800
    },
    {
      "epoch": 24.115616911130285,
      "grad_norm": 0.008155403658747673,
      "learning_rate": 0.0005176934138625253,
      "loss": 0.0046,
      "step": 83850
    },
    {
      "epoch": 24.129997123957434,
      "grad_norm": 0.004328722134232521,
      "learning_rate": 0.0005174058096059822,
      "loss": 0.0013,
      "step": 83900
    },
    {
      "epoch": 24.144377336784583,
      "grad_norm": 0.0004274047678336501,
      "learning_rate": 0.0005171182053494392,
      "loss": 0.0029,
      "step": 83950
    },
    {
      "epoch": 24.158757549611735,
      "grad_norm": 5.476170917972922e-05,
      "learning_rate": 0.0005168306010928962,
      "loss": 0.0032,
      "step": 84000
    },
    {
      "epoch": 24.173137762438884,
      "grad_norm": 0.02265986055135727,
      "learning_rate": 0.0005165429968363532,
      "loss": 0.0037,
      "step": 84050
    },
    {
      "epoch": 24.187517975266033,
      "grad_norm": 0.014685914851725101,
      "learning_rate": 0.0005162553925798102,
      "loss": 0.0013,
      "step": 84100
    },
    {
      "epoch": 24.201898188093185,
      "grad_norm": 0.00010493322042748332,
      "learning_rate": 0.0005159677883232672,
      "loss": 0.0007,
      "step": 84150
    },
    {
      "epoch": 24.216278400920334,
      "grad_norm": 0.01887262426316738,
      "learning_rate": 0.0005156801840667242,
      "loss": 0.0011,
      "step": 84200
    },
    {
      "epoch": 24.230658613747483,
      "grad_norm": 0.00012058832362527028,
      "learning_rate": 0.0005153925798101811,
      "loss": 0.0036,
      "step": 84250
    },
    {
      "epoch": 24.245038826574632,
      "grad_norm": 0.00010383265180280432,
      "learning_rate": 0.0005151049755536383,
      "loss": 0.0043,
      "step": 84300
    },
    {
      "epoch": 24.259419039401784,
      "grad_norm": 0.0016472128918394446,
      "learning_rate": 0.0005148173712970952,
      "loss": 0.0062,
      "step": 84350
    },
    {
      "epoch": 24.273799252228933,
      "grad_norm": 0.0015362804988399148,
      "learning_rate": 0.0005145297670405522,
      "loss": 0.0003,
      "step": 84400
    },
    {
      "epoch": 24.288179465056082,
      "grad_norm": 5.445145507110283e-05,
      "learning_rate": 0.0005142421627840093,
      "loss": 0.0029,
      "step": 84450
    },
    {
      "epoch": 24.30255967788323,
      "grad_norm": 3.433584060985595e-05,
      "learning_rate": 0.0005139545585274662,
      "loss": 0.0032,
      "step": 84500
    },
    {
      "epoch": 24.316939890710383,
      "grad_norm": 0.005952975247055292,
      "learning_rate": 0.0005136669542709232,
      "loss": 0.0041,
      "step": 84550
    },
    {
      "epoch": 24.331320103537532,
      "grad_norm": 0.015050632879137993,
      "learning_rate": 0.0005133793500143802,
      "loss": 0.0027,
      "step": 84600
    },
    {
      "epoch": 24.34570031636468,
      "grad_norm": 5.603639510809444e-05,
      "learning_rate": 0.0005130917457578372,
      "loss": 0.0025,
      "step": 84650
    },
    {
      "epoch": 24.360080529191833,
      "grad_norm": 0.005960165522992611,
      "learning_rate": 0.0005128041415012941,
      "loss": 0.0061,
      "step": 84700
    },
    {
      "epoch": 24.374460742018982,
      "grad_norm": 0.00027415677323006094,
      "learning_rate": 0.0005125165372447513,
      "loss": 0.0054,
      "step": 84750
    },
    {
      "epoch": 24.38884095484613,
      "grad_norm": 0.00871284119784832,
      "learning_rate": 0.0005122289329882083,
      "loss": 0.0014,
      "step": 84800
    },
    {
      "epoch": 24.40322116767328,
      "grad_norm": 0.04914720728993416,
      "learning_rate": 0.0005119413287316652,
      "loss": 0.0019,
      "step": 84850
    },
    {
      "epoch": 24.417601380500432,
      "grad_norm": 7.26116995792836e-05,
      "learning_rate": 0.0005116537244751223,
      "loss": 0.0045,
      "step": 84900
    },
    {
      "epoch": 24.43198159332758,
      "grad_norm": 0.00023618846898898482,
      "learning_rate": 0.0005113661202185792,
      "loss": 0.0006,
      "step": 84950
    },
    {
      "epoch": 24.44636180615473,
      "grad_norm": 0.0001252370566362515,
      "learning_rate": 0.0005110785159620362,
      "loss": 0.0013,
      "step": 85000
    },
    {
      "epoch": 24.460742018981882,
      "grad_norm": 0.00019396132847759873,
      "learning_rate": 0.0005107909117054933,
      "loss": 0.0012,
      "step": 85050
    },
    {
      "epoch": 24.47512223180903,
      "grad_norm": 3.214413300156593e-05,
      "learning_rate": 0.0005105033074489502,
      "loss": 0.0028,
      "step": 85100
    },
    {
      "epoch": 24.48950244463618,
      "grad_norm": 0.0002427929430268705,
      "learning_rate": 0.0005102157031924073,
      "loss": 0.002,
      "step": 85150
    },
    {
      "epoch": 24.50388265746333,
      "grad_norm": 0.00024629183462820947,
      "learning_rate": 0.0005099280989358643,
      "loss": 0.0036,
      "step": 85200
    },
    {
      "epoch": 24.51826287029048,
      "grad_norm": 9.912895620800555e-05,
      "learning_rate": 0.0005096404946793213,
      "loss": 0.0031,
      "step": 85250
    },
    {
      "epoch": 24.53264308311763,
      "grad_norm": 0.02525901235640049,
      "learning_rate": 0.0005093528904227782,
      "loss": 0.0027,
      "step": 85300
    },
    {
      "epoch": 24.54702329594478,
      "grad_norm": 0.000528659438714385,
      "learning_rate": 0.0005090652861662353,
      "loss": 0.0013,
      "step": 85350
    },
    {
      "epoch": 24.56140350877193,
      "grad_norm": 0.0001053403684636578,
      "learning_rate": 0.0005087776819096923,
      "loss": 0.0022,
      "step": 85400
    },
    {
      "epoch": 24.57578372159908,
      "grad_norm": 2.4908145860536024e-05,
      "learning_rate": 0.0005084900776531492,
      "loss": 0.0006,
      "step": 85450
    },
    {
      "epoch": 24.59016393442623,
      "grad_norm": 0.010781400837004185,
      "learning_rate": 0.0005082024733966063,
      "loss": 0.0004,
      "step": 85500
    },
    {
      "epoch": 24.604544147253378,
      "grad_norm": 0.0028044513892382383,
      "learning_rate": 0.0005079148691400632,
      "loss": 0.0013,
      "step": 85550
    },
    {
      "epoch": 24.61892436008053,
      "grad_norm": 0.002264712005853653,
      "learning_rate": 0.0005076272648835203,
      "loss": 0.0007,
      "step": 85600
    },
    {
      "epoch": 24.63330457290768,
      "grad_norm": 0.042822692543268204,
      "learning_rate": 0.0005073396606269774,
      "loss": 0.001,
      "step": 85650
    },
    {
      "epoch": 24.64768478573483,
      "grad_norm": 6.185259553603828e-05,
      "learning_rate": 0.0005070520563704343,
      "loss": 0.0028,
      "step": 85700
    },
    {
      "epoch": 24.662064998561977,
      "grad_norm": 0.0001876041933428496,
      "learning_rate": 0.0005067644521138913,
      "loss": 0.0017,
      "step": 85750
    },
    {
      "epoch": 24.67644521138913,
      "grad_norm": 0.00429140729829669,
      "learning_rate": 0.0005064768478573483,
      "loss": 0.0014,
      "step": 85800
    },
    {
      "epoch": 24.69082542421628,
      "grad_norm": 0.00012532141408883035,
      "learning_rate": 0.0005061892436008053,
      "loss": 0.0021,
      "step": 85850
    },
    {
      "epoch": 24.705205637043427,
      "grad_norm": 0.00012033848179271445,
      "learning_rate": 0.0005059016393442622,
      "loss": 0.0049,
      "step": 85900
    },
    {
      "epoch": 24.71958584987058,
      "grad_norm": 0.00016616465291008353,
      "learning_rate": 0.0005056140350877193,
      "loss": 0.0008,
      "step": 85950
    },
    {
      "epoch": 24.73396606269773,
      "grad_norm": 4.1334675188409165e-05,
      "learning_rate": 0.0005053264308311764,
      "loss": 0.0045,
      "step": 86000
    },
    {
      "epoch": 24.748346275524877,
      "grad_norm": 0.0008527179597876966,
      "learning_rate": 0.0005050388265746333,
      "loss": 0.0005,
      "step": 86050
    },
    {
      "epoch": 24.762726488352026,
      "grad_norm": 0.003119518281891942,
      "learning_rate": 0.0005047512223180904,
      "loss": 0.0031,
      "step": 86100
    },
    {
      "epoch": 24.77710670117918,
      "grad_norm": 0.014258628711104393,
      "learning_rate": 0.0005044636180615473,
      "loss": 0.0047,
      "step": 86150
    },
    {
      "epoch": 24.791486914006327,
      "grad_norm": 9.885046893032268e-05,
      "learning_rate": 0.0005041760138050043,
      "loss": 0.0021,
      "step": 86200
    },
    {
      "epoch": 24.805867126833476,
      "grad_norm": 0.00010144431871594861,
      "learning_rate": 0.0005038884095484614,
      "loss": 0.0018,
      "step": 86250
    },
    {
      "epoch": 24.82024733966063,
      "grad_norm": 0.026218261569738388,
      "learning_rate": 0.0005036008052919183,
      "loss": 0.0035,
      "step": 86300
    },
    {
      "epoch": 24.834627552487778,
      "grad_norm": 0.0006925676134414971,
      "learning_rate": 0.0005033132010353753,
      "loss": 0.0015,
      "step": 86350
    },
    {
      "epoch": 24.849007765314926,
      "grad_norm": 0.0001429002732038498,
      "learning_rate": 0.0005030255967788323,
      "loss": 0.0017,
      "step": 86400
    },
    {
      "epoch": 24.863387978142075,
      "grad_norm": 0.00833219289779663,
      "learning_rate": 0.0005027379925222894,
      "loss": 0.0026,
      "step": 86450
    },
    {
      "epoch": 24.877768190969228,
      "grad_norm": 0.0078998152166605,
      "learning_rate": 0.0005024503882657463,
      "loss": 0.0006,
      "step": 86500
    },
    {
      "epoch": 24.892148403796376,
      "grad_norm": 0.0009164816583506763,
      "learning_rate": 0.0005021627840092034,
      "loss": 0.0024,
      "step": 86550
    },
    {
      "epoch": 24.906528616623525,
      "grad_norm": 0.07766880095005035,
      "learning_rate": 0.0005018751797526604,
      "loss": 0.001,
      "step": 86600
    },
    {
      "epoch": 24.920908829450674,
      "grad_norm": 0.006016298662871122,
      "learning_rate": 0.0005015875754961173,
      "loss": 0.0013,
      "step": 86650
    },
    {
      "epoch": 24.935289042277827,
      "grad_norm": 0.01523568108677864,
      "learning_rate": 0.0005012999712395744,
      "loss": 0.0031,
      "step": 86700
    },
    {
      "epoch": 24.949669255104975,
      "grad_norm": 0.0006104558706283569,
      "learning_rate": 0.0005010123669830313,
      "loss": 0.0026,
      "step": 86750
    },
    {
      "epoch": 24.964049467932124,
      "grad_norm": 0.002418997697532177,
      "learning_rate": 0.0005007247627264883,
      "loss": 0.0047,
      "step": 86800
    },
    {
      "epoch": 24.978429680759277,
      "grad_norm": 0.00026769511168822646,
      "learning_rate": 0.0005004371584699455,
      "loss": 0.0036,
      "step": 86850
    },
    {
      "epoch": 24.992809893586426,
      "grad_norm": 5.32543781446293e-05,
      "learning_rate": 0.0005001495542134024,
      "loss": 0.0002,
      "step": 86900
    },
    {
      "epoch": 25.0,
      "eval_loss": 0.010287797078490257,
      "eval_runtime": 17.1707,
      "eval_samples_per_second": 2779.099,
      "eval_steps_per_second": 43.446,
      "step": 86925
    },
    {
      "epoch": 25.007190106413574,
      "grad_norm": 0.00017397815827280283,
      "learning_rate": 0.0004998619499568594,
      "loss": 0.0058,
      "step": 86950
    },
    {
      "epoch": 25.021570319240723,
      "grad_norm": 0.0002688774839043617,
      "learning_rate": 0.0004995743457003164,
      "loss": 0.002,
      "step": 87000
    },
    {
      "epoch": 25.035950532067876,
      "grad_norm": 0.00015212134167086333,
      "learning_rate": 0.0004992867414437734,
      "loss": 0.0046,
      "step": 87050
    },
    {
      "epoch": 25.050330744895025,
      "grad_norm": 0.0005546041065827012,
      "learning_rate": 0.0004989991371872304,
      "loss": 0.0006,
      "step": 87100
    },
    {
      "epoch": 25.064710957722173,
      "grad_norm": 0.006848423276096582,
      "learning_rate": 0.0004987115329306874,
      "loss": 0.0042,
      "step": 87150
    },
    {
      "epoch": 25.079091170549326,
      "grad_norm": 0.017434576526284218,
      "learning_rate": 0.0004984239286741444,
      "loss": 0.0006,
      "step": 87200
    },
    {
      "epoch": 25.093471383376475,
      "grad_norm": 0.0029206385370343924,
      "learning_rate": 0.0004981363244176013,
      "loss": 0.0042,
      "step": 87250
    },
    {
      "epoch": 25.107851596203624,
      "grad_norm": 0.0002646137145347893,
      "learning_rate": 0.0004978487201610585,
      "loss": 0.0033,
      "step": 87300
    },
    {
      "epoch": 25.122231809030772,
      "grad_norm": 0.001597721129655838,
      "learning_rate": 0.0004975611159045155,
      "loss": 0.0023,
      "step": 87350
    },
    {
      "epoch": 25.136612021857925,
      "grad_norm": 0.002085479674860835,
      "learning_rate": 0.0004972735116479724,
      "loss": 0.0011,
      "step": 87400
    },
    {
      "epoch": 25.150992234685074,
      "grad_norm": 0.00010083181405207142,
      "learning_rate": 0.0004969859073914294,
      "loss": 0.0035,
      "step": 87450
    },
    {
      "epoch": 25.165372447512222,
      "grad_norm": 0.0015782781410962343,
      "learning_rate": 0.0004966983031348864,
      "loss": 0.0024,
      "step": 87500
    },
    {
      "epoch": 25.17975266033937,
      "grad_norm": 0.0004675067903008312,
      "learning_rate": 0.0004964106988783434,
      "loss": 0.0005,
      "step": 87550
    },
    {
      "epoch": 25.194132873166524,
      "grad_norm": 2.163704084523488e-05,
      "learning_rate": 0.0004961230946218004,
      "loss": 0.0017,
      "step": 87600
    },
    {
      "epoch": 25.208513085993673,
      "grad_norm": 7.792349060764536e-05,
      "learning_rate": 0.0004958354903652574,
      "loss": 0.0011,
      "step": 87650
    },
    {
      "epoch": 25.22289329882082,
      "grad_norm": 0.008610961958765984,
      "learning_rate": 0.0004955478861087144,
      "loss": 0.0025,
      "step": 87700
    },
    {
      "epoch": 25.237273511647974,
      "grad_norm": 0.002568443538621068,
      "learning_rate": 0.0004952602818521715,
      "loss": 0.0017,
      "step": 87750
    },
    {
      "epoch": 25.251653724475123,
      "grad_norm": 0.0001445859088562429,
      "learning_rate": 0.0004949726775956285,
      "loss": 0.0021,
      "step": 87800
    },
    {
      "epoch": 25.26603393730227,
      "grad_norm": 0.002523524221032858,
      "learning_rate": 0.0004946850733390854,
      "loss": 0.0014,
      "step": 87850
    },
    {
      "epoch": 25.28041415012942,
      "grad_norm": 0.011196737177670002,
      "learning_rate": 0.0004943974690825424,
      "loss": 0.0014,
      "step": 87900
    },
    {
      "epoch": 25.294794362956573,
      "grad_norm": 5.4019859817344695e-05,
      "learning_rate": 0.0004941098648259995,
      "loss": 0.0013,
      "step": 87950
    },
    {
      "epoch": 25.30917457578372,
      "grad_norm": 0.003036409616470337,
      "learning_rate": 0.0004938222605694564,
      "loss": 0.0026,
      "step": 88000
    },
    {
      "epoch": 25.32355478861087,
      "grad_norm": 0.027713946998119354,
      "learning_rate": 0.0004935346563129134,
      "loss": 0.004,
      "step": 88050
    },
    {
      "epoch": 25.337935001438023,
      "grad_norm": 0.0004853679856751114,
      "learning_rate": 0.0004932470520563704,
      "loss": 0.0015,
      "step": 88100
    },
    {
      "epoch": 25.35231521426517,
      "grad_norm": 2.4370405299123377e-05,
      "learning_rate": 0.0004929594477998274,
      "loss": 0.004,
      "step": 88150
    },
    {
      "epoch": 25.36669542709232,
      "grad_norm": 0.0002354901225771755,
      "learning_rate": 0.0004926718435432845,
      "loss": 0.0023,
      "step": 88200
    },
    {
      "epoch": 25.38107563991947,
      "grad_norm": 7.831319817341864e-05,
      "learning_rate": 0.0004923842392867415,
      "loss": 0.0008,
      "step": 88250
    },
    {
      "epoch": 25.395455852746622,
      "grad_norm": 0.0006351747433654964,
      "learning_rate": 0.0004920966350301985,
      "loss": 0.0072,
      "step": 88300
    },
    {
      "epoch": 25.40983606557377,
      "grad_norm": 0.0008360347128473222,
      "learning_rate": 0.0004918090307736555,
      "loss": 0.002,
      "step": 88350
    },
    {
      "epoch": 25.42421627840092,
      "grad_norm": 0.004586861003190279,
      "learning_rate": 0.0004915214265171125,
      "loss": 0.0022,
      "step": 88400
    },
    {
      "epoch": 25.43859649122807,
      "grad_norm": 0.00021744714467786252,
      "learning_rate": 0.0004912338222605694,
      "loss": 0.0014,
      "step": 88450
    },
    {
      "epoch": 25.45297670405522,
      "grad_norm": 0.009130547754466534,
      "learning_rate": 0.0004909462180040264,
      "loss": 0.0023,
      "step": 88500
    },
    {
      "epoch": 25.46735691688237,
      "grad_norm": 0.008328490890562534,
      "learning_rate": 0.0004906586137474835,
      "loss": 0.0011,
      "step": 88550
    },
    {
      "epoch": 25.48173712970952,
      "grad_norm": 0.003934995271265507,
      "learning_rate": 0.0004903710094909404,
      "loss": 0.0012,
      "step": 88600
    },
    {
      "epoch": 25.49611734253667,
      "grad_norm": 0.004946186672896147,
      "learning_rate": 0.0004900834052343975,
      "loss": 0.0004,
      "step": 88650
    },
    {
      "epoch": 25.51049755536382,
      "grad_norm": 0.04082024097442627,
      "learning_rate": 0.0004897958009778545,
      "loss": 0.0044,
      "step": 88700
    },
    {
      "epoch": 25.52487776819097,
      "grad_norm": 4.244019146426581e-05,
      "learning_rate": 0.0004895081967213115,
      "loss": 0.0009,
      "step": 88750
    },
    {
      "epoch": 25.539257981018118,
      "grad_norm": 0.000605056993663311,
      "learning_rate": 0.0004892205924647685,
      "loss": 0.0028,
      "step": 88800
    },
    {
      "epoch": 25.55363819384527,
      "grad_norm": 0.00356338475830853,
      "learning_rate": 0.0004889329882082255,
      "loss": 0.0036,
      "step": 88850
    },
    {
      "epoch": 25.56801840667242,
      "grad_norm": 6.829464109614491e-05,
      "learning_rate": 0.0004886453839516825,
      "loss": 0.0031,
      "step": 88900
    },
    {
      "epoch": 25.582398619499568,
      "grad_norm": 0.027554530650377274,
      "learning_rate": 0.0004883577796951395,
      "loss": 0.0047,
      "step": 88950
    },
    {
      "epoch": 25.59677883232672,
      "grad_norm": 9.158905595541e-05,
      "learning_rate": 0.0004880701754385965,
      "loss": 0.0013,
      "step": 89000
    },
    {
      "epoch": 25.61115904515387,
      "grad_norm": 0.0007164476555772126,
      "learning_rate": 0.0004877825711820535,
      "loss": 0.0012,
      "step": 89050
    },
    {
      "epoch": 25.625539257981018,
      "grad_norm": 0.00012072784011252224,
      "learning_rate": 0.00048749496692551046,
      "loss": 0.0011,
      "step": 89100
    },
    {
      "epoch": 25.639919470808167,
      "grad_norm": 0.0024944294709712267,
      "learning_rate": 0.00048720736266896753,
      "loss": 0.0005,
      "step": 89150
    },
    {
      "epoch": 25.65429968363532,
      "grad_norm": 0.00022958643967285752,
      "learning_rate": 0.00048691975841242454,
      "loss": 0.0027,
      "step": 89200
    },
    {
      "epoch": 25.668679896462468,
      "grad_norm": 3.093564009759575e-05,
      "learning_rate": 0.0004866321541558815,
      "loss": 0.0046,
      "step": 89250
    },
    {
      "epoch": 25.683060109289617,
      "grad_norm": 0.0002179598668590188,
      "learning_rate": 0.0004863445498993385,
      "loss": 0.0026,
      "step": 89300
    },
    {
      "epoch": 25.69744032211677,
      "grad_norm": 0.0007770983502268791,
      "learning_rate": 0.0004860569456427955,
      "loss": 0.0023,
      "step": 89350
    },
    {
      "epoch": 25.711820534943918,
      "grad_norm": 7.26315047359094e-05,
      "learning_rate": 0.0004857693413862525,
      "loss": 0.0023,
      "step": 89400
    },
    {
      "epoch": 25.726200747771067,
      "grad_norm": 0.05410510301589966,
      "learning_rate": 0.00048548173712970955,
      "loss": 0.0005,
      "step": 89450
    },
    {
      "epoch": 25.740580960598216,
      "grad_norm": 1.3500475688488223e-05,
      "learning_rate": 0.00048519413287316656,
      "loss": 0.0037,
      "step": 89500
    },
    {
      "epoch": 25.754961173425368,
      "grad_norm": 0.005830009933561087,
      "learning_rate": 0.0004849065286166235,
      "loss": 0.0054,
      "step": 89550
    },
    {
      "epoch": 25.769341386252517,
      "grad_norm": 0.0044067008420825005,
      "learning_rate": 0.00048461892436008053,
      "loss": 0.0033,
      "step": 89600
    },
    {
      "epoch": 25.783721599079666,
      "grad_norm": 4.966150299878791e-05,
      "learning_rate": 0.00048433132010353754,
      "loss": 0.0018,
      "step": 89650
    },
    {
      "epoch": 25.798101811906815,
      "grad_norm": 0.05743562802672386,
      "learning_rate": 0.0004840437158469945,
      "loss": 0.0035,
      "step": 89700
    },
    {
      "epoch": 25.812482024733967,
      "grad_norm": 7.07752478774637e-05,
      "learning_rate": 0.00048375611159045157,
      "loss": 0.0013,
      "step": 89750
    },
    {
      "epoch": 25.826862237561116,
      "grad_norm": 9.601984493201599e-05,
      "learning_rate": 0.0004834685073339086,
      "loss": 0.0037,
      "step": 89800
    },
    {
      "epoch": 25.841242450388265,
      "grad_norm": 0.0004242465947754681,
      "learning_rate": 0.00048318090307736554,
      "loss": 0.0015,
      "step": 89850
    },
    {
      "epoch": 25.855622663215417,
      "grad_norm": 8.057080776779912e-06,
      "learning_rate": 0.00048289329882082255,
      "loss": 0.0036,
      "step": 89900
    },
    {
      "epoch": 25.870002876042566,
      "grad_norm": 0.017143934965133667,
      "learning_rate": 0.00048260569456427956,
      "loss": 0.0042,
      "step": 89950
    },
    {
      "epoch": 25.884383088869715,
      "grad_norm": 0.00020987764582969248,
      "learning_rate": 0.0004823180903077365,
      "loss": 0.0022,
      "step": 90000
    },
    {
      "epoch": 25.898763301696864,
      "grad_norm": 0.00024302629753947258,
      "learning_rate": 0.0004820304860511936,
      "loss": 0.0024,
      "step": 90050
    },
    {
      "epoch": 25.913143514524016,
      "grad_norm": 0.023437242954969406,
      "learning_rate": 0.0004817428817946506,
      "loss": 0.0038,
      "step": 90100
    },
    {
      "epoch": 25.927523727351165,
      "grad_norm": 0.002087798435240984,
      "learning_rate": 0.00048145527753810756,
      "loss": 0.0022,
      "step": 90150
    },
    {
      "epoch": 25.941903940178314,
      "grad_norm": 0.0024389042519032955,
      "learning_rate": 0.00048116767328156457,
      "loss": 0.0021,
      "step": 90200
    },
    {
      "epoch": 25.956284153005463,
      "grad_norm": 0.0006336572114378214,
      "learning_rate": 0.0004808800690250216,
      "loss": 0.0007,
      "step": 90250
    },
    {
      "epoch": 25.970664365832615,
      "grad_norm": 7.454101432813331e-05,
      "learning_rate": 0.00048059246476847854,
      "loss": 0.0021,
      "step": 90300
    },
    {
      "epoch": 25.985044578659764,
      "grad_norm": 0.0003786668530665338,
      "learning_rate": 0.00048030486051193555,
      "loss": 0.0031,
      "step": 90350
    },
    {
      "epoch": 25.999424791486913,
      "grad_norm": 0.00010364759509684518,
      "learning_rate": 0.0004800172562553926,
      "loss": 0.0016,
      "step": 90400
    },
    {
      "epoch": 26.0,
      "eval_loss": 0.011209884658455849,
      "eval_runtime": 17.4691,
      "eval_samples_per_second": 2731.618,
      "eval_steps_per_second": 42.704,
      "step": 90402
    },
    {
      "epoch": 26.013805004314065,
      "grad_norm": 0.07235097140073776,
      "learning_rate": 0.0004797296519988496,
      "loss": 0.0026,
      "step": 90450
    },
    {
      "epoch": 26.028185217141214,
      "grad_norm": 0.00011877061479026452,
      "learning_rate": 0.0004794420477423066,
      "loss": 0.0003,
      "step": 90500
    },
    {
      "epoch": 26.042565429968363,
      "grad_norm": 0.001144175068475306,
      "learning_rate": 0.0004791544434857636,
      "loss": 0.0046,
      "step": 90550
    },
    {
      "epoch": 26.05694564279551,
      "grad_norm": 0.003484888467937708,
      "learning_rate": 0.00047886683922922056,
      "loss": 0.0018,
      "step": 90600
    },
    {
      "epoch": 26.071325855622664,
      "grad_norm": 9.535194840282202e-05,
      "learning_rate": 0.00047857923497267757,
      "loss": 0.0024,
      "step": 90650
    },
    {
      "epoch": 26.085706068449813,
      "grad_norm": 0.0452418215572834,
      "learning_rate": 0.00047829163071613464,
      "loss": 0.0009,
      "step": 90700
    },
    {
      "epoch": 26.100086281276962,
      "grad_norm": 4.045988680445589e-05,
      "learning_rate": 0.0004780040264595916,
      "loss": 0.0029,
      "step": 90750
    },
    {
      "epoch": 26.114466494104114,
      "grad_norm": 9.361484262626618e-05,
      "learning_rate": 0.0004777164222030486,
      "loss": 0.0068,
      "step": 90800
    },
    {
      "epoch": 26.128846706931263,
      "grad_norm": 0.011164448224008083,
      "learning_rate": 0.0004774288179465056,
      "loss": 0.0016,
      "step": 90850
    },
    {
      "epoch": 26.143226919758412,
      "grad_norm": 0.0008135800017043948,
      "learning_rate": 0.0004771412136899626,
      "loss": 0.002,
      "step": 90900
    },
    {
      "epoch": 26.15760713258556,
      "grad_norm": 3.2769381505204365e-05,
      "learning_rate": 0.0004768536094334196,
      "loss": 0.0037,
      "step": 90950
    },
    {
      "epoch": 26.171987345412713,
      "grad_norm": 0.00010273973020957783,
      "learning_rate": 0.00047656600517687666,
      "loss": 0.0019,
      "step": 91000
    },
    {
      "epoch": 26.186367558239862,
      "grad_norm": 0.01771964132785797,
      "learning_rate": 0.0004762784009203336,
      "loss": 0.0019,
      "step": 91050
    },
    {
      "epoch": 26.20074777106701,
      "grad_norm": 0.00010090549039887264,
      "learning_rate": 0.00047599079666379063,
      "loss": 0.0042,
      "step": 91100
    },
    {
      "epoch": 26.215127983894163,
      "grad_norm": 5.273170972941443e-05,
      "learning_rate": 0.00047570319240724764,
      "loss": 0.0032,
      "step": 91150
    },
    {
      "epoch": 26.229508196721312,
      "grad_norm": 0.0007811413379386067,
      "learning_rate": 0.0004754155881507046,
      "loss": 0.0018,
      "step": 91200
    },
    {
      "epoch": 26.24388840954846,
      "grad_norm": 0.06694024801254272,
      "learning_rate": 0.0004751279838941616,
      "loss": 0.0047,
      "step": 91250
    },
    {
      "epoch": 26.25826862237561,
      "grad_norm": 0.008774246089160442,
      "learning_rate": 0.0004748403796376187,
      "loss": 0.0029,
      "step": 91300
    },
    {
      "epoch": 26.272648835202762,
      "grad_norm": 0.0003439060237724334,
      "learning_rate": 0.00047455277538107564,
      "loss": 0.0046,
      "step": 91350
    },
    {
      "epoch": 26.28702904802991,
      "grad_norm": 0.000644051528070122,
      "learning_rate": 0.00047426517112453265,
      "loss": 0.0005,
      "step": 91400
    },
    {
      "epoch": 26.30140926085706,
      "grad_norm": 0.0010036482708528638,
      "learning_rate": 0.00047397756686798966,
      "loss": 0.0057,
      "step": 91450
    },
    {
      "epoch": 26.31578947368421,
      "grad_norm": 0.0004997477517463267,
      "learning_rate": 0.0004736899626114466,
      "loss": 0.0005,
      "step": 91500
    },
    {
      "epoch": 26.33016968651136,
      "grad_norm": 0.05218178406357765,
      "learning_rate": 0.00047340235835490363,
      "loss": 0.0045,
      "step": 91550
    },
    {
      "epoch": 26.34454989933851,
      "grad_norm": 0.0003565050137694925,
      "learning_rate": 0.0004731147540983607,
      "loss": 0.0019,
      "step": 91600
    },
    {
      "epoch": 26.35893011216566,
      "grad_norm": 5.557757685892284e-05,
      "learning_rate": 0.0004728271498418177,
      "loss": 0.0015,
      "step": 91650
    },
    {
      "epoch": 26.37331032499281,
      "grad_norm": 0.012669513002038002,
      "learning_rate": 0.00047253954558527467,
      "loss": 0.0044,
      "step": 91700
    },
    {
      "epoch": 26.38769053781996,
      "grad_norm": 7.133646431611851e-05,
      "learning_rate": 0.0004722519413287317,
      "loss": 0.0033,
      "step": 91750
    },
    {
      "epoch": 26.40207075064711,
      "grad_norm": 0.0007338786963373423,
      "learning_rate": 0.00047196433707218864,
      "loss": 0.0032,
      "step": 91800
    },
    {
      "epoch": 26.416450963474258,
      "grad_norm": 0.000595022807829082,
      "learning_rate": 0.00047167673281564565,
      "loss": 0.0008,
      "step": 91850
    },
    {
      "epoch": 26.43083117630141,
      "grad_norm": 6.261269300011918e-05,
      "learning_rate": 0.0004713891285591027,
      "loss": 0.0022,
      "step": 91900
    },
    {
      "epoch": 26.44521138912856,
      "grad_norm": 0.00031487050000578165,
      "learning_rate": 0.00047110152430255973,
      "loss": 0.0032,
      "step": 91950
    },
    {
      "epoch": 26.459591601955708,
      "grad_norm": 7.333536632359028e-05,
      "learning_rate": 0.0004708139200460167,
      "loss": 0.0031,
      "step": 92000
    },
    {
      "epoch": 26.47397181478286,
      "grad_norm": 2.640988168423064e-05,
      "learning_rate": 0.0004705263157894737,
      "loss": 0.0003,
      "step": 92050
    },
    {
      "epoch": 26.48835202761001,
      "grad_norm": 0.008142444305121899,
      "learning_rate": 0.0004702387115329307,
      "loss": 0.0003,
      "step": 92100
    },
    {
      "epoch": 26.502732240437158,
      "grad_norm": 2.9910937882959843e-05,
      "learning_rate": 0.00046995110727638767,
      "loss": 0.0061,
      "step": 92150
    },
    {
      "epoch": 26.517112453264307,
      "grad_norm": 1.8088003344018944e-05,
      "learning_rate": 0.00046966350301984474,
      "loss": 0.0013,
      "step": 92200
    },
    {
      "epoch": 26.53149266609146,
      "grad_norm": 0.001209614216350019,
      "learning_rate": 0.00046937589876330175,
      "loss": 0.0021,
      "step": 92250
    },
    {
      "epoch": 26.54587287891861,
      "grad_norm": 0.00014595680113416165,
      "learning_rate": 0.0004690882945067587,
      "loss": 0.0011,
      "step": 92300
    },
    {
      "epoch": 26.560253091745757,
      "grad_norm": 0.0002521555870771408,
      "learning_rate": 0.0004688006902502157,
      "loss": 0.0035,
      "step": 92350
    },
    {
      "epoch": 26.574633304572906,
      "grad_norm": 0.04709532856941223,
      "learning_rate": 0.00046851308599367273,
      "loss": 0.0011,
      "step": 92400
    },
    {
      "epoch": 26.58901351740006,
      "grad_norm": 0.00013458117609843612,
      "learning_rate": 0.0004682254817371297,
      "loss": 0.0038,
      "step": 92450
    },
    {
      "epoch": 26.603393730227207,
      "grad_norm": 0.00014400393411051482,
      "learning_rate": 0.00046793787748058676,
      "loss": 0.0043,
      "step": 92500
    },
    {
      "epoch": 26.617773943054356,
      "grad_norm": 0.00042240796028636396,
      "learning_rate": 0.00046765027322404377,
      "loss": 0.0025,
      "step": 92550
    },
    {
      "epoch": 26.63215415588151,
      "grad_norm": 0.0005856936913914979,
      "learning_rate": 0.0004673626689675007,
      "loss": 0.0008,
      "step": 92600
    },
    {
      "epoch": 26.646534368708657,
      "grad_norm": 8.758780313655734e-05,
      "learning_rate": 0.00046707506471095774,
      "loss": 0.0006,
      "step": 92650
    },
    {
      "epoch": 26.660914581535806,
      "grad_norm": 3.246230699005537e-05,
      "learning_rate": 0.00046678746045441475,
      "loss": 0.0015,
      "step": 92700
    },
    {
      "epoch": 26.675294794362955,
      "grad_norm": 0.011581228114664555,
      "learning_rate": 0.0004664998561978717,
      "loss": 0.0017,
      "step": 92750
    },
    {
      "epoch": 26.689675007190107,
      "grad_norm": 8.087705646175891e-05,
      "learning_rate": 0.0004662122519413288,
      "loss": 0.0024,
      "step": 92800
    },
    {
      "epoch": 26.704055220017256,
      "grad_norm": 8.014755439944565e-05,
      "learning_rate": 0.0004659246476847858,
      "loss": 0.0023,
      "step": 92850
    },
    {
      "epoch": 26.718435432844405,
      "grad_norm": 0.0007414231076836586,
      "learning_rate": 0.00046563704342824275,
      "loss": 0.0025,
      "step": 92900
    },
    {
      "epoch": 26.732815645671558,
      "grad_norm": 8.99577426025644e-05,
      "learning_rate": 0.00046534943917169976,
      "loss": 0.002,
      "step": 92950
    },
    {
      "epoch": 26.747195858498706,
      "grad_norm": 0.0009518138249404728,
      "learning_rate": 0.00046506183491515677,
      "loss": 0.001,
      "step": 93000
    },
    {
      "epoch": 26.761576071325855,
      "grad_norm": 0.014959204941987991,
      "learning_rate": 0.00046477423065861373,
      "loss": 0.0022,
      "step": 93050
    },
    {
      "epoch": 26.775956284153004,
      "grad_norm": 0.001420102664269507,
      "learning_rate": 0.00046448662640207074,
      "loss": 0.0019,
      "step": 93100
    },
    {
      "epoch": 26.790336496980157,
      "grad_norm": 0.001189386355690658,
      "learning_rate": 0.0004641990221455278,
      "loss": 0.0024,
      "step": 93150
    },
    {
      "epoch": 26.804716709807305,
      "grad_norm": 0.046548694372177124,
      "learning_rate": 0.00046391141788898477,
      "loss": 0.0031,
      "step": 93200
    },
    {
      "epoch": 26.819096922634454,
      "grad_norm": 0.011433981359004974,
      "learning_rate": 0.0004636238136324418,
      "loss": 0.0029,
      "step": 93250
    },
    {
      "epoch": 26.833477135461607,
      "grad_norm": 0.003823724575340748,
      "learning_rate": 0.0004633362093758988,
      "loss": 0.0013,
      "step": 93300
    },
    {
      "epoch": 26.847857348288755,
      "grad_norm": 0.00012051202065777034,
      "learning_rate": 0.00046304860511935575,
      "loss": 0.0035,
      "step": 93350
    },
    {
      "epoch": 26.862237561115904,
      "grad_norm": 0.0006672962917946279,
      "learning_rate": 0.00046276100086281276,
      "loss": 0.0014,
      "step": 93400
    },
    {
      "epoch": 26.876617773943053,
      "grad_norm": 0.006845468655228615,
      "learning_rate": 0.00046247339660626983,
      "loss": 0.0011,
      "step": 93450
    },
    {
      "epoch": 26.890997986770206,
      "grad_norm": 0.0014017227804288268,
      "learning_rate": 0.0004621857923497268,
      "loss": 0.0016,
      "step": 93500
    },
    {
      "epoch": 26.905378199597354,
      "grad_norm": 0.0001472301228204742,
      "learning_rate": 0.0004618981880931838,
      "loss": 0.0024,
      "step": 93550
    },
    {
      "epoch": 26.919758412424503,
      "grad_norm": 3.383548391866498e-05,
      "learning_rate": 0.0004616105838366408,
      "loss": 0.0015,
      "step": 93600
    },
    {
      "epoch": 26.934138625251652,
      "grad_norm": 0.0020438700448721647,
      "learning_rate": 0.00046132297958009777,
      "loss": 0.002,
      "step": 93650
    },
    {
      "epoch": 26.948518838078805,
      "grad_norm": 0.028103047981858253,
      "learning_rate": 0.0004610353753235548,
      "loss": 0.0033,
      "step": 93700
    },
    {
      "epoch": 26.962899050905953,
      "grad_norm": 0.0001394198916386813,
      "learning_rate": 0.00046074777106701185,
      "loss": 0.0044,
      "step": 93750
    },
    {
      "epoch": 26.977279263733102,
      "grad_norm": 0.0001695569953881204,
      "learning_rate": 0.0004604601668104688,
      "loss": 0.0018,
      "step": 93800
    },
    {
      "epoch": 26.991659476560255,
      "grad_norm": 0.00013041991041973233,
      "learning_rate": 0.0004601725625539258,
      "loss": 0.0004,
      "step": 93850
    },
    {
      "epoch": 27.0,
      "eval_loss": 0.010195691138505936,
      "eval_runtime": 18.2215,
      "eval_samples_per_second": 2618.827,
      "eval_steps_per_second": 40.941,
      "step": 93879
    },
    {
      "epoch": 27.006039689387404,
      "grad_norm": 0.10950540006160736,
      "learning_rate": 0.00045988495829738283,
      "loss": 0.003,
      "step": 93900
    },
    {
      "epoch": 27.020419902214552,
      "grad_norm": 0.0017195908585563302,
      "learning_rate": 0.0004595973540408398,
      "loss": 0.0026,
      "step": 93950
    },
    {
      "epoch": 27.0348001150417,
      "grad_norm": 0.0004627854214049876,
      "learning_rate": 0.0004593097497842968,
      "loss": 0.0035,
      "step": 94000
    },
    {
      "epoch": 27.049180327868854,
      "grad_norm": 0.000524404866155237,
      "learning_rate": 0.00045902214552775387,
      "loss": 0.0005,
      "step": 94050
    },
    {
      "epoch": 27.063560540696002,
      "grad_norm": 6.311148899840191e-05,
      "learning_rate": 0.0004587345412712108,
      "loss": 0.0011,
      "step": 94100
    },
    {
      "epoch": 27.07794075352315,
      "grad_norm": 5.931345003773458e-05,
      "learning_rate": 0.00045844693701466784,
      "loss": 0.0024,
      "step": 94150
    },
    {
      "epoch": 27.092320966350304,
      "grad_norm": 0.00010016118176281452,
      "learning_rate": 0.00045815933275812485,
      "loss": 0.0026,
      "step": 94200
    },
    {
      "epoch": 27.106701179177453,
      "grad_norm": 0.006568511016666889,
      "learning_rate": 0.0004578717285015818,
      "loss": 0.0045,
      "step": 94250
    },
    {
      "epoch": 27.1210813920046,
      "grad_norm": 0.0002652309776749462,
      "learning_rate": 0.0004575841242450388,
      "loss": 0.0037,
      "step": 94300
    },
    {
      "epoch": 27.13546160483175,
      "grad_norm": 0.013404064811766148,
      "learning_rate": 0.0004572965199884959,
      "loss": 0.0014,
      "step": 94350
    },
    {
      "epoch": 27.149841817658903,
      "grad_norm": 0.00033470356720499694,
      "learning_rate": 0.00045700891573195284,
      "loss": 0.0037,
      "step": 94400
    },
    {
      "epoch": 27.16422203048605,
      "grad_norm": 0.0002279884647578001,
      "learning_rate": 0.00045672131147540986,
      "loss": 0.0015,
      "step": 94450
    },
    {
      "epoch": 27.1786022433132,
      "grad_norm": 0.00045430538011714816,
      "learning_rate": 0.00045643370721886687,
      "loss": 0.0004,
      "step": 94500
    },
    {
      "epoch": 27.19298245614035,
      "grad_norm": 6.436389230657369e-05,
      "learning_rate": 0.0004561461029623238,
      "loss": 0.0035,
      "step": 94550
    },
    {
      "epoch": 27.2073626689675,
      "grad_norm": 0.00010711505456129089,
      "learning_rate": 0.00045585849870578084,
      "loss": 0.0011,
      "step": 94600
    },
    {
      "epoch": 27.22174288179465,
      "grad_norm": 9.004617459140718e-05,
      "learning_rate": 0.0004555708944492379,
      "loss": 0.0027,
      "step": 94650
    },
    {
      "epoch": 27.2361230946218,
      "grad_norm": 0.00014950659533496946,
      "learning_rate": 0.00045528329019269486,
      "loss": 0.0048,
      "step": 94700
    },
    {
      "epoch": 27.25050330744895,
      "grad_norm": 0.01046866737306118,
      "learning_rate": 0.0004549956859361519,
      "loss": 0.0007,
      "step": 94750
    },
    {
      "epoch": 27.2648835202761,
      "grad_norm": 0.0001548779255244881,
      "learning_rate": 0.0004547080816796089,
      "loss": 0.0023,
      "step": 94800
    },
    {
      "epoch": 27.27926373310325,
      "grad_norm": 0.00025849099620245397,
      "learning_rate": 0.00045442047742306585,
      "loss": 0.0014,
      "step": 94850
    },
    {
      "epoch": 27.2936439459304,
      "grad_norm": 0.00044306690688245,
      "learning_rate": 0.00045413287316652286,
      "loss": 0.0029,
      "step": 94900
    },
    {
      "epoch": 27.30802415875755,
      "grad_norm": 0.00044430088018998504,
      "learning_rate": 0.0004538452689099799,
      "loss": 0.0049,
      "step": 94950
    },
    {
      "epoch": 27.3224043715847,
      "grad_norm": 0.0003234600299037993,
      "learning_rate": 0.0004535576646534369,
      "loss": 0.0031,
      "step": 95000
    },
    {
      "epoch": 27.33678458441185,
      "grad_norm": 7.41444164304994e-05,
      "learning_rate": 0.0004532700603968939,
      "loss": 0.0036,
      "step": 95050
    },
    {
      "epoch": 27.351164797239,
      "grad_norm": 0.0010730016510933638,
      "learning_rate": 0.0004529824561403509,
      "loss": 0.0033,
      "step": 95100
    },
    {
      "epoch": 27.36554501006615,
      "grad_norm": 0.00014401489170268178,
      "learning_rate": 0.00045269485188380787,
      "loss": 0.0026,
      "step": 95150
    },
    {
      "epoch": 27.3799252228933,
      "grad_norm": 0.008010940626263618,
      "learning_rate": 0.0004524072476272649,
      "loss": 0.0002,
      "step": 95200
    },
    {
      "epoch": 27.394305435720447,
      "grad_norm": 0.0002833661565091461,
      "learning_rate": 0.00045211964337072194,
      "loss": 0.0022,
      "step": 95250
    },
    {
      "epoch": 27.4086856485476,
      "grad_norm": 0.00016255705850198865,
      "learning_rate": 0.0004518320391141789,
      "loss": 0.0008,
      "step": 95300
    },
    {
      "epoch": 27.42306586137475,
      "grad_norm": 0.00029444388928823173,
      "learning_rate": 0.0004515444348576359,
      "loss": 0.0077,
      "step": 95350
    },
    {
      "epoch": 27.437446074201898,
      "grad_norm": 0.0005422816029749811,
      "learning_rate": 0.00045125683060109293,
      "loss": 0.0019,
      "step": 95400
    },
    {
      "epoch": 27.451826287029046,
      "grad_norm": 0.0002927374735008925,
      "learning_rate": 0.0004509692263445499,
      "loss": 0.0048,
      "step": 95450
    },
    {
      "epoch": 27.4662064998562,
      "grad_norm": 5.0304370233789086e-05,
      "learning_rate": 0.0004506816220880069,
      "loss": 0.0019,
      "step": 95500
    },
    {
      "epoch": 27.480586712683348,
      "grad_norm": 0.002673918381333351,
      "learning_rate": 0.00045039401783146396,
      "loss": 0.0018,
      "step": 95550
    },
    {
      "epoch": 27.494966925510496,
      "grad_norm": 0.0005737673491239548,
      "learning_rate": 0.0004501064135749209,
      "loss": 0.0005,
      "step": 95600
    },
    {
      "epoch": 27.50934713833765,
      "grad_norm": 0.0011281287297606468,
      "learning_rate": 0.00044981880931837793,
      "loss": 0.0009,
      "step": 95650
    },
    {
      "epoch": 27.523727351164798,
      "grad_norm": 0.0012124100467190146,
      "learning_rate": 0.00044953120506183495,
      "loss": 0.0031,
      "step": 95700
    },
    {
      "epoch": 27.538107563991947,
      "grad_norm": 2.8414897315087728e-05,
      "learning_rate": 0.0004492436008052919,
      "loss": 0.0036,
      "step": 95750
    },
    {
      "epoch": 27.552487776819095,
      "grad_norm": 0.00017036435019690543,
      "learning_rate": 0.0004489559965487489,
      "loss": 0.0035,
      "step": 95800
    },
    {
      "epoch": 27.566867989646248,
      "grad_norm": 4.540770532912575e-05,
      "learning_rate": 0.00044866839229220593,
      "loss": 0.0036,
      "step": 95850
    },
    {
      "epoch": 27.581248202473397,
      "grad_norm": 0.00013588066212832928,
      "learning_rate": 0.00044838078803566294,
      "loss": 0.0028,
      "step": 95900
    },
    {
      "epoch": 27.595628415300546,
      "grad_norm": 0.0005423259572125971,
      "learning_rate": 0.00044809318377911995,
      "loss": 0.0018,
      "step": 95950
    },
    {
      "epoch": 27.610008628127698,
      "grad_norm": 0.0035042252857238054,
      "learning_rate": 0.00044780557952257697,
      "loss": 0.0015,
      "step": 96000
    },
    {
      "epoch": 27.624388840954847,
      "grad_norm": 0.00014054788334760815,
      "learning_rate": 0.0004475179752660339,
      "loss": 0.0015,
      "step": 96050
    },
    {
      "epoch": 27.638769053781996,
      "grad_norm": 0.0002007696166401729,
      "learning_rate": 0.00044723037100949094,
      "loss": 0.0014,
      "step": 96100
    },
    {
      "epoch": 27.653149266609145,
      "grad_norm": 0.010582654736936092,
      "learning_rate": 0.00044694276675294795,
      "loss": 0.0058,
      "step": 96150
    },
    {
      "epoch": 27.667529479436297,
      "grad_norm": 0.009287850931286812,
      "learning_rate": 0.00044665516249640496,
      "loss": 0.0035,
      "step": 96200
    },
    {
      "epoch": 27.681909692263446,
      "grad_norm": 0.004486055113375187,
      "learning_rate": 0.000446367558239862,
      "loss": 0.0014,
      "step": 96250
    },
    {
      "epoch": 27.696289905090595,
      "grad_norm": 0.00020033979672007263,
      "learning_rate": 0.000446079953983319,
      "loss": 0.0016,
      "step": 96300
    },
    {
      "epoch": 27.710670117917743,
      "grad_norm": 9.954398410627618e-05,
      "learning_rate": 0.00044579234972677594,
      "loss": 0.0026,
      "step": 96350
    },
    {
      "epoch": 27.725050330744896,
      "grad_norm": 0.00028020923491567373,
      "learning_rate": 0.00044550474547023296,
      "loss": 0.0011,
      "step": 96400
    },
    {
      "epoch": 27.739430543572045,
      "grad_norm": 0.00017728112288750708,
      "learning_rate": 0.00044521714121368997,
      "loss": 0.0009,
      "step": 96450
    },
    {
      "epoch": 27.753810756399194,
      "grad_norm": 0.00011307783279335126,
      "learning_rate": 0.000444929536957147,
      "loss": 0.0066,
      "step": 96500
    },
    {
      "epoch": 27.768190969226346,
      "grad_norm": 0.012713269330561161,
      "learning_rate": 0.000444641932700604,
      "loss": 0.004,
      "step": 96550
    },
    {
      "epoch": 27.782571182053495,
      "grad_norm": 0.005325584672391415,
      "learning_rate": 0.000444354328444061,
      "loss": 0.0049,
      "step": 96600
    },
    {
      "epoch": 27.796951394880644,
      "grad_norm": 2.8618069336516783e-05,
      "learning_rate": 0.00044406672418751796,
      "loss": 0.0035,
      "step": 96650
    },
    {
      "epoch": 27.811331607707793,
      "grad_norm": 1.8404343791189604e-05,
      "learning_rate": 0.000443779119930975,
      "loss": 0.0025,
      "step": 96700
    },
    {
      "epoch": 27.825711820534945,
      "grad_norm": 0.0005524317384697497,
      "learning_rate": 0.000443491515674432,
      "loss": 0.0029,
      "step": 96750
    },
    {
      "epoch": 27.840092033362094,
      "grad_norm": 7.908120460342616e-05,
      "learning_rate": 0.000443203911417889,
      "loss": 0.0005,
      "step": 96800
    },
    {
      "epoch": 27.854472246189243,
      "grad_norm": 4.389230161905289e-05,
      "learning_rate": 0.000442916307161346,
      "loss": 0.0015,
      "step": 96850
    },
    {
      "epoch": 27.868852459016395,
      "grad_norm": 3.300997195765376e-05,
      "learning_rate": 0.000442628702904803,
      "loss": 0.0012,
      "step": 96900
    },
    {
      "epoch": 27.883232671843544,
      "grad_norm": 0.00011554241791600361,
      "learning_rate": 0.00044234109864826,
      "loss": 0.0009,
      "step": 96950
    },
    {
      "epoch": 27.897612884670693,
      "grad_norm": 0.00018642951908987015,
      "learning_rate": 0.000442053494391717,
      "loss": 0.0021,
      "step": 97000
    },
    {
      "epoch": 27.91199309749784,
      "grad_norm": 3.233970710425638e-05,
      "learning_rate": 0.000441765890135174,
      "loss": 0.0017,
      "step": 97050
    },
    {
      "epoch": 27.926373310324994,
      "grad_norm": 0.000735404493752867,
      "learning_rate": 0.000441478285878631,
      "loss": 0.0008,
      "step": 97100
    },
    {
      "epoch": 27.940753523152143,
      "grad_norm": 0.0010742582380771637,
      "learning_rate": 0.00044119068162208803,
      "loss": 0.0002,
      "step": 97150
    },
    {
      "epoch": 27.95513373597929,
      "grad_norm": 0.00032905579428188503,
      "learning_rate": 0.00044090307736554504,
      "loss": 0.0012,
      "step": 97200
    },
    {
      "epoch": 27.969513948806444,
      "grad_norm": 0.0005286778905428946,
      "learning_rate": 0.000440615473109002,
      "loss": 0.0011,
      "step": 97250
    },
    {
      "epoch": 27.983894161633593,
      "grad_norm": 0.00047489689313806593,
      "learning_rate": 0.000440327868852459,
      "loss": 0.0007,
      "step": 97300
    },
    {
      "epoch": 27.998274374460742,
      "grad_norm": 0.0004227411118336022,
      "learning_rate": 0.00044004026459591603,
      "loss": 0.0015,
      "step": 97350
    },
    {
      "epoch": 28.0,
      "eval_loss": 0.00998491421341896,
      "eval_runtime": 16.8243,
      "eval_samples_per_second": 2836.317,
      "eval_steps_per_second": 44.341,
      "step": 97356
    },
    {
      "epoch": 28.01265458728789,
      "grad_norm": 0.00015719774819444865,
      "learning_rate": 0.00043975266033937304,
      "loss": 0.0006,
      "step": 97400
    },
    {
      "epoch": 28.027034800115043,
      "grad_norm": 0.005097691901028156,
      "learning_rate": 0.00043946505608283005,
      "loss": 0.0021,
      "step": 97450
    },
    {
      "epoch": 28.041415012942192,
      "grad_norm": 7.563483814010397e-05,
      "learning_rate": 0.00043917745182628706,
      "loss": 0.0014,
      "step": 97500
    },
    {
      "epoch": 28.05579522576934,
      "grad_norm": 0.0036431339103728533,
      "learning_rate": 0.000438889847569744,
      "loss": 0.002,
      "step": 97550
    },
    {
      "epoch": 28.07017543859649,
      "grad_norm": 8.84466790012084e-05,
      "learning_rate": 0.00043860224331320103,
      "loss": 0.0009,
      "step": 97600
    },
    {
      "epoch": 28.084555651423642,
      "grad_norm": 0.00020974452490918338,
      "learning_rate": 0.00043831463905665805,
      "loss": 0.0034,
      "step": 97650
    },
    {
      "epoch": 28.09893586425079,
      "grad_norm": 0.00779716856777668,
      "learning_rate": 0.00043802703480011506,
      "loss": 0.0008,
      "step": 97700
    },
    {
      "epoch": 28.11331607707794,
      "grad_norm": 8.903209527488798e-05,
      "learning_rate": 0.00043773943054357207,
      "loss": 0.0016,
      "step": 97750
    },
    {
      "epoch": 28.127696289905092,
      "grad_norm": 0.00027900139684788883,
      "learning_rate": 0.0004374518262870291,
      "loss": 0.0022,
      "step": 97800
    },
    {
      "epoch": 28.14207650273224,
      "grad_norm": 0.00046943032066337764,
      "learning_rate": 0.00043716422203048604,
      "loss": 0.0053,
      "step": 97850
    },
    {
      "epoch": 28.15645671555939,
      "grad_norm": 0.0004305381153244525,
      "learning_rate": 0.00043687661777394305,
      "loss": 0.0018,
      "step": 97900
    },
    {
      "epoch": 28.17083692838654,
      "grad_norm": 0.052270323038101196,
      "learning_rate": 0.00043658901351740007,
      "loss": 0.0004,
      "step": 97950
    },
    {
      "epoch": 28.18521714121369,
      "grad_norm": 2.3920896637719125e-05,
      "learning_rate": 0.0004363014092608571,
      "loss": 0.0036,
      "step": 98000
    },
    {
      "epoch": 28.19959735404084,
      "grad_norm": 0.028539832681417465,
      "learning_rate": 0.0004360138050043141,
      "loss": 0.0025,
      "step": 98050
    },
    {
      "epoch": 28.21397756686799,
      "grad_norm": 0.0008224935736507177,
      "learning_rate": 0.0004357262007477711,
      "loss": 0.0046,
      "step": 98100
    },
    {
      "epoch": 28.228357779695138,
      "grad_norm": 0.0003081648319493979,
      "learning_rate": 0.00043543859649122806,
      "loss": 0.0023,
      "step": 98150
    },
    {
      "epoch": 28.24273799252229,
      "grad_norm": 7.506876136176288e-05,
      "learning_rate": 0.0004351509922346851,
      "loss": 0.002,
      "step": 98200
    },
    {
      "epoch": 28.25711820534944,
      "grad_norm": 0.0006358379032462835,
      "learning_rate": 0.0004348633879781421,
      "loss": 0.0022,
      "step": 98250
    },
    {
      "epoch": 28.271498418176588,
      "grad_norm": 0.007136605214327574,
      "learning_rate": 0.0004345757837215991,
      "loss": 0.0033,
      "step": 98300
    },
    {
      "epoch": 28.28587863100374,
      "grad_norm": 4.33396126027219e-05,
      "learning_rate": 0.0004342881794650561,
      "loss": 0.0005,
      "step": 98350
    },
    {
      "epoch": 28.30025884383089,
      "grad_norm": 0.019842548295855522,
      "learning_rate": 0.0004340005752085131,
      "loss": 0.0034,
      "step": 98400
    },
    {
      "epoch": 28.314639056658038,
      "grad_norm": 0.020539740100502968,
      "learning_rate": 0.0004337129709519701,
      "loss": 0.0016,
      "step": 98450
    },
    {
      "epoch": 28.329019269485187,
      "grad_norm": 5.7852696045301855e-05,
      "learning_rate": 0.0004334253666954271,
      "loss": 0.0025,
      "step": 98500
    },
    {
      "epoch": 28.34339948231234,
      "grad_norm": 7.349188672378659e-05,
      "learning_rate": 0.0004331377624388841,
      "loss": 0.002,
      "step": 98550
    },
    {
      "epoch": 28.357779695139488,
      "grad_norm": 6.884101458126679e-05,
      "learning_rate": 0.00043285015818234106,
      "loss": 0.0015,
      "step": 98600
    },
    {
      "epoch": 28.372159907966637,
      "grad_norm": 0.007465708535164595,
      "learning_rate": 0.00043256255392579813,
      "loss": 0.0018,
      "step": 98650
    },
    {
      "epoch": 28.38654012079379,
      "grad_norm": 0.004006430506706238,
      "learning_rate": 0.00043227494966925514,
      "loss": 0.0025,
      "step": 98700
    },
    {
      "epoch": 28.400920333620938,
      "grad_norm": 0.0009561615879647434,
      "learning_rate": 0.0004319873454127121,
      "loss": 0.0011,
      "step": 98750
    },
    {
      "epoch": 28.415300546448087,
      "grad_norm": 0.0008161233854480088,
      "learning_rate": 0.0004316997411561691,
      "loss": 0.0056,
      "step": 98800
    },
    {
      "epoch": 28.429680759275236,
      "grad_norm": 0.00017918167577590793,
      "learning_rate": 0.0004314121368996261,
      "loss": 0.0037,
      "step": 98850
    },
    {
      "epoch": 28.44406097210239,
      "grad_norm": 0.009367731399834156,
      "learning_rate": 0.0004311245326430831,
      "loss": 0.0023,
      "step": 98900
    },
    {
      "epoch": 28.458441184929537,
      "grad_norm": 0.00010237939568469301,
      "learning_rate": 0.00043083692838654015,
      "loss": 0.0008,
      "step": 98950
    },
    {
      "epoch": 28.472821397756686,
      "grad_norm": 0.00015445007011294365,
      "learning_rate": 0.00043054932412999716,
      "loss": 0.0031,
      "step": 99000
    },
    {
      "epoch": 28.48720161058384,
      "grad_norm": 0.00023004834656603634,
      "learning_rate": 0.0004302617198734541,
      "loss": 0.0018,
      "step": 99050
    },
    {
      "epoch": 28.501581823410987,
      "grad_norm": 0.00040503026684746146,
      "learning_rate": 0.00042997411561691113,
      "loss": 0.0012,
      "step": 99100
    },
    {
      "epoch": 28.515962036238136,
      "grad_norm": 0.0008266686927527189,
      "learning_rate": 0.00042968651136036814,
      "loss": 0.0016,
      "step": 99150
    },
    {
      "epoch": 28.530342249065285,
      "grad_norm": 0.00010520670184632763,
      "learning_rate": 0.0004293989071038251,
      "loss": 0.0007,
      "step": 99200
    },
    {
      "epoch": 28.544722461892437,
      "grad_norm": 0.0010420756880193949,
      "learning_rate": 0.00042911130284728217,
      "loss": 0.0019,
      "step": 99250
    },
    {
      "epoch": 28.559102674719586,
      "grad_norm": 0.013501537032425404,
      "learning_rate": 0.0004288236985907392,
      "loss": 0.003,
      "step": 99300
    },
    {
      "epoch": 28.573482887546735,
      "grad_norm": 8.397096098633483e-05,
      "learning_rate": 0.00042853609433419614,
      "loss": 0.002,
      "step": 99350
    },
    {
      "epoch": 28.587863100373884,
      "grad_norm": 0.0014229788212105632,
      "learning_rate": 0.00042824849007765315,
      "loss": 0.0035,
      "step": 99400
    },
    {
      "epoch": 28.602243313201036,
      "grad_norm": 0.008329560048878193,
      "learning_rate": 0.00042796088582111016,
      "loss": 0.0023,
      "step": 99450
    },
    {
      "epoch": 28.616623526028185,
      "grad_norm": 0.00021424198348540813,
      "learning_rate": 0.0004276732815645671,
      "loss": 0.004,
      "step": 99500
    },
    {
      "epoch": 28.631003738855334,
      "grad_norm": 0.0003758303355425596,
      "learning_rate": 0.0004273856773080242,
      "loss": 0.0043,
      "step": 99550
    },
    {
      "epoch": 28.645383951682486,
      "grad_norm": 0.010431854985654354,
      "learning_rate": 0.0004270980730514812,
      "loss": 0.0032,
      "step": 99600
    },
    {
      "epoch": 28.659764164509635,
      "grad_norm": 8.001653623068705e-05,
      "learning_rate": 0.00042681046879493816,
      "loss": 0.003,
      "step": 99650
    },
    {
      "epoch": 28.674144377336784,
      "grad_norm": 0.003518147859722376,
      "learning_rate": 0.00042652286453839517,
      "loss": 0.0007,
      "step": 99700
    },
    {
      "epoch": 28.688524590163933,
      "grad_norm": 0.013755099847912788,
      "learning_rate": 0.0004262352602818522,
      "loss": 0.0014,
      "step": 99750
    },
    {
      "epoch": 28.702904802991085,
      "grad_norm": 0.003841794328764081,
      "learning_rate": 0.00042594765602530914,
      "loss": 0.0027,
      "step": 99800
    },
    {
      "epoch": 28.717285015818234,
      "grad_norm": 0.00027690737624652684,
      "learning_rate": 0.0004256600517687662,
      "loss": 0.0019,
      "step": 99850
    },
    {
      "epoch": 28.731665228645383,
      "grad_norm": 4.4215674279257655e-05,
      "learning_rate": 0.0004253724475122232,
      "loss": 0.0029,
      "step": 99900
    },
    {
      "epoch": 28.746045441472535,
      "grad_norm": 0.0005960424314253032,
      "learning_rate": 0.0004250848432556802,
      "loss": 0.0014,
      "step": 99950
    },
    {
      "epoch": 28.760425654299684,
      "grad_norm": 0.0017518493114039302,
      "learning_rate": 0.0004247972389991372,
      "loss": 0.0031,
      "step": 100000
    },
    {
      "epoch": 28.774805867126833,
      "grad_norm": 0.012644501402974129,
      "learning_rate": 0.0004245096347425942,
      "loss": 0.0038,
      "step": 100050
    },
    {
      "epoch": 28.789186079953982,
      "grad_norm": 6.30614158581011e-05,
      "learning_rate": 0.00042422203048605116,
      "loss": 0.0016,
      "step": 100100
    },
    {
      "epoch": 28.803566292781134,
      "grad_norm": 0.049073051661252975,
      "learning_rate": 0.00042393442622950823,
      "loss": 0.0021,
      "step": 100150
    },
    {
      "epoch": 28.817946505608283,
      "grad_norm": 7.44932476663962e-05,
      "learning_rate": 0.00042364682197296524,
      "loss": 0.0018,
      "step": 100200
    },
    {
      "epoch": 28.832326718435432,
      "grad_norm": 0.0031341747380793095,
      "learning_rate": 0.0004233592177164222,
      "loss": 0.0037,
      "step": 100250
    },
    {
      "epoch": 28.84670693126258,
      "grad_norm": 0.005467911716550589,
      "learning_rate": 0.0004230716134598792,
      "loss": 0.0021,
      "step": 100300
    },
    {
      "epoch": 28.861087144089733,
      "grad_norm": 0.0004811014805454761,
      "learning_rate": 0.0004227840092033362,
      "loss": 0.0023,
      "step": 100350
    },
    {
      "epoch": 28.875467356916882,
      "grad_norm": 6.0847552958875895e-05,
      "learning_rate": 0.0004224964049467932,
      "loss": 0.0005,
      "step": 100400
    },
    {
      "epoch": 28.88984756974403,
      "grad_norm": 0.006392759270966053,
      "learning_rate": 0.00042220880069025025,
      "loss": 0.005,
      "step": 100450
    },
    {
      "epoch": 28.904227782571184,
      "grad_norm": 0.0021210010163486004,
      "learning_rate": 0.00042192119643370726,
      "loss": 0.0026,
      "step": 100500
    },
    {
      "epoch": 28.918607995398332,
      "grad_norm": 2.4211340132751502e-05,
      "learning_rate": 0.0004216335921771642,
      "loss": 0.0032,
      "step": 100550
    },
    {
      "epoch": 28.93298820822548,
      "grad_norm": 0.00317630497738719,
      "learning_rate": 0.00042134598792062123,
      "loss": 0.0007,
      "step": 100600
    },
    {
      "epoch": 28.94736842105263,
      "grad_norm": 0.005149192176759243,
      "learning_rate": 0.00042105838366407824,
      "loss": 0.0042,
      "step": 100650
    },
    {
      "epoch": 28.961748633879782,
      "grad_norm": 0.00017538457177579403,
      "learning_rate": 0.0004207707794075352,
      "loss": 0.006,
      "step": 100700
    },
    {
      "epoch": 28.97612884670693,
      "grad_norm": 0.000138730276376009,
      "learning_rate": 0.00042048317515099227,
      "loss": 0.0016,
      "step": 100750
    },
    {
      "epoch": 28.99050905953408,
      "grad_norm": 3.067658326472156e-05,
      "learning_rate": 0.0004201955708944493,
      "loss": 0.0025,
      "step": 100800
    },
    {
      "epoch": 29.0,
      "eval_loss": 0.009930758737027645,
      "eval_runtime": 17.6934,
      "eval_samples_per_second": 2696.99,
      "eval_steps_per_second": 42.163,
      "step": 100833
    },
    {
      "epoch": 29.004889272361233,
      "grad_norm": 1.8888549675466493e-05,
      "learning_rate": 0.00041990796663790624,
      "loss": 0.0024,
      "step": 100850
    },
    {
      "epoch": 29.01926948518838,
      "grad_norm": 0.014177733100950718,
      "learning_rate": 0.00041962036238136325,
      "loss": 0.0024,
      "step": 100900
    },
    {
      "epoch": 29.03364969801553,
      "grad_norm": 0.01822500489652157,
      "learning_rate": 0.00041933275812482026,
      "loss": 0.0005,
      "step": 100950
    },
    {
      "epoch": 29.04802991084268,
      "grad_norm": 5.190278170630336e-05,
      "learning_rate": 0.0004190451538682772,
      "loss": 0.0006,
      "step": 101000
    },
    {
      "epoch": 29.06241012366983,
      "grad_norm": 0.00019042217172682285,
      "learning_rate": 0.0004187575496117343,
      "loss": 0.0032,
      "step": 101050
    },
    {
      "epoch": 29.07679033649698,
      "grad_norm": 3.5700704756891355e-05,
      "learning_rate": 0.0004184699453551913,
      "loss": 0.0033,
      "step": 101100
    },
    {
      "epoch": 29.09117054932413,
      "grad_norm": 2.810469777614344e-05,
      "learning_rate": 0.00041818234109864826,
      "loss": 0.0042,
      "step": 101150
    },
    {
      "epoch": 29.105550762151278,
      "grad_norm": 0.00019091804279014468,
      "learning_rate": 0.00041789473684210527,
      "loss": 0.0019,
      "step": 101200
    },
    {
      "epoch": 29.11993097497843,
      "grad_norm": 0.0004819688037969172,
      "learning_rate": 0.0004176071325855623,
      "loss": 0.0055,
      "step": 101250
    },
    {
      "epoch": 29.13431118780558,
      "grad_norm": 6.588186079170555e-05,
      "learning_rate": 0.00041731952832901924,
      "loss": 0.001,
      "step": 101300
    },
    {
      "epoch": 29.14869140063273,
      "grad_norm": 0.00015644638915546238,
      "learning_rate": 0.00041703192407247625,
      "loss": 0.0021,
      "step": 101350
    },
    {
      "epoch": 29.16307161345988,
      "grad_norm": 0.0004324222099967301,
      "learning_rate": 0.0004167443198159333,
      "loss": 0.002,
      "step": 101400
    },
    {
      "epoch": 29.17745182628703,
      "grad_norm": 0.00016488757682964206,
      "learning_rate": 0.0004164567155593903,
      "loss": 0.0028,
      "step": 101450
    },
    {
      "epoch": 29.19183203911418,
      "grad_norm": 0.002232795348390937,
      "learning_rate": 0.0004161691113028473,
      "loss": 0.0059,
      "step": 101500
    },
    {
      "epoch": 29.206212251941327,
      "grad_norm": 0.005195378325879574,
      "learning_rate": 0.0004158815070463043,
      "loss": 0.0015,
      "step": 101550
    },
    {
      "epoch": 29.22059246476848,
      "grad_norm": 0.0006520558963529766,
      "learning_rate": 0.00041559390278976126,
      "loss": 0.0051,
      "step": 101600
    },
    {
      "epoch": 29.23497267759563,
      "grad_norm": 0.01145865861326456,
      "learning_rate": 0.00041530629853321827,
      "loss": 0.0021,
      "step": 101650
    },
    {
      "epoch": 29.249352890422777,
      "grad_norm": 0.0006700691883452237,
      "learning_rate": 0.00041501869427667534,
      "loss": 0.0011,
      "step": 101700
    },
    {
      "epoch": 29.26373310324993,
      "grad_norm": 0.0041597806848585606,
      "learning_rate": 0.0004147310900201323,
      "loss": 0.0014,
      "step": 101750
    },
    {
      "epoch": 29.27811331607708,
      "grad_norm": 0.012901621870696545,
      "learning_rate": 0.0004144434857635893,
      "loss": 0.0061,
      "step": 101800
    },
    {
      "epoch": 29.292493528904227,
      "grad_norm": 0.00018341974646318704,
      "learning_rate": 0.0004141558815070463,
      "loss": 0.0021,
      "step": 101850
    },
    {
      "epoch": 29.306873741731376,
      "grad_norm": 2.6728721422841772e-05,
      "learning_rate": 0.0004138682772505033,
      "loss": 0.001,
      "step": 101900
    },
    {
      "epoch": 29.32125395455853,
      "grad_norm": 0.014779060147702694,
      "learning_rate": 0.0004135806729939603,
      "loss": 0.002,
      "step": 101950
    },
    {
      "epoch": 29.335634167385678,
      "grad_norm": 0.053817059844732285,
      "learning_rate": 0.00041329306873741736,
      "loss": 0.0034,
      "step": 102000
    },
    {
      "epoch": 29.350014380212826,
      "grad_norm": 0.009698636829853058,
      "learning_rate": 0.0004130054644808743,
      "loss": 0.0008,
      "step": 102050
    },
    {
      "epoch": 29.364394593039975,
      "grad_norm": 7.280801219167188e-05,
      "learning_rate": 0.00041271786022433133,
      "loss": 0.0039,
      "step": 102100
    },
    {
      "epoch": 29.378774805867128,
      "grad_norm": 0.024441499263048172,
      "learning_rate": 0.00041243025596778834,
      "loss": 0.0005,
      "step": 102150
    },
    {
      "epoch": 29.393155018694276,
      "grad_norm": 0.00022280322446022183,
      "learning_rate": 0.0004121426517112453,
      "loss": 0.0018,
      "step": 102200
    },
    {
      "epoch": 29.407535231521425,
      "grad_norm": 5.504448199644685e-05,
      "learning_rate": 0.0004118550474547023,
      "loss": 0.004,
      "step": 102250
    },
    {
      "epoch": 29.421915444348578,
      "grad_norm": 0.007751100230962038,
      "learning_rate": 0.0004115674431981594,
      "loss": 0.001,
      "step": 102300
    },
    {
      "epoch": 29.436295657175727,
      "grad_norm": 4.625854126061313e-05,
      "learning_rate": 0.00041127983894161634,
      "loss": 0.0012,
      "step": 102350
    },
    {
      "epoch": 29.450675870002875,
      "grad_norm": 7.130314770620316e-05,
      "learning_rate": 0.00041099223468507335,
      "loss": 0.0016,
      "step": 102400
    },
    {
      "epoch": 29.465056082830024,
      "grad_norm": 0.020930824801325798,
      "learning_rate": 0.00041070463042853036,
      "loss": 0.0061,
      "step": 102450
    },
    {
      "epoch": 29.479436295657177,
      "grad_norm": 0.010843916796147823,
      "learning_rate": 0.0004104170261719873,
      "loss": 0.0016,
      "step": 102500
    },
    {
      "epoch": 29.493816508484326,
      "grad_norm": 3.720915628946386e-05,
      "learning_rate": 0.00041012942191544433,
      "loss": 0.002,
      "step": 102550
    },
    {
      "epoch": 29.508196721311474,
      "grad_norm": 0.00023847118427511305,
      "learning_rate": 0.0004098418176589014,
      "loss": 0.0017,
      "step": 102600
    },
    {
      "epoch": 29.522576934138627,
      "grad_norm": 9.226710244547576e-05,
      "learning_rate": 0.00040955421340235835,
      "loss": 0.0024,
      "step": 102650
    },
    {
      "epoch": 29.536957146965776,
      "grad_norm": 7.42004849598743e-05,
      "learning_rate": 0.00040926660914581537,
      "loss": 0.001,
      "step": 102700
    },
    {
      "epoch": 29.551337359792925,
      "grad_norm": 0.002844297094270587,
      "learning_rate": 0.0004089790048892724,
      "loss": 0.0021,
      "step": 102750
    },
    {
      "epoch": 29.565717572620073,
      "grad_norm": 0.019092367962002754,
      "learning_rate": 0.00040869140063272934,
      "loss": 0.003,
      "step": 102800
    },
    {
      "epoch": 29.580097785447226,
      "grad_norm": 0.0033487332984805107,
      "learning_rate": 0.00040840379637618635,
      "loss": 0.0024,
      "step": 102850
    },
    {
      "epoch": 29.594477998274375,
      "grad_norm": 0.015311404131352901,
      "learning_rate": 0.0004081161921196434,
      "loss": 0.0029,
      "step": 102900
    },
    {
      "epoch": 29.608858211101523,
      "grad_norm": 0.009082846343517303,
      "learning_rate": 0.0004078285878631004,
      "loss": 0.0015,
      "step": 102950
    },
    {
      "epoch": 29.623238423928676,
      "grad_norm": 0.006776877213269472,
      "learning_rate": 0.0004075409836065574,
      "loss": 0.0002,
      "step": 103000
    },
    {
      "epoch": 29.637618636755825,
      "grad_norm": 0.0017654176335781813,
      "learning_rate": 0.0004072533793500144,
      "loss": 0.0017,
      "step": 103050
    },
    {
      "epoch": 29.651998849582974,
      "grad_norm": 0.00017360327183268964,
      "learning_rate": 0.00040696577509347136,
      "loss": 0.0018,
      "step": 103100
    },
    {
      "epoch": 29.666379062410122,
      "grad_norm": 2.4691767976037227e-05,
      "learning_rate": 0.00040667817083692837,
      "loss": 0.0039,
      "step": 103150
    },
    {
      "epoch": 29.680759275237275,
      "grad_norm": 0.0037571110296994448,
      "learning_rate": 0.00040639056658038544,
      "loss": 0.0024,
      "step": 103200
    },
    {
      "epoch": 29.695139488064424,
      "grad_norm": 0.0012385704321786761,
      "learning_rate": 0.0004061029623238424,
      "loss": 0.002,
      "step": 103250
    },
    {
      "epoch": 29.709519700891573,
      "grad_norm": 0.0005463332054205239,
      "learning_rate": 0.0004058153580672994,
      "loss": 0.0013,
      "step": 103300
    },
    {
      "epoch": 29.72389991371872,
      "grad_norm": 0.001459387713111937,
      "learning_rate": 0.0004055277538107564,
      "loss": 0.0021,
      "step": 103350
    },
    {
      "epoch": 29.738280126545874,
      "grad_norm": 3.3709558920236304e-05,
      "learning_rate": 0.0004052401495542134,
      "loss": 0.0036,
      "step": 103400
    },
    {
      "epoch": 29.752660339373023,
      "grad_norm": 0.02177872695028782,
      "learning_rate": 0.0004049525452976704,
      "loss": 0.0047,
      "step": 103450
    },
    {
      "epoch": 29.76704055220017,
      "grad_norm": 3.950982863898389e-05,
      "learning_rate": 0.00040466494104112746,
      "loss": 0.0045,
      "step": 103500
    },
    {
      "epoch": 29.781420765027324,
      "grad_norm": 0.00010107815614901483,
      "learning_rate": 0.0004043773367845844,
      "loss": 0.0011,
      "step": 103550
    },
    {
      "epoch": 29.795800977854473,
      "grad_norm": 0.00012641007197089493,
      "learning_rate": 0.0004040897325280414,
      "loss": 0.0012,
      "step": 103600
    },
    {
      "epoch": 29.81018119068162,
      "grad_norm": 0.0011422643437981606,
      "learning_rate": 0.00040380212827149844,
      "loss": 0.005,
      "step": 103650
    },
    {
      "epoch": 29.82456140350877,
      "grad_norm": 0.007088990416377783,
      "learning_rate": 0.0004035145240149554,
      "loss": 0.0028,
      "step": 103700
    },
    {
      "epoch": 29.838941616335923,
      "grad_norm": 0.0039598080329597,
      "learning_rate": 0.0004032269197584124,
      "loss": 0.0026,
      "step": 103750
    },
    {
      "epoch": 29.85332182916307,
      "grad_norm": 3.089303208980709e-05,
      "learning_rate": 0.0004029393155018695,
      "loss": 0.0008,
      "step": 103800
    },
    {
      "epoch": 29.86770204199022,
      "grad_norm": 0.014021936804056168,
      "learning_rate": 0.00040265171124532643,
      "loss": 0.001,
      "step": 103850
    },
    {
      "epoch": 29.882082254817373,
      "grad_norm": 3.965272844652645e-05,
      "learning_rate": 0.00040236410698878345,
      "loss": 0.0023,
      "step": 103900
    },
    {
      "epoch": 29.896462467644522,
      "grad_norm": 0.008684749715030193,
      "learning_rate": 0.00040207650273224046,
      "loss": 0.0028,
      "step": 103950
    },
    {
      "epoch": 29.91084268047167,
      "grad_norm": 0.00803772546350956,
      "learning_rate": 0.0004017888984756974,
      "loss": 0.005,
      "step": 104000
    },
    {
      "epoch": 29.92522289329882,
      "grad_norm": 4.053530574310571e-05,
      "learning_rate": 0.00040150129421915443,
      "loss": 0.0014,
      "step": 104050
    },
    {
      "epoch": 29.939603106125972,
      "grad_norm": 0.0005829267902299762,
      "learning_rate": 0.00040121368996261144,
      "loss": 0.001,
      "step": 104100
    },
    {
      "epoch": 29.95398331895312,
      "grad_norm": 4.130592060391791e-05,
      "learning_rate": 0.00040092608570606845,
      "loss": 0.001,
      "step": 104150
    },
    {
      "epoch": 29.96836353178027,
      "grad_norm": 0.0008889808668754995,
      "learning_rate": 0.00040063848144952546,
      "loss": 0.003,
      "step": 104200
    },
    {
      "epoch": 29.98274374460742,
      "grad_norm": 0.027412980794906616,
      "learning_rate": 0.0004003508771929825,
      "loss": 0.0005,
      "step": 104250
    },
    {
      "epoch": 29.99712395743457,
      "grad_norm": 0.002643438521772623,
      "learning_rate": 0.00040006327293643944,
      "loss": 0.0018,
      "step": 104300
    },
    {
      "epoch": 30.0,
      "eval_loss": 0.010259132832288742,
      "eval_runtime": 16.9421,
      "eval_samples_per_second": 2816.592,
      "eval_steps_per_second": 44.032,
      "step": 104310
    },
    {
      "epoch": 30.01150417026172,
      "grad_norm": 8.31978177302517e-05,
      "learning_rate": 0.00039977566867989645,
      "loss": 0.0006,
      "step": 104350
    },
    {
      "epoch": 30.02588438308887,
      "grad_norm": 7.585150888189673e-05,
      "learning_rate": 0.00039948806442335346,
      "loss": 0.005,
      "step": 104400
    },
    {
      "epoch": 30.04026459591602,
      "grad_norm": 1.4829704923613463e-05,
      "learning_rate": 0.00039920046016681047,
      "loss": 0.0009,
      "step": 104450
    },
    {
      "epoch": 30.05464480874317,
      "grad_norm": 0.00018086438649334013,
      "learning_rate": 0.0003989128559102675,
      "loss": 0.0011,
      "step": 104500
    },
    {
      "epoch": 30.06902502157032,
      "grad_norm": 8.168760541593656e-05,
      "learning_rate": 0.0003986252516537245,
      "loss": 0.0034,
      "step": 104550
    },
    {
      "epoch": 30.083405234397468,
      "grad_norm": 0.0003837563854176551,
      "learning_rate": 0.00039833764739718145,
      "loss": 0.0027,
      "step": 104600
    },
    {
      "epoch": 30.09778544722462,
      "grad_norm": 0.0035834740847349167,
      "learning_rate": 0.00039805004314063847,
      "loss": 0.001,
      "step": 104650
    },
    {
      "epoch": 30.11216566005177,
      "grad_norm": 3.0574301490560174e-05,
      "learning_rate": 0.0003977624388840955,
      "loss": 0.0012,
      "step": 104700
    },
    {
      "epoch": 30.126545872878918,
      "grad_norm": 8.499350951751694e-05,
      "learning_rate": 0.0003974748346275525,
      "loss": 0.0018,
      "step": 104750
    },
    {
      "epoch": 30.14092608570607,
      "grad_norm": 4.5853692427044734e-05,
      "learning_rate": 0.0003971872303710095,
      "loss": 0.0042,
      "step": 104800
    },
    {
      "epoch": 30.15530629853322,
      "grad_norm": 3.678470238810405e-05,
      "learning_rate": 0.0003968996261144665,
      "loss": 0.0041,
      "step": 104850
    },
    {
      "epoch": 30.169686511360368,
      "grad_norm": 0.017411306500434875,
      "learning_rate": 0.0003966120218579235,
      "loss": 0.0008,
      "step": 104900
    },
    {
      "epoch": 30.184066724187517,
      "grad_norm": 0.0029507610015571117,
      "learning_rate": 0.0003963244176013805,
      "loss": 0.001,
      "step": 104950
    },
    {
      "epoch": 30.19844693701467,
      "grad_norm": 0.015252179466187954,
      "learning_rate": 0.0003960368133448375,
      "loss": 0.003,
      "step": 105000
    },
    {
      "epoch": 30.212827149841818,
      "grad_norm": 0.0036368174478411674,
      "learning_rate": 0.0003957492090882945,
      "loss": 0.0024,
      "step": 105050
    },
    {
      "epoch": 30.227207362668967,
      "grad_norm": 4.966091728419997e-05,
      "learning_rate": 0.0003954616048317515,
      "loss": 0.0019,
      "step": 105100
    },
    {
      "epoch": 30.241587575496116,
      "grad_norm": 0.005086591467261314,
      "learning_rate": 0.00039517400057520854,
      "loss": 0.005,
      "step": 105150
    },
    {
      "epoch": 30.255967788323268,
      "grad_norm": 1.9430484826443717e-05,
      "learning_rate": 0.0003948863963186655,
      "loss": 0.0024,
      "step": 105200
    },
    {
      "epoch": 30.270348001150417,
      "grad_norm": 3.2195206586038694e-05,
      "learning_rate": 0.0003945987920621225,
      "loss": 0.0025,
      "step": 105250
    },
    {
      "epoch": 30.284728213977566,
      "grad_norm": 1.7183778254548088e-05,
      "learning_rate": 0.0003943111878055795,
      "loss": 0.0037,
      "step": 105300
    },
    {
      "epoch": 30.299108426804718,
      "grad_norm": 0.00020820916688535362,
      "learning_rate": 0.00039402358354903653,
      "loss": 0.0015,
      "step": 105350
    },
    {
      "epoch": 30.313488639631867,
      "grad_norm": 2.6932304535876028e-05,
      "learning_rate": 0.00039373597929249354,
      "loss": 0.0049,
      "step": 105400
    },
    {
      "epoch": 30.327868852459016,
      "grad_norm": 0.00015418793191201985,
      "learning_rate": 0.00039344837503595056,
      "loss": 0.0025,
      "step": 105450
    },
    {
      "epoch": 30.342249065286165,
      "grad_norm": 0.0030540297739207745,
      "learning_rate": 0.0003931607707794075,
      "loss": 0.0059,
      "step": 105500
    },
    {
      "epoch": 30.356629278113317,
      "grad_norm": 4.414487921167165e-05,
      "learning_rate": 0.0003928731665228645,
      "loss": 0.0016,
      "step": 105550
    },
    {
      "epoch": 30.371009490940466,
      "grad_norm": 0.0001366953074466437,
      "learning_rate": 0.00039258556226632154,
      "loss": 0.0009,
      "step": 105600
    },
    {
      "epoch": 30.385389703767615,
      "grad_norm": 0.0008562836446799338,
      "learning_rate": 0.00039229795800977855,
      "loss": 0.004,
      "step": 105650
    },
    {
      "epoch": 30.399769916594767,
      "grad_norm": 7.672831998206675e-05,
      "learning_rate": 0.00039201035375323556,
      "loss": 0.0027,
      "step": 105700
    },
    {
      "epoch": 30.414150129421916,
      "grad_norm": 0.004793088883161545,
      "learning_rate": 0.0003917227494966926,
      "loss": 0.0023,
      "step": 105750
    },
    {
      "epoch": 30.428530342249065,
      "grad_norm": 0.0002087283064611256,
      "learning_rate": 0.00039143514524014953,
      "loss": 0.0058,
      "step": 105800
    },
    {
      "epoch": 30.442910555076214,
      "grad_norm": 0.00011490416363812983,
      "learning_rate": 0.00039114754098360655,
      "loss": 0.002,
      "step": 105850
    },
    {
      "epoch": 30.457290767903366,
      "grad_norm": 0.021831203252077103,
      "learning_rate": 0.00039085993672706356,
      "loss": 0.001,
      "step": 105900
    },
    {
      "epoch": 30.471670980730515,
      "grad_norm": 0.0038082068786025047,
      "learning_rate": 0.0003905723324705206,
      "loss": 0.0008,
      "step": 105950
    },
    {
      "epoch": 30.486051193557664,
      "grad_norm": 6.113514245953411e-05,
      "learning_rate": 0.0003902847282139776,
      "loss": 0.0032,
      "step": 106000
    },
    {
      "epoch": 30.500431406384813,
      "grad_norm": 0.001545158913359046,
      "learning_rate": 0.0003899971239574346,
      "loss": 0.0022,
      "step": 106050
    },
    {
      "epoch": 30.514811619211965,
      "grad_norm": 7.184046262409538e-05,
      "learning_rate": 0.00038970951970089155,
      "loss": 0.0004,
      "step": 106100
    },
    {
      "epoch": 30.529191832039114,
      "grad_norm": 0.010945065878331661,
      "learning_rate": 0.00038942191544434856,
      "loss": 0.002,
      "step": 106150
    },
    {
      "epoch": 30.543572044866263,
      "grad_norm": 0.02305430732667446,
      "learning_rate": 0.0003891343111878056,
      "loss": 0.0007,
      "step": 106200
    },
    {
      "epoch": 30.557952257693415,
      "grad_norm": 2.4971881430246867e-05,
      "learning_rate": 0.00038884670693126264,
      "loss": 0.0027,
      "step": 106250
    },
    {
      "epoch": 30.572332470520564,
      "grad_norm": 0.0020911842584609985,
      "learning_rate": 0.0003885591026747196,
      "loss": 0.003,
      "step": 106300
    },
    {
      "epoch": 30.586712683347713,
      "grad_norm": 0.00034615120966918766,
      "learning_rate": 0.0003882714984181766,
      "loss": 0.0024,
      "step": 106350
    },
    {
      "epoch": 30.601092896174862,
      "grad_norm": 7.742552406853065e-05,
      "learning_rate": 0.0003879838941616336,
      "loss": 0.0011,
      "step": 106400
    },
    {
      "epoch": 30.615473109002014,
      "grad_norm": 0.0001559947122586891,
      "learning_rate": 0.0003876962899050906,
      "loss": 0.0033,
      "step": 106450
    },
    {
      "epoch": 30.629853321829163,
      "grad_norm": 0.00029471825109794736,
      "learning_rate": 0.0003874086856485476,
      "loss": 0.0006,
      "step": 106500
    },
    {
      "epoch": 30.644233534656312,
      "grad_norm": 0.0005884785205125809,
      "learning_rate": 0.00038712108139200466,
      "loss": 0.0029,
      "step": 106550
    },
    {
      "epoch": 30.658613747483464,
      "grad_norm": 0.0001755606645019725,
      "learning_rate": 0.0003868334771354616,
      "loss": 0.0007,
      "step": 106600
    },
    {
      "epoch": 30.672993960310613,
      "grad_norm": 9.346454316983e-05,
      "learning_rate": 0.00038654587287891863,
      "loss": 0.0036,
      "step": 106650
    },
    {
      "epoch": 30.687374173137762,
      "grad_norm": 0.00010877096065087244,
      "learning_rate": 0.00038625826862237565,
      "loss": 0.0028,
      "step": 106700
    },
    {
      "epoch": 30.70175438596491,
      "grad_norm": 0.0002460413670632988,
      "learning_rate": 0.0003859706643658326,
      "loss": 0.0032,
      "step": 106750
    },
    {
      "epoch": 30.716134598792063,
      "grad_norm": 6.749250314896926e-05,
      "learning_rate": 0.0003856830601092896,
      "loss": 0.0036,
      "step": 106800
    },
    {
      "epoch": 30.730514811619212,
      "grad_norm": 4.314458055887371e-05,
      "learning_rate": 0.0003853954558527466,
      "loss": 0.0006,
      "step": 106850
    },
    {
      "epoch": 30.74489502444636,
      "grad_norm": 1.841743323893752e-05,
      "learning_rate": 0.00038510785159620364,
      "loss": 0.0015,
      "step": 106900
    },
    {
      "epoch": 30.759275237273513,
      "grad_norm": 5.569490895140916e-05,
      "learning_rate": 0.00038482024733966065,
      "loss": 0.0034,
      "step": 106950
    },
    {
      "epoch": 30.773655450100662,
      "grad_norm": 0.0002536701795179397,
      "learning_rate": 0.00038453264308311767,
      "loss": 0.0024,
      "step": 107000
    },
    {
      "epoch": 30.78803566292781,
      "grad_norm": 0.0001091595840989612,
      "learning_rate": 0.0003842450388265746,
      "loss": 0.0012,
      "step": 107050
    },
    {
      "epoch": 30.80241587575496,
      "grad_norm": 0.0001927464472828433,
      "learning_rate": 0.00038395743457003164,
      "loss": 0.0004,
      "step": 107100
    },
    {
      "epoch": 30.816796088582112,
      "grad_norm": 0.00014992115029599518,
      "learning_rate": 0.00038366983031348865,
      "loss": 0.0021,
      "step": 107150
    },
    {
      "epoch": 30.83117630140926,
      "grad_norm": 4.0641400119056925e-05,
      "learning_rate": 0.00038338222605694566,
      "loss": 0.001,
      "step": 107200
    },
    {
      "epoch": 30.84555651423641,
      "grad_norm": 0.039278239011764526,
      "learning_rate": 0.00038309462180040267,
      "loss": 0.0028,
      "step": 107250
    },
    {
      "epoch": 30.85993672706356,
      "grad_norm": 0.0013992968015372753,
      "learning_rate": 0.0003828070175438597,
      "loss": 0.0055,
      "step": 107300
    },
    {
      "epoch": 30.87431693989071,
      "grad_norm": 0.0010964707471430302,
      "learning_rate": 0.00038251941328731664,
      "loss": 0.0012,
      "step": 107350
    },
    {
      "epoch": 30.88869715271786,
      "grad_norm": 0.03731805458664894,
      "learning_rate": 0.00038223180903077366,
      "loss": 0.0016,
      "step": 107400
    },
    {
      "epoch": 30.90307736554501,
      "grad_norm": 8.11117934063077e-05,
      "learning_rate": 0.00038194420477423067,
      "loss": 0.0007,
      "step": 107450
    },
    {
      "epoch": 30.91745757837216,
      "grad_norm": 0.025272343307733536,
      "learning_rate": 0.0003816566005176877,
      "loss": 0.0017,
      "step": 107500
    },
    {
      "epoch": 30.93183779119931,
      "grad_norm": 0.0007416975568048656,
      "learning_rate": 0.0003813689962611447,
      "loss": 0.0049,
      "step": 107550
    },
    {
      "epoch": 30.94621800402646,
      "grad_norm": 5.100615817354992e-05,
      "learning_rate": 0.0003810813920046017,
      "loss": 0.0017,
      "step": 107600
    },
    {
      "epoch": 30.960598216853608,
      "grad_norm": 0.027848243713378906,
      "learning_rate": 0.00038079378774805866,
      "loss": 0.0014,
      "step": 107650
    },
    {
      "epoch": 30.97497842968076,
      "grad_norm": 5.8320070820627734e-05,
      "learning_rate": 0.0003805061834915157,
      "loss": 0.0016,
      "step": 107700
    },
    {
      "epoch": 30.98935864250791,
      "grad_norm": 0.0008945155423134565,
      "learning_rate": 0.0003802185792349727,
      "loss": 0.0022,
      "step": 107750
    },
    {
      "epoch": 31.0,
      "eval_loss": 0.009971260093152523,
      "eval_runtime": 17.1065,
      "eval_samples_per_second": 2789.531,
      "eval_steps_per_second": 43.609,
      "step": 107787
    },
    {
      "epoch": 31.003738855335058,
      "grad_norm": 0.00112176313996315,
      "learning_rate": 0.0003799309749784297,
      "loss": 0.0041,
      "step": 107800
    },
    {
      "epoch": 31.01811906816221,
      "grad_norm": 3.5145978472428396e-05,
      "learning_rate": 0.0003796433707218867,
      "loss": 0.0048,
      "step": 107850
    },
    {
      "epoch": 31.03249928098936,
      "grad_norm": 0.0020280773751437664,
      "learning_rate": 0.0003793557664653437,
      "loss": 0.0044,
      "step": 107900
    },
    {
      "epoch": 31.04687949381651,
      "grad_norm": 0.0050555262714624405,
      "learning_rate": 0.0003790681622088007,
      "loss": 0.0031,
      "step": 107950
    },
    {
      "epoch": 31.061259706643657,
      "grad_norm": 0.0007693229708820581,
      "learning_rate": 0.0003787805579522577,
      "loss": 0.0017,
      "step": 108000
    },
    {
      "epoch": 31.07563991947081,
      "grad_norm": 3.035741792700719e-05,
      "learning_rate": 0.0003784929536957147,
      "loss": 0.0006,
      "step": 108050
    },
    {
      "epoch": 31.09002013229796,
      "grad_norm": 7.762547465972602e-05,
      "learning_rate": 0.0003782053494391717,
      "loss": 0.0008,
      "step": 108100
    },
    {
      "epoch": 31.104400345125107,
      "grad_norm": 7.831185939721763e-05,
      "learning_rate": 0.00037791774518262873,
      "loss": 0.001,
      "step": 108150
    },
    {
      "epoch": 31.118780557952256,
      "grad_norm": 0.03498172387480736,
      "learning_rate": 0.00037763014092608574,
      "loss": 0.0015,
      "step": 108200
    },
    {
      "epoch": 31.13316077077941,
      "grad_norm": 2.9598082619486377e-05,
      "learning_rate": 0.0003773425366695427,
      "loss": 0.0028,
      "step": 108250
    },
    {
      "epoch": 31.147540983606557,
      "grad_norm": 0.00554691581055522,
      "learning_rate": 0.0003770549324129997,
      "loss": 0.0036,
      "step": 108300
    },
    {
      "epoch": 31.161921196433706,
      "grad_norm": 0.00011381623335182667,
      "learning_rate": 0.0003767673281564567,
      "loss": 0.0003,
      "step": 108350
    },
    {
      "epoch": 31.17630140926086,
      "grad_norm": 0.008060180582106113,
      "learning_rate": 0.00037647972389991374,
      "loss": 0.0014,
      "step": 108400
    },
    {
      "epoch": 31.190681622088007,
      "grad_norm": 3.2753392588347197e-05,
      "learning_rate": 0.00037619211964337075,
      "loss": 0.0029,
      "step": 108450
    },
    {
      "epoch": 31.205061834915156,
      "grad_norm": 5.955985761829652e-05,
      "learning_rate": 0.00037590451538682776,
      "loss": 0.0008,
      "step": 108500
    },
    {
      "epoch": 31.219442047742305,
      "grad_norm": 0.0006165322847664356,
      "learning_rate": 0.0003756169111302847,
      "loss": 0.0033,
      "step": 108550
    },
    {
      "epoch": 31.233822260569458,
      "grad_norm": 0.012830495834350586,
      "learning_rate": 0.00037532930687374173,
      "loss": 0.0017,
      "step": 108600
    },
    {
      "epoch": 31.248202473396606,
      "grad_norm": 0.010674601420760155,
      "learning_rate": 0.00037504170261719875,
      "loss": 0.0026,
      "step": 108650
    },
    {
      "epoch": 31.262582686223755,
      "grad_norm": 3.974295032094233e-05,
      "learning_rate": 0.00037475409836065576,
      "loss": 0.0033,
      "step": 108700
    },
    {
      "epoch": 31.276962899050908,
      "grad_norm": 0.000220975125557743,
      "learning_rate": 0.00037446649410411277,
      "loss": 0.0022,
      "step": 108750
    },
    {
      "epoch": 31.291343111878057,
      "grad_norm": 0.006443439982831478,
      "learning_rate": 0.0003741788898475698,
      "loss": 0.0012,
      "step": 108800
    },
    {
      "epoch": 31.305723324705205,
      "grad_norm": 7.519430801039562e-05,
      "learning_rate": 0.00037389128559102674,
      "loss": 0.0029,
      "step": 108850
    },
    {
      "epoch": 31.320103537532354,
      "grad_norm": 0.00993869174271822,
      "learning_rate": 0.00037360368133448375,
      "loss": 0.0048,
      "step": 108900
    },
    {
      "epoch": 31.334483750359507,
      "grad_norm": 6.17325640632771e-05,
      "learning_rate": 0.00037331607707794077,
      "loss": 0.004,
      "step": 108950
    },
    {
      "epoch": 31.348863963186655,
      "grad_norm": 2.8721606213366613e-05,
      "learning_rate": 0.0003730284728213978,
      "loss": 0.0014,
      "step": 109000
    },
    {
      "epoch": 31.363244176013804,
      "grad_norm": 0.02873477339744568,
      "learning_rate": 0.0003727408685648548,
      "loss": 0.0008,
      "step": 109050
    },
    {
      "epoch": 31.377624388840953,
      "grad_norm": 9.802162821870297e-05,
      "learning_rate": 0.0003724532643083118,
      "loss": 0.001,
      "step": 109100
    },
    {
      "epoch": 31.392004601668106,
      "grad_norm": 0.0038787845987826586,
      "learning_rate": 0.00037216566005176876,
      "loss": 0.0008,
      "step": 109150
    },
    {
      "epoch": 31.406384814495254,
      "grad_norm": 0.00017967252642847598,
      "learning_rate": 0.00037187805579522577,
      "loss": 0.0017,
      "step": 109200
    },
    {
      "epoch": 31.420765027322403,
      "grad_norm": 0.008523590862751007,
      "learning_rate": 0.0003715904515386828,
      "loss": 0.002,
      "step": 109250
    },
    {
      "epoch": 31.435145240149556,
      "grad_norm": 0.014204844832420349,
      "learning_rate": 0.0003713028472821398,
      "loss": 0.0013,
      "step": 109300
    },
    {
      "epoch": 31.449525452976705,
      "grad_norm": 0.00011646308848867193,
      "learning_rate": 0.0003710152430255968,
      "loss": 0.0009,
      "step": 109350
    },
    {
      "epoch": 31.463905665803853,
      "grad_norm": 0.0007349495426751673,
      "learning_rate": 0.0003707276387690538,
      "loss": 0.0026,
      "step": 109400
    },
    {
      "epoch": 31.478285878631002,
      "grad_norm": 7.136718340916559e-05,
      "learning_rate": 0.0003704400345125108,
      "loss": 0.0045,
      "step": 109450
    },
    {
      "epoch": 31.492666091458155,
      "grad_norm": 4.105881089344621e-05,
      "learning_rate": 0.0003701524302559678,
      "loss": 0.0026,
      "step": 109500
    },
    {
      "epoch": 31.507046304285304,
      "grad_norm": 0.0013094648020341992,
      "learning_rate": 0.0003698648259994248,
      "loss": 0.0054,
      "step": 109550
    },
    {
      "epoch": 31.521426517112452,
      "grad_norm": 0.007906537503004074,
      "learning_rate": 0.00036957722174288176,
      "loss": 0.0023,
      "step": 109600
    },
    {
      "epoch": 31.535806729939605,
      "grad_norm": 9.866991604212672e-05,
      "learning_rate": 0.00036928961748633883,
      "loss": 0.0009,
      "step": 109650
    },
    {
      "epoch": 31.550186942766754,
      "grad_norm": 0.001704649650491774,
      "learning_rate": 0.00036900201322979584,
      "loss": 0.0025,
      "step": 109700
    },
    {
      "epoch": 31.564567155593902,
      "grad_norm": 0.008847719989717007,
      "learning_rate": 0.0003687144089732528,
      "loss": 0.0032,
      "step": 109750
    },
    {
      "epoch": 31.57894736842105,
      "grad_norm": 2.5297274987678975e-05,
      "learning_rate": 0.0003684268047167098,
      "loss": 0.0022,
      "step": 109800
    },
    {
      "epoch": 31.593327581248204,
      "grad_norm": 0.007017322350293398,
      "learning_rate": 0.0003681392004601668,
      "loss": 0.0029,
      "step": 109850
    },
    {
      "epoch": 31.607707794075353,
      "grad_norm": 6.11234427196905e-05,
      "learning_rate": 0.0003678515962036238,
      "loss": 0.0036,
      "step": 109900
    },
    {
      "epoch": 31.6220880069025,
      "grad_norm": 0.00010899195331148803,
      "learning_rate": 0.00036756399194708085,
      "loss": 0.0025,
      "step": 109950
    },
    {
      "epoch": 31.63646821972965,
      "grad_norm": 0.00014706338697578758,
      "learning_rate": 0.00036727638769053786,
      "loss": 0.0026,
      "step": 110000
    },
    {
      "epoch": 31.650848432556803,
      "grad_norm": 4.724817699752748e-05,
      "learning_rate": 0.0003669887834339948,
      "loss": 0.0022,
      "step": 110050
    },
    {
      "epoch": 31.66522864538395,
      "grad_norm": 0.004108825232833624,
      "learning_rate": 0.00036670117917745183,
      "loss": 0.0021,
      "step": 110100
    },
    {
      "epoch": 31.6796088582111,
      "grad_norm": 0.0394723005592823,
      "learning_rate": 0.00036641357492090884,
      "loss": 0.0021,
      "step": 110150
    },
    {
      "epoch": 31.693989071038253,
      "grad_norm": 0.00010199131793342531,
      "learning_rate": 0.0003661259706643658,
      "loss": 0.0003,
      "step": 110200
    },
    {
      "epoch": 31.7083692838654,
      "grad_norm": 0.008651412092149258,
      "learning_rate": 0.00036583836640782287,
      "loss": 0.0033,
      "step": 110250
    },
    {
      "epoch": 31.72274949669255,
      "grad_norm": 0.003285998245701194,
      "learning_rate": 0.0003655507621512799,
      "loss": 0.002,
      "step": 110300
    },
    {
      "epoch": 31.7371297095197,
      "grad_norm": 9.168143151327968e-05,
      "learning_rate": 0.00036526315789473684,
      "loss": 0.0002,
      "step": 110350
    },
    {
      "epoch": 31.75150992234685,
      "grad_norm": 5.632330066873692e-05,
      "learning_rate": 0.00036497555363819385,
      "loss": 0.0017,
      "step": 110400
    },
    {
      "epoch": 31.765890135174,
      "grad_norm": 0.03426435962319374,
      "learning_rate": 0.00036468794938165086,
      "loss": 0.0025,
      "step": 110450
    },
    {
      "epoch": 31.78027034800115,
      "grad_norm": 0.00794866681098938,
      "learning_rate": 0.0003644003451251078,
      "loss": 0.001,
      "step": 110500
    },
    {
      "epoch": 31.794650560828302,
      "grad_norm": 7.299279241124168e-05,
      "learning_rate": 0.0003641127408685649,
      "loss": 0.0008,
      "step": 110550
    },
    {
      "epoch": 31.80903077365545,
      "grad_norm": 0.0009972421685233712,
      "learning_rate": 0.0003638251366120219,
      "loss": 0.0029,
      "step": 110600
    },
    {
      "epoch": 31.8234109864826,
      "grad_norm": 3.261106030549854e-05,
      "learning_rate": 0.00036353753235547886,
      "loss": 0.0011,
      "step": 110650
    },
    {
      "epoch": 31.83779119930975,
      "grad_norm": 0.0013904921943321824,
      "learning_rate": 0.00036324992809893587,
      "loss": 0.0037,
      "step": 110700
    },
    {
      "epoch": 31.8521714121369,
      "grad_norm": 0.002584130270406604,
      "learning_rate": 0.0003629623238423929,
      "loss": 0.0052,
      "step": 110750
    },
    {
      "epoch": 31.86655162496405,
      "grad_norm": 1.1435126907599624e-05,
      "learning_rate": 0.00036267471958584984,
      "loss": 0.0016,
      "step": 110800
    },
    {
      "epoch": 31.8809318377912,
      "grad_norm": 0.008197304792702198,
      "learning_rate": 0.0003623871153293069,
      "loss": 0.0058,
      "step": 110850
    },
    {
      "epoch": 31.89531205061835,
      "grad_norm": 5.608826904790476e-05,
      "learning_rate": 0.0003620995110727639,
      "loss": 0.0008,
      "step": 110900
    },
    {
      "epoch": 31.9096922634455,
      "grad_norm": 9.093310654861853e-05,
      "learning_rate": 0.0003618119068162209,
      "loss": 0.0028,
      "step": 110950
    },
    {
      "epoch": 31.92407247627265,
      "grad_norm": 0.0012838079128414392,
      "learning_rate": 0.0003615243025596779,
      "loss": 0.0038,
      "step": 111000
    },
    {
      "epoch": 31.938452689099798,
      "grad_norm": 0.015296096913516521,
      "learning_rate": 0.0003612366983031349,
      "loss": 0.0033,
      "step": 111050
    },
    {
      "epoch": 31.95283290192695,
      "grad_norm": 0.004563081078231335,
      "learning_rate": 0.00036094909404659186,
      "loss": 0.0022,
      "step": 111100
    },
    {
      "epoch": 31.9672131147541,
      "grad_norm": 0.0014740299666300416,
      "learning_rate": 0.0003606614897900489,
      "loss": 0.0024,
      "step": 111150
    },
    {
      "epoch": 31.981593327581248,
      "grad_norm": 0.0004041117208544165,
      "learning_rate": 0.00036037388553350594,
      "loss": 0.0024,
      "step": 111200
    },
    {
      "epoch": 31.995973540408396,
      "grad_norm": 0.003983724396675825,
      "learning_rate": 0.0003600862812769629,
      "loss": 0.0042,
      "step": 111250
    },
    {
      "epoch": 32.0,
      "eval_loss": 0.0100483987480402,
      "eval_runtime": 16.988,
      "eval_samples_per_second": 2808.983,
      "eval_steps_per_second": 43.913,
      "step": 111264
    },
    {
      "epoch": 32.01035375323555,
      "grad_norm": 4.072134834132157e-05,
      "learning_rate": 0.0003597986770204199,
      "loss": 0.0031,
      "step": 111300
    },
    {
      "epoch": 32.0247339660627,
      "grad_norm": 0.008141313679516315,
      "learning_rate": 0.0003595110727638769,
      "loss": 0.0033,
      "step": 111350
    },
    {
      "epoch": 32.03911417888985,
      "grad_norm": 0.0046579306945204735,
      "learning_rate": 0.0003592234685073339,
      "loss": 0.0028,
      "step": 111400
    },
    {
      "epoch": 32.053494391716995,
      "grad_norm": 0.0037860784213989973,
      "learning_rate": 0.00035893586425079095,
      "loss": 0.0018,
      "step": 111450
    },
    {
      "epoch": 32.067874604544144,
      "grad_norm": 3.9107231714297086e-05,
      "learning_rate": 0.00035864825999424796,
      "loss": 0.0018,
      "step": 111500
    },
    {
      "epoch": 32.0822548173713,
      "grad_norm": 0.0032313589472323656,
      "learning_rate": 0.0003583606557377049,
      "loss": 0.0024,
      "step": 111550
    },
    {
      "epoch": 32.09663503019845,
      "grad_norm": 0.0001381097681587562,
      "learning_rate": 0.00035807305148116193,
      "loss": 0.002,
      "step": 111600
    },
    {
      "epoch": 32.1110152430256,
      "grad_norm": 0.009422165341675282,
      "learning_rate": 0.00035778544722461894,
      "loss": 0.0027,
      "step": 111650
    },
    {
      "epoch": 32.12539545585275,
      "grad_norm": 0.038120582699775696,
      "learning_rate": 0.0003574978429680759,
      "loss": 0.0033,
      "step": 111700
    },
    {
      "epoch": 32.139775668679896,
      "grad_norm": 4.08740270358976e-05,
      "learning_rate": 0.00035721023871153297,
      "loss": 0.0026,
      "step": 111750
    },
    {
      "epoch": 32.154155881507045,
      "grad_norm": 3.9479924453189597e-05,
      "learning_rate": 0.00035692263445499,
      "loss": 0.0006,
      "step": 111800
    },
    {
      "epoch": 32.16853609433419,
      "grad_norm": 0.001616229536011815,
      "learning_rate": 0.00035663503019844694,
      "loss": 0.003,
      "step": 111850
    },
    {
      "epoch": 32.18291630716135,
      "grad_norm": 5.78134749957826e-05,
      "learning_rate": 0.00035634742594190395,
      "loss": 0.0011,
      "step": 111900
    },
    {
      "epoch": 32.1972965199885,
      "grad_norm": 0.0107793640345335,
      "learning_rate": 0.00035605982168536096,
      "loss": 0.0013,
      "step": 111950
    },
    {
      "epoch": 32.21167673281565,
      "grad_norm": 0.02322843298316002,
      "learning_rate": 0.0003557722174288179,
      "loss": 0.0025,
      "step": 112000
    },
    {
      "epoch": 32.226056945642796,
      "grad_norm": 0.0005444005946628749,
      "learning_rate": 0.000355484613172275,
      "loss": 0.0021,
      "step": 112050
    },
    {
      "epoch": 32.240437158469945,
      "grad_norm": 0.007265500258654356,
      "learning_rate": 0.000355197008915732,
      "loss": 0.0032,
      "step": 112100
    },
    {
      "epoch": 32.254817371297094,
      "grad_norm": 4.0356368117500097e-05,
      "learning_rate": 0.00035490940465918896,
      "loss": 0.0022,
      "step": 112150
    },
    {
      "epoch": 32.26919758412424,
      "grad_norm": 0.03297958895564079,
      "learning_rate": 0.00035462180040264597,
      "loss": 0.0032,
      "step": 112200
    },
    {
      "epoch": 32.2835777969514,
      "grad_norm": 0.0003364515141583979,
      "learning_rate": 0.000354334196146103,
      "loss": 0.003,
      "step": 112250
    },
    {
      "epoch": 32.29795800977855,
      "grad_norm": 0.0013972673332318664,
      "learning_rate": 0.00035404659188955994,
      "loss": 0.005,
      "step": 112300
    },
    {
      "epoch": 32.312338222605696,
      "grad_norm": 0.009864859282970428,
      "learning_rate": 0.00035375898763301695,
      "loss": 0.0043,
      "step": 112350
    },
    {
      "epoch": 32.326718435432845,
      "grad_norm": 0.00855234358459711,
      "learning_rate": 0.000353471383376474,
      "loss": 0.0031,
      "step": 112400
    },
    {
      "epoch": 32.341098648259994,
      "grad_norm": 4.541665839497e-05,
      "learning_rate": 0.000353183779119931,
      "loss": 0.002,
      "step": 112450
    },
    {
      "epoch": 32.35547886108714,
      "grad_norm": 0.004829091485589743,
      "learning_rate": 0.000352896174863388,
      "loss": 0.0026,
      "step": 112500
    },
    {
      "epoch": 32.36985907391429,
      "grad_norm": 0.0008009276352822781,
      "learning_rate": 0.000352608570606845,
      "loss": 0.0021,
      "step": 112550
    },
    {
      "epoch": 32.38423928674144,
      "grad_norm": 3.467702481430024e-05,
      "learning_rate": 0.00035232096635030196,
      "loss": 0.0034,
      "step": 112600
    },
    {
      "epoch": 32.398619499568596,
      "grad_norm": 0.00031828091596253216,
      "learning_rate": 0.00035203336209375897,
      "loss": 0.0034,
      "step": 112650
    },
    {
      "epoch": 32.412999712395745,
      "grad_norm": 0.00021557592845056206,
      "learning_rate": 0.00035174575783721604,
      "loss": 0.0017,
      "step": 112700
    },
    {
      "epoch": 32.427379925222894,
      "grad_norm": 4.602396802511066e-05,
      "learning_rate": 0.000351458153580673,
      "loss": 0.004,
      "step": 112750
    },
    {
      "epoch": 32.44176013805004,
      "grad_norm": 0.02130693756043911,
      "learning_rate": 0.00035117054932413,
      "loss": 0.0023,
      "step": 112800
    },
    {
      "epoch": 32.45614035087719,
      "grad_norm": 0.003607418155297637,
      "learning_rate": 0.000350882945067587,
      "loss": 0.0022,
      "step": 112850
    },
    {
      "epoch": 32.47052056370434,
      "grad_norm": 2.3065964342094958e-05,
      "learning_rate": 0.000350595340811044,
      "loss": 0.0027,
      "step": 112900
    },
    {
      "epoch": 32.48490077653149,
      "grad_norm": 2.080875674437266e-05,
      "learning_rate": 0.000350307736554501,
      "loss": 0.0009,
      "step": 112950
    },
    {
      "epoch": 32.499280989358645,
      "grad_norm": 0.0006208745762705803,
      "learning_rate": 0.00035002013229795806,
      "loss": 0.0014,
      "step": 113000
    },
    {
      "epoch": 32.513661202185794,
      "grad_norm": 0.00013379775919020176,
      "learning_rate": 0.000349732528041415,
      "loss": 0.002,
      "step": 113050
    },
    {
      "epoch": 32.52804141501294,
      "grad_norm": 4.2464136640774086e-05,
      "learning_rate": 0.000349444923784872,
      "loss": 0.0009,
      "step": 113100
    },
    {
      "epoch": 32.54242162784009,
      "grad_norm": 0.00017471297178417444,
      "learning_rate": 0.00034915731952832904,
      "loss": 0.0027,
      "step": 113150
    },
    {
      "epoch": 32.55680184066724,
      "grad_norm": 0.0001418828614987433,
      "learning_rate": 0.000348869715271786,
      "loss": 0.0026,
      "step": 113200
    },
    {
      "epoch": 32.57118205349439,
      "grad_norm": 0.0007974931504577398,
      "learning_rate": 0.000348582111015243,
      "loss": 0.0025,
      "step": 113250
    },
    {
      "epoch": 32.58556226632154,
      "grad_norm": 8.769898704485968e-05,
      "learning_rate": 0.0003482945067587001,
      "loss": 0.0008,
      "step": 113300
    },
    {
      "epoch": 32.599942479148694,
      "grad_norm": 0.00014616061525885016,
      "learning_rate": 0.00034800690250215703,
      "loss": 0.0013,
      "step": 113350
    },
    {
      "epoch": 32.61432269197584,
      "grad_norm": 0.00038503389805555344,
      "learning_rate": 0.00034771929824561405,
      "loss": 0.0012,
      "step": 113400
    },
    {
      "epoch": 32.62870290480299,
      "grad_norm": 0.0004167129809502512,
      "learning_rate": 0.00034743169398907106,
      "loss": 0.0042,
      "step": 113450
    },
    {
      "epoch": 32.64308311763014,
      "grad_norm": 7.142368849599734e-05,
      "learning_rate": 0.000347144089732528,
      "loss": 0.0019,
      "step": 113500
    },
    {
      "epoch": 32.65746333045729,
      "grad_norm": 0.00047167998855002224,
      "learning_rate": 0.00034685648547598503,
      "loss": 0.0027,
      "step": 113550
    },
    {
      "epoch": 32.67184354328444,
      "grad_norm": 0.000167852223967202,
      "learning_rate": 0.0003465688812194421,
      "loss": 0.0018,
      "step": 113600
    },
    {
      "epoch": 32.68622375611159,
      "grad_norm": 0.011988054029643536,
      "learning_rate": 0.00034628127696289905,
      "loss": 0.003,
      "step": 113650
    },
    {
      "epoch": 32.70060396893874,
      "grad_norm": 0.00015304121188819408,
      "learning_rate": 0.00034599367270635607,
      "loss": 0.0023,
      "step": 113700
    },
    {
      "epoch": 32.71498418176589,
      "grad_norm": 3.016572372871451e-05,
      "learning_rate": 0.0003457060684498131,
      "loss": 0.0009,
      "step": 113750
    },
    {
      "epoch": 32.72936439459304,
      "grad_norm": 0.015261517837643623,
      "learning_rate": 0.00034541846419327004,
      "loss": 0.0004,
      "step": 113800
    },
    {
      "epoch": 32.74374460742019,
      "grad_norm": 0.0004474329180084169,
      "learning_rate": 0.00034513085993672705,
      "loss": 0.0056,
      "step": 113850
    },
    {
      "epoch": 32.75812482024734,
      "grad_norm": 6.562812632182613e-05,
      "learning_rate": 0.0003448432556801841,
      "loss": 0.0024,
      "step": 113900
    },
    {
      "epoch": 32.77250503307449,
      "grad_norm": 0.00048639322631061077,
      "learning_rate": 0.0003445556514236411,
      "loss": 0.0022,
      "step": 113950
    },
    {
      "epoch": 32.78688524590164,
      "grad_norm": 0.00018697611812967807,
      "learning_rate": 0.0003442680471670981,
      "loss": 0.0039,
      "step": 114000
    },
    {
      "epoch": 32.80126545872879,
      "grad_norm": 0.0005484059802256525,
      "learning_rate": 0.0003439804429105551,
      "loss": 0.0027,
      "step": 114050
    },
    {
      "epoch": 32.81564567155594,
      "grad_norm": 0.030698850750923157,
      "learning_rate": 0.00034369283865401206,
      "loss": 0.0021,
      "step": 114100
    },
    {
      "epoch": 32.83002588438309,
      "grad_norm": 0.00012859905837103724,
      "learning_rate": 0.00034340523439746907,
      "loss": 0.001,
      "step": 114150
    },
    {
      "epoch": 32.84440609721024,
      "grad_norm": 8.309838449349627e-05,
      "learning_rate": 0.00034311763014092613,
      "loss": 0.0008,
      "step": 114200
    },
    {
      "epoch": 32.85878631003739,
      "grad_norm": 0.007785387337207794,
      "learning_rate": 0.0003428300258843831,
      "loss": 0.0032,
      "step": 114250
    },
    {
      "epoch": 32.87316652286454,
      "grad_norm": 6.404527084669098e-05,
      "learning_rate": 0.0003425424216278401,
      "loss": 0.0028,
      "step": 114300
    },
    {
      "epoch": 32.887546735691686,
      "grad_norm": 3.0044388040550984e-05,
      "learning_rate": 0.0003422548173712971,
      "loss": 0.0014,
      "step": 114350
    },
    {
      "epoch": 32.901926948518835,
      "grad_norm": 1.625237382540945e-05,
      "learning_rate": 0.0003419672131147541,
      "loss": 0.0012,
      "step": 114400
    },
    {
      "epoch": 32.91630716134599,
      "grad_norm": 7.806054782122374e-05,
      "learning_rate": 0.0003416796088582111,
      "loss": 0.0002,
      "step": 114450
    },
    {
      "epoch": 32.93068737417314,
      "grad_norm": 0.0035632054787129164,
      "learning_rate": 0.00034139200460166815,
      "loss": 0.0017,
      "step": 114500
    },
    {
      "epoch": 32.94506758700029,
      "grad_norm": 0.003695448162034154,
      "learning_rate": 0.0003411044003451251,
      "loss": 0.0039,
      "step": 114550
    },
    {
      "epoch": 32.95944779982744,
      "grad_norm": 0.055053699761629105,
      "learning_rate": 0.0003408167960885821,
      "loss": 0.004,
      "step": 114600
    },
    {
      "epoch": 32.973828012654586,
      "grad_norm": 0.006450647488236427,
      "learning_rate": 0.00034052919183203914,
      "loss": 0.0017,
      "step": 114650
    },
    {
      "epoch": 32.988208225481735,
      "grad_norm": 2.823191425704863e-05,
      "learning_rate": 0.0003402415875754961,
      "loss": 0.0011,
      "step": 114700
    },
    {
      "epoch": 33.0,
      "eval_loss": 0.01003698818385601,
      "eval_runtime": 17.6871,
      "eval_samples_per_second": 2697.956,
      "eval_steps_per_second": 42.178,
      "step": 114741
    },
    {
      "epoch": 33.002588438308884,
      "grad_norm": 0.003341344418004155,
      "learning_rate": 0.0003399539833189531,
      "loss": 0.0023,
      "step": 114750
    },
    {
      "epoch": 33.01696865113604,
      "grad_norm": 7.207582530099899e-05,
      "learning_rate": 0.0003396663790624102,
      "loss": 0.0034,
      "step": 114800
    },
    {
      "epoch": 33.03134886396319,
      "grad_norm": 5.608186256722547e-05,
      "learning_rate": 0.00033937877480586713,
      "loss": 0.0007,
      "step": 114850
    },
    {
      "epoch": 33.04572907679034,
      "grad_norm": 0.02313091792166233,
      "learning_rate": 0.00033909117054932414,
      "loss": 0.0026,
      "step": 114900
    },
    {
      "epoch": 33.060109289617486,
      "grad_norm": 0.000480698945466429,
      "learning_rate": 0.00033880356629278116,
      "loss": 0.0018,
      "step": 114950
    },
    {
      "epoch": 33.074489502444635,
      "grad_norm": 8.66736372699961e-05,
      "learning_rate": 0.0003385159620362381,
      "loss": 0.0023,
      "step": 115000
    },
    {
      "epoch": 33.088869715271784,
      "grad_norm": 0.00010268970800098032,
      "learning_rate": 0.0003382283577796951,
      "loss": 0.0011,
      "step": 115050
    },
    {
      "epoch": 33.10324992809893,
      "grad_norm": 0.008590948767960072,
      "learning_rate": 0.00033794075352315214,
      "loss": 0.0021,
      "step": 115100
    },
    {
      "epoch": 33.11763014092609,
      "grad_norm": 4.603331763064489e-05,
      "learning_rate": 0.00033765314926660915,
      "loss": 0.0018,
      "step": 115150
    },
    {
      "epoch": 33.13201035375324,
      "grad_norm": 1.651208549446892e-05,
      "learning_rate": 0.00033736554501006616,
      "loss": 0.0005,
      "step": 115200
    },
    {
      "epoch": 33.146390566580386,
      "grad_norm": 8.902959962142631e-05,
      "learning_rate": 0.0003370779407535232,
      "loss": 0.0085,
      "step": 115250
    },
    {
      "epoch": 33.160770779407535,
      "grad_norm": 0.00387036195024848,
      "learning_rate": 0.00033679033649698013,
      "loss": 0.0031,
      "step": 115300
    },
    {
      "epoch": 33.175150992234684,
      "grad_norm": 2.795861473714467e-05,
      "learning_rate": 0.00033650273224043715,
      "loss": 0.0008,
      "step": 115350
    },
    {
      "epoch": 33.18953120506183,
      "grad_norm": 0.00027266269898973405,
      "learning_rate": 0.00033621512798389416,
      "loss": 0.0013,
      "step": 115400
    },
    {
      "epoch": 33.20391141788898,
      "grad_norm": 2.6157975298701786e-05,
      "learning_rate": 0.00033592752372735117,
      "loss": 0.002,
      "step": 115450
    },
    {
      "epoch": 33.21829163071614,
      "grad_norm": 0.0003211737785022706,
      "learning_rate": 0.0003356399194708082,
      "loss": 0.0011,
      "step": 115500
    },
    {
      "epoch": 33.23267184354329,
      "grad_norm": 0.001044253702275455,
      "learning_rate": 0.0003353523152142652,
      "loss": 0.0027,
      "step": 115550
    },
    {
      "epoch": 33.247052056370435,
      "grad_norm": 2.533874794607982e-05,
      "learning_rate": 0.00033506471095772215,
      "loss": 0.0039,
      "step": 115600
    },
    {
      "epoch": 33.261432269197584,
      "grad_norm": 2.372237759118434e-05,
      "learning_rate": 0.00033477710670117917,
      "loss": 0.0025,
      "step": 115650
    },
    {
      "epoch": 33.27581248202473,
      "grad_norm": 4.892851211479865e-05,
      "learning_rate": 0.0003344895024446362,
      "loss": 0.0018,
      "step": 115700
    },
    {
      "epoch": 33.29019269485188,
      "grad_norm": 0.00016573569155298173,
      "learning_rate": 0.0003342018981880932,
      "loss": 0.0031,
      "step": 115750
    },
    {
      "epoch": 33.30457290767903,
      "grad_norm": 0.006897975690662861,
      "learning_rate": 0.0003339142939315502,
      "loss": 0.0009,
      "step": 115800
    },
    {
      "epoch": 33.31895312050619,
      "grad_norm": 9.354859503218904e-05,
      "learning_rate": 0.0003336266896750072,
      "loss": 0.0039,
      "step": 115850
    },
    {
      "epoch": 33.333333333333336,
      "grad_norm": 0.011420798487961292,
      "learning_rate": 0.0003333390854184642,
      "loss": 0.0016,
      "step": 115900
    },
    {
      "epoch": 33.347713546160485,
      "grad_norm": 0.00016843940829858184,
      "learning_rate": 0.0003330514811619212,
      "loss": 0.0023,
      "step": 115950
    },
    {
      "epoch": 33.36209375898763,
      "grad_norm": 0.0002723868237808347,
      "learning_rate": 0.0003327638769053782,
      "loss": 0.0034,
      "step": 116000
    },
    {
      "epoch": 33.37647397181478,
      "grad_norm": 3.566688974387944e-05,
      "learning_rate": 0.0003324762726488352,
      "loss": 0.0007,
      "step": 116050
    },
    {
      "epoch": 33.39085418464193,
      "grad_norm": 0.0003440931614022702,
      "learning_rate": 0.0003321886683922922,
      "loss": 0.003,
      "step": 116100
    },
    {
      "epoch": 33.40523439746908,
      "grad_norm": 0.0006242849049158394,
      "learning_rate": 0.00033190106413574923,
      "loss": 0.0049,
      "step": 116150
    },
    {
      "epoch": 33.419614610296236,
      "grad_norm": 0.010757348500192165,
      "learning_rate": 0.0003316134598792062,
      "loss": 0.002,
      "step": 116200
    },
    {
      "epoch": 33.433994823123385,
      "grad_norm": 0.0007047798717394471,
      "learning_rate": 0.0003313258556226632,
      "loss": 0.0027,
      "step": 116250
    },
    {
      "epoch": 33.448375035950534,
      "grad_norm": 0.0027146115899086,
      "learning_rate": 0.0003310382513661202,
      "loss": 0.0025,
      "step": 116300
    },
    {
      "epoch": 33.46275524877768,
      "grad_norm": 9.329709428129718e-05,
      "learning_rate": 0.00033075064710957723,
      "loss": 0.001,
      "step": 116350
    },
    {
      "epoch": 33.47713546160483,
      "grad_norm": 0.00032856606412678957,
      "learning_rate": 0.00033046304285303424,
      "loss": 0.0034,
      "step": 116400
    },
    {
      "epoch": 33.49151567443198,
      "grad_norm": 6.281852256506681e-05,
      "learning_rate": 0.00033017543859649125,
      "loss": 0.0021,
      "step": 116450
    },
    {
      "epoch": 33.50589588725913,
      "grad_norm": 0.0002009247982641682,
      "learning_rate": 0.0003298878343399482,
      "loss": 0.0015,
      "step": 116500
    },
    {
      "epoch": 33.52027610008628,
      "grad_norm": 0.0002274155558552593,
      "learning_rate": 0.0003296002300834052,
      "loss": 0.0009,
      "step": 116550
    },
    {
      "epoch": 33.534656312913434,
      "grad_norm": 0.018092963844537735,
      "learning_rate": 0.00032931262582686224,
      "loss": 0.002,
      "step": 116600
    },
    {
      "epoch": 33.54903652574058,
      "grad_norm": 4.157501462032087e-05,
      "learning_rate": 0.00032902502157031925,
      "loss": 0.0004,
      "step": 116650
    },
    {
      "epoch": 33.56341673856773,
      "grad_norm": 5.29989592905622e-05,
      "learning_rate": 0.00032873741731377626,
      "loss": 0.0018,
      "step": 116700
    },
    {
      "epoch": 33.57779695139488,
      "grad_norm": 0.0013597819488495588,
      "learning_rate": 0.0003284498130572333,
      "loss": 0.0031,
      "step": 116750
    },
    {
      "epoch": 33.59217716422203,
      "grad_norm": 0.0001963009126484394,
      "learning_rate": 0.00032816220880069023,
      "loss": 0.0017,
      "step": 116800
    },
    {
      "epoch": 33.60655737704918,
      "grad_norm": 1.7925656720763072e-05,
      "learning_rate": 0.00032787460454414724,
      "loss": 0.0021,
      "step": 116850
    },
    {
      "epoch": 33.62093758987633,
      "grad_norm": 0.0029651320073753595,
      "learning_rate": 0.00032758700028760426,
      "loss": 0.0011,
      "step": 116900
    },
    {
      "epoch": 33.63531780270348,
      "grad_norm": 0.0001613497588550672,
      "learning_rate": 0.00032729939603106127,
      "loss": 0.0007,
      "step": 116950
    },
    {
      "epoch": 33.64969801553063,
      "grad_norm": 9.014385432237759e-05,
      "learning_rate": 0.0003270117917745183,
      "loss": 0.0047,
      "step": 117000
    },
    {
      "epoch": 33.66407822835778,
      "grad_norm": 0.00026053658802993596,
      "learning_rate": 0.0003267241875179753,
      "loss": 0.0003,
      "step": 117050
    },
    {
      "epoch": 33.67845844118493,
      "grad_norm": 0.007068032398819923,
      "learning_rate": 0.00032643658326143225,
      "loss": 0.0037,
      "step": 117100
    },
    {
      "epoch": 33.69283865401208,
      "grad_norm": 0.0011853346368297935,
      "learning_rate": 0.00032614897900488926,
      "loss": 0.0009,
      "step": 117150
    },
    {
      "epoch": 33.70721886683923,
      "grad_norm": 0.00018622027710080147,
      "learning_rate": 0.0003258613747483463,
      "loss": 0.0024,
      "step": 117200
    },
    {
      "epoch": 33.721599079666376,
      "grad_norm": 0.0018104629125446081,
      "learning_rate": 0.0003255737704918033,
      "loss": 0.0013,
      "step": 117250
    },
    {
      "epoch": 33.73597929249353,
      "grad_norm": 6.429327913792804e-05,
      "learning_rate": 0.0003252861662352603,
      "loss": 0.0026,
      "step": 117300
    },
    {
      "epoch": 33.75035950532068,
      "grad_norm": 4.797030851477757e-05,
      "learning_rate": 0.0003249985619787173,
      "loss": 0.0023,
      "step": 117350
    },
    {
      "epoch": 33.76473971814783,
      "grad_norm": 0.00012014666572213173,
      "learning_rate": 0.00032471095772217427,
      "loss": 0.004,
      "step": 117400
    },
    {
      "epoch": 33.77911993097498,
      "grad_norm": 0.004670336376875639,
      "learning_rate": 0.0003244233534656313,
      "loss": 0.0014,
      "step": 117450
    },
    {
      "epoch": 33.79350014380213,
      "grad_norm": 0.009191382676362991,
      "learning_rate": 0.0003241357492090883,
      "loss": 0.0027,
      "step": 117500
    },
    {
      "epoch": 33.807880356629276,
      "grad_norm": 0.00020308347302488983,
      "learning_rate": 0.0003238481449525453,
      "loss": 0.0027,
      "step": 117550
    },
    {
      "epoch": 33.822260569456425,
      "grad_norm": 7.794540579197928e-05,
      "learning_rate": 0.0003235605406960023,
      "loss": 0.0041,
      "step": 117600
    },
    {
      "epoch": 33.83664078228358,
      "grad_norm": 0.0008660978637635708,
      "learning_rate": 0.00032327293643945933,
      "loss": 0.0017,
      "step": 117650
    },
    {
      "epoch": 33.85102099511073,
      "grad_norm": 0.002282501896843314,
      "learning_rate": 0.0003229853321829163,
      "loss": 0.0023,
      "step": 117700
    },
    {
      "epoch": 33.86540120793788,
      "grad_norm": 0.0022381304297596216,
      "learning_rate": 0.0003226977279263733,
      "loss": 0.0046,
      "step": 117750
    },
    {
      "epoch": 33.87978142076503,
      "grad_norm": 6.157601455925032e-05,
      "learning_rate": 0.0003224101236698303,
      "loss": 0.0032,
      "step": 117800
    },
    {
      "epoch": 33.89416163359218,
      "grad_norm": 0.0022239340469241142,
      "learning_rate": 0.0003221225194132873,
      "loss": 0.0037,
      "step": 117850
    },
    {
      "epoch": 33.908541846419325,
      "grad_norm": 4.637972961063497e-05,
      "learning_rate": 0.00032183491515674434,
      "loss": 0.0022,
      "step": 117900
    },
    {
      "epoch": 33.922922059246474,
      "grad_norm": 0.003973337821662426,
      "learning_rate": 0.00032154731090020135,
      "loss": 0.001,
      "step": 117950
    },
    {
      "epoch": 33.93730227207363,
      "grad_norm": 0.0033672533463686705,
      "learning_rate": 0.0003212597066436583,
      "loss": 0.0035,
      "step": 118000
    },
    {
      "epoch": 33.95168248490078,
      "grad_norm": 2.8710885089822114e-05,
      "learning_rate": 0.0003209721023871153,
      "loss": 0.0028,
      "step": 118050
    },
    {
      "epoch": 33.96606269772793,
      "grad_norm": 5.636023706756532e-05,
      "learning_rate": 0.00032068449813057233,
      "loss": 0.0037,
      "step": 118100
    },
    {
      "epoch": 33.98044291055508,
      "grad_norm": 0.0013378801522776484,
      "learning_rate": 0.0003203968938740293,
      "loss": 0.0021,
      "step": 118150
    },
    {
      "epoch": 33.994823123382226,
      "grad_norm": 0.0007084865937940776,
      "learning_rate": 0.00032010928961748636,
      "loss": 0.0016,
      "step": 118200
    },
    {
      "epoch": 34.0,
      "eval_loss": 0.009998423978686333,
      "eval_runtime": 17.3751,
      "eval_samples_per_second": 2746.409,
      "eval_steps_per_second": 42.935,
      "step": 118218
    },
    {
      "epoch": 34.009203336209374,
      "grad_norm": 0.0001224624429596588,
      "learning_rate": 0.00031982168536094337,
      "loss": 0.0019,
      "step": 118250
    },
    {
      "epoch": 34.02358354903652,
      "grad_norm": 6.506647332571447e-05,
      "learning_rate": 0.00031953408110440033,
      "loss": 0.002,
      "step": 118300
    },
    {
      "epoch": 34.03796376186367,
      "grad_norm": 0.004225042648613453,
      "learning_rate": 0.00031924647684785734,
      "loss": 0.0041,
      "step": 118350
    },
    {
      "epoch": 34.05234397469083,
      "grad_norm": 2.1266696421662346e-05,
      "learning_rate": 0.00031895887259131435,
      "loss": 0.0044,
      "step": 118400
    },
    {
      "epoch": 34.06672418751798,
      "grad_norm": 0.0004991533933207393,
      "learning_rate": 0.0003186712683347713,
      "loss": 0.0043,
      "step": 118450
    },
    {
      "epoch": 34.081104400345126,
      "grad_norm": 0.0001509060530224815,
      "learning_rate": 0.0003183836640782284,
      "loss": 0.0008,
      "step": 118500
    },
    {
      "epoch": 34.095484613172275,
      "grad_norm": 0.002382326638326049,
      "learning_rate": 0.0003180960598216854,
      "loss": 0.0007,
      "step": 118550
    },
    {
      "epoch": 34.10986482599942,
      "grad_norm": 0.0015133593697100878,
      "learning_rate": 0.00031780845556514235,
      "loss": 0.0033,
      "step": 118600
    },
    {
      "epoch": 34.12424503882657,
      "grad_norm": 2.6021443773061037e-05,
      "learning_rate": 0.00031752085130859936,
      "loss": 0.0029,
      "step": 118650
    },
    {
      "epoch": 34.13862525165372,
      "grad_norm": 0.03607107698917389,
      "learning_rate": 0.0003172332470520564,
      "loss": 0.0043,
      "step": 118700
    },
    {
      "epoch": 34.15300546448088,
      "grad_norm": 0.0009576562442816794,
      "learning_rate": 0.00031694564279551333,
      "loss": 0.0012,
      "step": 118750
    },
    {
      "epoch": 34.167385677308026,
      "grad_norm": 6.994730938458815e-05,
      "learning_rate": 0.0003166580385389704,
      "loss": 0.0004,
      "step": 118800
    },
    {
      "epoch": 34.181765890135175,
      "grad_norm": 0.006884097587317228,
      "learning_rate": 0.0003163704342824274,
      "loss": 0.0019,
      "step": 118850
    },
    {
      "epoch": 34.196146102962324,
      "grad_norm": 0.017734674736857414,
      "learning_rate": 0.00031608283002588437,
      "loss": 0.0023,
      "step": 118900
    },
    {
      "epoch": 34.21052631578947,
      "grad_norm": 0.0004930913564749062,
      "learning_rate": 0.0003157952257693414,
      "loss": 0.0029,
      "step": 118950
    },
    {
      "epoch": 34.22490652861662,
      "grad_norm": 0.0017821816727519035,
      "learning_rate": 0.0003155076215127984,
      "loss": 0.0028,
      "step": 119000
    },
    {
      "epoch": 34.23928674144377,
      "grad_norm": 0.008177018724381924,
      "learning_rate": 0.00031522001725625535,
      "loss": 0.0026,
      "step": 119050
    },
    {
      "epoch": 34.253666954270926,
      "grad_norm": 0.00021824469149578363,
      "learning_rate": 0.0003149324129997124,
      "loss": 0.0006,
      "step": 119100
    },
    {
      "epoch": 34.268047167098075,
      "grad_norm": 6.428972847061232e-05,
      "learning_rate": 0.00031464480874316943,
      "loss": 0.0007,
      "step": 119150
    },
    {
      "epoch": 34.282427379925224,
      "grad_norm": 0.00013320852303877473,
      "learning_rate": 0.0003143572044866264,
      "loss": 0.0033,
      "step": 119200
    },
    {
      "epoch": 34.29680759275237,
      "grad_norm": 0.021323103457689285,
      "learning_rate": 0.0003140696002300834,
      "loss": 0.004,
      "step": 119250
    },
    {
      "epoch": 34.31118780557952,
      "grad_norm": 0.0003804153820965439,
      "learning_rate": 0.0003137819959735404,
      "loss": 0.0016,
      "step": 119300
    },
    {
      "epoch": 34.32556801840667,
      "grad_norm": 0.006889502517879009,
      "learning_rate": 0.00031349439171699737,
      "loss": 0.0004,
      "step": 119350
    },
    {
      "epoch": 34.33994823123382,
      "grad_norm": 1.347263969364576e-05,
      "learning_rate": 0.00031320678746045444,
      "loss": 0.0035,
      "step": 119400
    },
    {
      "epoch": 34.354328444060975,
      "grad_norm": 6.016931729391217e-05,
      "learning_rate": 0.00031291918320391145,
      "loss": 0.0025,
      "step": 119450
    },
    {
      "epoch": 34.368708656888124,
      "grad_norm": 0.0007932658772915602,
      "learning_rate": 0.0003126315789473684,
      "loss": 0.0027,
      "step": 119500
    },
    {
      "epoch": 34.38308886971527,
      "grad_norm": 3.5740333260037005e-05,
      "learning_rate": 0.0003123439746908254,
      "loss": 0.0044,
      "step": 119550
    },
    {
      "epoch": 34.39746908254242,
      "grad_norm": 0.00010132920579053462,
      "learning_rate": 0.00031205637043428243,
      "loss": 0.004,
      "step": 119600
    },
    {
      "epoch": 34.41184929536957,
      "grad_norm": 0.008208390325307846,
      "learning_rate": 0.0003117687661777394,
      "loss": 0.0037,
      "step": 119650
    },
    {
      "epoch": 34.42622950819672,
      "grad_norm": 0.009375681169331074,
      "learning_rate": 0.00031148116192119646,
      "loss": 0.003,
      "step": 119700
    },
    {
      "epoch": 34.44060972102387,
      "grad_norm": 4.578562220558524e-05,
      "learning_rate": 0.00031119355766465347,
      "loss": 0.0003,
      "step": 119750
    },
    {
      "epoch": 34.454989933851024,
      "grad_norm": 0.00014557092799805105,
      "learning_rate": 0.00031090595340811043,
      "loss": 0.0043,
      "step": 119800
    },
    {
      "epoch": 34.46937014667817,
      "grad_norm": 2.196431341872085e-05,
      "learning_rate": 0.00031061834915156744,
      "loss": 0.0025,
      "step": 119850
    },
    {
      "epoch": 34.48375035950532,
      "grad_norm": 1.9457071175565943e-05,
      "learning_rate": 0.00031033074489502445,
      "loss": 0.0043,
      "step": 119900
    },
    {
      "epoch": 34.49813057233247,
      "grad_norm": 0.00010558333451626822,
      "learning_rate": 0.0003100431406384814,
      "loss": 0.0004,
      "step": 119950
    },
    {
      "epoch": 34.51251078515962,
      "grad_norm": 0.0035941177047789097,
      "learning_rate": 0.0003097555363819385,
      "loss": 0.0012,
      "step": 120000
    },
    {
      "epoch": 34.52689099798677,
      "grad_norm": 2.1519974325201474e-05,
      "learning_rate": 0.0003094679321253955,
      "loss": 0.0018,
      "step": 120050
    },
    {
      "epoch": 34.54127121081392,
      "grad_norm": 0.0014833135064691305,
      "learning_rate": 0.00030918032786885245,
      "loss": 0.0028,
      "step": 120100
    },
    {
      "epoch": 34.555651423641066,
      "grad_norm": 0.0075597381219267845,
      "learning_rate": 0.00030889272361230946,
      "loss": 0.0012,
      "step": 120150
    },
    {
      "epoch": 34.57003163646822,
      "grad_norm": 3.413192098378204e-05,
      "learning_rate": 0.00030860511935576647,
      "loss": 0.0027,
      "step": 120200
    },
    {
      "epoch": 34.58441184929537,
      "grad_norm": 1.6651323676342145e-05,
      "learning_rate": 0.00030831751509922343,
      "loss": 0.0014,
      "step": 120250
    },
    {
      "epoch": 34.59879206212252,
      "grad_norm": 5.783926462754607e-05,
      "learning_rate": 0.0003080299108426805,
      "loss": 0.0044,
      "step": 120300
    },
    {
      "epoch": 34.61317227494967,
      "grad_norm": 0.0010825276840478182,
      "learning_rate": 0.0003077423065861375,
      "loss": 0.0005,
      "step": 120350
    },
    {
      "epoch": 34.62755248777682,
      "grad_norm": 2.5138890123344027e-05,
      "learning_rate": 0.00030745470232959447,
      "loss": 0.002,
      "step": 120400
    },
    {
      "epoch": 34.64193270060397,
      "grad_norm": 0.00010317489068256691,
      "learning_rate": 0.0003071670980730515,
      "loss": 0.0022,
      "step": 120450
    },
    {
      "epoch": 34.656312913431115,
      "grad_norm": 0.0007954866159707308,
      "learning_rate": 0.0003068794938165085,
      "loss": 0.0025,
      "step": 120500
    },
    {
      "epoch": 34.67069312625827,
      "grad_norm": 4.2072591895703226e-05,
      "learning_rate": 0.00030659188955996545,
      "loss": 0.0038,
      "step": 120550
    },
    {
      "epoch": 34.68507333908542,
      "grad_norm": 0.0005391289014369249,
      "learning_rate": 0.00030630428530342246,
      "loss": 0.0024,
      "step": 120600
    },
    {
      "epoch": 34.69945355191257,
      "grad_norm": 0.0032163909636437893,
      "learning_rate": 0.00030601668104687953,
      "loss": 0.0006,
      "step": 120650
    },
    {
      "epoch": 34.71383376473972,
      "grad_norm": 0.005003872327506542,
      "learning_rate": 0.00030572907679033654,
      "loss": 0.002,
      "step": 120700
    },
    {
      "epoch": 34.72821397756687,
      "grad_norm": 0.0002457537630107254,
      "learning_rate": 0.0003054414725337935,
      "loss": 0.0027,
      "step": 120750
    },
    {
      "epoch": 34.742594190394016,
      "grad_norm": 3.5184093576390296e-05,
      "learning_rate": 0.0003051538682772505,
      "loss": 0.0039,
      "step": 120800
    },
    {
      "epoch": 34.756974403221164,
      "grad_norm": 1.919779424497392e-05,
      "learning_rate": 0.00030486626402070747,
      "loss": 0.0011,
      "step": 120850
    },
    {
      "epoch": 34.77135461604832,
      "grad_norm": 2.966845568153076e-05,
      "learning_rate": 0.0003045786597641645,
      "loss": 0.0021,
      "step": 120900
    },
    {
      "epoch": 34.78573482887547,
      "grad_norm": 3.7920428439974785e-05,
      "learning_rate": 0.00030429105550762155,
      "loss": 0.0018,
      "step": 120950
    },
    {
      "epoch": 34.80011504170262,
      "grad_norm": 8.33102676551789e-05,
      "learning_rate": 0.00030400345125107856,
      "loss": 0.0014,
      "step": 121000
    },
    {
      "epoch": 34.81449525452977,
      "grad_norm": 0.0034454588312655687,
      "learning_rate": 0.0003037158469945355,
      "loss": 0.002,
      "step": 121050
    },
    {
      "epoch": 34.828875467356916,
      "grad_norm": 0.006651910953223705,
      "learning_rate": 0.00030342824273799253,
      "loss": 0.0033,
      "step": 121100
    },
    {
      "epoch": 34.843255680184065,
      "grad_norm": 3.018119969055988e-05,
      "learning_rate": 0.00030314063848144954,
      "loss": 0.0012,
      "step": 121150
    },
    {
      "epoch": 34.857635893011214,
      "grad_norm": 0.003005756065249443,
      "learning_rate": 0.0003028530342249065,
      "loss": 0.0021,
      "step": 121200
    },
    {
      "epoch": 34.87201610583837,
      "grad_norm": 0.00011751744750654325,
      "learning_rate": 0.00030256542996836357,
      "loss": 0.0028,
      "step": 121250
    },
    {
      "epoch": 34.88639631866552,
      "grad_norm": 0.014759375713765621,
      "learning_rate": 0.0003022778257118206,
      "loss": 0.0016,
      "step": 121300
    },
    {
      "epoch": 34.90077653149267,
      "grad_norm": 0.058682169765233994,
      "learning_rate": 0.00030199022145527754,
      "loss": 0.0035,
      "step": 121350
    },
    {
      "epoch": 34.915156744319816,
      "grad_norm": 0.000670148350764066,
      "learning_rate": 0.00030170261719873455,
      "loss": 0.0031,
      "step": 121400
    },
    {
      "epoch": 34.929536957146965,
      "grad_norm": 0.028373150154948235,
      "learning_rate": 0.00030141501294219156,
      "loss": 0.002,
      "step": 121450
    },
    {
      "epoch": 34.943917169974114,
      "grad_norm": 0.0007451887940987945,
      "learning_rate": 0.0003011274086856485,
      "loss": 0.0009,
      "step": 121500
    },
    {
      "epoch": 34.95829738280126,
      "grad_norm": 4.963009632774629e-05,
      "learning_rate": 0.0003008398044291056,
      "loss": 0.0013,
      "step": 121550
    },
    {
      "epoch": 34.97267759562842,
      "grad_norm": 0.0004630086768884212,
      "learning_rate": 0.0003005522001725626,
      "loss": 0.002,
      "step": 121600
    },
    {
      "epoch": 34.98705780845557,
      "grad_norm": 0.0001307229103986174,
      "learning_rate": 0.00030026459591601956,
      "loss": 0.0013,
      "step": 121650
    },
    {
      "epoch": 35.0,
      "eval_loss": 0.009903731755912304,
      "eval_runtime": 16.6784,
      "eval_samples_per_second": 2861.122,
      "eval_steps_per_second": 44.728,
      "step": 121695
    },
    {
      "epoch": 35.001438021282716,
      "grad_norm": 0.007185717113316059,
      "learning_rate": 0.00029997699165947657,
      "loss": 0.0033,
      "step": 121700
    },
    {
      "epoch": 35.015818234109865,
      "grad_norm": 0.00024064286844804883,
      "learning_rate": 0.0002996893874029336,
      "loss": 0.0032,
      "step": 121750
    },
    {
      "epoch": 35.030198446937014,
      "grad_norm": 6.864342140033841e-05,
      "learning_rate": 0.00029940178314639054,
      "loss": 0.0032,
      "step": 121800
    },
    {
      "epoch": 35.04457865976416,
      "grad_norm": 0.013487353920936584,
      "learning_rate": 0.0002991141788898476,
      "loss": 0.0016,
      "step": 121850
    },
    {
      "epoch": 35.05895887259131,
      "grad_norm": 0.00018983719928655773,
      "learning_rate": 0.0002988265746333046,
      "loss": 0.0029,
      "step": 121900
    },
    {
      "epoch": 35.07333908541847,
      "grad_norm": 0.0003135603037662804,
      "learning_rate": 0.0002985389703767616,
      "loss": 0.0016,
      "step": 121950
    },
    {
      "epoch": 35.08771929824562,
      "grad_norm": 0.0011691011022776365,
      "learning_rate": 0.0002982513661202186,
      "loss": 0.0023,
      "step": 122000
    },
    {
      "epoch": 35.102099511072765,
      "grad_norm": 0.0005362904630601406,
      "learning_rate": 0.0002979637618636756,
      "loss": 0.0014,
      "step": 122050
    },
    {
      "epoch": 35.116479723899914,
      "grad_norm": 0.00022872001864016056,
      "learning_rate": 0.00029767615760713256,
      "loss": 0.0016,
      "step": 122100
    },
    {
      "epoch": 35.13085993672706,
      "grad_norm": 0.00019537769549060613,
      "learning_rate": 0.0002973885533505896,
      "loss": 0.0007,
      "step": 122150
    },
    {
      "epoch": 35.14524014955421,
      "grad_norm": 4.1964140109485015e-05,
      "learning_rate": 0.00029710094909404664,
      "loss": 0.0022,
      "step": 122200
    },
    {
      "epoch": 35.15962036238136,
      "grad_norm": 8.485974103678018e-05,
      "learning_rate": 0.0002968133448375036,
      "loss": 0.0006,
      "step": 122250
    },
    {
      "epoch": 35.17400057520851,
      "grad_norm": 0.003339101793244481,
      "learning_rate": 0.0002965257405809606,
      "loss": 0.0018,
      "step": 122300
    },
    {
      "epoch": 35.188380788035666,
      "grad_norm": 0.023425176739692688,
      "learning_rate": 0.0002962381363244176,
      "loss": 0.0031,
      "step": 122350
    },
    {
      "epoch": 35.202761000862814,
      "grad_norm": 8.048812014749274e-05,
      "learning_rate": 0.0002959505320678746,
      "loss": 0.0014,
      "step": 122400
    },
    {
      "epoch": 35.21714121368996,
      "grad_norm": 8.626702765468508e-05,
      "learning_rate": 0.00029566292781133165,
      "loss": 0.0019,
      "step": 122450
    },
    {
      "epoch": 35.23152142651711,
      "grad_norm": 0.01119534857571125,
      "learning_rate": 0.00029537532355478866,
      "loss": 0.0019,
      "step": 122500
    },
    {
      "epoch": 35.24590163934426,
      "grad_norm": 0.02276420034468174,
      "learning_rate": 0.0002950877192982456,
      "loss": 0.0004,
      "step": 122550
    },
    {
      "epoch": 35.26028185217141,
      "grad_norm": 0.0006382012506946921,
      "learning_rate": 0.00029480011504170263,
      "loss": 0.0023,
      "step": 122600
    },
    {
      "epoch": 35.27466206499856,
      "grad_norm": 5.748600415245164e-06,
      "learning_rate": 0.00029451251078515964,
      "loss": 0.0024,
      "step": 122650
    },
    {
      "epoch": 35.289042277825715,
      "grad_norm": 4.3970198021270335e-05,
      "learning_rate": 0.0002942249065286166,
      "loss": 0.0029,
      "step": 122700
    },
    {
      "epoch": 35.30342249065286,
      "grad_norm": 0.00047941147931851447,
      "learning_rate": 0.00029393730227207366,
      "loss": 0.0029,
      "step": 122750
    },
    {
      "epoch": 35.31780270348001,
      "grad_norm": 5.43942951480858e-05,
      "learning_rate": 0.0002936496980155307,
      "loss": 0.0024,
      "step": 122800
    },
    {
      "epoch": 35.33218291630716,
      "grad_norm": 3.2878146157599986e-05,
      "learning_rate": 0.00029336209375898764,
      "loss": 0.0018,
      "step": 122850
    },
    {
      "epoch": 35.34656312913431,
      "grad_norm": 0.012834039516746998,
      "learning_rate": 0.00029307448950244465,
      "loss": 0.0006,
      "step": 122900
    },
    {
      "epoch": 35.36094334196146,
      "grad_norm": 4.884653026238084e-05,
      "learning_rate": 0.00029278688524590166,
      "loss": 0.0026,
      "step": 122950
    },
    {
      "epoch": 35.37532355478861,
      "grad_norm": 0.0004188010934740305,
      "learning_rate": 0.0002924992809893586,
      "loss": 0.0006,
      "step": 123000
    },
    {
      "epoch": 35.389703767615764,
      "grad_norm": 0.0009206962422467768,
      "learning_rate": 0.0002922116767328157,
      "loss": 0.0007,
      "step": 123050
    },
    {
      "epoch": 35.40408398044291,
      "grad_norm": 1.5020015780464746e-05,
      "learning_rate": 0.0002919240724762727,
      "loss": 0.0025,
      "step": 123100
    },
    {
      "epoch": 35.41846419327006,
      "grad_norm": 5.046675869380124e-05,
      "learning_rate": 0.00029163646821972965,
      "loss": 0.0044,
      "step": 123150
    },
    {
      "epoch": 35.43284440609721,
      "grad_norm": 0.00010625694267218933,
      "learning_rate": 0.00029134886396318667,
      "loss": 0.0053,
      "step": 123200
    },
    {
      "epoch": 35.44722461892436,
      "grad_norm": 0.00015182413335423917,
      "learning_rate": 0.0002910612597066437,
      "loss": 0.0016,
      "step": 123250
    },
    {
      "epoch": 35.46160483175151,
      "grad_norm": 0.003255597548559308,
      "learning_rate": 0.00029077365545010064,
      "loss": 0.0014,
      "step": 123300
    },
    {
      "epoch": 35.47598504457866,
      "grad_norm": 7.461971108568832e-05,
      "learning_rate": 0.00029048605119355765,
      "loss": 0.0037,
      "step": 123350
    },
    {
      "epoch": 35.49036525740581,
      "grad_norm": 1.872591565188486e-05,
      "learning_rate": 0.0002901984469370147,
      "loss": 0.0019,
      "step": 123400
    },
    {
      "epoch": 35.50474547023296,
      "grad_norm": 7.177149382187054e-05,
      "learning_rate": 0.0002899108426804717,
      "loss": 0.0027,
      "step": 123450
    },
    {
      "epoch": 35.51912568306011,
      "grad_norm": 0.00015919034194666892,
      "learning_rate": 0.0002896232384239287,
      "loss": 0.0051,
      "step": 123500
    },
    {
      "epoch": 35.53350589588726,
      "grad_norm": 0.0005902390112169087,
      "learning_rate": 0.0002893356341673857,
      "loss": 0.0027,
      "step": 123550
    },
    {
      "epoch": 35.54788610871441,
      "grad_norm": 0.03482171520590782,
      "learning_rate": 0.00028904802991084266,
      "loss": 0.0033,
      "step": 123600
    },
    {
      "epoch": 35.56226632154156,
      "grad_norm": 0.0022006456274539232,
      "learning_rate": 0.00028876042565429967,
      "loss": 0.0013,
      "step": 123650
    },
    {
      "epoch": 35.576646534368706,
      "grad_norm": 0.00414701784029603,
      "learning_rate": 0.00028847282139775674,
      "loss": 0.0038,
      "step": 123700
    },
    {
      "epoch": 35.59102674719586,
      "grad_norm": 0.00034048999077640474,
      "learning_rate": 0.0002881852171412137,
      "loss": 0.003,
      "step": 123750
    },
    {
      "epoch": 35.60540696002301,
      "grad_norm": 7.253233343362808e-05,
      "learning_rate": 0.0002878976128846707,
      "loss": 0.0026,
      "step": 123800
    },
    {
      "epoch": 35.61978717285016,
      "grad_norm": 0.009165780618786812,
      "learning_rate": 0.0002876100086281277,
      "loss": 0.0022,
      "step": 123850
    },
    {
      "epoch": 35.63416738567731,
      "grad_norm": 0.009348706342279911,
      "learning_rate": 0.0002873224043715847,
      "loss": 0.0012,
      "step": 123900
    },
    {
      "epoch": 35.64854759850446,
      "grad_norm": 0.00041100673843175173,
      "learning_rate": 0.0002870348001150417,
      "loss": 0.002,
      "step": 123950
    },
    {
      "epoch": 35.662927811331606,
      "grad_norm": 2.4048651539487764e-05,
      "learning_rate": 0.00028674719585849876,
      "loss": 0.006,
      "step": 124000
    },
    {
      "epoch": 35.677308024158755,
      "grad_norm": 0.0005035009817220271,
      "learning_rate": 0.0002864595916019557,
      "loss": 0.0024,
      "step": 124050
    },
    {
      "epoch": 35.69168823698591,
      "grad_norm": 0.004232469480484724,
      "learning_rate": 0.0002861719873454127,
      "loss": 0.002,
      "step": 124100
    },
    {
      "epoch": 35.70606844981306,
      "grad_norm": 4.8319467168767005e-05,
      "learning_rate": 0.00028588438308886974,
      "loss": 0.0044,
      "step": 124150
    },
    {
      "epoch": 35.72044866264021,
      "grad_norm": 1.5190208614512812e-05,
      "learning_rate": 0.0002855967788323267,
      "loss": 0.0002,
      "step": 124200
    },
    {
      "epoch": 35.73482887546736,
      "grad_norm": 1.1171237929374911e-05,
      "learning_rate": 0.0002853091745757837,
      "loss": 0.001,
      "step": 124250
    },
    {
      "epoch": 35.749209088294506,
      "grad_norm": 0.00291340216062963,
      "learning_rate": 0.0002850215703192408,
      "loss": 0.0015,
      "step": 124300
    },
    {
      "epoch": 35.763589301121655,
      "grad_norm": 0.005932190455496311,
      "learning_rate": 0.00028473396606269773,
      "loss": 0.0018,
      "step": 124350
    },
    {
      "epoch": 35.777969513948804,
      "grad_norm": 0.0002641960745677352,
      "learning_rate": 0.00028444636180615475,
      "loss": 0.0012,
      "step": 124400
    },
    {
      "epoch": 35.79234972677595,
      "grad_norm": 9.33537885430269e-05,
      "learning_rate": 0.00028415875754961176,
      "loss": 0.0027,
      "step": 124450
    },
    {
      "epoch": 35.80672993960311,
      "grad_norm": 0.017780005931854248,
      "learning_rate": 0.0002838711532930687,
      "loss": 0.0021,
      "step": 124500
    },
    {
      "epoch": 35.82111015243026,
      "grad_norm": 5.268317545414902e-05,
      "learning_rate": 0.00028358354903652573,
      "loss": 0.002,
      "step": 124550
    },
    {
      "epoch": 35.83549036525741,
      "grad_norm": 8.557849650969729e-05,
      "learning_rate": 0.0002832959447799828,
      "loss": 0.0018,
      "step": 124600
    },
    {
      "epoch": 35.849870578084555,
      "grad_norm": 6.888309872010723e-05,
      "learning_rate": 0.00028300834052343975,
      "loss": 0.0056,
      "step": 124650
    },
    {
      "epoch": 35.864250790911704,
      "grad_norm": 9.600088378647342e-05,
      "learning_rate": 0.00028272073626689676,
      "loss": 0.0048,
      "step": 124700
    },
    {
      "epoch": 35.87863100373885,
      "grad_norm": 1.7879494407679886e-05,
      "learning_rate": 0.0002824331320103538,
      "loss": 0.0026,
      "step": 124750
    },
    {
      "epoch": 35.893011216566,
      "grad_norm": 0.0037607720587402582,
      "learning_rate": 0.00028214552775381074,
      "loss": 0.0019,
      "step": 124800
    },
    {
      "epoch": 35.90739142939316,
      "grad_norm": 1.8103182810591534e-05,
      "learning_rate": 0.00028185792349726775,
      "loss": 0.0014,
      "step": 124850
    },
    {
      "epoch": 35.92177164222031,
      "grad_norm": 0.02226698026061058,
      "learning_rate": 0.0002815703192407248,
      "loss": 0.0011,
      "step": 124900
    },
    {
      "epoch": 35.936151855047456,
      "grad_norm": 4.7911296860547736e-05,
      "learning_rate": 0.00028128271498418177,
      "loss": 0.0043,
      "step": 124950
    },
    {
      "epoch": 35.950532067874605,
      "grad_norm": 0.0004295984108466655,
      "learning_rate": 0.0002809951107276388,
      "loss": 0.0064,
      "step": 125000
    },
    {
      "epoch": 35.96491228070175,
      "grad_norm": 2.4942610252765007e-05,
      "learning_rate": 0.0002807075064710958,
      "loss": 0.0006,
      "step": 125050
    },
    {
      "epoch": 35.9792924935289,
      "grad_norm": 2.2781077859690413e-05,
      "learning_rate": 0.00028041990221455275,
      "loss": 0.0015,
      "step": 125100
    },
    {
      "epoch": 35.99367270635605,
      "grad_norm": 0.0006609999109059572,
      "learning_rate": 0.00028013229795800977,
      "loss": 0.0003,
      "step": 125150
    },
    {
      "epoch": 36.0,
      "eval_loss": 0.010382350534200668,
      "eval_runtime": 16.6188,
      "eval_samples_per_second": 2871.381,
      "eval_steps_per_second": 44.889,
      "step": 125172
    },
    {
      "epoch": 36.00805291918321,
      "grad_norm": 0.013358088210225105,
      "learning_rate": 0.00027984469370146683,
      "loss": 0.0018,
      "step": 125200
    },
    {
      "epoch": 36.022433132010356,
      "grad_norm": 0.0018528855871409178,
      "learning_rate": 0.0002795570894449238,
      "loss": 0.0029,
      "step": 125250
    },
    {
      "epoch": 36.036813344837505,
      "grad_norm": 0.007458404637873173,
      "learning_rate": 0.0002792694851883808,
      "loss": 0.0009,
      "step": 125300
    },
    {
      "epoch": 36.051193557664654,
      "grad_norm": 1.813345261325594e-05,
      "learning_rate": 0.0002789818809318378,
      "loss": 0.0004,
      "step": 125350
    },
    {
      "epoch": 36.0655737704918,
      "grad_norm": 0.0001400008623022586,
      "learning_rate": 0.0002786942766752948,
      "loss": 0.0018,
      "step": 125400
    },
    {
      "epoch": 36.07995398331895,
      "grad_norm": 0.005228110123425722,
      "learning_rate": 0.0002784066724187518,
      "loss": 0.0032,
      "step": 125450
    },
    {
      "epoch": 36.0943341961461,
      "grad_norm": 6.229329301277176e-05,
      "learning_rate": 0.00027811906816220885,
      "loss": 0.0001,
      "step": 125500
    },
    {
      "epoch": 36.108714408973256,
      "grad_norm": 0.005200452636927366,
      "learning_rate": 0.0002778314639056658,
      "loss": 0.0026,
      "step": 125550
    },
    {
      "epoch": 36.123094621800405,
      "grad_norm": 6.115883297752589e-05,
      "learning_rate": 0.0002775438596491228,
      "loss": 0.0044,
      "step": 125600
    },
    {
      "epoch": 36.137474834627554,
      "grad_norm": 0.00042060136911459267,
      "learning_rate": 0.00027725625539257984,
      "loss": 0.0017,
      "step": 125650
    },
    {
      "epoch": 36.1518550474547,
      "grad_norm": 4.9395330279367045e-05,
      "learning_rate": 0.0002769686511360368,
      "loss": 0.0038,
      "step": 125700
    },
    {
      "epoch": 36.16623526028185,
      "grad_norm": 0.00042764798854477704,
      "learning_rate": 0.0002766810468794938,
      "loss": 0.0007,
      "step": 125750
    },
    {
      "epoch": 36.180615473109,
      "grad_norm": 8.970292401500046e-05,
      "learning_rate": 0.00027639344262295087,
      "loss": 0.0024,
      "step": 125800
    },
    {
      "epoch": 36.19499568593615,
      "grad_norm": 0.00015286155394278467,
      "learning_rate": 0.00027610583836640783,
      "loss": 0.001,
      "step": 125850
    },
    {
      "epoch": 36.209375898763305,
      "grad_norm": 0.00020128775213379413,
      "learning_rate": 0.00027581823410986484,
      "loss": 0.0016,
      "step": 125900
    },
    {
      "epoch": 36.223756111590454,
      "grad_norm": 0.013737978413701057,
      "learning_rate": 0.00027553062985332186,
      "loss": 0.0021,
      "step": 125950
    },
    {
      "epoch": 36.2381363244176,
      "grad_norm": 7.158332300605252e-05,
      "learning_rate": 0.0002752430255967788,
      "loss": 0.0007,
      "step": 126000
    },
    {
      "epoch": 36.25251653724475,
      "grad_norm": 5.8058005379280075e-05,
      "learning_rate": 0.0002749554213402358,
      "loss": 0.002,
      "step": 126050
    },
    {
      "epoch": 36.2668967500719,
      "grad_norm": 0.010898732580244541,
      "learning_rate": 0.00027466781708369284,
      "loss": 0.0008,
      "step": 126100
    },
    {
      "epoch": 36.28127696289905,
      "grad_norm": 6.703818507958204e-05,
      "learning_rate": 0.00027438021282714985,
      "loss": 0.001,
      "step": 126150
    },
    {
      "epoch": 36.2956571757262,
      "grad_norm": 0.00020973593927919865,
      "learning_rate": 0.00027409260857060686,
      "loss": 0.005,
      "step": 126200
    },
    {
      "epoch": 36.31003738855335,
      "grad_norm": 0.0014571823412552476,
      "learning_rate": 0.0002738050043140639,
      "loss": 0.0013,
      "step": 126250
    },
    {
      "epoch": 36.3244176013805,
      "grad_norm": 0.00035882601514458656,
      "learning_rate": 0.00027351740005752083,
      "loss": 0.0014,
      "step": 126300
    },
    {
      "epoch": 36.33879781420765,
      "grad_norm": 7.94561710790731e-05,
      "learning_rate": 0.00027322979580097785,
      "loss": 0.0032,
      "step": 126350
    },
    {
      "epoch": 36.3531780270348,
      "grad_norm": 0.000267995783360675,
      "learning_rate": 0.00027294219154443486,
      "loss": 0.0043,
      "step": 126400
    },
    {
      "epoch": 36.36755823986195,
      "grad_norm": 0.00012041429727105424,
      "learning_rate": 0.00027265458728789187,
      "loss": 0.002,
      "step": 126450
    },
    {
      "epoch": 36.3819384526891,
      "grad_norm": 0.00037866117781959474,
      "learning_rate": 0.0002723669830313489,
      "loss": 0.0019,
      "step": 126500
    },
    {
      "epoch": 36.39631866551625,
      "grad_norm": 0.0005954744410701096,
      "learning_rate": 0.0002720793787748059,
      "loss": 0.0057,
      "step": 126550
    },
    {
      "epoch": 36.410698878343396,
      "grad_norm": 0.002394809853285551,
      "learning_rate": 0.00027179177451826285,
      "loss": 0.0005,
      "step": 126600
    },
    {
      "epoch": 36.42507909117055,
      "grad_norm": 3.475858466117643e-05,
      "learning_rate": 0.00027150417026171986,
      "loss": 0.0028,
      "step": 126650
    },
    {
      "epoch": 36.4394593039977,
      "grad_norm": 0.00046559469774365425,
      "learning_rate": 0.0002712165660051769,
      "loss": 0.0024,
      "step": 126700
    },
    {
      "epoch": 36.45383951682485,
      "grad_norm": 2.5498198738205247e-05,
      "learning_rate": 0.0002709289617486339,
      "loss": 0.0019,
      "step": 126750
    },
    {
      "epoch": 36.468219729652,
      "grad_norm": 2.148323255823925e-05,
      "learning_rate": 0.0002706413574920909,
      "loss": 0.002,
      "step": 126800
    },
    {
      "epoch": 36.48259994247915,
      "grad_norm": 0.017114728689193726,
      "learning_rate": 0.0002703537532355479,
      "loss": 0.0033,
      "step": 126850
    },
    {
      "epoch": 36.4969801553063,
      "grad_norm": 0.010935652069747448,
      "learning_rate": 0.00027006614897900487,
      "loss": 0.0052,
      "step": 126900
    },
    {
      "epoch": 36.511360368133445,
      "grad_norm": 7.339239527937025e-05,
      "learning_rate": 0.0002697785447224619,
      "loss": 0.0009,
      "step": 126950
    },
    {
      "epoch": 36.5257405809606,
      "grad_norm": 0.010437951423227787,
      "learning_rate": 0.0002694909404659189,
      "loss": 0.0043,
      "step": 127000
    },
    {
      "epoch": 36.54012079378775,
      "grad_norm": 0.00013881105405744165,
      "learning_rate": 0.0002692033362093759,
      "loss": 0.0014,
      "step": 127050
    },
    {
      "epoch": 36.5545010066149,
      "grad_norm": 0.003967515658587217,
      "learning_rate": 0.0002689157319528329,
      "loss": 0.0019,
      "step": 127100
    },
    {
      "epoch": 36.56888121944205,
      "grad_norm": 0.0001840695331338793,
      "learning_rate": 0.00026862812769628993,
      "loss": 0.0009,
      "step": 127150
    },
    {
      "epoch": 36.5832614322692,
      "grad_norm": 0.007602743338793516,
      "learning_rate": 0.0002683405234397469,
      "loss": 0.0013,
      "step": 127200
    },
    {
      "epoch": 36.597641645096346,
      "grad_norm": 0.00017969604232348502,
      "learning_rate": 0.0002680529191832039,
      "loss": 0.0027,
      "step": 127250
    },
    {
      "epoch": 36.612021857923494,
      "grad_norm": 1.628678546694573e-05,
      "learning_rate": 0.0002677653149266609,
      "loss": 0.0013,
      "step": 127300
    },
    {
      "epoch": 36.62640207075065,
      "grad_norm": 0.00011982132127741352,
      "learning_rate": 0.00026747771067011793,
      "loss": 0.0025,
      "step": 127350
    },
    {
      "epoch": 36.6407822835778,
      "grad_norm": 0.0005912656197324395,
      "learning_rate": 0.00026719010641357494,
      "loss": 0.0017,
      "step": 127400
    },
    {
      "epoch": 36.65516249640495,
      "grad_norm": 0.0004243604198563844,
      "learning_rate": 0.00026690250215703195,
      "loss": 0.0005,
      "step": 127450
    },
    {
      "epoch": 36.6695427092321,
      "grad_norm": 2.0451912860153243e-05,
      "learning_rate": 0.0002666148979004889,
      "loss": 0.0036,
      "step": 127500
    },
    {
      "epoch": 36.683922922059246,
      "grad_norm": 2.7053980375058018e-05,
      "learning_rate": 0.0002663272936439459,
      "loss": 0.0058,
      "step": 127550
    },
    {
      "epoch": 36.698303134886395,
      "grad_norm": 0.004172195680439472,
      "learning_rate": 0.00026603968938740294,
      "loss": 0.0027,
      "step": 127600
    },
    {
      "epoch": 36.71268334771354,
      "grad_norm": 0.0005582214216701686,
      "learning_rate": 0.00026575208513085995,
      "loss": 0.0023,
      "step": 127650
    },
    {
      "epoch": 36.7270635605407,
      "grad_norm": 7.694902160437778e-05,
      "learning_rate": 0.00026546448087431696,
      "loss": 0.0022,
      "step": 127700
    },
    {
      "epoch": 36.74144377336785,
      "grad_norm": 2.4441236746497452e-05,
      "learning_rate": 0.00026517687661777397,
      "loss": 0.0023,
      "step": 127750
    },
    {
      "epoch": 36.755823986195,
      "grad_norm": 0.00011920525867026299,
      "learning_rate": 0.00026488927236123093,
      "loss": 0.0015,
      "step": 127800
    },
    {
      "epoch": 36.770204199022146,
      "grad_norm": 6.400610436685383e-05,
      "learning_rate": 0.00026460166810468794,
      "loss": 0.0023,
      "step": 127850
    },
    {
      "epoch": 36.784584411849295,
      "grad_norm": 5.125411553308368e-05,
      "learning_rate": 0.00026431406384814496,
      "loss": 0.0021,
      "step": 127900
    },
    {
      "epoch": 36.798964624676444,
      "grad_norm": 0.0013240593252703547,
      "learning_rate": 0.00026402645959160197,
      "loss": 0.0017,
      "step": 127950
    },
    {
      "epoch": 36.81334483750359,
      "grad_norm": 0.00029959302628412843,
      "learning_rate": 0.000263738855335059,
      "loss": 0.0031,
      "step": 128000
    },
    {
      "epoch": 36.82772505033074,
      "grad_norm": 0.0006073485710658133,
      "learning_rate": 0.000263451251078516,
      "loss": 0.0037,
      "step": 128050
    },
    {
      "epoch": 36.8421052631579,
      "grad_norm": 0.037221264094114304,
      "learning_rate": 0.00026316364682197295,
      "loss": 0.0009,
      "step": 128100
    },
    {
      "epoch": 36.856485475985046,
      "grad_norm": 0.002470980165526271,
      "learning_rate": 0.00026287604256542996,
      "loss": 0.0049,
      "step": 128150
    },
    {
      "epoch": 36.870865688812195,
      "grad_norm": 5.658691225107759e-05,
      "learning_rate": 0.000262588438308887,
      "loss": 0.0029,
      "step": 128200
    },
    {
      "epoch": 36.885245901639344,
      "grad_norm": 6.883823516545817e-05,
      "learning_rate": 0.000262300834052344,
      "loss": 0.0013,
      "step": 128250
    },
    {
      "epoch": 36.89962611446649,
      "grad_norm": 0.0002582875022199005,
      "learning_rate": 0.000262013229795801,
      "loss": 0.0039,
      "step": 128300
    },
    {
      "epoch": 36.91400632729364,
      "grad_norm": 0.0005147031624801457,
      "learning_rate": 0.000261725625539258,
      "loss": 0.0029,
      "step": 128350
    },
    {
      "epoch": 36.92838654012079,
      "grad_norm": 3.687725620693527e-05,
      "learning_rate": 0.00026143802128271497,
      "loss": 0.0021,
      "step": 128400
    },
    {
      "epoch": 36.942766752947946,
      "grad_norm": 2.3900573069113307e-05,
      "learning_rate": 0.000261150417026172,
      "loss": 0.0033,
      "step": 128450
    },
    {
      "epoch": 36.957146965775095,
      "grad_norm": 8.986912143882364e-05,
      "learning_rate": 0.000260862812769629,
      "loss": 0.0049,
      "step": 128500
    },
    {
      "epoch": 36.971527178602244,
      "grad_norm": 1.9765378965530545e-05,
      "learning_rate": 0.000260575208513086,
      "loss": 0.0009,
      "step": 128550
    },
    {
      "epoch": 36.98590739142939,
      "grad_norm": 0.0010836130240932107,
      "learning_rate": 0.000260287604256543,
      "loss": 0.0017,
      "step": 128600
    },
    {
      "epoch": 37.0,
      "eval_loss": 0.009971539489924908,
      "eval_runtime": 16.8253,
      "eval_samples_per_second": 2836.137,
      "eval_steps_per_second": 44.338,
      "step": 128649
    },
    {
      "epoch": 37.00028760425654,
      "grad_norm": 0.00011907681619049981,
      "learning_rate": 0.00026000000000000003,
      "loss": 0.0013,
      "step": 128650
    },
    {
      "epoch": 37.01466781708369,
      "grad_norm": 0.0016260079573839903,
      "learning_rate": 0.000259712395743457,
      "loss": 0.0025,
      "step": 128700
    },
    {
      "epoch": 37.02904802991084,
      "grad_norm": 0.0015427679754793644,
      "learning_rate": 0.000259424791486914,
      "loss": 0.0026,
      "step": 128750
    },
    {
      "epoch": 37.043428242737996,
      "grad_norm": 0.0023343900684267282,
      "learning_rate": 0.000259137187230371,
      "loss": 0.0006,
      "step": 128800
    },
    {
      "epoch": 37.057808455565144,
      "grad_norm": 7.399309106403962e-05,
      "learning_rate": 0.00025884958297382797,
      "loss": 0.0016,
      "step": 128850
    },
    {
      "epoch": 37.07218866839229,
      "grad_norm": 0.004990093410015106,
      "learning_rate": 0.00025856197871728504,
      "loss": 0.0037,
      "step": 128900
    },
    {
      "epoch": 37.08656888121944,
      "grad_norm": 1.813213930290658e-05,
      "learning_rate": 0.00025827437446074205,
      "loss": 0.001,
      "step": 128950
    },
    {
      "epoch": 37.10094909404659,
      "grad_norm": 0.0012303953990340233,
      "learning_rate": 0.000257986770204199,
      "loss": 0.0007,
      "step": 129000
    },
    {
      "epoch": 37.11532930687374,
      "grad_norm": 1.0074279089167248e-05,
      "learning_rate": 0.000257699165947656,
      "loss": 0.0008,
      "step": 129050
    },
    {
      "epoch": 37.12970951970089,
      "grad_norm": 0.0005533768562600017,
      "learning_rate": 0.00025741156169111303,
      "loss": 0.003,
      "step": 129100
    },
    {
      "epoch": 37.144089732528045,
      "grad_norm": 0.0004005937953479588,
      "learning_rate": 0.00025712395743457,
      "loss": 0.0027,
      "step": 129150
    },
    {
      "epoch": 37.15846994535519,
      "grad_norm": 4.1931965824915096e-05,
      "learning_rate": 0.00025683635317802706,
      "loss": 0.0022,
      "step": 129200
    },
    {
      "epoch": 37.17285015818234,
      "grad_norm": 0.003827576758340001,
      "learning_rate": 0.00025654874892148407,
      "loss": 0.0006,
      "step": 129250
    },
    {
      "epoch": 37.18723037100949,
      "grad_norm": 0.00020202877931296825,
      "learning_rate": 0.00025626114466494103,
      "loss": 0.0016,
      "step": 129300
    },
    {
      "epoch": 37.20161058383664,
      "grad_norm": 0.00017220922745764256,
      "learning_rate": 0.00025597354040839804,
      "loss": 0.0012,
      "step": 129350
    },
    {
      "epoch": 37.21599079666379,
      "grad_norm": 8.819732465781271e-05,
      "learning_rate": 0.00025568593615185505,
      "loss": 0.0031,
      "step": 129400
    },
    {
      "epoch": 37.23037100949094,
      "grad_norm": 0.020536283031105995,
      "learning_rate": 0.000255398331895312,
      "loss": 0.0017,
      "step": 129450
    },
    {
      "epoch": 37.244751222318094,
      "grad_norm": 0.005133090540766716,
      "learning_rate": 0.0002551107276387691,
      "loss": 0.0005,
      "step": 129500
    },
    {
      "epoch": 37.25913143514524,
      "grad_norm": 0.00485428748652339,
      "learning_rate": 0.0002548231233822261,
      "loss": 0.0053,
      "step": 129550
    },
    {
      "epoch": 37.27351164797239,
      "grad_norm": 3.57077551598195e-05,
      "learning_rate": 0.00025453551912568305,
      "loss": 0.002,
      "step": 129600
    },
    {
      "epoch": 37.28789186079954,
      "grad_norm": 3.492536779958755e-05,
      "learning_rate": 0.00025424791486914006,
      "loss": 0.0022,
      "step": 129650
    },
    {
      "epoch": 37.30227207362669,
      "grad_norm": 1.2172387869213708e-05,
      "learning_rate": 0.00025396031061259707,
      "loss": 0.0003,
      "step": 129700
    },
    {
      "epoch": 37.31665228645384,
      "grad_norm": 4.7615987568860874e-05,
      "learning_rate": 0.00025367270635605403,
      "loss": 0.0019,
      "step": 129750
    },
    {
      "epoch": 37.33103249928099,
      "grad_norm": 0.0007308291387744248,
      "learning_rate": 0.0002533851020995111,
      "loss": 0.0031,
      "step": 129800
    },
    {
      "epoch": 37.34541271210814,
      "grad_norm": 6.144705548649654e-05,
      "learning_rate": 0.0002530974978429681,
      "loss": 0.002,
      "step": 129850
    },
    {
      "epoch": 37.35979292493529,
      "grad_norm": 0.0002651993709150702,
      "learning_rate": 0.00025280989358642507,
      "loss": 0.0057,
      "step": 129900
    },
    {
      "epoch": 37.37417313776244,
      "grad_norm": 0.00018801946134772152,
      "learning_rate": 0.0002525222893298821,
      "loss": 0.0027,
      "step": 129950
    },
    {
      "epoch": 37.38855335058959,
      "grad_norm": 0.013137045316398144,
      "learning_rate": 0.0002522346850733391,
      "loss": 0.0027,
      "step": 130000
    },
    {
      "epoch": 37.40293356341674,
      "grad_norm": 0.0014998854603618383,
      "learning_rate": 0.00025194708081679605,
      "loss": 0.0028,
      "step": 130050
    },
    {
      "epoch": 37.41731377624389,
      "grad_norm": 0.00013898432371206582,
      "learning_rate": 0.0002516594765602531,
      "loss": 0.0059,
      "step": 130100
    },
    {
      "epoch": 37.431693989071036,
      "grad_norm": 0.0005961767164990306,
      "learning_rate": 0.00025137187230371013,
      "loss": 0.001,
      "step": 130150
    },
    {
      "epoch": 37.446074201898185,
      "grad_norm": 0.0007205449510365725,
      "learning_rate": 0.0002510842680471671,
      "loss": 0.0029,
      "step": 130200
    },
    {
      "epoch": 37.46045441472534,
      "grad_norm": 1.8919981812359765e-05,
      "learning_rate": 0.0002507966637906241,
      "loss": 0.0037,
      "step": 130250
    },
    {
      "epoch": 37.47483462755249,
      "grad_norm": 8.815663022687659e-05,
      "learning_rate": 0.0002505090595340811,
      "loss": 0.0024,
      "step": 130300
    },
    {
      "epoch": 37.48921484037964,
      "grad_norm": 0.000189579397556372,
      "learning_rate": 0.00025022145527753807,
      "loss": 0.0032,
      "step": 130350
    },
    {
      "epoch": 37.50359505320679,
      "grad_norm": 7.500483479816467e-05,
      "learning_rate": 0.00024993385102099514,
      "loss": 0.0015,
      "step": 130400
    },
    {
      "epoch": 37.517975266033936,
      "grad_norm": 0.00010312416270608082,
      "learning_rate": 0.0002496462467644521,
      "loss": 0.0025,
      "step": 130450
    },
    {
      "epoch": 37.532355478861085,
      "grad_norm": 0.002147659892216325,
      "learning_rate": 0.0002493586425079091,
      "loss": 0.0041,
      "step": 130500
    },
    {
      "epoch": 37.546735691688234,
      "grad_norm": 0.0003040149458684027,
      "learning_rate": 0.0002490710382513661,
      "loss": 0.0019,
      "step": 130550
    },
    {
      "epoch": 37.56111590451539,
      "grad_norm": 0.00040192794403992593,
      "learning_rate": 0.00024878343399482313,
      "loss": 0.0026,
      "step": 130600
    },
    {
      "epoch": 37.57549611734254,
      "grad_norm": 6.0839684010716155e-05,
      "learning_rate": 0.00024849582973828014,
      "loss": 0.0009,
      "step": 130650
    },
    {
      "epoch": 37.58987633016969,
      "grad_norm": 0.012010318227112293,
      "learning_rate": 0.00024820822548173716,
      "loss": 0.0023,
      "step": 130700
    },
    {
      "epoch": 37.604256542996836,
      "grad_norm": 0.00016028825484681875,
      "learning_rate": 0.0002479206212251941,
      "loss": 0.0008,
      "step": 130750
    },
    {
      "epoch": 37.618636755823985,
      "grad_norm": 5.016607974539511e-05,
      "learning_rate": 0.0002476330169686511,
      "loss": 0.0032,
      "step": 130800
    },
    {
      "epoch": 37.633016968651134,
      "grad_norm": 0.008051096461713314,
      "learning_rate": 0.00024734541271210814,
      "loss": 0.0047,
      "step": 130850
    },
    {
      "epoch": 37.64739718147828,
      "grad_norm": 0.007750803604722023,
      "learning_rate": 0.00024705780845556515,
      "loss": 0.0019,
      "step": 130900
    },
    {
      "epoch": 37.66177739430544,
      "grad_norm": 0.0018659336492419243,
      "learning_rate": 0.00024677020419902216,
      "loss": 0.0005,
      "step": 130950
    },
    {
      "epoch": 37.67615760713259,
      "grad_norm": 0.005434991791844368,
      "learning_rate": 0.0002464825999424792,
      "loss": 0.0042,
      "step": 131000
    },
    {
      "epoch": 37.69053781995974,
      "grad_norm": 0.00047593514318577945,
      "learning_rate": 0.00024619499568593613,
      "loss": 0.0029,
      "step": 131050
    },
    {
      "epoch": 37.704918032786885,
      "grad_norm": 0.022251121699810028,
      "learning_rate": 0.00024590739142939315,
      "loss": 0.0039,
      "step": 131100
    },
    {
      "epoch": 37.719298245614034,
      "grad_norm": 0.005990950390696526,
      "learning_rate": 0.00024561978717285016,
      "loss": 0.0019,
      "step": 131150
    },
    {
      "epoch": 37.73367845844118,
      "grad_norm": 0.0002626632049214095,
      "learning_rate": 0.00024533218291630717,
      "loss": 0.006,
      "step": 131200
    },
    {
      "epoch": 37.74805867126833,
      "grad_norm": 0.0020824670791625977,
      "learning_rate": 0.0002450445786597642,
      "loss": 0.0011,
      "step": 131250
    },
    {
      "epoch": 37.76243888409549,
      "grad_norm": 0.0002689945395104587,
      "learning_rate": 0.0002447569744032212,
      "loss": 0.002,
      "step": 131300
    },
    {
      "epoch": 37.77681909692264,
      "grad_norm": 0.0040203589014709,
      "learning_rate": 0.00024446937014667815,
      "loss": 0.0016,
      "step": 131350
    },
    {
      "epoch": 37.791199309749786,
      "grad_norm": 0.005270320922136307,
      "learning_rate": 0.00024418176589013517,
      "loss": 0.0015,
      "step": 131400
    },
    {
      "epoch": 37.805579522576934,
      "grad_norm": 0.00010674529039533809,
      "learning_rate": 0.00024389416163359218,
      "loss": 0.0021,
      "step": 131450
    },
    {
      "epoch": 37.81995973540408,
      "grad_norm": 1.2612522368726786e-05,
      "learning_rate": 0.00024360655737704916,
      "loss": 0.0045,
      "step": 131500
    },
    {
      "epoch": 37.83433994823123,
      "grad_norm": 0.011745997704565525,
      "learning_rate": 0.0002433189531205062,
      "loss": 0.0005,
      "step": 131550
    },
    {
      "epoch": 37.84872016105838,
      "grad_norm": 0.0018968877848237753,
      "learning_rate": 0.0002430313488639632,
      "loss": 0.0008,
      "step": 131600
    },
    {
      "epoch": 37.86310037388554,
      "grad_norm": 1.6776122720330022e-05,
      "learning_rate": 0.00024274374460742017,
      "loss": 0.0003,
      "step": 131650
    },
    {
      "epoch": 37.877480586712686,
      "grad_norm": 0.0001099982691812329,
      "learning_rate": 0.0002424561403508772,
      "loss": 0.0013,
      "step": 131700
    },
    {
      "epoch": 37.891860799539835,
      "grad_norm": 0.0009960213210433722,
      "learning_rate": 0.0002421685360943342,
      "loss": 0.0019,
      "step": 131750
    },
    {
      "epoch": 37.90624101236698,
      "grad_norm": 9.756206418387592e-05,
      "learning_rate": 0.00024188093183779118,
      "loss": 0.0005,
      "step": 131800
    },
    {
      "epoch": 37.92062122519413,
      "grad_norm": 0.0021456084214150906,
      "learning_rate": 0.00024159332758124822,
      "loss": 0.004,
      "step": 131850
    },
    {
      "epoch": 37.93500143802128,
      "grad_norm": 4.751833330374211e-05,
      "learning_rate": 0.0002413057233247052,
      "loss": 0.0002,
      "step": 131900
    },
    {
      "epoch": 37.94938165084843,
      "grad_norm": 4.273616650607437e-05,
      "learning_rate": 0.00024101811906816222,
      "loss": 0.0026,
      "step": 131950
    },
    {
      "epoch": 37.96376186367558,
      "grad_norm": 0.0020122823771089315,
      "learning_rate": 0.00024073051481161923,
      "loss": 0.0013,
      "step": 132000
    },
    {
      "epoch": 37.978142076502735,
      "grad_norm": 0.0028165241237729788,
      "learning_rate": 0.00024044291055507622,
      "loss": 0.0049,
      "step": 132050
    },
    {
      "epoch": 37.992522289329884,
      "grad_norm": 0.012047610245645046,
      "learning_rate": 0.00024015530629853323,
      "loss": 0.0036,
      "step": 132100
    },
    {
      "epoch": 38.0,
      "eval_loss": 0.009879986755549908,
      "eval_runtime": 17.7675,
      "eval_samples_per_second": 2685.749,
      "eval_steps_per_second": 41.987,
      "step": 132126
    },
    {
      "epoch": 38.00690250215703,
      "grad_norm": 0.0006505087367258966,
      "learning_rate": 0.00023986770204199024,
      "loss": 0.0009,
      "step": 132150
    },
    {
      "epoch": 38.02128271498418,
      "grad_norm": 0.00011277698649792,
      "learning_rate": 0.00023958009778544723,
      "loss": 0.0002,
      "step": 132200
    },
    {
      "epoch": 38.03566292781133,
      "grad_norm": 0.0003120491746813059,
      "learning_rate": 0.00023929249352890424,
      "loss": 0.0022,
      "step": 132250
    },
    {
      "epoch": 38.05004314063848,
      "grad_norm": 2.9189281121944077e-05,
      "learning_rate": 0.00023900488927236125,
      "loss": 0.0023,
      "step": 132300
    },
    {
      "epoch": 38.06442335346563,
      "grad_norm": 0.012198901735246181,
      "learning_rate": 0.00023871728501581824,
      "loss": 0.0014,
      "step": 132350
    },
    {
      "epoch": 38.078803566292784,
      "grad_norm": 0.0020587127655744553,
      "learning_rate": 0.00023842968075927525,
      "loss": 0.0039,
      "step": 132400
    },
    {
      "epoch": 38.09318377911993,
      "grad_norm": 0.0001813528942875564,
      "learning_rate": 0.00023814207650273226,
      "loss": 0.0046,
      "step": 132450
    },
    {
      "epoch": 38.10756399194708,
      "grad_norm": 0.002560378983616829,
      "learning_rate": 0.00023785447224618925,
      "loss": 0.0021,
      "step": 132500
    },
    {
      "epoch": 38.12194420477423,
      "grad_norm": 0.00014489331806544214,
      "learning_rate": 0.00023756686798964626,
      "loss": 0.0025,
      "step": 132550
    },
    {
      "epoch": 38.13632441760138,
      "grad_norm": 0.0006545889773406088,
      "learning_rate": 0.00023727926373310327,
      "loss": 0.0029,
      "step": 132600
    },
    {
      "epoch": 38.15070463042853,
      "grad_norm": 4.4851669372292235e-05,
      "learning_rate": 0.00023699165947656026,
      "loss": 0.0006,
      "step": 132650
    },
    {
      "epoch": 38.16508484325568,
      "grad_norm": 2.4730881705181673e-05,
      "learning_rate": 0.00023670405522001727,
      "loss": 0.0017,
      "step": 132700
    },
    {
      "epoch": 38.17946505608283,
      "grad_norm": 0.012575882486999035,
      "learning_rate": 0.00023641645096347428,
      "loss": 0.0035,
      "step": 132750
    },
    {
      "epoch": 38.19384526890998,
      "grad_norm": 0.000249286531470716,
      "learning_rate": 0.00023612884670693127,
      "loss": 0.0024,
      "step": 132800
    },
    {
      "epoch": 38.20822548173713,
      "grad_norm": 0.00440627709031105,
      "learning_rate": 0.00023584124245038828,
      "loss": 0.0019,
      "step": 132850
    },
    {
      "epoch": 38.22260569456428,
      "grad_norm": 0.000564699643291533,
      "learning_rate": 0.00023555363819384526,
      "loss": 0.0011,
      "step": 132900
    },
    {
      "epoch": 38.23698590739143,
      "grad_norm": 0.023755984380841255,
      "learning_rate": 0.00023526603393730228,
      "loss": 0.0014,
      "step": 132950
    },
    {
      "epoch": 38.25136612021858,
      "grad_norm": 6.355166988214478e-05,
      "learning_rate": 0.0002349784296807593,
      "loss": 0.002,
      "step": 133000
    },
    {
      "epoch": 38.265746333045726,
      "grad_norm": 3.5547232982935384e-05,
      "learning_rate": 0.00023469082542421627,
      "loss": 0.0015,
      "step": 133050
    },
    {
      "epoch": 38.28012654587288,
      "grad_norm": 6.264220428420231e-05,
      "learning_rate": 0.00023440322116767329,
      "loss": 0.0049,
      "step": 133100
    },
    {
      "epoch": 38.29450675870003,
      "grad_norm": 0.007079252041876316,
      "learning_rate": 0.0002341156169111303,
      "loss": 0.0017,
      "step": 133150
    },
    {
      "epoch": 38.30888697152718,
      "grad_norm": 0.0003379731788299978,
      "learning_rate": 0.00023382801265458728,
      "loss": 0.002,
      "step": 133200
    },
    {
      "epoch": 38.32326718435433,
      "grad_norm": 0.006491777487099171,
      "learning_rate": 0.0002335404083980443,
      "loss": 0.0018,
      "step": 133250
    },
    {
      "epoch": 38.33764739718148,
      "grad_norm": 0.020912203937768936,
      "learning_rate": 0.0002332528041415013,
      "loss": 0.0003,
      "step": 133300
    },
    {
      "epoch": 38.352027610008626,
      "grad_norm": 6.67464264552109e-05,
      "learning_rate": 0.0002329651998849583,
      "loss": 0.0038,
      "step": 133350
    },
    {
      "epoch": 38.366407822835775,
      "grad_norm": 5.7344397646375e-05,
      "learning_rate": 0.0002326775956284153,
      "loss": 0.0024,
      "step": 133400
    },
    {
      "epoch": 38.38078803566293,
      "grad_norm": 5.4404888942372054e-05,
      "learning_rate": 0.00023238999137187232,
      "loss": 0.004,
      "step": 133450
    },
    {
      "epoch": 38.39516824849008,
      "grad_norm": 2.945351297967136e-05,
      "learning_rate": 0.0002321023871153293,
      "loss": 0.0014,
      "step": 133500
    },
    {
      "epoch": 38.40954846131723,
      "grad_norm": 0.000293751887511462,
      "learning_rate": 0.00023181478285878631,
      "loss": 0.0043,
      "step": 133550
    },
    {
      "epoch": 38.42392867414438,
      "grad_norm": 0.0001613796193851158,
      "learning_rate": 0.00023152717860224333,
      "loss": 0.0047,
      "step": 133600
    },
    {
      "epoch": 38.43830888697153,
      "grad_norm": 0.001928649260662496,
      "learning_rate": 0.0002312395743457003,
      "loss": 0.0017,
      "step": 133650
    },
    {
      "epoch": 38.452689099798675,
      "grad_norm": 0.00040745100704953074,
      "learning_rate": 0.00023095197008915732,
      "loss": 0.0025,
      "step": 133700
    },
    {
      "epoch": 38.467069312625824,
      "grad_norm": 3.266589919803664e-05,
      "learning_rate": 0.00023066436583261434,
      "loss": 0.0031,
      "step": 133750
    },
    {
      "epoch": 38.48144952545297,
      "grad_norm": 0.0011660464806482196,
      "learning_rate": 0.00023037676157607132,
      "loss": 0.0026,
      "step": 133800
    },
    {
      "epoch": 38.49582973828013,
      "grad_norm": 0.0017120076809078455,
      "learning_rate": 0.00023008915731952833,
      "loss": 0.0006,
      "step": 133850
    },
    {
      "epoch": 38.51020995110728,
      "grad_norm": 0.015052570030093193,
      "learning_rate": 0.00022980155306298535,
      "loss": 0.0006,
      "step": 133900
    },
    {
      "epoch": 38.52459016393443,
      "grad_norm": 7.2823568189051e-05,
      "learning_rate": 0.00022951394880644233,
      "loss": 0.0017,
      "step": 133950
    },
    {
      "epoch": 38.538970376761576,
      "grad_norm": 0.00037092616548761725,
      "learning_rate": 0.00022922634454989934,
      "loss": 0.0013,
      "step": 134000
    },
    {
      "epoch": 38.553350589588725,
      "grad_norm": 0.0006575884181074798,
      "learning_rate": 0.00022893874029335636,
      "loss": 0.0024,
      "step": 134050
    },
    {
      "epoch": 38.56773080241587,
      "grad_norm": 0.0009101448231376708,
      "learning_rate": 0.00022865113603681334,
      "loss": 0.0044,
      "step": 134100
    },
    {
      "epoch": 38.58211101524302,
      "grad_norm": 0.0005975381936877966,
      "learning_rate": 0.00022836353178027035,
      "loss": 0.0015,
      "step": 134150
    },
    {
      "epoch": 38.59649122807018,
      "grad_norm": 2.340483115403913e-05,
      "learning_rate": 0.00022807592752372737,
      "loss": 0.0015,
      "step": 134200
    },
    {
      "epoch": 38.61087144089733,
      "grad_norm": 0.008550279773771763,
      "learning_rate": 0.00022778832326718435,
      "loss": 0.0022,
      "step": 134250
    },
    {
      "epoch": 38.625251653724476,
      "grad_norm": 0.0017966064624488354,
      "learning_rate": 0.00022750071901064136,
      "loss": 0.0032,
      "step": 134300
    },
    {
      "epoch": 38.639631866551625,
      "grad_norm": 0.04233233630657196,
      "learning_rate": 0.00022721311475409838,
      "loss": 0.0036,
      "step": 134350
    },
    {
      "epoch": 38.654012079378774,
      "grad_norm": 0.006250564008951187,
      "learning_rate": 0.00022692551049755536,
      "loss": 0.0015,
      "step": 134400
    },
    {
      "epoch": 38.66839229220592,
      "grad_norm": 0.00011473151971586049,
      "learning_rate": 0.00022663790624101237,
      "loss": 0.0019,
      "step": 134450
    },
    {
      "epoch": 38.68277250503307,
      "grad_norm": 7.099694630596787e-05,
      "learning_rate": 0.00022635030198446939,
      "loss": 0.0032,
      "step": 134500
    },
    {
      "epoch": 38.69715271786023,
      "grad_norm": 8.483973215334117e-05,
      "learning_rate": 0.00022606269772792637,
      "loss": 0.0008,
      "step": 134550
    },
    {
      "epoch": 38.711532930687376,
      "grad_norm": 6.77767166052945e-05,
      "learning_rate": 0.00022577509347138338,
      "loss": 0.0008,
      "step": 134600
    },
    {
      "epoch": 38.725913143514525,
      "grad_norm": 2.2967782570049167e-05,
      "learning_rate": 0.0002254874892148404,
      "loss": 0.0028,
      "step": 134650
    },
    {
      "epoch": 38.740293356341674,
      "grad_norm": 3.892848690156825e-05,
      "learning_rate": 0.00022519988495829738,
      "loss": 0.0006,
      "step": 134700
    },
    {
      "epoch": 38.75467356916882,
      "grad_norm": 0.004621707834303379,
      "learning_rate": 0.0002249122807017544,
      "loss": 0.0052,
      "step": 134750
    },
    {
      "epoch": 38.76905378199597,
      "grad_norm": 2.5018445739988238e-05,
      "learning_rate": 0.0002246246764452114,
      "loss": 0.0009,
      "step": 134800
    },
    {
      "epoch": 38.78343399482312,
      "grad_norm": 0.0010659623658284545,
      "learning_rate": 0.0002243370721886684,
      "loss": 0.0013,
      "step": 134850
    },
    {
      "epoch": 38.797814207650276,
      "grad_norm": 0.001984239323064685,
      "learning_rate": 0.0002240494679321254,
      "loss": 0.0013,
      "step": 134900
    },
    {
      "epoch": 38.812194420477425,
      "grad_norm": 0.0007715236279182136,
      "learning_rate": 0.00022376186367558241,
      "loss": 0.0034,
      "step": 134950
    },
    {
      "epoch": 38.826574633304574,
      "grad_norm": 3.742452463484369e-05,
      "learning_rate": 0.0002234742594190394,
      "loss": 0.0048,
      "step": 135000
    },
    {
      "epoch": 38.84095484613172,
      "grad_norm": 2.1769337763544172e-05,
      "learning_rate": 0.0002231866551624964,
      "loss": 0.0007,
      "step": 135050
    },
    {
      "epoch": 38.85533505895887,
      "grad_norm": 0.0019090857822448015,
      "learning_rate": 0.00022289905090595342,
      "loss": 0.0057,
      "step": 135100
    },
    {
      "epoch": 38.86971527178602,
      "grad_norm": 0.005144490394741297,
      "learning_rate": 0.0002226114466494104,
      "loss": 0.0025,
      "step": 135150
    },
    {
      "epoch": 38.88409548461317,
      "grad_norm": 0.0005304727237671614,
      "learning_rate": 0.00022232384239286742,
      "loss": 0.0035,
      "step": 135200
    },
    {
      "epoch": 38.898475697440325,
      "grad_norm": 0.0004599441890604794,
      "learning_rate": 0.00022203623813632443,
      "loss": 0.0014,
      "step": 135250
    },
    {
      "epoch": 38.912855910267474,
      "grad_norm": 0.00040857522981241345,
      "learning_rate": 0.00022174863387978142,
      "loss": 0.0027,
      "step": 135300
    },
    {
      "epoch": 38.92723612309462,
      "grad_norm": 7.818474114174023e-05,
      "learning_rate": 0.00022146102962323843,
      "loss": 0.0003,
      "step": 135350
    },
    {
      "epoch": 38.94161633592177,
      "grad_norm": 0.0003412693040445447,
      "learning_rate": 0.00022117342536669544,
      "loss": 0.0049,
      "step": 135400
    },
    {
      "epoch": 38.95599654874892,
      "grad_norm": 0.00040747964521870017,
      "learning_rate": 0.00022088582111015243,
      "loss": 0.0031,
      "step": 135450
    },
    {
      "epoch": 38.97037676157607,
      "grad_norm": 6.292338639468653e-06,
      "learning_rate": 0.00022059821685360944,
      "loss": 0.002,
      "step": 135500
    },
    {
      "epoch": 38.98475697440322,
      "grad_norm": 0.0023929099552333355,
      "learning_rate": 0.00022031061259706645,
      "loss": 0.0012,
      "step": 135550
    },
    {
      "epoch": 38.999137187230374,
      "grad_norm": 0.00042088062036782503,
      "learning_rate": 0.00022002300834052344,
      "loss": 0.0014,
      "step": 135600
    },
    {
      "epoch": 39.0,
      "eval_loss": 0.010013116523623466,
      "eval_runtime": 16.5593,
      "eval_samples_per_second": 2881.707,
      "eval_steps_per_second": 45.05,
      "step": 135603
    },
    {
      "epoch": 39.01351740005752,
      "grad_norm": 0.0022407658398151398,
      "learning_rate": 0.00021973540408398045,
      "loss": 0.0018,
      "step": 135650
    },
    {
      "epoch": 39.02789761288467,
      "grad_norm": 0.0031869844533503056,
      "learning_rate": 0.00021944779982743746,
      "loss": 0.0027,
      "step": 135700
    },
    {
      "epoch": 39.04227782571182,
      "grad_norm": 0.009179575368762016,
      "learning_rate": 0.00021916019557089445,
      "loss": 0.0006,
      "step": 135750
    },
    {
      "epoch": 39.05665803853897,
      "grad_norm": 0.00014508927415590733,
      "learning_rate": 0.00021887259131435143,
      "loss": 0.0019,
      "step": 135800
    },
    {
      "epoch": 39.07103825136612,
      "grad_norm": 4.870297198067419e-05,
      "learning_rate": 0.00021858498705780847,
      "loss": 0.001,
      "step": 135850
    },
    {
      "epoch": 39.08541846419327,
      "grad_norm": 0.00020286701328586787,
      "learning_rate": 0.00021829738280126546,
      "loss": 0.0015,
      "step": 135900
    },
    {
      "epoch": 39.09979867702042,
      "grad_norm": 0.0006259778165258467,
      "learning_rate": 0.00021800977854472244,
      "loss": 0.001,
      "step": 135950
    },
    {
      "epoch": 39.11417888984757,
      "grad_norm": 1.546014755149372e-05,
      "learning_rate": 0.00021772217428817948,
      "loss": 0.002,
      "step": 136000
    },
    {
      "epoch": 39.12855910267472,
      "grad_norm": 0.00336167449131608,
      "learning_rate": 0.00021743457003163647,
      "loss": 0.0002,
      "step": 136050
    },
    {
      "epoch": 39.14293931550187,
      "grad_norm": 3.195460158167407e-05,
      "learning_rate": 0.00021714696577509345,
      "loss": 0.002,
      "step": 136100
    },
    {
      "epoch": 39.15731952832902,
      "grad_norm": 0.0001316030538873747,
      "learning_rate": 0.0002168593615185505,
      "loss": 0.002,
      "step": 136150
    },
    {
      "epoch": 39.17169974115617,
      "grad_norm": 0.009101709350943565,
      "learning_rate": 0.00021657175726200748,
      "loss": 0.0021,
      "step": 136200
    },
    {
      "epoch": 39.18607995398332,
      "grad_norm": 5.25709729117807e-05,
      "learning_rate": 0.00021628415300546446,
      "loss": 0.0015,
      "step": 136250
    },
    {
      "epoch": 39.200460166810466,
      "grad_norm": 0.00019531065481714904,
      "learning_rate": 0.0002159965487489215,
      "loss": 0.0018,
      "step": 136300
    },
    {
      "epoch": 39.21484037963762,
      "grad_norm": 0.00021146716608200222,
      "learning_rate": 0.0002157089444923785,
      "loss": 0.0007,
      "step": 136350
    },
    {
      "epoch": 39.22922059246477,
      "grad_norm": 8.441027603112161e-05,
      "learning_rate": 0.00021542134023583547,
      "loss": 0.001,
      "step": 136400
    },
    {
      "epoch": 39.24360080529192,
      "grad_norm": 0.00019226764561608434,
      "learning_rate": 0.0002151337359792925,
      "loss": 0.0008,
      "step": 136450
    },
    {
      "epoch": 39.25798101811907,
      "grad_norm": 5.413092003436759e-05,
      "learning_rate": 0.0002148461317227495,
      "loss": 0.001,
      "step": 136500
    },
    {
      "epoch": 39.27236123094622,
      "grad_norm": 0.0027423594146966934,
      "learning_rate": 0.00021455852746620648,
      "loss": 0.0031,
      "step": 136550
    },
    {
      "epoch": 39.286741443773366,
      "grad_norm": 0.00014327741519082338,
      "learning_rate": 0.00021427092320966352,
      "loss": 0.0032,
      "step": 136600
    },
    {
      "epoch": 39.301121656600515,
      "grad_norm": 0.0006134865689091384,
      "learning_rate": 0.0002139833189531205,
      "loss": 0.0036,
      "step": 136650
    },
    {
      "epoch": 39.31550186942767,
      "grad_norm": 5.312204302754253e-05,
      "learning_rate": 0.0002136957146965775,
      "loss": 0.0013,
      "step": 136700
    },
    {
      "epoch": 39.32988208225482,
      "grad_norm": 0.0006606120732612908,
      "learning_rate": 0.00021340811044003453,
      "loss": 0.0007,
      "step": 136750
    },
    {
      "epoch": 39.34426229508197,
      "grad_norm": 3.598582770791836e-05,
      "learning_rate": 0.00021312050618349152,
      "loss": 0.0035,
      "step": 136800
    },
    {
      "epoch": 39.35864250790912,
      "grad_norm": 0.01332444604486227,
      "learning_rate": 0.0002128329019269485,
      "loss": 0.0043,
      "step": 136850
    },
    {
      "epoch": 39.373022720736266,
      "grad_norm": 9.941573807736859e-05,
      "learning_rate": 0.00021254529767040554,
      "loss": 0.0035,
      "step": 136900
    },
    {
      "epoch": 39.387402933563415,
      "grad_norm": 0.0004069144488312304,
      "learning_rate": 0.00021225769341386253,
      "loss": 0.0023,
      "step": 136950
    },
    {
      "epoch": 39.401783146390564,
      "grad_norm": 0.006938531994819641,
      "learning_rate": 0.0002119700891573195,
      "loss": 0.0022,
      "step": 137000
    },
    {
      "epoch": 39.41616335921772,
      "grad_norm": 0.0007317104027606547,
      "learning_rate": 0.00021168248490077655,
      "loss": 0.002,
      "step": 137050
    },
    {
      "epoch": 39.43054357204487,
      "grad_norm": 0.0003744497080333531,
      "learning_rate": 0.00021139488064423354,
      "loss": 0.0041,
      "step": 137100
    },
    {
      "epoch": 39.44492378487202,
      "grad_norm": 0.011067922227084637,
      "learning_rate": 0.00021110727638769052,
      "loss": 0.0014,
      "step": 137150
    },
    {
      "epoch": 39.459303997699166,
      "grad_norm": 0.00012572127161547542,
      "learning_rate": 0.00021081967213114756,
      "loss": 0.0043,
      "step": 137200
    },
    {
      "epoch": 39.473684210526315,
      "grad_norm": 0.003997953608632088,
      "learning_rate": 0.00021053206787460455,
      "loss": 0.0007,
      "step": 137250
    },
    {
      "epoch": 39.488064423353464,
      "grad_norm": 0.008359178900718689,
      "learning_rate": 0.00021024446361806153,
      "loss": 0.0011,
      "step": 137300
    },
    {
      "epoch": 39.50244463618061,
      "grad_norm": 1.601644908078015e-05,
      "learning_rate": 0.00020995685936151857,
      "loss": 0.0016,
      "step": 137350
    },
    {
      "epoch": 39.51682484900777,
      "grad_norm": 0.0002875546633731574,
      "learning_rate": 0.00020966925510497556,
      "loss": 0.0038,
      "step": 137400
    },
    {
      "epoch": 39.53120506183492,
      "grad_norm": 0.0006790504558011889,
      "learning_rate": 0.00020938165084843254,
      "loss": 0.001,
      "step": 137450
    },
    {
      "epoch": 39.545585274662066,
      "grad_norm": 0.00039899273542687297,
      "learning_rate": 0.00020909404659188958,
      "loss": 0.0039,
      "step": 137500
    },
    {
      "epoch": 39.559965487489215,
      "grad_norm": 0.00019480256014503539,
      "learning_rate": 0.00020880644233534657,
      "loss": 0.0029,
      "step": 137550
    },
    {
      "epoch": 39.574345700316364,
      "grad_norm": 0.001620598486624658,
      "learning_rate": 0.00020851883807880355,
      "loss": 0.0008,
      "step": 137600
    },
    {
      "epoch": 39.58872591314351,
      "grad_norm": 0.0003857267729472369,
      "learning_rate": 0.0002082312338222606,
      "loss": 0.0012,
      "step": 137650
    },
    {
      "epoch": 39.60310612597066,
      "grad_norm": 0.0185401551425457,
      "learning_rate": 0.00020794362956571758,
      "loss": 0.0056,
      "step": 137700
    },
    {
      "epoch": 39.61748633879782,
      "grad_norm": 0.0001528592692920938,
      "learning_rate": 0.00020765602530917456,
      "loss": 0.0019,
      "step": 137750
    },
    {
      "epoch": 39.63186655162497,
      "grad_norm": 0.000121010270959232,
      "learning_rate": 0.0002073684210526316,
      "loss": 0.0003,
      "step": 137800
    },
    {
      "epoch": 39.646246764452115,
      "grad_norm": 3.110904435743578e-05,
      "learning_rate": 0.00020708081679608859,
      "loss": 0.0041,
      "step": 137850
    },
    {
      "epoch": 39.660626977279264,
      "grad_norm": 0.004092792980372906,
      "learning_rate": 0.00020679321253954557,
      "loss": 0.0037,
      "step": 137900
    },
    {
      "epoch": 39.67500719010641,
      "grad_norm": 0.004320528358221054,
      "learning_rate": 0.0002065056082830026,
      "loss": 0.0036,
      "step": 137950
    },
    {
      "epoch": 39.68938740293356,
      "grad_norm": 6.117626617196947e-05,
      "learning_rate": 0.0002062180040264596,
      "loss": 0.0038,
      "step": 138000
    },
    {
      "epoch": 39.70376761576071,
      "grad_norm": 2.848170515790116e-05,
      "learning_rate": 0.00020593039976991658,
      "loss": 0.0008,
      "step": 138050
    },
    {
      "epoch": 39.71814782858786,
      "grad_norm": 0.014474884606897831,
      "learning_rate": 0.00020564279551337362,
      "loss": 0.004,
      "step": 138100
    },
    {
      "epoch": 39.732528041415016,
      "grad_norm": 2.088194560201373e-05,
      "learning_rate": 0.0002053551912568306,
      "loss": 0.0003,
      "step": 138150
    },
    {
      "epoch": 39.746908254242165,
      "grad_norm": 0.00036911104689352214,
      "learning_rate": 0.0002050675870002876,
      "loss": 0.0029,
      "step": 138200
    },
    {
      "epoch": 39.76128846706931,
      "grad_norm": 1.1772004654631019e-05,
      "learning_rate": 0.00020477998274374463,
      "loss": 0.004,
      "step": 138250
    },
    {
      "epoch": 39.77566867989646,
      "grad_norm": 0.003611332504078746,
      "learning_rate": 0.00020449237848720162,
      "loss": 0.0045,
      "step": 138300
    },
    {
      "epoch": 39.79004889272361,
      "grad_norm": 0.0002794325118884444,
      "learning_rate": 0.0002042047742306586,
      "loss": 0.0015,
      "step": 138350
    },
    {
      "epoch": 39.80442910555076,
      "grad_norm": 0.0001826479274313897,
      "learning_rate": 0.00020391716997411564,
      "loss": 0.0013,
      "step": 138400
    },
    {
      "epoch": 39.81880931837791,
      "grad_norm": 4.840939800487831e-05,
      "learning_rate": 0.00020362956571757263,
      "loss": 0.0034,
      "step": 138450
    },
    {
      "epoch": 39.833189531205065,
      "grad_norm": 0.0001416744344169274,
      "learning_rate": 0.0002033419614610296,
      "loss": 0.0049,
      "step": 138500
    },
    {
      "epoch": 39.847569744032214,
      "grad_norm": 0.004993248265236616,
      "learning_rate": 0.00020305435720448662,
      "loss": 0.0033,
      "step": 138550
    },
    {
      "epoch": 39.86194995685936,
      "grad_norm": 0.0045370371080935,
      "learning_rate": 0.00020276675294794363,
      "loss": 0.0021,
      "step": 138600
    },
    {
      "epoch": 39.87633016968651,
      "grad_norm": 2.0203744497848675e-05,
      "learning_rate": 0.00020247914869140062,
      "loss": 0.001,
      "step": 138650
    },
    {
      "epoch": 39.89071038251366,
      "grad_norm": 0.000517805281560868,
      "learning_rate": 0.00020219154443485763,
      "loss": 0.002,
      "step": 138700
    },
    {
      "epoch": 39.90509059534081,
      "grad_norm": 0.0008382481755688787,
      "learning_rate": 0.00020190394017831464,
      "loss": 0.0011,
      "step": 138750
    },
    {
      "epoch": 39.91947080816796,
      "grad_norm": 0.0001539189397590235,
      "learning_rate": 0.00020161633592177163,
      "loss": 0.0053,
      "step": 138800
    },
    {
      "epoch": 39.933851020995114,
      "grad_norm": 0.0008531436324119568,
      "learning_rate": 0.00020132873166522864,
      "loss": 0.0028,
      "step": 138850
    },
    {
      "epoch": 39.94823123382226,
      "grad_norm": 0.0003151061537209898,
      "learning_rate": 0.00020104112740868565,
      "loss": 0.0033,
      "step": 138900
    },
    {
      "epoch": 39.96261144664941,
      "grad_norm": 9.434660023543984e-05,
      "learning_rate": 0.00020075352315214264,
      "loss": 0.0012,
      "step": 138950
    },
    {
      "epoch": 39.97699165947656,
      "grad_norm": 0.00014419799845200032,
      "learning_rate": 0.00020046591889559965,
      "loss": 0.0026,
      "step": 139000
    },
    {
      "epoch": 39.99137187230371,
      "grad_norm": 0.0002877037914004177,
      "learning_rate": 0.00020017831463905666,
      "loss": 0.0015,
      "step": 139050
    },
    {
      "epoch": 40.0,
      "eval_loss": 0.009952714666724205,
      "eval_runtime": 17.5012,
      "eval_samples_per_second": 2726.607,
      "eval_steps_per_second": 42.626,
      "step": 139080
    },
    {
      "epoch": 40.00575208513086,
      "grad_norm": 0.00019124060054309666,
      "learning_rate": 0.00019989071038251368,
      "loss": 0.0014,
      "step": 139100
    },
    {
      "epoch": 40.02013229795801,
      "grad_norm": 3.4613520256243646e-05,
      "learning_rate": 0.00019960310612597066,
      "loss": 0.0013,
      "step": 139150
    },
    {
      "epoch": 40.03451251078516,
      "grad_norm": 0.000274463789537549,
      "learning_rate": 0.00019931550186942767,
      "loss": 0.0027,
      "step": 139200
    },
    {
      "epoch": 40.04889272361231,
      "grad_norm": 0.007564453408122063,
      "learning_rate": 0.00019902789761288469,
      "loss": 0.001,
      "step": 139250
    },
    {
      "epoch": 40.06327293643946,
      "grad_norm": 0.01546408049762249,
      "learning_rate": 0.00019874029335634167,
      "loss": 0.0001,
      "step": 139300
    },
    {
      "epoch": 40.07765314926661,
      "grad_norm": 2.954125011456199e-05,
      "learning_rate": 0.00019845268909979868,
      "loss": 0.003,
      "step": 139350
    },
    {
      "epoch": 40.09203336209376,
      "grad_norm": 5.8699504734249786e-05,
      "learning_rate": 0.0001981650848432557,
      "loss": 0.0037,
      "step": 139400
    },
    {
      "epoch": 40.10641357492091,
      "grad_norm": 0.0012894473038613796,
      "learning_rate": 0.00019787748058671268,
      "loss": 0.0042,
      "step": 139450
    },
    {
      "epoch": 40.120793787748056,
      "grad_norm": 0.0020365749951452017,
      "learning_rate": 0.0001975898763301697,
      "loss": 0.0003,
      "step": 139500
    },
    {
      "epoch": 40.13517400057521,
      "grad_norm": 0.0017495204228907824,
      "learning_rate": 0.0001973022720736267,
      "loss": 0.0015,
      "step": 139550
    },
    {
      "epoch": 40.14955421340236,
      "grad_norm": 7.952540909172967e-05,
      "learning_rate": 0.0001970146678170837,
      "loss": 0.0016,
      "step": 139600
    },
    {
      "epoch": 40.16393442622951,
      "grad_norm": 2.660901373019442e-05,
      "learning_rate": 0.0001967270635605407,
      "loss": 0.0025,
      "step": 139650
    },
    {
      "epoch": 40.17831463905666,
      "grad_norm": 0.004550410434603691,
      "learning_rate": 0.00019643945930399772,
      "loss": 0.003,
      "step": 139700
    },
    {
      "epoch": 40.19269485188381,
      "grad_norm": 0.0006144842482171953,
      "learning_rate": 0.0001961518550474547,
      "loss": 0.0033,
      "step": 139750
    },
    {
      "epoch": 40.207075064710956,
      "grad_norm": 2.0885410776827484e-05,
      "learning_rate": 0.0001958642507909117,
      "loss": 0.0034,
      "step": 139800
    },
    {
      "epoch": 40.221455277538105,
      "grad_norm": 0.004940144717693329,
      "learning_rate": 0.00019557664653436873,
      "loss": 0.0039,
      "step": 139850
    },
    {
      "epoch": 40.235835490365254,
      "grad_norm": 0.00018504337640479207,
      "learning_rate": 0.0001952890422778257,
      "loss": 0.0015,
      "step": 139900
    },
    {
      "epoch": 40.25021570319241,
      "grad_norm": 0.0010746438056230545,
      "learning_rate": 0.00019500143802128272,
      "loss": 0.0026,
      "step": 139950
    },
    {
      "epoch": 40.26459591601956,
      "grad_norm": 0.0006305305287241936,
      "learning_rate": 0.00019471383376473974,
      "loss": 0.0023,
      "step": 140000
    },
    {
      "epoch": 40.27897612884671,
      "grad_norm": 5.6950182624859735e-05,
      "learning_rate": 0.00019442622950819672,
      "loss": 0.0039,
      "step": 140050
    },
    {
      "epoch": 40.29335634167386,
      "grad_norm": 0.000238764492678456,
      "learning_rate": 0.00019413862525165373,
      "loss": 0.0022,
      "step": 140100
    },
    {
      "epoch": 40.307736554501005,
      "grad_norm": 0.00011282377818133682,
      "learning_rate": 0.00019385102099511074,
      "loss": 0.0012,
      "step": 140150
    },
    {
      "epoch": 40.322116767328154,
      "grad_norm": 0.0014992615906521678,
      "learning_rate": 0.00019356341673856773,
      "loss": 0.0038,
      "step": 140200
    },
    {
      "epoch": 40.3364969801553,
      "grad_norm": 0.0002823367540258914,
      "learning_rate": 0.00019327581248202474,
      "loss": 0.0016,
      "step": 140250
    },
    {
      "epoch": 40.35087719298246,
      "grad_norm": 0.0004961218219250441,
      "learning_rate": 0.00019298820822548175,
      "loss": 0.002,
      "step": 140300
    },
    {
      "epoch": 40.36525740580961,
      "grad_norm": 0.0031051377300173044,
      "learning_rate": 0.00019270060396893874,
      "loss": 0.0012,
      "step": 140350
    },
    {
      "epoch": 40.37963761863676,
      "grad_norm": 0.0008512245258316398,
      "learning_rate": 0.00019241299971239575,
      "loss": 0.0025,
      "step": 140400
    },
    {
      "epoch": 40.394017831463906,
      "grad_norm": 0.0008653258555568755,
      "learning_rate": 0.00019212539545585276,
      "loss": 0.0024,
      "step": 140450
    },
    {
      "epoch": 40.408398044291054,
      "grad_norm": 2.156123628083151e-05,
      "learning_rate": 0.00019183779119930975,
      "loss": 0.003,
      "step": 140500
    },
    {
      "epoch": 40.4227782571182,
      "grad_norm": 7.615966023877263e-05,
      "learning_rate": 0.00019155018694276676,
      "loss": 0.0036,
      "step": 140550
    },
    {
      "epoch": 40.43715846994535,
      "grad_norm": 4.8281566705554724e-05,
      "learning_rate": 0.00019126258268622377,
      "loss": 0.0036,
      "step": 140600
    },
    {
      "epoch": 40.45153868277251,
      "grad_norm": 0.0027091961819678545,
      "learning_rate": 0.00019097497842968076,
      "loss": 0.0007,
      "step": 140650
    },
    {
      "epoch": 40.46591889559966,
      "grad_norm": 4.323820030549541e-05,
      "learning_rate": 0.00019068737417313777,
      "loss": 0.0015,
      "step": 140700
    },
    {
      "epoch": 40.480299108426806,
      "grad_norm": 0.00016251944180112332,
      "learning_rate": 0.00019039976991659478,
      "loss": 0.001,
      "step": 140750
    },
    {
      "epoch": 40.494679321253955,
      "grad_norm": 2.1250742065603845e-05,
      "learning_rate": 0.00019011216566005177,
      "loss": 0.0019,
      "step": 140800
    },
    {
      "epoch": 40.5090595340811,
      "grad_norm": 2.226934884674847e-05,
      "learning_rate": 0.00018982456140350878,
      "loss": 0.0029,
      "step": 140850
    },
    {
      "epoch": 40.52343974690825,
      "grad_norm": 4.88927325932309e-05,
      "learning_rate": 0.0001895369571469658,
      "loss": 0.0048,
      "step": 140900
    },
    {
      "epoch": 40.5378199597354,
      "grad_norm": 0.00317292264662683,
      "learning_rate": 0.00018924935289042278,
      "loss": 0.0018,
      "step": 140950
    },
    {
      "epoch": 40.55220017256256,
      "grad_norm": 0.00174044631421566,
      "learning_rate": 0.0001889617486338798,
      "loss": 0.0033,
      "step": 141000
    },
    {
      "epoch": 40.566580385389706,
      "grad_norm": 0.005261365789920092,
      "learning_rate": 0.0001886741443773368,
      "loss": 0.0029,
      "step": 141050
    },
    {
      "epoch": 40.580960598216855,
      "grad_norm": 4.7858684411039576e-05,
      "learning_rate": 0.0001883865401207938,
      "loss": 0.0013,
      "step": 141100
    },
    {
      "epoch": 40.595340811044004,
      "grad_norm": 0.0010323546594008803,
      "learning_rate": 0.0001880989358642508,
      "loss": 0.0009,
      "step": 141150
    },
    {
      "epoch": 40.60972102387115,
      "grad_norm": 6.264668627409264e-05,
      "learning_rate": 0.0001878113316077078,
      "loss": 0.003,
      "step": 141200
    },
    {
      "epoch": 40.6241012366983,
      "grad_norm": 1.5597062883898616e-05,
      "learning_rate": 0.0001875237273511648,
      "loss": 0.0016,
      "step": 141250
    },
    {
      "epoch": 40.63848144952545,
      "grad_norm": 0.00020284672791603953,
      "learning_rate": 0.00018723612309462178,
      "loss": 0.001,
      "step": 141300
    },
    {
      "epoch": 40.652861662352606,
      "grad_norm": 0.00012427696492522955,
      "learning_rate": 0.00018694851883807882,
      "loss": 0.0002,
      "step": 141350
    },
    {
      "epoch": 40.667241875179755,
      "grad_norm": 9.070854139281437e-05,
      "learning_rate": 0.0001866609145815358,
      "loss": 0.0019,
      "step": 141400
    },
    {
      "epoch": 40.681622088006904,
      "grad_norm": 0.00013636738003697246,
      "learning_rate": 0.0001863733103249928,
      "loss": 0.003,
      "step": 141450
    },
    {
      "epoch": 40.69600230083405,
      "grad_norm": 3.603360164561309e-05,
      "learning_rate": 0.00018608570606844983,
      "loss": 0.0014,
      "step": 141500
    },
    {
      "epoch": 40.7103825136612,
      "grad_norm": 5.7644170738058165e-05,
      "learning_rate": 0.00018579810181190682,
      "loss": 0.004,
      "step": 141550
    },
    {
      "epoch": 40.72476272648835,
      "grad_norm": 4.0361148421652615e-05,
      "learning_rate": 0.0001855104975553638,
      "loss": 0.0019,
      "step": 141600
    },
    {
      "epoch": 40.7391429393155,
      "grad_norm": 6.172848225105554e-05,
      "learning_rate": 0.00018522289329882084,
      "loss": 0.0014,
      "step": 141650
    },
    {
      "epoch": 40.75352315214265,
      "grad_norm": 0.002888597548007965,
      "learning_rate": 0.00018493528904227783,
      "loss": 0.0018,
      "step": 141700
    },
    {
      "epoch": 40.767903364969804,
      "grad_norm": 0.015598469413816929,
      "learning_rate": 0.0001846476847857348,
      "loss": 0.0021,
      "step": 141750
    },
    {
      "epoch": 40.78228357779695,
      "grad_norm": 9.663991659181193e-05,
      "learning_rate": 0.00018436008052919185,
      "loss": 0.0013,
      "step": 141800
    },
    {
      "epoch": 40.7966637906241,
      "grad_norm": 5.3734016546513885e-05,
      "learning_rate": 0.00018407247627264884,
      "loss": 0.0016,
      "step": 141850
    },
    {
      "epoch": 40.81104400345125,
      "grad_norm": 4.4388121750671417e-05,
      "learning_rate": 0.00018378487201610582,
      "loss": 0.0014,
      "step": 141900
    },
    {
      "epoch": 40.8254242162784,
      "grad_norm": 1.356640859739855e-05,
      "learning_rate": 0.00018349726775956286,
      "loss": 0.0048,
      "step": 141950
    },
    {
      "epoch": 40.83980442910555,
      "grad_norm": 0.007038523908704519,
      "learning_rate": 0.00018320966350301985,
      "loss": 0.0025,
      "step": 142000
    },
    {
      "epoch": 40.8541846419327,
      "grad_norm": 0.0007386623765341938,
      "learning_rate": 0.00018292205924647683,
      "loss": 0.0024,
      "step": 142050
    },
    {
      "epoch": 40.86856485475985,
      "grad_norm": 0.0004935674369335175,
      "learning_rate": 0.00018263445498993387,
      "loss": 0.003,
      "step": 142100
    },
    {
      "epoch": 40.882945067587,
      "grad_norm": 0.0010378272272646427,
      "learning_rate": 0.00018234685073339086,
      "loss": 0.0039,
      "step": 142150
    },
    {
      "epoch": 40.89732528041415,
      "grad_norm": 0.010393303819000721,
      "learning_rate": 0.00018205924647684784,
      "loss": 0.0045,
      "step": 142200
    },
    {
      "epoch": 40.9117054932413,
      "grad_norm": 6.225579272722825e-05,
      "learning_rate": 0.00018177164222030488,
      "loss": 0.0036,
      "step": 142250
    },
    {
      "epoch": 40.92608570606845,
      "grad_norm": 7.91192869655788e-05,
      "learning_rate": 0.00018148403796376187,
      "loss": 0.0017,
      "step": 142300
    },
    {
      "epoch": 40.9404659188956,
      "grad_norm": 4.945623368257657e-05,
      "learning_rate": 0.00018119643370721885,
      "loss": 0.0012,
      "step": 142350
    },
    {
      "epoch": 40.954846131722746,
      "grad_norm": 0.0008313175640068948,
      "learning_rate": 0.0001809088294506759,
      "loss": 0.0029,
      "step": 142400
    },
    {
      "epoch": 40.9692263445499,
      "grad_norm": 5.8265693951398134e-05,
      "learning_rate": 0.00018062122519413288,
      "loss": 0.001,
      "step": 142450
    },
    {
      "epoch": 40.98360655737705,
      "grad_norm": 0.00583924725651741,
      "learning_rate": 0.00018033362093758986,
      "loss": 0.0035,
      "step": 142500
    },
    {
      "epoch": 40.9979867702042,
      "grad_norm": 0.0005827769637107849,
      "learning_rate": 0.0001800460166810469,
      "loss": 0.0002,
      "step": 142550
    },
    {
      "epoch": 41.0,
      "eval_loss": 0.009921163320541382,
      "eval_runtime": 18.0001,
      "eval_samples_per_second": 2651.045,
      "eval_steps_per_second": 41.444,
      "step": 142557
    },
    {
      "epoch": 41.01236698303135,
      "grad_norm": 5.061965566710569e-05,
      "learning_rate": 0.0001797584124245039,
      "loss": 0.005,
      "step": 142600
    },
    {
      "epoch": 41.0267471958585,
      "grad_norm": 4.932468436891213e-05,
      "learning_rate": 0.00017947080816796087,
      "loss": 0.0014,
      "step": 142650
    },
    {
      "epoch": 41.04112740868565,
      "grad_norm": 0.00023420454817824066,
      "learning_rate": 0.0001791832039114179,
      "loss": 0.0009,
      "step": 142700
    },
    {
      "epoch": 41.055507621512795,
      "grad_norm": 4.2525756725808606e-05,
      "learning_rate": 0.0001788955996548749,
      "loss": 0.0008,
      "step": 142750
    },
    {
      "epoch": 41.06988783433995,
      "grad_norm": 1.1038356205972377e-05,
      "learning_rate": 0.00017860799539833188,
      "loss": 0.001,
      "step": 142800
    },
    {
      "epoch": 41.0842680471671,
      "grad_norm": 0.003997026011347771,
      "learning_rate": 0.00017832039114178892,
      "loss": 0.0032,
      "step": 142850
    },
    {
      "epoch": 41.09864825999425,
      "grad_norm": 0.0004244576848577708,
      "learning_rate": 0.0001780327868852459,
      "loss": 0.0029,
      "step": 142900
    },
    {
      "epoch": 41.1130284728214,
      "grad_norm": 0.0005380086367949843,
      "learning_rate": 0.0001777451826287029,
      "loss": 0.0003,
      "step": 142950
    },
    {
      "epoch": 41.12740868564855,
      "grad_norm": 0.00048313807928934693,
      "learning_rate": 0.00017745757837215993,
      "loss": 0.0038,
      "step": 143000
    },
    {
      "epoch": 41.141788898475696,
      "grad_norm": 4.099641591892578e-05,
      "learning_rate": 0.00017716997411561692,
      "loss": 0.0012,
      "step": 143050
    },
    {
      "epoch": 41.156169111302844,
      "grad_norm": 0.0012921327725052834,
      "learning_rate": 0.0001768823698590739,
      "loss": 0.0022,
      "step": 143100
    },
    {
      "epoch": 41.17054932413,
      "grad_norm": 0.0008771038264967501,
      "learning_rate": 0.00017659476560253094,
      "loss": 0.0043,
      "step": 143150
    },
    {
      "epoch": 41.18492953695715,
      "grad_norm": 2.6589481421979144e-05,
      "learning_rate": 0.00017630716134598793,
      "loss": 0.0006,
      "step": 143200
    },
    {
      "epoch": 41.1993097497843,
      "grad_norm": 0.004432564601302147,
      "learning_rate": 0.0001760195570894449,
      "loss": 0.0023,
      "step": 143250
    },
    {
      "epoch": 41.21368996261145,
      "grad_norm": 3.499922968330793e-05,
      "learning_rate": 0.00017573195283290195,
      "loss": 0.0022,
      "step": 143300
    },
    {
      "epoch": 41.228070175438596,
      "grad_norm": 0.0007056080503389239,
      "learning_rate": 0.00017544434857635894,
      "loss": 0.0003,
      "step": 143350
    },
    {
      "epoch": 41.242450388265745,
      "grad_norm": 0.0006353669450618327,
      "learning_rate": 0.00017515674431981592,
      "loss": 0.002,
      "step": 143400
    },
    {
      "epoch": 41.256830601092894,
      "grad_norm": 0.00033133075339719653,
      "learning_rate": 0.00017486914006327296,
      "loss": 0.0037,
      "step": 143450
    },
    {
      "epoch": 41.27121081392005,
      "grad_norm": 0.0001596412039361894,
      "learning_rate": 0.00017458153580672995,
      "loss": 0.002,
      "step": 143500
    },
    {
      "epoch": 41.2855910267472,
      "grad_norm": 5.2592567953979596e-05,
      "learning_rate": 0.00017429393155018693,
      "loss": 0.0046,
      "step": 143550
    },
    {
      "epoch": 41.29997123957435,
      "grad_norm": 2.2856436771689914e-05,
      "learning_rate": 0.00017400632729364397,
      "loss": 0.0028,
      "step": 143600
    },
    {
      "epoch": 41.314351452401496,
      "grad_norm": 0.0004055760509800166,
      "learning_rate": 0.00017371872303710095,
      "loss": 0.0036,
      "step": 143650
    },
    {
      "epoch": 41.328731665228645,
      "grad_norm": 0.00022191490279510617,
      "learning_rate": 0.00017343111878055794,
      "loss": 0.0008,
      "step": 143700
    },
    {
      "epoch": 41.343111878055794,
      "grad_norm": 0.00012467874330468476,
      "learning_rate": 0.00017314351452401498,
      "loss": 0.0028,
      "step": 143750
    },
    {
      "epoch": 41.35749209088294,
      "grad_norm": 0.014325635507702827,
      "learning_rate": 0.00017285591026747196,
      "loss": 0.0026,
      "step": 143800
    },
    {
      "epoch": 41.37187230371009,
      "grad_norm": 0.02092527598142624,
      "learning_rate": 0.00017256830601092895,
      "loss": 0.003,
      "step": 143850
    },
    {
      "epoch": 41.38625251653725,
      "grad_norm": 7.692846702411771e-05,
      "learning_rate": 0.000172280701754386,
      "loss": 0.0041,
      "step": 143900
    },
    {
      "epoch": 41.400632729364396,
      "grad_norm": 2.515174310246948e-05,
      "learning_rate": 0.00017199309749784297,
      "loss": 0.0008,
      "step": 143950
    },
    {
      "epoch": 41.415012942191545,
      "grad_norm": 4.8057128879008815e-05,
      "learning_rate": 0.00017170549324129996,
      "loss": 0.0024,
      "step": 144000
    },
    {
      "epoch": 41.429393155018694,
      "grad_norm": 2.316873360541649e-05,
      "learning_rate": 0.00017141788898475697,
      "loss": 0.001,
      "step": 144050
    },
    {
      "epoch": 41.44377336784584,
      "grad_norm": 0.0005727511597797275,
      "learning_rate": 0.00017113028472821398,
      "loss": 0.0023,
      "step": 144100
    },
    {
      "epoch": 41.45815358067299,
      "grad_norm": 2.326884714420885e-05,
      "learning_rate": 0.00017084268047167097,
      "loss": 0.0003,
      "step": 144150
    },
    {
      "epoch": 41.47253379350014,
      "grad_norm": 0.00031038682209327817,
      "learning_rate": 0.00017055507621512798,
      "loss": 0.0007,
      "step": 144200
    },
    {
      "epoch": 41.4869140063273,
      "grad_norm": 0.0034894950222223997,
      "learning_rate": 0.000170267471958585,
      "loss": 0.0012,
      "step": 144250
    },
    {
      "epoch": 41.501294219154445,
      "grad_norm": 1.2523235454864334e-05,
      "learning_rate": 0.00016997986770204198,
      "loss": 0.0019,
      "step": 144300
    },
    {
      "epoch": 41.515674431981594,
      "grad_norm": 9.103685442823917e-05,
      "learning_rate": 0.000169692263445499,
      "loss": 0.0055,
      "step": 144350
    },
    {
      "epoch": 41.53005464480874,
      "grad_norm": 0.0022595778573304415,
      "learning_rate": 0.000169404659188956,
      "loss": 0.0032,
      "step": 144400
    },
    {
      "epoch": 41.54443485763589,
      "grad_norm": 5.0273276428924873e-05,
      "learning_rate": 0.000169117054932413,
      "loss": 0.0014,
      "step": 144450
    },
    {
      "epoch": 41.55881507046304,
      "grad_norm": 0.007690283004194498,
      "learning_rate": 0.00016882945067587,
      "loss": 0.002,
      "step": 144500
    },
    {
      "epoch": 41.57319528329019,
      "grad_norm": 0.012941413559019566,
      "learning_rate": 0.00016854184641932701,
      "loss": 0.0042,
      "step": 144550
    },
    {
      "epoch": 41.587575496117346,
      "grad_norm": 0.0011433199979364872,
      "learning_rate": 0.000168254242162784,
      "loss": 0.0018,
      "step": 144600
    },
    {
      "epoch": 41.601955708944494,
      "grad_norm": 0.0001725983602227643,
      "learning_rate": 0.000167966637906241,
      "loss": 0.003,
      "step": 144650
    },
    {
      "epoch": 41.61633592177164,
      "grad_norm": 0.00017243980255443603,
      "learning_rate": 0.00016767903364969802,
      "loss": 0.0026,
      "step": 144700
    },
    {
      "epoch": 41.63071613459879,
      "grad_norm": 0.00544365681707859,
      "learning_rate": 0.000167391429393155,
      "loss": 0.0016,
      "step": 144750
    },
    {
      "epoch": 41.64509634742594,
      "grad_norm": 0.0004295164253562689,
      "learning_rate": 0.00016710382513661202,
      "loss": 0.0005,
      "step": 144800
    },
    {
      "epoch": 41.65947656025309,
      "grad_norm": 2.8650414606090635e-05,
      "learning_rate": 0.00016681622088006903,
      "loss": 0.0016,
      "step": 144850
    },
    {
      "epoch": 41.67385677308024,
      "grad_norm": 0.0010427706874907017,
      "learning_rate": 0.00016652861662352602,
      "loss": 0.0019,
      "step": 144900
    },
    {
      "epoch": 41.688236985907395,
      "grad_norm": 6.294139166129753e-05,
      "learning_rate": 0.00016624101236698303,
      "loss": 0.0024,
      "step": 144950
    },
    {
      "epoch": 41.70261719873454,
      "grad_norm": 5.953633808530867e-05,
      "learning_rate": 0.00016595340811044004,
      "loss": 0.0041,
      "step": 145000
    },
    {
      "epoch": 41.71699741156169,
      "grad_norm": 4.21221338910982e-05,
      "learning_rate": 0.00016566580385389703,
      "loss": 0.0025,
      "step": 145050
    },
    {
      "epoch": 41.73137762438884,
      "grad_norm": 0.0024428516626358032,
      "learning_rate": 0.00016537819959735404,
      "loss": 0.0047,
      "step": 145100
    },
    {
      "epoch": 41.74575783721599,
      "grad_norm": 1.886263271444477e-05,
      "learning_rate": 0.00016509059534081105,
      "loss": 0.0039,
      "step": 145150
    },
    {
      "epoch": 41.76013805004314,
      "grad_norm": 0.001689284690655768,
      "learning_rate": 0.00016480299108426804,
      "loss": 0.0023,
      "step": 145200
    },
    {
      "epoch": 41.77451826287029,
      "grad_norm": 0.0002663898339960724,
      "learning_rate": 0.00016451538682772505,
      "loss": 0.0003,
      "step": 145250
    },
    {
      "epoch": 41.788898475697444,
      "grad_norm": 1.4969987205404323e-05,
      "learning_rate": 0.00016422778257118206,
      "loss": 0.0011,
      "step": 145300
    },
    {
      "epoch": 41.80327868852459,
      "grad_norm": 0.00024248780391644686,
      "learning_rate": 0.00016394017831463905,
      "loss": 0.0011,
      "step": 145350
    },
    {
      "epoch": 41.81765890135174,
      "grad_norm": 9.356161172036082e-05,
      "learning_rate": 0.00016365257405809606,
      "loss": 0.0002,
      "step": 145400
    },
    {
      "epoch": 41.83203911417889,
      "grad_norm": 0.00015097045979928225,
      "learning_rate": 0.00016336496980155307,
      "loss": 0.0008,
      "step": 145450
    },
    {
      "epoch": 41.84641932700604,
      "grad_norm": 3.063942858716473e-05,
      "learning_rate": 0.00016307736554501006,
      "loss": 0.0035,
      "step": 145500
    },
    {
      "epoch": 41.86079953983319,
      "grad_norm": 0.001107734045945108,
      "learning_rate": 0.00016278976128846707,
      "loss": 0.0036,
      "step": 145550
    },
    {
      "epoch": 41.87517975266034,
      "grad_norm": 0.0031363111920654774,
      "learning_rate": 0.00016250215703192408,
      "loss": 0.0021,
      "step": 145600
    },
    {
      "epoch": 41.889559965487486,
      "grad_norm": 0.0007499885396100581,
      "learning_rate": 0.00016221455277538107,
      "loss": 0.0028,
      "step": 145650
    },
    {
      "epoch": 41.90394017831464,
      "grad_norm": 1.801854341465514e-05,
      "learning_rate": 0.00016192694851883808,
      "loss": 0.0056,
      "step": 145700
    },
    {
      "epoch": 41.91832039114179,
      "grad_norm": 0.0016787848435342312,
      "learning_rate": 0.0001616393442622951,
      "loss": 0.0016,
      "step": 145750
    },
    {
      "epoch": 41.93270060396894,
      "grad_norm": 0.0028533877339214087,
      "learning_rate": 0.00016135174000575208,
      "loss": 0.0021,
      "step": 145800
    },
    {
      "epoch": 41.94708081679609,
      "grad_norm": 0.008996338583528996,
      "learning_rate": 0.0001610641357492091,
      "loss": 0.0022,
      "step": 145850
    },
    {
      "epoch": 41.96146102962324,
      "grad_norm": 0.00019504499505273998,
      "learning_rate": 0.0001607765314926661,
      "loss": 0.0008,
      "step": 145900
    },
    {
      "epoch": 41.975841242450386,
      "grad_norm": 4.855038059758954e-05,
      "learning_rate": 0.0001604889272361231,
      "loss": 0.0017,
      "step": 145950
    },
    {
      "epoch": 41.990221455277535,
      "grad_norm": 4.664426523959264e-05,
      "learning_rate": 0.0001602013229795801,
      "loss": 0.0042,
      "step": 146000
    },
    {
      "epoch": 42.0,
      "eval_loss": 0.009875989519059658,
      "eval_runtime": 17.0101,
      "eval_samples_per_second": 2805.341,
      "eval_steps_per_second": 43.856,
      "step": 146034
    },
    {
      "epoch": 42.00460166810469,
      "grad_norm": 0.0003175449382979423,
      "learning_rate": 0.0001599137187230371,
      "loss": 0.0011,
      "step": 146050
    },
    {
      "epoch": 42.01898188093184,
      "grad_norm": 1.9215309293940663e-05,
      "learning_rate": 0.0001596261144664941,
      "loss": 0.0025,
      "step": 146100
    },
    {
      "epoch": 42.03336209375899,
      "grad_norm": 0.00014424935216084123,
      "learning_rate": 0.0001593385102099511,
      "loss": 0.0026,
      "step": 146150
    },
    {
      "epoch": 42.04774230658614,
      "grad_norm": 7.586393621750176e-05,
      "learning_rate": 0.00015905090595340812,
      "loss": 0.0009,
      "step": 146200
    },
    {
      "epoch": 42.062122519413286,
      "grad_norm": 3.2625262974761426e-05,
      "learning_rate": 0.00015876330169686513,
      "loss": 0.0008,
      "step": 146250
    },
    {
      "epoch": 42.076502732240435,
      "grad_norm": 0.0002878628147300333,
      "learning_rate": 0.00015847569744032212,
      "loss": 0.0019,
      "step": 146300
    },
    {
      "epoch": 42.090882945067584,
      "grad_norm": 0.0006212344160303473,
      "learning_rate": 0.00015818809318377913,
      "loss": 0.0011,
      "step": 146350
    },
    {
      "epoch": 42.10526315789474,
      "grad_norm": 1.4497239135380369e-05,
      "learning_rate": 0.00015790048892723614,
      "loss": 0.0012,
      "step": 146400
    },
    {
      "epoch": 42.11964337072189,
      "grad_norm": 0.0015374014619737864,
      "learning_rate": 0.00015761288467069313,
      "loss": 0.0022,
      "step": 146450
    },
    {
      "epoch": 42.13402358354904,
      "grad_norm": 7.049117266433313e-05,
      "learning_rate": 0.00015732528041415014,
      "loss": 0.0038,
      "step": 146500
    },
    {
      "epoch": 42.148403796376186,
      "grad_norm": 0.0001490077847847715,
      "learning_rate": 0.00015703767615760715,
      "loss": 0.0013,
      "step": 146550
    },
    {
      "epoch": 42.162784009203335,
      "grad_norm": 2.586384471214842e-05,
      "learning_rate": 0.00015675007190106414,
      "loss": 0.0031,
      "step": 146600
    },
    {
      "epoch": 42.177164222030484,
      "grad_norm": 0.00029828783590346575,
      "learning_rate": 0.00015646246764452115,
      "loss": 0.003,
      "step": 146650
    },
    {
      "epoch": 42.19154443485763,
      "grad_norm": 0.0003932005201932043,
      "learning_rate": 0.00015617486338797816,
      "loss": 0.0017,
      "step": 146700
    },
    {
      "epoch": 42.20592464768479,
      "grad_norm": 5.784483073512092e-05,
      "learning_rate": 0.00015588725913143515,
      "loss": 0.003,
      "step": 146750
    },
    {
      "epoch": 42.22030486051194,
      "grad_norm": 0.00010344106703996658,
      "learning_rate": 0.00015559965487489213,
      "loss": 0.0036,
      "step": 146800
    },
    {
      "epoch": 42.23468507333909,
      "grad_norm": 0.0013610812602564692,
      "learning_rate": 0.00015531205061834917,
      "loss": 0.0009,
      "step": 146850
    },
    {
      "epoch": 42.249065286166235,
      "grad_norm": 0.0001642665738472715,
      "learning_rate": 0.00015502444636180616,
      "loss": 0.0028,
      "step": 146900
    },
    {
      "epoch": 42.263445498993384,
      "grad_norm": 0.00014910267782397568,
      "learning_rate": 0.00015473684210526314,
      "loss": 0.004,
      "step": 146950
    },
    {
      "epoch": 42.27782571182053,
      "grad_norm": 0.00010261154238833115,
      "learning_rate": 0.00015444923784872018,
      "loss": 0.002,
      "step": 147000
    },
    {
      "epoch": 42.29220592464768,
      "grad_norm": 2.7454225346446037e-05,
      "learning_rate": 0.00015416163359217717,
      "loss": 0.0038,
      "step": 147050
    },
    {
      "epoch": 42.30658613747484,
      "grad_norm": 5.4910800827201456e-05,
      "learning_rate": 0.00015387402933563415,
      "loss": 0.0012,
      "step": 147100
    },
    {
      "epoch": 42.32096635030199,
      "grad_norm": 3.988482421846129e-05,
      "learning_rate": 0.0001535864250790912,
      "loss": 0.0027,
      "step": 147150
    },
    {
      "epoch": 42.335346563129136,
      "grad_norm": 0.006533811334520578,
      "learning_rate": 0.00015329882082254818,
      "loss": 0.0018,
      "step": 147200
    },
    {
      "epoch": 42.349726775956285,
      "grad_norm": 2.863580266421195e-05,
      "learning_rate": 0.00015301121656600516,
      "loss": 0.0007,
      "step": 147250
    },
    {
      "epoch": 42.36410698878343,
      "grad_norm": 9.976801084121689e-05,
      "learning_rate": 0.0001527236123094622,
      "loss": 0.0008,
      "step": 147300
    },
    {
      "epoch": 42.37848720161058,
      "grad_norm": 0.0007494546007364988,
      "learning_rate": 0.0001524360080529192,
      "loss": 0.0031,
      "step": 147350
    },
    {
      "epoch": 42.39286741443773,
      "grad_norm": 0.008906765840947628,
      "learning_rate": 0.00015214840379637617,
      "loss": 0.004,
      "step": 147400
    },
    {
      "epoch": 42.40724762726488,
      "grad_norm": 0.006526449229568243,
      "learning_rate": 0.0001518607995398332,
      "loss": 0.001,
      "step": 147450
    },
    {
      "epoch": 42.421627840092036,
      "grad_norm": 0.000665036728605628,
      "learning_rate": 0.0001515731952832902,
      "loss": 0.0015,
      "step": 147500
    },
    {
      "epoch": 42.436008052919185,
      "grad_norm": 0.0039499783888459206,
      "learning_rate": 0.00015128559102674718,
      "loss": 0.003,
      "step": 147550
    },
    {
      "epoch": 42.450388265746334,
      "grad_norm": 5.1408744184300303e-05,
      "learning_rate": 0.00015099798677020422,
      "loss": 0.0017,
      "step": 147600
    },
    {
      "epoch": 42.46476847857348,
      "grad_norm": 0.00775147182866931,
      "learning_rate": 0.0001507103825136612,
      "loss": 0.0025,
      "step": 147650
    },
    {
      "epoch": 42.47914869140063,
      "grad_norm": 6.795572699047625e-05,
      "learning_rate": 0.0001504227782571182,
      "loss": 0.0007,
      "step": 147700
    },
    {
      "epoch": 42.49352890422778,
      "grad_norm": 0.0001458603801438585,
      "learning_rate": 0.00015013517400057523,
      "loss": 0.0008,
      "step": 147750
    },
    {
      "epoch": 42.50790911705493,
      "grad_norm": 0.0024650567211210728,
      "learning_rate": 0.00014984756974403222,
      "loss": 0.0032,
      "step": 147800
    },
    {
      "epoch": 42.522289329882085,
      "grad_norm": 1.9066121240030043e-05,
      "learning_rate": 0.0001495599654874892,
      "loss": 0.0043,
      "step": 147850
    },
    {
      "epoch": 42.536669542709234,
      "grad_norm": 0.007711358834058046,
      "learning_rate": 0.00014927236123094624,
      "loss": 0.0024,
      "step": 147900
    },
    {
      "epoch": 42.55104975553638,
      "grad_norm": 0.013506777584552765,
      "learning_rate": 0.00014898475697440323,
      "loss": 0.0003,
      "step": 147950
    },
    {
      "epoch": 42.56542996836353,
      "grad_norm": 5.211581810726784e-05,
      "learning_rate": 0.0001486971527178602,
      "loss": 0.0016,
      "step": 148000
    },
    {
      "epoch": 42.57981018119068,
      "grad_norm": 0.0011900953250005841,
      "learning_rate": 0.00014840954846131725,
      "loss": 0.0029,
      "step": 148050
    },
    {
      "epoch": 42.59419039401783,
      "grad_norm": 0.0011468491284176707,
      "learning_rate": 0.00014812194420477424,
      "loss": 0.0035,
      "step": 148100
    },
    {
      "epoch": 42.60857060684498,
      "grad_norm": 0.0003279599768575281,
      "learning_rate": 0.00014783433994823122,
      "loss": 0.0051,
      "step": 148150
    },
    {
      "epoch": 42.622950819672134,
      "grad_norm": 0.00016537675401195884,
      "learning_rate": 0.00014754673569168826,
      "loss": 0.0026,
      "step": 148200
    },
    {
      "epoch": 42.63733103249928,
      "grad_norm": 2.2093219740781933e-05,
      "learning_rate": 0.00014725913143514525,
      "loss": 0.0007,
      "step": 148250
    },
    {
      "epoch": 42.65171124532643,
      "grad_norm": 0.005717857275158167,
      "learning_rate": 0.00014697152717860223,
      "loss": 0.0035,
      "step": 148300
    },
    {
      "epoch": 42.66609145815358,
      "grad_norm": 4.602298213285394e-05,
      "learning_rate": 0.00014668392292205927,
      "loss": 0.002,
      "step": 148350
    },
    {
      "epoch": 42.68047167098073,
      "grad_norm": 0.0008180732256732881,
      "learning_rate": 0.00014639631866551626,
      "loss": 0.0012,
      "step": 148400
    },
    {
      "epoch": 42.69485188380788,
      "grad_norm": 3.226508488296531e-05,
      "learning_rate": 0.00014610871440897324,
      "loss": 0.0011,
      "step": 148450
    },
    {
      "epoch": 42.70923209663503,
      "grad_norm": 0.004854332655668259,
      "learning_rate": 0.00014582111015243028,
      "loss": 0.0008,
      "step": 148500
    },
    {
      "epoch": 42.72361230946218,
      "grad_norm": 4.1904571844497696e-05,
      "learning_rate": 0.00014553350589588727,
      "loss": 0.0029,
      "step": 148550
    },
    {
      "epoch": 42.73799252228933,
      "grad_norm": 0.0003044959157705307,
      "learning_rate": 0.00014524590163934425,
      "loss": 0.003,
      "step": 148600
    },
    {
      "epoch": 42.75237273511648,
      "grad_norm": 0.0003118345921393484,
      "learning_rate": 0.0001449582973828013,
      "loss": 0.0006,
      "step": 148650
    },
    {
      "epoch": 42.76675294794363,
      "grad_norm": 0.00921140518039465,
      "learning_rate": 0.00014467069312625828,
      "loss": 0.0012,
      "step": 148700
    },
    {
      "epoch": 42.78113316077078,
      "grad_norm": 0.0006614056183025241,
      "learning_rate": 0.00014438308886971526,
      "loss": 0.0039,
      "step": 148750
    },
    {
      "epoch": 42.79551337359793,
      "grad_norm": 0.0031715084332972765,
      "learning_rate": 0.0001440954846131723,
      "loss": 0.0032,
      "step": 148800
    },
    {
      "epoch": 42.809893586425076,
      "grad_norm": 8.685206557856873e-05,
      "learning_rate": 0.00014380788035662928,
      "loss": 0.0023,
      "step": 148850
    },
    {
      "epoch": 42.82427379925223,
      "grad_norm": 0.00018460732826497406,
      "learning_rate": 0.00014352027610008627,
      "loss": 0.0039,
      "step": 148900
    },
    {
      "epoch": 42.83865401207938,
      "grad_norm": 0.00010699321137508377,
      "learning_rate": 0.0001432326718435433,
      "loss": 0.001,
      "step": 148950
    },
    {
      "epoch": 42.85303422490653,
      "grad_norm": 0.00983631331473589,
      "learning_rate": 0.0001429450675870003,
      "loss": 0.0016,
      "step": 149000
    },
    {
      "epoch": 42.86741443773368,
      "grad_norm": 0.00456529064103961,
      "learning_rate": 0.00014265746333045728,
      "loss": 0.0055,
      "step": 149050
    },
    {
      "epoch": 42.88179465056083,
      "grad_norm": 0.007731572724878788,
      "learning_rate": 0.00014236985907391432,
      "loss": 0.0011,
      "step": 149100
    },
    {
      "epoch": 42.89617486338798,
      "grad_norm": 7.007976819295436e-05,
      "learning_rate": 0.0001420822548173713,
      "loss": 0.0028,
      "step": 149150
    },
    {
      "epoch": 42.910555076215125,
      "grad_norm": 0.028588294982910156,
      "learning_rate": 0.0001417946505608283,
      "loss": 0.0035,
      "step": 149200
    },
    {
      "epoch": 42.92493528904228,
      "grad_norm": 1.921785224112682e-05,
      "learning_rate": 0.00014150704630428533,
      "loss": 0.0011,
      "step": 149250
    },
    {
      "epoch": 42.93931550186943,
      "grad_norm": 0.00027965978370048106,
      "learning_rate": 0.00014121944204774231,
      "loss": 0.0013,
      "step": 149300
    },
    {
      "epoch": 42.95369571469658,
      "grad_norm": 0.00016621709801256657,
      "learning_rate": 0.0001409318377911993,
      "loss": 0.0007,
      "step": 149350
    },
    {
      "epoch": 42.96807592752373,
      "grad_norm": 5.1615992560982704e-05,
      "learning_rate": 0.00014064423353465634,
      "loss": 0.0035,
      "step": 149400
    },
    {
      "epoch": 42.98245614035088,
      "grad_norm": 0.006326440721750259,
      "learning_rate": 0.00014035662927811332,
      "loss": 0.0043,
      "step": 149450
    },
    {
      "epoch": 42.996836353178026,
      "grad_norm": 4.107779386686161e-05,
      "learning_rate": 0.0001400690250215703,
      "loss": 0.002,
      "step": 149500
    },
    {
      "epoch": 43.0,
      "eval_loss": 0.010089055635035038,
      "eval_runtime": 17.9569,
      "eval_samples_per_second": 2657.421,
      "eval_steps_per_second": 41.544,
      "step": 149511
    },
    {
      "epoch": 43.011216566005174,
      "grad_norm": 2.9626167815877125e-05,
      "learning_rate": 0.00013978142076502732,
      "loss": 0.0061,
      "step": 149550
    },
    {
      "epoch": 43.02559677883232,
      "grad_norm": 0.00013346831838134676,
      "learning_rate": 0.00013949381650848433,
      "loss": 0.0018,
      "step": 149600
    },
    {
      "epoch": 43.03997699165948,
      "grad_norm": 0.000180532515514642,
      "learning_rate": 0.00013920621225194132,
      "loss": 0.002,
      "step": 149650
    },
    {
      "epoch": 43.05435720448663,
      "grad_norm": 0.021875232458114624,
      "learning_rate": 0.00013891860799539833,
      "loss": 0.0039,
      "step": 149700
    },
    {
      "epoch": 43.06873741731378,
      "grad_norm": 0.01206390094012022,
      "learning_rate": 0.00013863100373885534,
      "loss": 0.0022,
      "step": 149750
    },
    {
      "epoch": 43.083117630140926,
      "grad_norm": 0.005714135244488716,
      "learning_rate": 0.00013834339948231233,
      "loss": 0.0057,
      "step": 149800
    },
    {
      "epoch": 43.097497842968075,
      "grad_norm": 0.0010556410998106003,
      "learning_rate": 0.00013805579522576934,
      "loss": 0.004,
      "step": 149850
    },
    {
      "epoch": 43.11187805579522,
      "grad_norm": 0.0009755087667144835,
      "learning_rate": 0.00013776819096922635,
      "loss": 0.0028,
      "step": 149900
    },
    {
      "epoch": 43.12625826862237,
      "grad_norm": 4.810610334970988e-05,
      "learning_rate": 0.00013748058671268334,
      "loss": 0.0024,
      "step": 149950
    },
    {
      "epoch": 43.14063848144953,
      "grad_norm": 0.00015261612134054303,
      "learning_rate": 0.00013719298245614035,
      "loss": 0.0019,
      "step": 150000
    },
    {
      "epoch": 43.15501869427668,
      "grad_norm": 7.56793306209147e-05,
      "learning_rate": 0.00013690537819959736,
      "loss": 0.0021,
      "step": 150050
    },
    {
      "epoch": 43.169398907103826,
      "grad_norm": 4.89562917209696e-05,
      "learning_rate": 0.00013661777394305435,
      "loss": 0.0044,
      "step": 150100
    },
    {
      "epoch": 43.183779119930975,
      "grad_norm": 2.1515163098229095e-05,
      "learning_rate": 0.00013633016968651136,
      "loss": 0.0046,
      "step": 150150
    },
    {
      "epoch": 43.198159332758124,
      "grad_norm": 4.4363667257130146e-05,
      "learning_rate": 0.00013604256542996837,
      "loss": 0.0008,
      "step": 150200
    },
    {
      "epoch": 43.21253954558527,
      "grad_norm": 3.8244485040195286e-05,
      "learning_rate": 0.00013575496117342536,
      "loss": 0.0006,
      "step": 150250
    },
    {
      "epoch": 43.22691975841242,
      "grad_norm": 0.0017442199168726802,
      "learning_rate": 0.00013546735691688237,
      "loss": 0.003,
      "step": 150300
    },
    {
      "epoch": 43.24129997123958,
      "grad_norm": 4.957948112860322e-05,
      "learning_rate": 0.00013517975266033938,
      "loss": 0.0044,
      "step": 150350
    },
    {
      "epoch": 43.255680184066726,
      "grad_norm": 6.725065759383142e-05,
      "learning_rate": 0.00013489214840379637,
      "loss": 0.002,
      "step": 150400
    },
    {
      "epoch": 43.270060396893875,
      "grad_norm": 9.70066903391853e-05,
      "learning_rate": 0.00013460454414725338,
      "loss": 0.0003,
      "step": 150450
    },
    {
      "epoch": 43.284440609721024,
      "grad_norm": 4.362269464763813e-05,
      "learning_rate": 0.0001343169398907104,
      "loss": 0.0019,
      "step": 150500
    },
    {
      "epoch": 43.29882082254817,
      "grad_norm": 0.004605965223163366,
      "learning_rate": 0.00013402933563416738,
      "loss": 0.0019,
      "step": 150550
    },
    {
      "epoch": 43.31320103537532,
      "grad_norm": 0.0003926964709535241,
      "learning_rate": 0.0001337417313776244,
      "loss": 0.0035,
      "step": 150600
    },
    {
      "epoch": 43.32758124820247,
      "grad_norm": 0.00015931548841763288,
      "learning_rate": 0.0001334541271210814,
      "loss": 0.0024,
      "step": 150650
    },
    {
      "epoch": 43.341961461029626,
      "grad_norm": 2.8247972295503132e-05,
      "learning_rate": 0.0001331665228645384,
      "loss": 0.0022,
      "step": 150700
    },
    {
      "epoch": 43.356341673856775,
      "grad_norm": 0.0051435562781989574,
      "learning_rate": 0.0001328789186079954,
      "loss": 0.0013,
      "step": 150750
    },
    {
      "epoch": 43.370721886683924,
      "grad_norm": 1.3251799828140065e-05,
      "learning_rate": 0.0001325913143514524,
      "loss": 0.0027,
      "step": 150800
    },
    {
      "epoch": 43.38510209951107,
      "grad_norm": 0.0004833433195017278,
      "learning_rate": 0.0001323037100949094,
      "loss": 0.002,
      "step": 150850
    },
    {
      "epoch": 43.39948231233822,
      "grad_norm": 0.0022091164719313383,
      "learning_rate": 0.0001320161058383664,
      "loss": 0.0041,
      "step": 150900
    },
    {
      "epoch": 43.41386252516537,
      "grad_norm": 2.800507536449004e-05,
      "learning_rate": 0.00013172850158182342,
      "loss": 0.0018,
      "step": 150950
    },
    {
      "epoch": 43.42824273799252,
      "grad_norm": 1.6627827790216543e-05,
      "learning_rate": 0.0001314408973252804,
      "loss": 0.0012,
      "step": 151000
    },
    {
      "epoch": 43.442622950819676,
      "grad_norm": 7.637967792106792e-05,
      "learning_rate": 0.00013115329306873742,
      "loss": 0.0007,
      "step": 151050
    },
    {
      "epoch": 43.457003163646824,
      "grad_norm": 9.168974065687507e-05,
      "learning_rate": 0.00013086568881219443,
      "loss": 0.0011,
      "step": 151100
    },
    {
      "epoch": 43.47138337647397,
      "grad_norm": 7.200049003586173e-05,
      "learning_rate": 0.00013057808455565142,
      "loss": 0.0048,
      "step": 151150
    },
    {
      "epoch": 43.48576358930112,
      "grad_norm": 0.00044520385563373566,
      "learning_rate": 0.00013029048029910843,
      "loss": 0.0011,
      "step": 151200
    },
    {
      "epoch": 43.50014380212827,
      "grad_norm": 0.0023915658239275217,
      "learning_rate": 0.00013000287604256544,
      "loss": 0.0014,
      "step": 151250
    },
    {
      "epoch": 43.51452401495542,
      "grad_norm": 5.146821422385983e-05,
      "learning_rate": 0.00012971527178602243,
      "loss": 0.002,
      "step": 151300
    },
    {
      "epoch": 43.52890422778257,
      "grad_norm": 0.004698335658758879,
      "learning_rate": 0.00012942766752947944,
      "loss": 0.0017,
      "step": 151350
    },
    {
      "epoch": 43.543284440609725,
      "grad_norm": 0.011277476325631142,
      "learning_rate": 0.00012914006327293645,
      "loss": 0.001,
      "step": 151400
    },
    {
      "epoch": 43.55766465343687,
      "grad_norm": 0.00899431947618723,
      "learning_rate": 0.00012885245901639344,
      "loss": 0.0015,
      "step": 151450
    },
    {
      "epoch": 43.57204486626402,
      "grad_norm": 2.6103583877556957e-05,
      "learning_rate": 0.00012856485475985045,
      "loss": 0.0014,
      "step": 151500
    },
    {
      "epoch": 43.58642507909117,
      "grad_norm": 0.0004074719618074596,
      "learning_rate": 0.00012827725050330746,
      "loss": 0.0012,
      "step": 151550
    },
    {
      "epoch": 43.60080529191832,
      "grad_norm": 5.8340810937806964e-05,
      "learning_rate": 0.00012798964624676445,
      "loss": 0.0018,
      "step": 151600
    },
    {
      "epoch": 43.61518550474547,
      "grad_norm": 0.0032446980476379395,
      "learning_rate": 0.00012770204199022146,
      "loss": 0.0033,
      "step": 151650
    },
    {
      "epoch": 43.62956571757262,
      "grad_norm": 0.0001402771595166996,
      "learning_rate": 0.00012741443773367847,
      "loss": 0.0029,
      "step": 151700
    },
    {
      "epoch": 43.64394593039977,
      "grad_norm": 0.0003903714823536575,
      "learning_rate": 0.00012712683347713546,
      "loss": 0.0017,
      "step": 151750
    },
    {
      "epoch": 43.65832614322692,
      "grad_norm": 8.964435255620629e-05,
      "learning_rate": 0.00012683922922059247,
      "loss": 0.0037,
      "step": 151800
    },
    {
      "epoch": 43.67270635605407,
      "grad_norm": 0.019401686266064644,
      "learning_rate": 0.00012655162496404948,
      "loss": 0.0026,
      "step": 151850
    },
    {
      "epoch": 43.68708656888122,
      "grad_norm": 4.4092208554502577e-05,
      "learning_rate": 0.00012626402070750647,
      "loss": 0.0019,
      "step": 151900
    },
    {
      "epoch": 43.70146678170837,
      "grad_norm": 0.0006268786382861435,
      "learning_rate": 0.00012597641645096348,
      "loss": 0.0011,
      "step": 151950
    },
    {
      "epoch": 43.71584699453552,
      "grad_norm": 0.010196253657341003,
      "learning_rate": 0.0001256888121944205,
      "loss": 0.0002,
      "step": 152000
    },
    {
      "epoch": 43.73022720736267,
      "grad_norm": 0.0004160510143265128,
      "learning_rate": 0.00012540120793787748,
      "loss": 0.0036,
      "step": 152050
    },
    {
      "epoch": 43.744607420189816,
      "grad_norm": 0.011452077887952328,
      "learning_rate": 0.0001251136036813345,
      "loss": 0.001,
      "step": 152100
    },
    {
      "epoch": 43.75898763301697,
      "grad_norm": 2.116146424668841e-05,
      "learning_rate": 0.0001248259994247915,
      "loss": 0.0011,
      "step": 152150
    },
    {
      "epoch": 43.77336784584412,
      "grad_norm": 0.006207014434039593,
      "learning_rate": 0.00012453839516824849,
      "loss": 0.0034,
      "step": 152200
    },
    {
      "epoch": 43.78774805867127,
      "grad_norm": 0.0066971066407859325,
      "learning_rate": 0.0001242507909117055,
      "loss": 0.0024,
      "step": 152250
    },
    {
      "epoch": 43.80212827149842,
      "grad_norm": 0.0004099737561773509,
      "learning_rate": 0.0001239631866551625,
      "loss": 0.0024,
      "step": 152300
    },
    {
      "epoch": 43.81650848432557,
      "grad_norm": 7.147351425373927e-05,
      "learning_rate": 0.0001236755823986195,
      "loss": 0.0029,
      "step": 152350
    },
    {
      "epoch": 43.830888697152716,
      "grad_norm": 0.00015920266741886735,
      "learning_rate": 0.0001233879781420765,
      "loss": 0.001,
      "step": 152400
    },
    {
      "epoch": 43.845268909979865,
      "grad_norm": 0.00028249915339984,
      "learning_rate": 0.00012310037388553352,
      "loss": 0.0076,
      "step": 152450
    },
    {
      "epoch": 43.85964912280702,
      "grad_norm": 2.0289508029236458e-05,
      "learning_rate": 0.0001228127696289905,
      "loss": 0.0004,
      "step": 152500
    },
    {
      "epoch": 43.87402933563417,
      "grad_norm": 0.00028149710851721466,
      "learning_rate": 0.00012252516537244752,
      "loss": 0.0016,
      "step": 152550
    },
    {
      "epoch": 43.88840954846132,
      "grad_norm": 0.02448384277522564,
      "learning_rate": 0.00012223756111590453,
      "loss": 0.0004,
      "step": 152600
    },
    {
      "epoch": 43.90278976128847,
      "grad_norm": 0.0002875773352570832,
      "learning_rate": 0.00012194995685936153,
      "loss": 0.0026,
      "step": 152650
    },
    {
      "epoch": 43.917169974115616,
      "grad_norm": 0.0002388964348938316,
      "learning_rate": 0.00012166235260281853,
      "loss": 0.0021,
      "step": 152700
    },
    {
      "epoch": 43.931550186942765,
      "grad_norm": 0.00021380925318226218,
      "learning_rate": 0.00012137474834627553,
      "loss": 0.0016,
      "step": 152750
    },
    {
      "epoch": 43.945930399769914,
      "grad_norm": 6.01784122409299e-05,
      "learning_rate": 0.00012108714408973254,
      "loss": 0.0017,
      "step": 152800
    },
    {
      "epoch": 43.96031061259707,
      "grad_norm": 0.0018006432801485062,
      "learning_rate": 0.00012079953983318954,
      "loss": 0.0013,
      "step": 152850
    },
    {
      "epoch": 43.97469082542422,
      "grad_norm": 0.0031375924590975046,
      "learning_rate": 0.00012051193557664654,
      "loss": 0.0017,
      "step": 152900
    },
    {
      "epoch": 43.98907103825137,
      "grad_norm": 0.0002785466786008328,
      "learning_rate": 0.00012022433132010353,
      "loss": 0.0017,
      "step": 152950
    },
    {
      "epoch": 44.0,
      "eval_loss": 0.009976007044315338,
      "eval_runtime": 17.2635,
      "eval_samples_per_second": 2764.162,
      "eval_steps_per_second": 43.213,
      "step": 152988
    },
    {
      "epoch": 44.003451251078516,
      "grad_norm": 0.001581417047418654,
      "learning_rate": 0.00011993672706356055,
      "loss": 0.0049,
      "step": 153000
    },
    {
      "epoch": 44.017831463905665,
      "grad_norm": 0.0007811124087311327,
      "learning_rate": 0.00011964912280701755,
      "loss": 0.0011,
      "step": 153050
    },
    {
      "epoch": 44.032211676732814,
      "grad_norm": 0.00017854238103609532,
      "learning_rate": 0.00011936151855047454,
      "loss": 0.0013,
      "step": 153100
    },
    {
      "epoch": 44.04659188955996,
      "grad_norm": 3.0207225790945813e-05,
      "learning_rate": 0.00011907391429393156,
      "loss": 0.0036,
      "step": 153150
    },
    {
      "epoch": 44.06097210238712,
      "grad_norm": 0.02101081795990467,
      "learning_rate": 0.00011878631003738855,
      "loss": 0.0034,
      "step": 153200
    },
    {
      "epoch": 44.07535231521427,
      "grad_norm": 7.132879545679316e-05,
      "learning_rate": 0.00011849870578084555,
      "loss": 0.0051,
      "step": 153250
    },
    {
      "epoch": 44.08973252804142,
      "grad_norm": 0.0006081248284317553,
      "learning_rate": 0.00011821110152430257,
      "loss": 0.0005,
      "step": 153300
    },
    {
      "epoch": 44.104112740868565,
      "grad_norm": 0.0001674853847362101,
      "learning_rate": 0.00011792349726775956,
      "loss": 0.0012,
      "step": 153350
    },
    {
      "epoch": 44.118492953695714,
      "grad_norm": 0.005773817654699087,
      "learning_rate": 0.00011763589301121656,
      "loss": 0.002,
      "step": 153400
    },
    {
      "epoch": 44.13287316652286,
      "grad_norm": 0.03365814685821533,
      "learning_rate": 0.00011734828875467358,
      "loss": 0.0003,
      "step": 153450
    },
    {
      "epoch": 44.14725337935001,
      "grad_norm": 0.000116419316327665,
      "learning_rate": 0.00011706068449813057,
      "loss": 0.0012,
      "step": 153500
    },
    {
      "epoch": 44.16163359217716,
      "grad_norm": 0.00456414557993412,
      "learning_rate": 0.00011677308024158757,
      "loss": 0.0026,
      "step": 153550
    },
    {
      "epoch": 44.17601380500432,
      "grad_norm": 0.00468467129394412,
      "learning_rate": 0.00011648547598504459,
      "loss": 0.0015,
      "step": 153600
    },
    {
      "epoch": 44.190394017831466,
      "grad_norm": 0.00015139502647798508,
      "learning_rate": 0.00011619787172850158,
      "loss": 0.0023,
      "step": 153650
    },
    {
      "epoch": 44.204774230658614,
      "grad_norm": 0.00015912037633825094,
      "learning_rate": 0.00011591026747195858,
      "loss": 0.0019,
      "step": 153700
    },
    {
      "epoch": 44.21915444348576,
      "grad_norm": 0.007061795797199011,
      "learning_rate": 0.0001156226632154156,
      "loss": 0.0053,
      "step": 153750
    },
    {
      "epoch": 44.23353465631291,
      "grad_norm": 0.0004766892525367439,
      "learning_rate": 0.0001153350589588726,
      "loss": 0.0012,
      "step": 153800
    },
    {
      "epoch": 44.24791486914006,
      "grad_norm": 5.128508564666845e-05,
      "learning_rate": 0.00011504745470232959,
      "loss": 0.0025,
      "step": 153850
    },
    {
      "epoch": 44.26229508196721,
      "grad_norm": 2.853333717212081e-05,
      "learning_rate": 0.0001147598504457866,
      "loss": 0.0014,
      "step": 153900
    },
    {
      "epoch": 44.276675294794366,
      "grad_norm": 0.0008498815004713833,
      "learning_rate": 0.0001144722461892436,
      "loss": 0.0023,
      "step": 153950
    },
    {
      "epoch": 44.291055507621515,
      "grad_norm": 3.013236892002169e-05,
      "learning_rate": 0.0001141846419327006,
      "loss": 0.0052,
      "step": 154000
    },
    {
      "epoch": 44.30543572044866,
      "grad_norm": 8.384243119508028e-05,
      "learning_rate": 0.00011389703767615761,
      "loss": 0.0028,
      "step": 154050
    },
    {
      "epoch": 44.31981593327581,
      "grad_norm": 0.002896106569096446,
      "learning_rate": 0.00011360943341961461,
      "loss": 0.0004,
      "step": 154100
    },
    {
      "epoch": 44.33419614610296,
      "grad_norm": 2.0808545741601847e-05,
      "learning_rate": 0.00011332182916307161,
      "loss": 0.0014,
      "step": 154150
    },
    {
      "epoch": 44.34857635893011,
      "grad_norm": 2.360906728426926e-05,
      "learning_rate": 0.00011303422490652862,
      "loss": 0.0007,
      "step": 154200
    },
    {
      "epoch": 44.36295657175726,
      "grad_norm": 0.00014370660937856883,
      "learning_rate": 0.00011274662064998562,
      "loss": 0.0018,
      "step": 154250
    },
    {
      "epoch": 44.377336784584415,
      "grad_norm": 0.00011211790842935443,
      "learning_rate": 0.00011245901639344262,
      "loss": 0.0018,
      "step": 154300
    },
    {
      "epoch": 44.391716997411564,
      "grad_norm": 0.000132171597215347,
      "learning_rate": 0.00011217141213689963,
      "loss": 0.001,
      "step": 154350
    },
    {
      "epoch": 44.40609721023871,
      "grad_norm": 6.301200482994318e-05,
      "learning_rate": 0.00011188380788035662,
      "loss": 0.0036,
      "step": 154400
    },
    {
      "epoch": 44.42047742306586,
      "grad_norm": 0.0004245190357323736,
      "learning_rate": 0.00011159620362381363,
      "loss": 0.0012,
      "step": 154450
    },
    {
      "epoch": 44.43485763589301,
      "grad_norm": 0.000491333135869354,
      "learning_rate": 0.00011130859936727064,
      "loss": 0.0018,
      "step": 154500
    },
    {
      "epoch": 44.44923784872016,
      "grad_norm": 2.4384833523072302e-05,
      "learning_rate": 0.00011102099511072763,
      "loss": 0.0018,
      "step": 154550
    },
    {
      "epoch": 44.46361806154731,
      "grad_norm": 3.66882341040764e-05,
      "learning_rate": 0.00011073339085418464,
      "loss": 0.0022,
      "step": 154600
    },
    {
      "epoch": 44.477998274374464,
      "grad_norm": 2.2451271433965303e-05,
      "learning_rate": 0.00011044578659764165,
      "loss": 0.0017,
      "step": 154650
    },
    {
      "epoch": 44.49237848720161,
      "grad_norm": 0.005995594430714846,
      "learning_rate": 0.00011015818234109864,
      "loss": 0.0026,
      "step": 154700
    },
    {
      "epoch": 44.50675870002876,
      "grad_norm": 0.004075685050338507,
      "learning_rate": 0.00010987057808455565,
      "loss": 0.0017,
      "step": 154750
    },
    {
      "epoch": 44.52113891285591,
      "grad_norm": 1.503791645518504e-05,
      "learning_rate": 0.00010958297382801266,
      "loss": 0.0015,
      "step": 154800
    },
    {
      "epoch": 44.53551912568306,
      "grad_norm": 0.0002236561122117564,
      "learning_rate": 0.00010929536957146965,
      "loss": 0.0003,
      "step": 154850
    },
    {
      "epoch": 44.54989933851021,
      "grad_norm": 8.710224938113242e-05,
      "learning_rate": 0.00010900776531492666,
      "loss": 0.0024,
      "step": 154900
    },
    {
      "epoch": 44.56427955133736,
      "grad_norm": 0.0002106508327415213,
      "learning_rate": 0.00010872016105838367,
      "loss": 0.001,
      "step": 154950
    },
    {
      "epoch": 44.57865976416451,
      "grad_norm": 3.242646926082671e-05,
      "learning_rate": 0.00010843255680184066,
      "loss": 0.0022,
      "step": 155000
    },
    {
      "epoch": 44.59303997699166,
      "grad_norm": 0.00268451776355505,
      "learning_rate": 0.00010814495254529767,
      "loss": 0.0044,
      "step": 155050
    },
    {
      "epoch": 44.60742018981881,
      "grad_norm": 0.001479871803894639,
      "learning_rate": 0.00010785734828875468,
      "loss": 0.0027,
      "step": 155100
    },
    {
      "epoch": 44.62180040264596,
      "grad_norm": 6.133080751169473e-05,
      "learning_rate": 0.00010756974403221167,
      "loss": 0.0029,
      "step": 155150
    },
    {
      "epoch": 44.63618061547311,
      "grad_norm": 0.003168710507452488,
      "learning_rate": 0.00010728213977566868,
      "loss": 0.002,
      "step": 155200
    },
    {
      "epoch": 44.65056082830026,
      "grad_norm": 0.0006136686424724758,
      "learning_rate": 0.00010699453551912569,
      "loss": 0.0056,
      "step": 155250
    },
    {
      "epoch": 44.664941041127406,
      "grad_norm": 0.0025678270030766726,
      "learning_rate": 0.00010670693126258268,
      "loss": 0.0063,
      "step": 155300
    },
    {
      "epoch": 44.679321253954555,
      "grad_norm": 0.0023991703055799007,
      "learning_rate": 0.00010641932700603969,
      "loss": 0.0047,
      "step": 155350
    },
    {
      "epoch": 44.69370146678171,
      "grad_norm": 0.002583087421953678,
      "learning_rate": 0.0001061317227494967,
      "loss": 0.0006,
      "step": 155400
    },
    {
      "epoch": 44.70808167960886,
      "grad_norm": 0.008484644815325737,
      "learning_rate": 0.0001058441184929537,
      "loss": 0.0046,
      "step": 155450
    },
    {
      "epoch": 44.72246189243601,
      "grad_norm": 0.0005034795613028109,
      "learning_rate": 0.0001055565142364107,
      "loss": 0.0049,
      "step": 155500
    },
    {
      "epoch": 44.73684210526316,
      "grad_norm": 1.1899524906766601e-05,
      "learning_rate": 0.00010526890997986771,
      "loss": 0.0017,
      "step": 155550
    },
    {
      "epoch": 44.751222318090306,
      "grad_norm": 0.01478109136223793,
      "learning_rate": 0.00010498130572332471,
      "loss": 0.0011,
      "step": 155600
    },
    {
      "epoch": 44.765602530917455,
      "grad_norm": 0.0003459816798567772,
      "learning_rate": 0.00010469370146678171,
      "loss": 0.0008,
      "step": 155650
    },
    {
      "epoch": 44.779982743744604,
      "grad_norm": 0.0004114724579267204,
      "learning_rate": 0.00010440609721023871,
      "loss": 0.0008,
      "step": 155700
    },
    {
      "epoch": 44.79436295657176,
      "grad_norm": 0.020271386951208115,
      "learning_rate": 0.00010411849295369572,
      "loss": 0.0022,
      "step": 155750
    },
    {
      "epoch": 44.80874316939891,
      "grad_norm": 1.2114239325455856e-05,
      "learning_rate": 0.00010383088869715272,
      "loss": 0.0025,
      "step": 155800
    },
    {
      "epoch": 44.82312338222606,
      "grad_norm": 3.126054798485711e-05,
      "learning_rate": 0.00010354328444060972,
      "loss": 0.0016,
      "step": 155850
    },
    {
      "epoch": 44.83750359505321,
      "grad_norm": 0.0004968190914951265,
      "learning_rate": 0.00010325568018406673,
      "loss": 0.0011,
      "step": 155900
    },
    {
      "epoch": 44.851883807880355,
      "grad_norm": 0.0004417684976942837,
      "learning_rate": 0.00010296807592752373,
      "loss": 0.0022,
      "step": 155950
    },
    {
      "epoch": 44.866264020707504,
      "grad_norm": 7.231580093502998e-05,
      "learning_rate": 0.00010268047167098073,
      "loss": 0.001,
      "step": 156000
    },
    {
      "epoch": 44.88064423353465,
      "grad_norm": 0.0008434807532466948,
      "learning_rate": 0.00010239286741443774,
      "loss": 0.0025,
      "step": 156050
    },
    {
      "epoch": 44.89502444636181,
      "grad_norm": 0.00013380766904447228,
      "learning_rate": 0.00010210526315789474,
      "loss": 0.0027,
      "step": 156100
    },
    {
      "epoch": 44.90940465918896,
      "grad_norm": 0.0006036969134584069,
      "learning_rate": 0.00010181765890135174,
      "loss": 0.0028,
      "step": 156150
    },
    {
      "epoch": 44.92378487201611,
      "grad_norm": 0.0006191074498929083,
      "learning_rate": 0.00010153005464480875,
      "loss": 0.0002,
      "step": 156200
    },
    {
      "epoch": 44.938165084843256,
      "grad_norm": 0.00013288299669511616,
      "learning_rate": 0.00010124245038826575,
      "loss": 0.0031,
      "step": 156250
    },
    {
      "epoch": 44.952545297670405,
      "grad_norm": 0.00027752609457820654,
      "learning_rate": 0.00010095484613172275,
      "loss": 0.0027,
      "step": 156300
    },
    {
      "epoch": 44.96692551049755,
      "grad_norm": 0.003989178221672773,
      "learning_rate": 0.00010066724187517976,
      "loss": 0.0028,
      "step": 156350
    },
    {
      "epoch": 44.9813057233247,
      "grad_norm": 1.9781711671384983e-05,
      "learning_rate": 0.00010037963761863676,
      "loss": 0.0025,
      "step": 156400
    },
    {
      "epoch": 44.99568593615186,
      "grad_norm": 0.000777305627707392,
      "learning_rate": 0.00010009203336209376,
      "loss": 0.0015,
      "step": 156450
    },
    {
      "epoch": 45.0,
      "eval_loss": 0.009878890588879585,
      "eval_runtime": 16.6383,
      "eval_samples_per_second": 2868.019,
      "eval_steps_per_second": 44.836,
      "step": 156465
    },
    {
      "epoch": 45.01006614897901,
      "grad_norm": 2.0315355868660845e-05,
      "learning_rate": 9.980442910555077e-05,
      "loss": 0.0017,
      "step": 156500
    },
    {
      "epoch": 45.024446361806156,
      "grad_norm": 4.58283320767805e-05,
      "learning_rate": 9.951682484900777e-05,
      "loss": 0.0008,
      "step": 156550
    },
    {
      "epoch": 45.038826574633305,
      "grad_norm": 2.625431261549238e-05,
      "learning_rate": 9.922922059246477e-05,
      "loss": 0.0011,
      "step": 156600
    },
    {
      "epoch": 45.053206787460454,
      "grad_norm": 5.906133810640313e-05,
      "learning_rate": 9.894161633592178e-05,
      "loss": 0.0016,
      "step": 156650
    },
    {
      "epoch": 45.0675870002876,
      "grad_norm": 0.00013840828614775091,
      "learning_rate": 9.865401207937878e-05,
      "loss": 0.0049,
      "step": 156700
    },
    {
      "epoch": 45.08196721311475,
      "grad_norm": 3.9053542423062027e-05,
      "learning_rate": 9.836640782283578e-05,
      "loss": 0.0026,
      "step": 156750
    },
    {
      "epoch": 45.09634742594191,
      "grad_norm": 2.7764799597207457e-05,
      "learning_rate": 9.807880356629279e-05,
      "loss": 0.0044,
      "step": 156800
    },
    {
      "epoch": 45.110727638769056,
      "grad_norm": 0.00011703345808200538,
      "learning_rate": 9.779119930974979e-05,
      "loss": 0.003,
      "step": 156850
    },
    {
      "epoch": 45.125107851596205,
      "grad_norm": 0.004450536798685789,
      "learning_rate": 9.750359505320679e-05,
      "loss": 0.0011,
      "step": 156900
    },
    {
      "epoch": 45.139488064423354,
      "grad_norm": 3.741933323908597e-05,
      "learning_rate": 9.72159907966638e-05,
      "loss": 0.0012,
      "step": 156950
    },
    {
      "epoch": 45.1538682772505,
      "grad_norm": 6.46360422251746e-05,
      "learning_rate": 9.69283865401208e-05,
      "loss": 0.001,
      "step": 157000
    },
    {
      "epoch": 45.16824849007765,
      "grad_norm": 0.0006227954290807247,
      "learning_rate": 9.66407822835778e-05,
      "loss": 0.0014,
      "step": 157050
    },
    {
      "epoch": 45.1826287029048,
      "grad_norm": 0.00015608756802976131,
      "learning_rate": 9.635317802703481e-05,
      "loss": 0.0022,
      "step": 157100
    },
    {
      "epoch": 45.197008915731956,
      "grad_norm": 3.036257839994505e-05,
      "learning_rate": 9.60655737704918e-05,
      "loss": 0.0029,
      "step": 157150
    },
    {
      "epoch": 45.211389128559105,
      "grad_norm": 0.006033448502421379,
      "learning_rate": 9.57779695139488e-05,
      "loss": 0.0005,
      "step": 157200
    },
    {
      "epoch": 45.225769341386254,
      "grad_norm": 0.0017452939646318555,
      "learning_rate": 9.549036525740582e-05,
      "loss": 0.0025,
      "step": 157250
    },
    {
      "epoch": 45.2401495542134,
      "grad_norm": 0.012530960142612457,
      "learning_rate": 9.52027610008628e-05,
      "loss": 0.0025,
      "step": 157300
    },
    {
      "epoch": 45.25452976704055,
      "grad_norm": 6.619450869038701e-05,
      "learning_rate": 9.491515674431982e-05,
      "loss": 0.0022,
      "step": 157350
    },
    {
      "epoch": 45.2689099798677,
      "grad_norm": 6.172001303639263e-05,
      "learning_rate": 9.462755248777683e-05,
      "loss": 0.003,
      "step": 157400
    },
    {
      "epoch": 45.28329019269485,
      "grad_norm": 0.022997336462140083,
      "learning_rate": 9.433994823123381e-05,
      "loss": 0.002,
      "step": 157450
    },
    {
      "epoch": 45.297670405522,
      "grad_norm": 5.3002288041170686e-05,
      "learning_rate": 9.405234397469083e-05,
      "loss": 0.0053,
      "step": 157500
    },
    {
      "epoch": 45.312050618349154,
      "grad_norm": 0.0045092045329511166,
      "learning_rate": 9.376473971814784e-05,
      "loss": 0.0009,
      "step": 157550
    },
    {
      "epoch": 45.3264308311763,
      "grad_norm": 0.004365008324384689,
      "learning_rate": 9.347713546160482e-05,
      "loss": 0.0008,
      "step": 157600
    },
    {
      "epoch": 45.34081104400345,
      "grad_norm": 0.00013606634456664324,
      "learning_rate": 9.318953120506184e-05,
      "loss": 0.0048,
      "step": 157650
    },
    {
      "epoch": 45.3551912568306,
      "grad_norm": 0.00011929940228583291,
      "learning_rate": 9.290192694851885e-05,
      "loss": 0.0014,
      "step": 157700
    },
    {
      "epoch": 45.36957146965775,
      "grad_norm": 1.948166209331248e-05,
      "learning_rate": 9.261432269197583e-05,
      "loss": 0.0037,
      "step": 157750
    },
    {
      "epoch": 45.3839516824849,
      "grad_norm": 0.005705767776817083,
      "learning_rate": 9.232671843543285e-05,
      "loss": 0.0019,
      "step": 157800
    },
    {
      "epoch": 45.39833189531205,
      "grad_norm": 6.062390821170993e-05,
      "learning_rate": 9.203911417888986e-05,
      "loss": 0.003,
      "step": 157850
    },
    {
      "epoch": 45.4127121081392,
      "grad_norm": 0.0007049218402244151,
      "learning_rate": 9.175150992234684e-05,
      "loss": 0.0015,
      "step": 157900
    },
    {
      "epoch": 45.42709232096635,
      "grad_norm": 0.004536536987870932,
      "learning_rate": 9.146390566580386e-05,
      "loss": 0.0039,
      "step": 157950
    },
    {
      "epoch": 45.4414725337935,
      "grad_norm": 4.490035280468874e-05,
      "learning_rate": 9.117630140926087e-05,
      "loss": 0.0018,
      "step": 158000
    },
    {
      "epoch": 45.45585274662065,
      "grad_norm": 0.006923581939190626,
      "learning_rate": 9.088869715271785e-05,
      "loss": 0.0013,
      "step": 158050
    },
    {
      "epoch": 45.4702329594478,
      "grad_norm": 9.397422400070354e-05,
      "learning_rate": 9.060109289617487e-05,
      "loss": 0.0029,
      "step": 158100
    },
    {
      "epoch": 45.48461317227495,
      "grad_norm": 5.6970024161273614e-05,
      "learning_rate": 9.031348863963188e-05,
      "loss": 0.0014,
      "step": 158150
    },
    {
      "epoch": 45.4989933851021,
      "grad_norm": 8.912694465834647e-05,
      "learning_rate": 9.002588438308886e-05,
      "loss": 0.0022,
      "step": 158200
    },
    {
      "epoch": 45.51337359792925,
      "grad_norm": 0.005848397966474295,
      "learning_rate": 8.973828012654587e-05,
      "loss": 0.0025,
      "step": 158250
    },
    {
      "epoch": 45.5277538107564,
      "grad_norm": 0.006239964161068201,
      "learning_rate": 8.945067587000289e-05,
      "loss": 0.0032,
      "step": 158300
    },
    {
      "epoch": 45.54213402358355,
      "grad_norm": 0.005366708151996136,
      "learning_rate": 8.916307161345987e-05,
      "loss": 0.0026,
      "step": 158350
    },
    {
      "epoch": 45.5565142364107,
      "grad_norm": 0.0015645439270883799,
      "learning_rate": 8.887546735691688e-05,
      "loss": 0.0005,
      "step": 158400
    },
    {
      "epoch": 45.57089444923785,
      "grad_norm": 0.002575329039245844,
      "learning_rate": 8.858786310037388e-05,
      "loss": 0.0022,
      "step": 158450
    },
    {
      "epoch": 45.585274662065,
      "grad_norm": 9.674453031038865e-05,
      "learning_rate": 8.830025884383088e-05,
      "loss": 0.0027,
      "step": 158500
    },
    {
      "epoch": 45.599654874892146,
      "grad_norm": 0.0001260692224605009,
      "learning_rate": 8.80126545872879e-05,
      "loss": 0.0029,
      "step": 158550
    },
    {
      "epoch": 45.6140350877193,
      "grad_norm": 0.00010325986659154296,
      "learning_rate": 8.77250503307449e-05,
      "loss": 0.0031,
      "step": 158600
    },
    {
      "epoch": 45.62841530054645,
      "grad_norm": 0.006647363770753145,
      "learning_rate": 8.743744607420189e-05,
      "loss": 0.0021,
      "step": 158650
    },
    {
      "epoch": 45.6427955133736,
      "grad_norm": 3.1638730433769524e-05,
      "learning_rate": 8.71498418176589e-05,
      "loss": 0.0032,
      "step": 158700
    },
    {
      "epoch": 45.65717572620075,
      "grad_norm": 0.0007917757611721754,
      "learning_rate": 8.68622375611159e-05,
      "loss": 0.0048,
      "step": 158750
    },
    {
      "epoch": 45.6715559390279,
      "grad_norm": 4.107556742383167e-05,
      "learning_rate": 8.65746333045729e-05,
      "loss": 0.0028,
      "step": 158800
    },
    {
      "epoch": 45.685936151855046,
      "grad_norm": 0.00017792044673115015,
      "learning_rate": 8.628702904802991e-05,
      "loss": 0.0021,
      "step": 158850
    },
    {
      "epoch": 45.700316364682195,
      "grad_norm": 1.6876270819921046e-05,
      "learning_rate": 8.599942479148691e-05,
      "loss": 0.0028,
      "step": 158900
    },
    {
      "epoch": 45.71469657750935,
      "grad_norm": 0.00021358999947551638,
      "learning_rate": 8.571182053494393e-05,
      "loss": 0.0003,
      "step": 158950
    },
    {
      "epoch": 45.7290767903365,
      "grad_norm": 5.671468534274027e-05,
      "learning_rate": 8.542421627840092e-05,
      "loss": 0.0024,
      "step": 159000
    },
    {
      "epoch": 45.74345700316365,
      "grad_norm": 6.58947101328522e-05,
      "learning_rate": 8.513661202185792e-05,
      "loss": 0.0019,
      "step": 159050
    },
    {
      "epoch": 45.7578372159908,
      "grad_norm": 5.612705717794597e-05,
      "learning_rate": 8.484900776531493e-05,
      "loss": 0.0016,
      "step": 159100
    },
    {
      "epoch": 45.772217428817946,
      "grad_norm": 0.0007705366588197649,
      "learning_rate": 8.456140350877193e-05,
      "loss": 0.0011,
      "step": 159150
    },
    {
      "epoch": 45.786597641645095,
      "grad_norm": 0.000448320439318195,
      "learning_rate": 8.427379925222893e-05,
      "loss": 0.0015,
      "step": 159200
    },
    {
      "epoch": 45.800977854472244,
      "grad_norm": 0.000836862251162529,
      "learning_rate": 8.398619499568594e-05,
      "loss": 0.0005,
      "step": 159250
    },
    {
      "epoch": 45.8153580672994,
      "grad_norm": 0.0003971695841755718,
      "learning_rate": 8.369859073914294e-05,
      "loss": 0.0045,
      "step": 159300
    },
    {
      "epoch": 45.82973828012655,
      "grad_norm": 0.00025554082822054625,
      "learning_rate": 8.341098648259994e-05,
      "loss": 0.0007,
      "step": 159350
    },
    {
      "epoch": 45.8441184929537,
      "grad_norm": 2.9193641239544377e-05,
      "learning_rate": 8.312338222605695e-05,
      "loss": 0.0024,
      "step": 159400
    },
    {
      "epoch": 45.858498705780846,
      "grad_norm": 2.7017234970116988e-05,
      "learning_rate": 8.283577796951395e-05,
      "loss": 0.0015,
      "step": 159450
    },
    {
      "epoch": 45.872878918607995,
      "grad_norm": 8.70476069394499e-05,
      "learning_rate": 8.254817371297095e-05,
      "loss": 0.001,
      "step": 159500
    },
    {
      "epoch": 45.887259131435144,
      "grad_norm": 3.452602686593309e-05,
      "learning_rate": 8.226056945642796e-05,
      "loss": 0.0039,
      "step": 159550
    },
    {
      "epoch": 45.90163934426229,
      "grad_norm": 0.0007747868075966835,
      "learning_rate": 8.197296519988496e-05,
      "loss": 0.0036,
      "step": 159600
    },
    {
      "epoch": 45.91601955708944,
      "grad_norm": 3.0268258342402987e-05,
      "learning_rate": 8.168536094334196e-05,
      "loss": 0.0011,
      "step": 159650
    },
    {
      "epoch": 45.9303997699166,
      "grad_norm": 0.0019872330594807863,
      "learning_rate": 8.139775668679897e-05,
      "loss": 0.0032,
      "step": 159700
    },
    {
      "epoch": 45.944779982743746,
      "grad_norm": 1.2609234545379877e-05,
      "learning_rate": 8.111015243025597e-05,
      "loss": 0.0024,
      "step": 159750
    },
    {
      "epoch": 45.959160195570895,
      "grad_norm": 0.0003724413109011948,
      "learning_rate": 8.082254817371297e-05,
      "loss": 0.0004,
      "step": 159800
    },
    {
      "epoch": 45.973540408398044,
      "grad_norm": 0.0068703000433743,
      "learning_rate": 8.053494391716998e-05,
      "loss": 0.0007,
      "step": 159850
    },
    {
      "epoch": 45.98792062122519,
      "grad_norm": 0.006212145555764437,
      "learning_rate": 8.024733966062697e-05,
      "loss": 0.0036,
      "step": 159900
    },
    {
      "epoch": 46.0,
      "eval_loss": 0.009880755096673965,
      "eval_runtime": 17.5489,
      "eval_samples_per_second": 2719.204,
      "eval_steps_per_second": 42.51,
      "step": 159942
    },
    {
      "epoch": 46.00230083405234,
      "grad_norm": 0.001142118009738624,
      "learning_rate": 7.995973540408398e-05,
      "loss": 0.0019,
      "step": 159950
    },
    {
      "epoch": 46.01668104687949,
      "grad_norm": 0.005161397159099579,
      "learning_rate": 7.9672131147541e-05,
      "loss": 0.0024,
      "step": 160000
    },
    {
      "epoch": 46.03106125970665,
      "grad_norm": 0.00019610385061241686,
      "learning_rate": 7.938452689099798e-05,
      "loss": 0.0029,
      "step": 160050
    },
    {
      "epoch": 46.045441472533795,
      "grad_norm": 2.4187036615330726e-05,
      "learning_rate": 7.909692263445499e-05,
      "loss": 0.0022,
      "step": 160100
    },
    {
      "epoch": 46.059821685360944,
      "grad_norm": 0.0050730337388813496,
      "learning_rate": 7.8809318377912e-05,
      "loss": 0.0006,
      "step": 160150
    },
    {
      "epoch": 46.07420189818809,
      "grad_norm": 0.00042760203359648585,
      "learning_rate": 7.852171412136899e-05,
      "loss": 0.0007,
      "step": 160200
    },
    {
      "epoch": 46.08858211101524,
      "grad_norm": 0.0002072742790915072,
      "learning_rate": 7.8234109864826e-05,
      "loss": 0.0017,
      "step": 160250
    },
    {
      "epoch": 46.10296232384239,
      "grad_norm": 7.188219024101272e-05,
      "learning_rate": 7.794650560828301e-05,
      "loss": 0.0042,
      "step": 160300
    },
    {
      "epoch": 46.11734253666954,
      "grad_norm": 0.0009128044475801289,
      "learning_rate": 7.765890135174e-05,
      "loss": 0.004,
      "step": 160350
    },
    {
      "epoch": 46.131722749496696,
      "grad_norm": 0.002299047075212002,
      "learning_rate": 7.737129709519701e-05,
      "loss": 0.004,
      "step": 160400
    },
    {
      "epoch": 46.146102962323845,
      "grad_norm": 0.0009658241760917008,
      "learning_rate": 7.708369283865402e-05,
      "loss": 0.0072,
      "step": 160450
    },
    {
      "epoch": 46.16048317515099,
      "grad_norm": 0.00012170482659712434,
      "learning_rate": 7.679608858211101e-05,
      "loss": 0.0002,
      "step": 160500
    },
    {
      "epoch": 46.17486338797814,
      "grad_norm": 6.466376362368464e-05,
      "learning_rate": 7.650848432556802e-05,
      "loss": 0.0036,
      "step": 160550
    },
    {
      "epoch": 46.18924360080529,
      "grad_norm": 0.005839969962835312,
      "learning_rate": 7.622088006902503e-05,
      "loss": 0.0009,
      "step": 160600
    },
    {
      "epoch": 46.20362381363244,
      "grad_norm": 0.0001581578835612163,
      "learning_rate": 7.593327581248202e-05,
      "loss": 0.0023,
      "step": 160650
    },
    {
      "epoch": 46.21800402645959,
      "grad_norm": 0.0005379652720876038,
      "learning_rate": 7.564567155593903e-05,
      "loss": 0.0008,
      "step": 160700
    },
    {
      "epoch": 46.232384239286745,
      "grad_norm": 7.840932084945962e-05,
      "learning_rate": 7.535806729939604e-05,
      "loss": 0.0002,
      "step": 160750
    },
    {
      "epoch": 46.246764452113894,
      "grad_norm": 0.00010921640932792798,
      "learning_rate": 7.507046304285303e-05,
      "loss": 0.0014,
      "step": 160800
    },
    {
      "epoch": 46.26114466494104,
      "grad_norm": 0.004317816346883774,
      "learning_rate": 7.478285878631004e-05,
      "loss": 0.005,
      "step": 160850
    },
    {
      "epoch": 46.27552487776819,
      "grad_norm": 0.00017839547945186496,
      "learning_rate": 7.449525452976705e-05,
      "loss": 0.0024,
      "step": 160900
    },
    {
      "epoch": 46.28990509059534,
      "grad_norm": 0.0018262156518176198,
      "learning_rate": 7.420765027322404e-05,
      "loss": 0.0022,
      "step": 160950
    },
    {
      "epoch": 46.30428530342249,
      "grad_norm": 0.00026674929540604353,
      "learning_rate": 7.392004601668105e-05,
      "loss": 0.0053,
      "step": 161000
    },
    {
      "epoch": 46.31866551624964,
      "grad_norm": 0.00019398851145524532,
      "learning_rate": 7.363244176013806e-05,
      "loss": 0.0005,
      "step": 161050
    },
    {
      "epoch": 46.33304572907679,
      "grad_norm": 2.3471331587643363e-05,
      "learning_rate": 7.334483750359505e-05,
      "loss": 0.0033,
      "step": 161100
    },
    {
      "epoch": 46.34742594190394,
      "grad_norm": 4.431783963809721e-05,
      "learning_rate": 7.305723324705206e-05,
      "loss": 0.0014,
      "step": 161150
    },
    {
      "epoch": 46.36180615473109,
      "grad_norm": 0.0033360053785145283,
      "learning_rate": 7.276962899050906e-05,
      "loss": 0.003,
      "step": 161200
    },
    {
      "epoch": 46.37618636755824,
      "grad_norm": 0.03608769550919533,
      "learning_rate": 7.248202473396606e-05,
      "loss": 0.002,
      "step": 161250
    },
    {
      "epoch": 46.39056658038539,
      "grad_norm": 0.0001970220764633268,
      "learning_rate": 7.219442047742307e-05,
      "loss": 0.0014,
      "step": 161300
    },
    {
      "epoch": 46.40494679321254,
      "grad_norm": 0.002628314308822155,
      "learning_rate": 7.190681622088007e-05,
      "loss": 0.0057,
      "step": 161350
    },
    {
      "epoch": 46.41932700603969,
      "grad_norm": 2.0074718122486956e-05,
      "learning_rate": 7.161921196433707e-05,
      "loss": 0.0034,
      "step": 161400
    },
    {
      "epoch": 46.433707218866836,
      "grad_norm": 1.5123244338610675e-05,
      "learning_rate": 7.133160770779408e-05,
      "loss": 0.0009,
      "step": 161450
    },
    {
      "epoch": 46.44808743169399,
      "grad_norm": 0.002618237864226103,
      "learning_rate": 7.104400345125108e-05,
      "loss": 0.0013,
      "step": 161500
    },
    {
      "epoch": 46.46246764452114,
      "grad_norm": 5.5845473980298266e-05,
      "learning_rate": 7.075639919470808e-05,
      "loss": 0.0011,
      "step": 161550
    },
    {
      "epoch": 46.47684785734829,
      "grad_norm": 0.005247804336249828,
      "learning_rate": 7.046879493816509e-05,
      "loss": 0.002,
      "step": 161600
    },
    {
      "epoch": 46.49122807017544,
      "grad_norm": 0.0003078426525462419,
      "learning_rate": 7.018119068162209e-05,
      "loss": 0.0029,
      "step": 161650
    },
    {
      "epoch": 46.50560828300259,
      "grad_norm": 0.006362588610500097,
      "learning_rate": 6.989358642507909e-05,
      "loss": 0.0023,
      "step": 161700
    },
    {
      "epoch": 46.519988495829736,
      "grad_norm": 2.40566732827574e-05,
      "learning_rate": 6.96059821685361e-05,
      "loss": 0.0016,
      "step": 161750
    },
    {
      "epoch": 46.534368708656885,
      "grad_norm": 0.0003360047994647175,
      "learning_rate": 6.93183779119931e-05,
      "loss": 0.0014,
      "step": 161800
    },
    {
      "epoch": 46.54874892148404,
      "grad_norm": 0.002652199240401387,
      "learning_rate": 6.90307736554501e-05,
      "loss": 0.0017,
      "step": 161850
    },
    {
      "epoch": 46.56312913431119,
      "grad_norm": 7.949839346110821e-05,
      "learning_rate": 6.874316939890711e-05,
      "loss": 0.0002,
      "step": 161900
    },
    {
      "epoch": 46.57750934713834,
      "grad_norm": 0.0046930392272770405,
      "learning_rate": 6.845556514236411e-05,
      "loss": 0.0032,
      "step": 161950
    },
    {
      "epoch": 46.59188955996549,
      "grad_norm": 0.00014132194337435067,
      "learning_rate": 6.81679608858211e-05,
      "loss": 0.0012,
      "step": 162000
    },
    {
      "epoch": 46.606269772792636,
      "grad_norm": 0.0002789116988424212,
      "learning_rate": 6.788035662927812e-05,
      "loss": 0.0043,
      "step": 162050
    },
    {
      "epoch": 46.620649985619785,
      "grad_norm": 0.0001888050464913249,
      "learning_rate": 6.759275237273512e-05,
      "loss": 0.0038,
      "step": 162100
    },
    {
      "epoch": 46.635030198446934,
      "grad_norm": 0.0020954925566911697,
      "learning_rate": 6.730514811619212e-05,
      "loss": 0.0005,
      "step": 162150
    },
    {
      "epoch": 46.64941041127409,
      "grad_norm": 2.6572070055408403e-05,
      "learning_rate": 6.701754385964913e-05,
      "loss": 0.0022,
      "step": 162200
    },
    {
      "epoch": 46.66379062410124,
      "grad_norm": 0.002672879956662655,
      "learning_rate": 6.672993960310613e-05,
      "loss": 0.0045,
      "step": 162250
    },
    {
      "epoch": 46.67817083692839,
      "grad_norm": 3.3347085263812914e-05,
      "learning_rate": 6.644233534656313e-05,
      "loss": 0.0024,
      "step": 162300
    },
    {
      "epoch": 46.69255104975554,
      "grad_norm": 0.0006766196456737816,
      "learning_rate": 6.615473109002014e-05,
      "loss": 0.0037,
      "step": 162350
    },
    {
      "epoch": 46.706931262582685,
      "grad_norm": 0.010849309153854847,
      "learning_rate": 6.586712683347714e-05,
      "loss": 0.0025,
      "step": 162400
    },
    {
      "epoch": 46.721311475409834,
      "grad_norm": 0.00042997419950552285,
      "learning_rate": 6.557952257693415e-05,
      "loss": 0.0008,
      "step": 162450
    },
    {
      "epoch": 46.73569168823698,
      "grad_norm": 0.00027604063507169485,
      "learning_rate": 6.529191832039115e-05,
      "loss": 0.0022,
      "step": 162500
    },
    {
      "epoch": 46.75007190106414,
      "grad_norm": 8.603293827036396e-05,
      "learning_rate": 6.500431406384815e-05,
      "loss": 0.0029,
      "step": 162550
    },
    {
      "epoch": 46.76445211389129,
      "grad_norm": 0.0009637706098146737,
      "learning_rate": 6.471670980730516e-05,
      "loss": 0.0011,
      "step": 162600
    },
    {
      "epoch": 46.77883232671844,
      "grad_norm": 0.0002951494825538248,
      "learning_rate": 6.442910555076214e-05,
      "loss": 0.0014,
      "step": 162650
    },
    {
      "epoch": 46.793212539545586,
      "grad_norm": 0.0005861111567355692,
      "learning_rate": 6.414150129421916e-05,
      "loss": 0.0025,
      "step": 162700
    },
    {
      "epoch": 46.807592752372734,
      "grad_norm": 3.7896737921983004e-05,
      "learning_rate": 6.385389703767617e-05,
      "loss": 0.0072,
      "step": 162750
    },
    {
      "epoch": 46.82197296519988,
      "grad_norm": 3.6233162973076105e-05,
      "learning_rate": 6.356629278113315e-05,
      "loss": 0.0027,
      "step": 162800
    },
    {
      "epoch": 46.83635317802703,
      "grad_norm": 5.506621891981922e-05,
      "learning_rate": 6.327868852459017e-05,
      "loss": 0.0025,
      "step": 162850
    },
    {
      "epoch": 46.85073339085419,
      "grad_norm": 0.00040333563811145723,
      "learning_rate": 6.299108426804718e-05,
      "loss": 0.0006,
      "step": 162900
    },
    {
      "epoch": 46.86511360368134,
      "grad_norm": 5.306300226948224e-05,
      "learning_rate": 6.270348001150416e-05,
      "loss": 0.0008,
      "step": 162950
    },
    {
      "epoch": 46.879493816508486,
      "grad_norm": 0.0018667879048734903,
      "learning_rate": 6.241587575496118e-05,
      "loss": 0.0013,
      "step": 163000
    },
    {
      "epoch": 46.893874029335635,
      "grad_norm": 5.9213438362348825e-05,
      "learning_rate": 6.212827149841817e-05,
      "loss": 0.0017,
      "step": 163050
    },
    {
      "epoch": 46.90825424216278,
      "grad_norm": 6.190632848301902e-05,
      "learning_rate": 6.184066724187519e-05,
      "loss": 0.004,
      "step": 163100
    },
    {
      "epoch": 46.92263445498993,
      "grad_norm": 0.00011833632015623152,
      "learning_rate": 6.155306298533219e-05,
      "loss": 0.0012,
      "step": 163150
    },
    {
      "epoch": 46.93701466781708,
      "grad_norm": 4.501899820752442e-05,
      "learning_rate": 6.126545872878918e-05,
      "loss": 0.0011,
      "step": 163200
    },
    {
      "epoch": 46.95139488064423,
      "grad_norm": 2.7879113986273296e-05,
      "learning_rate": 6.097785447224619e-05,
      "loss": 0.0008,
      "step": 163250
    },
    {
      "epoch": 46.965775093471386,
      "grad_norm": 0.0026321925688534975,
      "learning_rate": 6.0690250215703195e-05,
      "loss": 0.0014,
      "step": 163300
    },
    {
      "epoch": 46.980155306298535,
      "grad_norm": 0.0013140399241819978,
      "learning_rate": 6.0402645959160194e-05,
      "loss": 0.0007,
      "step": 163350
    },
    {
      "epoch": 46.994535519125684,
      "grad_norm": 0.006693607661873102,
      "learning_rate": 6.0115041702617206e-05,
      "loss": 0.0005,
      "step": 163400
    },
    {
      "epoch": 47.0,
      "eval_loss": 0.009907341562211514,
      "eval_runtime": 17.2199,
      "eval_samples_per_second": 2771.156,
      "eval_steps_per_second": 43.322,
      "step": 163419
    },
    {
      "epoch": 47.00891573195283,
      "grad_norm": 0.0006128382519818842,
      "learning_rate": 5.9827437446074205e-05,
      "loss": 0.0017,
      "step": 163450
    },
    {
      "epoch": 47.02329594477998,
      "grad_norm": 0.00045033841161057353,
      "learning_rate": 5.9539833189531204e-05,
      "loss": 0.0025,
      "step": 163500
    },
    {
      "epoch": 47.03767615760713,
      "grad_norm": 0.0009900276781991124,
      "learning_rate": 5.9252228932988216e-05,
      "loss": 0.0015,
      "step": 163550
    },
    {
      "epoch": 47.05205637043428,
      "grad_norm": 0.004903791472315788,
      "learning_rate": 5.8964624676445215e-05,
      "loss": 0.0027,
      "step": 163600
    },
    {
      "epoch": 47.066436583261435,
      "grad_norm": 0.00023774146393407136,
      "learning_rate": 5.867702041990221e-05,
      "loss": 0.0023,
      "step": 163650
    },
    {
      "epoch": 47.080816796088584,
      "grad_norm": 3.4894823329523206e-05,
      "learning_rate": 5.838941616335922e-05,
      "loss": 0.0019,
      "step": 163700
    },
    {
      "epoch": 47.09519700891573,
      "grad_norm": 4.792544132214971e-05,
      "learning_rate": 5.8101811906816224e-05,
      "loss": 0.0049,
      "step": 163750
    },
    {
      "epoch": 47.10957722174288,
      "grad_norm": 0.048279959708452225,
      "learning_rate": 5.781420765027322e-05,
      "loss": 0.0045,
      "step": 163800
    },
    {
      "epoch": 47.12395743457003,
      "grad_norm": 2.5546003598719835e-05,
      "learning_rate": 5.752660339373023e-05,
      "loss": 0.0025,
      "step": 163850
    },
    {
      "epoch": 47.13833764739718,
      "grad_norm": 1.4976943020883482e-05,
      "learning_rate": 5.7238999137187234e-05,
      "loss": 0.0009,
      "step": 163900
    },
    {
      "epoch": 47.15271786022433,
      "grad_norm": 4.1791492549236864e-05,
      "learning_rate": 5.695139488064423e-05,
      "loss": 0.0011,
      "step": 163950
    },
    {
      "epoch": 47.167098073051484,
      "grad_norm": 0.006527415011078119,
      "learning_rate": 5.666379062410124e-05,
      "loss": 0.0033,
      "step": 164000
    },
    {
      "epoch": 47.18147828587863,
      "grad_norm": 0.004686371888965368,
      "learning_rate": 5.6376186367558244e-05,
      "loss": 0.0011,
      "step": 164050
    },
    {
      "epoch": 47.19585849870578,
      "grad_norm": 0.00025711007765494287,
      "learning_rate": 5.608858211101524e-05,
      "loss": 0.004,
      "step": 164100
    },
    {
      "epoch": 47.21023871153293,
      "grad_norm": 3.2049963920144364e-05,
      "learning_rate": 5.580097785447225e-05,
      "loss": 0.0002,
      "step": 164150
    },
    {
      "epoch": 47.22461892436008,
      "grad_norm": 0.0003071286191698164,
      "learning_rate": 5.5513373597929254e-05,
      "loss": 0.0015,
      "step": 164200
    },
    {
      "epoch": 47.23899913718723,
      "grad_norm": 5.415366103989072e-05,
      "learning_rate": 5.522576934138625e-05,
      "loss": 0.0025,
      "step": 164250
    },
    {
      "epoch": 47.25337935001438,
      "grad_norm": 0.0020486710127443075,
      "learning_rate": 5.493816508484326e-05,
      "loss": 0.003,
      "step": 164300
    },
    {
      "epoch": 47.26775956284153,
      "grad_norm": 0.0002585754555184394,
      "learning_rate": 5.465056082830026e-05,
      "loss": 0.0034,
      "step": 164350
    },
    {
      "epoch": 47.28213977566868,
      "grad_norm": 0.003553081536665559,
      "learning_rate": 5.436295657175726e-05,
      "loss": 0.0007,
      "step": 164400
    },
    {
      "epoch": 47.29651998849583,
      "grad_norm": 0.00010656197264324874,
      "learning_rate": 5.407535231521427e-05,
      "loss": 0.0022,
      "step": 164450
    },
    {
      "epoch": 47.31090020132298,
      "grad_norm": 1.8310529412701726e-05,
      "learning_rate": 5.3787748058671266e-05,
      "loss": 0.0014,
      "step": 164500
    },
    {
      "epoch": 47.32528041415013,
      "grad_norm": 0.0002705675142351538,
      "learning_rate": 5.350014380212827e-05,
      "loss": 0.0008,
      "step": 164550
    },
    {
      "epoch": 47.33966062697728,
      "grad_norm": 0.0143212229013443,
      "learning_rate": 5.321253954558528e-05,
      "loss": 0.0031,
      "step": 164600
    },
    {
      "epoch": 47.354040839804426,
      "grad_norm": 9.330107423011214e-05,
      "learning_rate": 5.2924935289042276e-05,
      "loss": 0.0031,
      "step": 164650
    },
    {
      "epoch": 47.36842105263158,
      "grad_norm": 0.0006278033251874149,
      "learning_rate": 5.263733103249928e-05,
      "loss": 0.0028,
      "step": 164700
    },
    {
      "epoch": 47.38280126545873,
      "grad_norm": 7.472063589375466e-05,
      "learning_rate": 5.234972677595629e-05,
      "loss": 0.0023,
      "step": 164750
    },
    {
      "epoch": 47.39718147828588,
      "grad_norm": 1.3692910215468146e-05,
      "learning_rate": 5.2062122519413286e-05,
      "loss": 0.0028,
      "step": 164800
    },
    {
      "epoch": 47.41156169111303,
      "grad_norm": 0.0033988896757364273,
      "learning_rate": 5.177451826287029e-05,
      "loss": 0.0024,
      "step": 164850
    },
    {
      "epoch": 47.42594190394018,
      "grad_norm": 2.236602631455753e-05,
      "learning_rate": 5.14869140063273e-05,
      "loss": 0.0008,
      "step": 164900
    },
    {
      "epoch": 47.44032211676733,
      "grad_norm": 0.0014869888545945287,
      "learning_rate": 5.1199309749784296e-05,
      "loss": 0.0022,
      "step": 164950
    },
    {
      "epoch": 47.454702329594475,
      "grad_norm": 3.314748755656183e-05,
      "learning_rate": 5.0911705493241295e-05,
      "loss": 0.0037,
      "step": 165000
    },
    {
      "epoch": 47.46908254242163,
      "grad_norm": 4.657251702155918e-05,
      "learning_rate": 5.062410123669831e-05,
      "loss": 0.0044,
      "step": 165050
    },
    {
      "epoch": 47.48346275524878,
      "grad_norm": 0.0008266206714324653,
      "learning_rate": 5.0336496980155306e-05,
      "loss": 0.0036,
      "step": 165100
    },
    {
      "epoch": 47.49784296807593,
      "grad_norm": 0.00015291610907297581,
      "learning_rate": 5.0048892723612304e-05,
      "loss": 0.0023,
      "step": 165150
    },
    {
      "epoch": 47.51222318090308,
      "grad_norm": 0.00027425092412158847,
      "learning_rate": 4.976128846706932e-05,
      "loss": 0.001,
      "step": 165200
    },
    {
      "epoch": 47.52660339373023,
      "grad_norm": 3.2374282454838976e-05,
      "learning_rate": 4.9473684210526315e-05,
      "loss": 0.0011,
      "step": 165250
    },
    {
      "epoch": 47.540983606557376,
      "grad_norm": 3.303135235910304e-05,
      "learning_rate": 4.918607995398332e-05,
      "loss": 0.001,
      "step": 165300
    },
    {
      "epoch": 47.555363819384525,
      "grad_norm": 8.474484639009461e-05,
      "learning_rate": 4.8898475697440326e-05,
      "loss": 0.0034,
      "step": 165350
    },
    {
      "epoch": 47.56974403221167,
      "grad_norm": 0.00019208318553864956,
      "learning_rate": 4.8610871440897325e-05,
      "loss": 0.0014,
      "step": 165400
    },
    {
      "epoch": 47.58412424503883,
      "grad_norm": 5.4007952712709084e-05,
      "learning_rate": 4.832326718435433e-05,
      "loss": 0.0022,
      "step": 165450
    },
    {
      "epoch": 47.59850445786598,
      "grad_norm": 2.9132383133401163e-05,
      "learning_rate": 4.8035662927811336e-05,
      "loss": 0.0023,
      "step": 165500
    },
    {
      "epoch": 47.61288467069313,
      "grad_norm": 0.001206897315569222,
      "learning_rate": 4.7748058671268335e-05,
      "loss": 0.0008,
      "step": 165550
    },
    {
      "epoch": 47.627264883520276,
      "grad_norm": 6.0093483625678346e-05,
      "learning_rate": 4.746045441472534e-05,
      "loss": 0.003,
      "step": 165600
    },
    {
      "epoch": 47.641645096347425,
      "grad_norm": 5.3463434596778825e-05,
      "learning_rate": 4.7172850158182346e-05,
      "loss": 0.0007,
      "step": 165650
    },
    {
      "epoch": 47.656025309174574,
      "grad_norm": 0.0006940517923794687,
      "learning_rate": 4.6885245901639345e-05,
      "loss": 0.0022,
      "step": 165700
    },
    {
      "epoch": 47.67040552200172,
      "grad_norm": 1.984462323889602e-05,
      "learning_rate": 4.659764164509635e-05,
      "loss": 0.0024,
      "step": 165750
    },
    {
      "epoch": 47.68478573482888,
      "grad_norm": 0.0001432975404895842,
      "learning_rate": 4.631003738855335e-05,
      "loss": 0.0005,
      "step": 165800
    },
    {
      "epoch": 47.69916594765603,
      "grad_norm": 0.013094942085444927,
      "learning_rate": 4.6022433132010354e-05,
      "loss": 0.0007,
      "step": 165850
    },
    {
      "epoch": 47.713546160483176,
      "grad_norm": 0.0008434526389464736,
      "learning_rate": 4.573482887546736e-05,
      "loss": 0.0014,
      "step": 165900
    },
    {
      "epoch": 47.727926373310325,
      "grad_norm": 0.0024023903533816338,
      "learning_rate": 4.544722461892436e-05,
      "loss": 0.0004,
      "step": 165950
    },
    {
      "epoch": 47.742306586137474,
      "grad_norm": 0.004236452281475067,
      "learning_rate": 4.5159620362381364e-05,
      "loss": 0.0067,
      "step": 166000
    },
    {
      "epoch": 47.75668679896462,
      "grad_norm": 6.730282620992512e-05,
      "learning_rate": 4.487201610583837e-05,
      "loss": 0.0057,
      "step": 166050
    },
    {
      "epoch": 47.77106701179177,
      "grad_norm": 0.009244122542440891,
      "learning_rate": 4.458441184929537e-05,
      "loss": 0.003,
      "step": 166100
    },
    {
      "epoch": 47.78544722461893,
      "grad_norm": 0.0004208236641716212,
      "learning_rate": 4.4296807592752374e-05,
      "loss": 0.0031,
      "step": 166150
    },
    {
      "epoch": 47.799827437446076,
      "grad_norm": 0.0005386060220189393,
      "learning_rate": 4.400920333620938e-05,
      "loss": 0.0044,
      "step": 166200
    },
    {
      "epoch": 47.814207650273225,
      "grad_norm": 4.936258847010322e-05,
      "learning_rate": 4.372159907966638e-05,
      "loss": 0.0013,
      "step": 166250
    },
    {
      "epoch": 47.828587863100374,
      "grad_norm": 0.0016305034514516592,
      "learning_rate": 4.3433994823123384e-05,
      "loss": 0.0027,
      "step": 166300
    },
    {
      "epoch": 47.84296807592752,
      "grad_norm": 7.584448030684143e-05,
      "learning_rate": 4.314639056658039e-05,
      "loss": 0.0008,
      "step": 166350
    },
    {
      "epoch": 47.85734828875467,
      "grad_norm": 4.499404167290777e-05,
      "learning_rate": 4.285878631003739e-05,
      "loss": 0.0005,
      "step": 166400
    },
    {
      "epoch": 47.87172850158182,
      "grad_norm": 0.0005744919762946665,
      "learning_rate": 4.257118205349439e-05,
      "loss": 0.0005,
      "step": 166450
    },
    {
      "epoch": 47.88610871440898,
      "grad_norm": 0.0006839251727797091,
      "learning_rate": 4.22835777969514e-05,
      "loss": 0.001,
      "step": 166500
    },
    {
      "epoch": 47.900488927236125,
      "grad_norm": 7.118423673091456e-05,
      "learning_rate": 4.19959735404084e-05,
      "loss": 0.0042,
      "step": 166550
    },
    {
      "epoch": 47.914869140063274,
      "grad_norm": 8.191160304704681e-05,
      "learning_rate": 4.1708369283865397e-05,
      "loss": 0.0021,
      "step": 166600
    },
    {
      "epoch": 47.92924935289042,
      "grad_norm": 0.0007940434152260423,
      "learning_rate": 4.142076502732241e-05,
      "loss": 0.0031,
      "step": 166650
    },
    {
      "epoch": 47.94362956571757,
      "grad_norm": 5.278013486531563e-05,
      "learning_rate": 4.113316077077941e-05,
      "loss": 0.0028,
      "step": 166700
    },
    {
      "epoch": 47.95800977854472,
      "grad_norm": 0.0038931965827941895,
      "learning_rate": 4.0845556514236406e-05,
      "loss": 0.0014,
      "step": 166750
    },
    {
      "epoch": 47.97238999137187,
      "grad_norm": 0.011236357502639294,
      "learning_rate": 4.055795225769342e-05,
      "loss": 0.0036,
      "step": 166800
    },
    {
      "epoch": 47.986770204199026,
      "grad_norm": 0.00024441309506073594,
      "learning_rate": 4.027034800115042e-05,
      "loss": 0.001,
      "step": 166850
    },
    {
      "epoch": 48.0,
      "eval_loss": 0.009933522902429104,
      "eval_runtime": 16.7046,
      "eval_samples_per_second": 2856.64,
      "eval_steps_per_second": 44.658,
      "step": 166896
    }
  ],
  "logging_steps": 50,
  "max_steps": 173850,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 10,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 6
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0578111809716224e+17,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
