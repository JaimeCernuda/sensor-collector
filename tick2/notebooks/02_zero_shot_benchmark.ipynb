{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ChronoTick 2: Zero-Shot TSFM Benchmark\n\nAutomated benchmark of time series foundation models on clock drift prediction.\nRuns all models across 4 machines, with and without covariates, at multiple\ncontext lengths and horizons. Just click **Run All**.\n\n## Models\n| Model | Origin | Params | Covariates | Probabilistic |\n|-------|--------|--------|------------|---------------|\n| Chronos-2 Small | Amazon | 28M | Yes | Yes |\n| Chronos-2 Base | Amazon | 120M | Yes | Yes |\n| TimesFM 2.5 | Google | 200M | Yes | Yes |\n| Granite TTM | IBM | 1-5M | No | No (point only) |\n| Toto | Datadog | 151M | Yes | Yes (samples) |\n| Moirai 1.1 Small | Salesforce | 14M | Yes | Yes (samples) |\n\n## Dependency Groups\nModels are ordered by dependency compatibility. Results are saved per-model,\nso later installs that override earlier packages don't affect completed runs.\n\n- **Group A** (compatible): Chronos-2, Granite TTM, TimesFM 2.5\n- **Group B** (exact-pinned deps): Toto\n- **Group C** (needs old torch): Moirai"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Environment Setup ===\nimport os\nimport subprocess\nimport sys\n\nIN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n\nif IN_COLAB:\n    REPO_DIR = \"/content/sensor-collector\"\n    REPO_URL = \"https://github.com/JaimeCernuda/sensor-collector.git\"\n\n    # Clone or pull latest tick2 code from GitHub\n    if os.path.exists(REPO_DIR):\n        subprocess.run([\"git\", \"-C\", REPO_DIR, \"pull\", \"-q\"], check=True)\n    else:\n        subprocess.run([\"git\", \"clone\", \"-q\", REPO_URL, REPO_DIR], check=True)\n\n    # Install tick2 package in editable mode\n    subprocess.run([\"pip\", \"install\", \"-q\", \"-e\", f\"{REPO_DIR}/tick2/\"], check=True)\n\n    # Ensure tick2 is importable (pip install via subprocess doesn't always\n    # update sys.path in the running kernel)\n    tick2_src = f\"{REPO_DIR}/tick2/src\"\n    if tick2_src not in sys.path:\n        sys.path.insert(0, tick2_src)\n\n    # Data: repo now includes the CSVs\n    DATA_DIR = f\"{REPO_DIR}/sensors/data\"\n    if not os.path.isdir(f\"{DATA_DIR}/24h_snapshot\"):\n        from google.colab import drive\n        drive.mount(\"/content/drive\")\n        DATA_DIR = \"/content/drive/MyDrive/chronotick2/data\"\n\n    RESULTS_DIR = \"/content/results/zero_shot\"\nelse:\n    DATA_DIR = None\n    RESULTS_DIR = \"../results/zero_shot\"\n\nprint(f\"Environment: {'Colab' if IN_COLAB else 'Local'}\")\nprint(f\"Data dir:    {DATA_DIR or '(default)'}\")\nprint(f\"Results dir: {RESULTS_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Model Definitions & Helpers ===\nimport importlib\nfrom pathlib import Path\n\nimport pandas as pd\n\n# Models ordered by dependency compatibility group.\n# Group A models share compatible deps and run first.\n# Before Group B/C, conflicting packages are uninstalled so pip can resolve\n# the new model's pinned versions cleanly.\n#\n# Dep conflicts (why we need the cleanup cycle):\n#   granite-tsfm pins transformers==4.56  vs  toto-ts pins transformers==4.52\n#   chronos needs scikit-learn>=1.6       vs  toto-ts pins scikit-learn==1.5\n#   toto-ts pins torch==2.7              vs  uni2ts needs torch<2.5\nMODEL_CONFIGS = [\n    # --- Group A: compatible (chronos + granite + timesfm) ---\n    {\n        \"name\": \"chronos2-small\",\n        \"cleanup\": [],\n        \"install\": ['pip install -q \"chronos-forecasting[extras]>=2.2\"'],\n        \"verify\": \"chronos\",\n        \"group\": \"A\",\n    },\n    {\n        \"name\": \"chronos2-base\",\n        \"cleanup\": [],\n        \"install\": [],  # same package as chronos2-small\n        \"verify\": \"chronos\",\n        \"group\": \"A\",\n    },\n    {\n        \"name\": \"granite-ttm\",\n        \"cleanup\": [],\n        \"install\": ['pip install -q \"granite-tsfm>=0.3.3\"'],\n        \"verify\": \"tsfm_public\",\n        \"group\": \"A\",\n    },\n    {\n        \"name\": \"timesfm-2.5\",\n        \"cleanup\": [],\n        \"install\": [\n            \"git clone -q https://github.com/google-research/timesfm /content/timesfm 2>/dev/null || true\",\n            'pip install -q -e \"/content/timesfm[torch]\"',\n        ],\n        \"verify\": \"timesfm\",\n        \"group\": \"A\",\n    },\n    # --- Group B: toto (exact-pinned deps) ---\n    # Uninstall granite + chronos first so pip can resolve toto's pins.\n    # toto-ts uses pkg_resources which was removed in setuptools>=82.\n    {\n        \"name\": \"toto\",\n        \"cleanup\": [\n            \"pip uninstall -y -q chronos-forecasting granite-tsfm 2>/dev/null; true\",\n        ],\n        \"install\": [\n            'pip install -q \"setuptools<81\"',\n            \"pip install -q toto-ts\",\n        ],\n        \"verify\": \"toto\",\n        \"group\": \"B\",\n    },\n    # --- Group C: moirai (needs torch<2.5, may fail on modern Colab) ---\n    # Uninstall toto first to free its version pins\n    {\n        \"name\": \"moirai-1.1-small\",\n        \"cleanup\": [\n            \"pip uninstall -y -q toto-ts 2>/dev/null; true\",\n        ],\n        \"install\": [\"pip install -q uni2ts\"],\n        \"verify\": \"uni2ts\",\n        \"group\": \"C\",\n    },\n]\n\n\ndef _can_import(module_name: str) -> bool:\n    \"\"\"Check if a Python module is importable.\"\"\"\n    try:\n        importlib.import_module(module_name)\n        return True\n    except (ImportError, ModuleNotFoundError):\n        return False\n\n\ndef install_model_deps(cfg: dict) -> bool:\n    \"\"\"Install dependencies for a model, cleaning up conflicts first.\n\n    Flow: check cache → cleanup conflicting packages → install → verify.\n    Returns True if the model is ready to run.\n    \"\"\"\n    name, verify = cfg[\"name\"], cfg[\"verify\"]\n\n    # Already importable — no install needed\n    if not cfg.get(\"cleanup\") and _can_import(verify):\n        print(f\"  [{verify}] already available\")\n        return True\n\n    if not IN_COLAB and not _can_import(verify):\n        print(f\"  [SKIP] {verify} not installed (install manually for local runs)\")\n        return False\n\n    # If no install commands, just verify (e.g., chronos2-base shares chronos pkg)\n    if not cfg[\"install\"] and not cfg[\"cleanup\"]:\n        return _can_import(verify)\n\n    # Cleanup: uninstall packages whose version pins conflict with this model\n    for cmd in cfg.get(\"cleanup\", []):\n        print(f\"  $ {cmd}\")\n        subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=120)\n\n    # After cleanup, check again (maybe we don't need to install)\n    importlib.invalidate_caches()\n    if _can_import(verify) and not cfg.get(\"cleanup\"):\n        print(f\"  [{verify}] already available\")\n        return True\n\n    # Install\n    for cmd in cfg[\"install\"]:\n        print(f\"  $ {cmd}\")\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=600)\n        if result.returncode != 0:\n            tail = result.stderr.strip().split(\"\\n\")[-3:]\n            print(\"  [FAIL]\", \"\\n        \".join(tail))\n            return False\n\n    importlib.invalidate_caches()\n    if _can_import(verify):\n        print(f\"  [{verify}] ready\")\n        return True\n\n    print(f\"  [FAIL] {verify} not importable after install\")\n    return False\n\n\ndef load_model_results(out_dir: Path, model_name: str) -> pd.DataFrame | None:\n    \"\"\"Load previously saved per-model results, or None.\"\"\"\n    csv_path = out_dir / f\"{model_name}.csv\"\n    return pd.read_csv(csv_path) if csv_path.exists() else None\n\n\ndef save_model_results(out_dir: Path, model_name: str, df: pd.DataFrame) -> None:\n    \"\"\"Save per-model results CSV for incremental resume.\"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    csv_path = out_dir / f\"{model_name}.csv\"\n    df.to_csv(csv_path, index=False)\n    print(f\"  Saved: {csv_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Imports, Config & Data ===\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\nfrom tick2.benchmark.runner import BenchmarkConfig, run_benchmark, results_to_dataframe\nfrom tick2.benchmark.reporting import format_summary, results_to_latex, save_results\nfrom tick2.data.preprocessing import TARGET_COL, get_feature_cols, load_all\nfrom tick2.models.registry import get_model, list_models\nfrom tick2.utils.gpu import clear_gpu_memory\n\nsns.set_theme(style=\"whitegrid\", font_scale=1.1)\n\n# --- GPU ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif device == \"cuda\":\n    print(f\"GPU:  {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")\nelse:\n    print(\"Running on CPU (Granite TTM only)\")\n\n# --- Benchmark config ---\nconfig = BenchmarkConfig(\n    context_lengths=[512, 1024],\n    horizons=[60, 120],\n    n_samples=25,\n    seed=42,\n    use_covariates=[False, True],\n    quantile_alpha=0.2,\n)\n\n# --- Load sensor data ---\ndata_dir = Path(DATA_DIR) if DATA_DIR else None\ndatasets = load_all(data_dir=data_dir, snapshot=\"24h_snapshot\")\nfor name, (df, cats) in datasets.items():\n    print(f\"  {name:16s}: {len(df):6d} rows, {len(get_feature_cols(df)):3d} features\")\n\n# --- Results directory ---\nresults_dir = Path(RESULTS_DIR)\nresults_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"\\nResults: {results_dir}\")\nprint(f\"Models:  {list_models()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run All Benchmarks\n\nFor each model: install deps, load, benchmark across all machines/configs,\nsave per-model CSV, unload. Cached results are loaded on resume.\nOn Colab T4, expect ~20-40 min per model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "completed = {}  # model_name -> DataFrame\nskipped = []\n\nfor cfg in MODEL_CONFIGS:\n    model_name = cfg[\"name\"]\n    print(f\"\\n{'='*60}\")\n    print(f\" {model_name}  [Group {cfg['group']}]\")\n    print(f\"{'='*60}\")\n\n    # Resume: load cached results if available\n    cached = load_model_results(results_dir, model_name)\n    if cached is not None:\n        print(f\"  [CACHED] {len(cached)} rows from previous run\")\n        completed[model_name] = cached\n        continue\n\n    # Install dependencies\n    if not install_model_deps(cfg):\n        skipped.append(model_name)\n        continue\n\n    # Benchmark\n    model = None\n    try:\n        clear_gpu_memory()\n        model = get_model(model_name)\n        model.load(device=device)\n        print(f\"  Loaded on {device}, ~{model.memory_footprint_mb():.0f} MB\")\n\n        run_results = run_benchmark(model, datasets, config, progress=True)\n        model_df = results_to_dataframe(run_results)\n\n        save_model_results(results_dir, model_name, model_df)\n        completed[model_name] = model_df\n        print(f\"  Done: {len(run_results)} configs, mean MAE = {model_df['mae'].mean():.4f}\")\n\n    except Exception as e:\n        print(f\"  [FAIL] {e}\")\n        import traceback\n        traceback.print_exc()\n        skipped.append(model_name)\n\n    finally:\n        del model\n        clear_gpu_memory()\n\n# --- Summary ---\nprint(f\"\\n{'='*60}\")\nprint(f\"  Completed: {list(completed.keys())}\")\nif skipped:\n    print(f\"  Skipped:   {skipped}\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if completed:\n    results_df = pd.concat(completed.values(), ignore_index=True)\n    print(format_summary(results_df))\n    display(results_df)\nelse:\n    results_df = pd.DataFrame()\n    print(\"No results collected. Check install/runtime failures above.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Visualizations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not results_df.empty:\n    df = results_df.copy()\n    uni_df = df[~df[\"with_covariates\"]]\n\n    # --- 1. MAE by Model and Machine (univariate) ---\n    fig, ax = plt.subplots(figsize=(12, 5))\n    if not uni_df.empty:\n        pivot = uni_df.pivot_table(values=\"mae\", index=\"model\", columns=\"machine\")\n        pivot.plot(kind=\"bar\", ax=ax)\n        ax.set_ylabel(\"MAE (ppm)\")\n        ax.set_title(\"Zero-Shot MAE by Model and Machine (Univariate)\")\n        ax.legend(title=\"Machine\")\n        plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n    # --- 2. Covariate Effect ---\n    cov_models = df[df[\"model\"].isin(df[df[\"with_covariates\"]][\"model\"].unique())]\n    if not cov_models.empty and len(cov_models[\"with_covariates\"].unique()) > 1:\n        fig, ax = plt.subplots(figsize=(10, 5))\n        sns.barplot(data=cov_models, x=\"model\", y=\"mae\", hue=\"with_covariates\", ax=ax)\n        ax.set_ylabel(\"MAE (ppm)\")\n        ax.set_title(\"Covariate Effect: Univariate vs. Multivariate\")\n        ax.legend(title=\"With Covariates\")\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n\n    # --- 3. Inference Latency ---\n    fig, ax = plt.subplots(figsize=(10, 5))\n    time_data = df.groupby(\"model\")[\"inference_ms\"].mean().sort_values()\n    time_data.plot(kind=\"barh\", ax=ax, color=\"steelblue\")\n    ax.set_xlabel(\"Mean Inference Time (ms)\")\n    ax.set_title(\"Inference Latency by Model\")\n    plt.tight_layout()\n    plt.show()\n\n    # --- 4. Context Length Sensitivity ---\n    if len(uni_df[\"context_length\"].unique()) > 1:\n        fig, ax = plt.subplots(figsize=(10, 5))\n        sns.lineplot(\n            data=uni_df, x=\"context_length\", y=\"mae\",\n            hue=\"model\", style=\"model\", markers=True, ax=ax,\n        )\n        ax.set_xlabel(\"Context Length (timesteps)\")\n        ax.set_ylabel(\"MAE (ppm)\")\n        ax.set_title(\"MAE vs. Context Length\")\n        plt.tight_layout()\n        plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Export Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not results_df.empty:\n    csv_path, latex_path = save_results(results_df, results_dir, prefix=\"zero_shot\")\n    print(f\"CSV:   {csv_path}\")\n    print(f\"LaTeX: {latex_path}\")\n    print(f\"\\n{results_to_latex(results_df)}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}