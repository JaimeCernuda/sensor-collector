{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ChronoTick 2: Zero-Shot TSFM Benchmark\n\nBenchmark all 5 time series foundation models on clock drift prediction\nacross 4 machines, with and without covariates, at multiple context lengths\nand horizons.\n\n## Models Tested\n| Model | Params | Covariates | Quantiles |\n|-------|--------|------------|----------|\n| Chronos-2 (Small) | 28M | Yes | Yes |\n| Chronos-2 (Base) | 120M | Yes | Yes |\n| TimesFM 2.5 | 200M | Yes (XReg) | Yes |\n| Moirai 1.1 (Small) | 14M | Yes | Yes |\n| Toto | 151M | Yes | Yes (samples) |\n| Granite TTM | 1-5M | No (zero-shot) | No |\n\n## Setup\n\n**Colab (recommended):** Use a T4 GPU runtime. Upload to `My Drive/chronotick2/`:\n1. `tick2/` -- the Python package (this repo's `tick2/` directory)\n2. `data/24h_snapshot/` -- the sensor CSVs (from `sensors/data/24h_snapshot/`)\n\n**Local:** Run from `tick2/` directory. Install model deps individually."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Detection & Setup ===\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DATA_DIR = \"/content/drive/MyDrive/chronotick2/data\"\n",
    "    RESULTS_DIR = \"/content/drive/MyDrive/chronotick2/results\"\n",
    "else:\n",
    "    DATA_DIR = None  # uses default\n",
    "    RESULTS_DIR = \"../results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Install Dependencies (Colab) ===\n# Run ONE model at a time to avoid dependency conflicts.\n# Uncomment the model you want to benchmark.\n\nif IN_COLAB:\n    # Install tick2 from Drive\n    !pip install -q /content/drive/MyDrive/chronotick2/tick2/\n\n    # --- Uncomment ONE model group at a time ---\n\n    # Chronos-2 (Amazon)\n    # !pip install -q \"chronos-forecasting[extras]>=2.2\"\n\n    # Moirai (Salesforce)\n    # !pip install -q \"uni2ts>=2.0\"\n\n    # Toto (Datadog)\n    # !pip install -q toto-ts\n\n    # Granite TTM (IBM)\n    # !pip install -q \"granite-tsfm>=0.3.3\"\n\n    # TimesFM 2.5 (Google) - requires source install\n    # !git clone https://github.com/google-research/timesfm /content/timesfm\n    # !cd /content/timesfm && pip install -q -e \".[torch]\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from tick2.benchmark.metrics import mae, rmse, coverage\n",
    "from tick2.benchmark.reporting import format_summary, results_to_latex, save_results\n",
    "from tick2.benchmark.runner import BenchmarkConfig, results_to_dataframe, run_benchmark\n",
    "from tick2.benchmark.timing import get_gpu_memory_mb, profile_inference, reset_gpu_memory_stats\n",
    "from tick2.data.preprocessing import TARGET_COL, get_feature_cols, load_all\n",
    "from tick2.models.registry import get_model, list_models\n",
    "from tick2.utils.gpu import clear_gpu_memory\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# GPU info\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "print(f\"Available models: {list_models()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(DATA_DIR) if DATA_DIR else None\n",
    "datasets = load_all(data_dir=data_dir, snapshot=\"24h_snapshot\")\n",
    "print(f\"\\nLoaded {len(datasets)} machines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BenchmarkConfig(\n",
    "    context_lengths=[512, 1024],\n",
    "    horizons=[60, 120],\n",
    "    n_samples=25,\n",
    "    seed=42,\n",
    "    use_covariates=[False, True],\n",
    "    quantile_alpha=0.2,\n",
    ")\n",
    "\n",
    "# Select which models to benchmark\n",
    "# Change this list based on which deps are installed\n",
    "MODELS_TO_RUN = [\n",
    "    # \"chronos2-small\",\n",
    "    # \"chronos2-base\",\n",
    "    # \"timesfm-2.5\",\n",
    "    # \"moirai-1.1-small\",\n",
    "    # \"toto\",\n",
    "    # \"granite-ttm\",\n",
    "]\n",
    "\n",
    "if not MODELS_TO_RUN:\n",
    "    print(\"WARNING: No models selected! Uncomment at least one model above.\")\n",
    "    print(\"Also make sure the corresponding pip package is installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Benchmarks\n",
    "\n",
    "Each model is loaded, benchmarked, then unloaded to free GPU memory.\n",
    "On a T4, expect ~20-40 minutes per model depending on the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for model_name in MODELS_TO_RUN:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Benchmarking: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    clear_gpu_memory()\n",
    "    reset_gpu_memory_stats()\n",
    "\n",
    "    try:\n",
    "        model = get_model(model_name)\n",
    "        model.load(device=device)\n",
    "        print(f\"  Loaded on {device}, ~{model.memory_footprint_mb():.0f} MB\")\n",
    "\n",
    "        results = run_benchmark(model, datasets, config, progress=True)\n",
    "        all_results.extend(results)\n",
    "\n",
    "        print(f\"  Completed: {len(results)} configurations\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Free model memory\n",
    "        del model\n",
    "        clear_gpu_memory()\n",
    "\n",
    "print(f\"\\nTotal results: {len(all_results)} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    results_df = results_to_dataframe(all_results)\n",
    "    print(format_summary(results_df))\n",
    "    display(results_df)\n",
    "else:\n",
    "    print(\"No results to display. Select and run models in section 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    df = results_df.copy()\n",
    "\n",
    "    # --- MAE by Model and Machine ---\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    uni_df = df[~df[\"with_covariates\"]]\n",
    "    if not uni_df.empty:\n",
    "        pivot = uni_df.pivot_table(values=\"mae\", index=\"model\", columns=\"machine\")\n",
    "        pivot.plot(kind=\"bar\", ax=ax)\n",
    "        ax.set_ylabel(\"MAE (ppm)\")\n",
    "        ax.set_title(\"Zero-Shot MAE by Model and Machine (Univariate)\")\n",
    "        ax.legend(title=\"Machine\")\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Covariate Effect ---\n",
    "    cov_models = df[df[\"model\"].isin(df[df[\"with_covariates\"]][\"model\"].unique())]\n",
    "    if not cov_models.empty and len(cov_models[\"with_covariates\"].unique()) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        sns.barplot(data=cov_models, x=\"model\", y=\"mae\", hue=\"with_covariates\", ax=ax)\n",
    "        ax.set_ylabel(\"MAE (ppm)\")\n",
    "        ax.set_title(\"Covariate Effect: Univariate vs. Multivariate\")\n",
    "        ax.legend(title=\"With Covariates\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Inference Time ---\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    time_data = df.groupby(\"model\")[\"inference_ms\"].mean().sort_values()\n",
    "    time_data.plot(kind=\"barh\", ax=ax, color=\"steelblue\")\n",
    "    ax.set_xlabel(\"Mean Inference Time (ms)\")\n",
    "    ax.set_title(\"Inference Latency by Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Context Length Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    uni_df = df[~df[\"with_covariates\"]]\n",
    "    if len(uni_df[\"context_length\"].unique()) > 1:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        sns.lineplot(\n",
    "            data=uni_df,\n",
    "            x=\"context_length\",\n",
    "            y=\"mae\",\n",
    "            hue=\"model\",\n",
    "            style=\"model\",\n",
    "            markers=True,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_xlabel(\"Context Length (timesteps)\")\n",
    "        ax.set_ylabel(\"MAE (ppm)\")\n",
    "        ax.set_title(\"MAE vs. Context Length\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    output_dir = Path(RESULTS_DIR)\n",
    "    csv_path, latex_path = save_results(results_df, output_dir, prefix=\"zero_shot\")\n",
    "    print(f\"CSV saved: {csv_path}\")\n",
    "    print(f\"LaTeX saved: {latex_path}\")\n",
    "\n",
    "    # Print LaTeX table for copy-paste into paper\n",
    "    print(\"\\n\" + results_to_latex(results_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Model Timing Profile (Optional)\n",
    "\n",
    "Detailed timing with warm-up runs for publication-quality latency numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run detailed timing for a specific model\n",
    "# This takes ~2-5 minutes per model.\n",
    "\n",
    "# TIMING_MODEL = \"chronos2-small\"\n",
    "# clear_gpu_memory()\n",
    "# model = get_model(TIMING_MODEL)\n",
    "# model.load(device=device)\n",
    "#\n",
    "# first_machine = list(datasets.keys())[0]\n",
    "# first_df = datasets[first_machine][0]\n",
    "# context = first_df[TARGET_COL].values[:1024]\n",
    "#\n",
    "# for hz in [60, 120]:\n",
    "#     timing = profile_inference(model, context, horizon=hz, n_warmup=5, n_repeats=20)\n",
    "#     print(f\"{TIMING_MODEL} hz={hz}: {timing.mean_ms:.1f} +/- {timing.std_ms:.1f} ms \"\n",
    "#           f\"(median={timing.median_ms:.1f}, p95={timing.p95_ms:.1f})\")\n",
    "#\n",
    "# del model\n",
    "# clear_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}