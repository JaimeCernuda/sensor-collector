{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ChronoTick 2: Zero-Shot TSFM Benchmark\n\nAutomated benchmark of time series foundation models on clock drift prediction.\nRuns all models across 4 machines, with and without covariates, at multiple\ncontext lengths and horizons. Just click **Run All**.\n\n## Models\n| Model | Origin | Params | Covariates | Probabilistic |\n|-------|--------|--------|------------|---------------|\n| Chronos-2 Small | Amazon | 28M | Yes | Yes |\n| Chronos-2 Base | Amazon | 120M | Yes | Yes |\n| TimesFM 2.5 | Google | 200M | Yes | Yes |\n| Granite TTM | IBM | 1-5M | No | No (point only) |\n| Toto | Datadog | 151M | Yes | Yes (samples) |\n| Moirai 1.1 Small | Salesforce | 14M | Yes | Yes (samples) |\n\n## Dependency Groups\nModels are ordered by dependency compatibility. Results are saved per-model,\nso later installs that override earlier packages don't affect completed runs.\n\n- **Group A** (compatible): Chronos-2, Granite TTM, TimesFM 2.5\n- **Group B** (exact-pinned deps): Toto\n- **Group C** (needs old torch): Moirai"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Environment Setup ===\nimport os\nimport subprocess\nimport sys\n\nIN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n\nif IN_COLAB:\n    REPO_DIR = \"/content/sensor-collector\"\n    REPO_URL = \"https://github.com/JaimeCernuda/sensor-collector.git\"\n\n    # Read GitHub token from Colab secrets (set via sidebar key icon).\n    # Required for git push; without it the notebook runs but cannot push.\n    GITHUB_TOKEN = None\n    try:\n        from google.colab import userdata\n        GITHUB_TOKEN = userdata.get(\"GITHUB_TOKEN\")\n    except Exception:\n        print(\"WARNING: GITHUB_TOKEN secret not available. Git push will be skipped.\")\n        print(\"  To enable: run from Colab UI with Secrets > GITHUB_TOKEN set.\")\n\n    # Build authenticated URL if token available\n    if GITHUB_TOKEN:\n        auth_url = f\"https://{GITHUB_TOKEN}@github.com/JaimeCernuda/sensor-collector.git\"\n    else:\n        auth_url = REPO_URL\n\n    # Clone or pull latest tick2 code from GitHub\n    if os.path.exists(REPO_DIR):\n        # Update remote URL in case token was added after initial clone\n        subprocess.run([\"git\", \"-C\", REPO_DIR, \"remote\", \"set-url\", \"origin\", auth_url], check=True)\n        # Reset to remote HEAD to avoid divergence from previous Colab commits.\n        # Colab outputs are regenerated each run, so local commits are disposable.\n        subprocess.run([\"git\", \"-C\", REPO_DIR, \"fetch\", \"-q\", \"origin\"], check=True)\n        subprocess.run([\"git\", \"-C\", REPO_DIR, \"reset\", \"--hard\", \"origin/main\"], check=True)\n    else:\n        subprocess.run([\"git\", \"clone\", \"-q\", auth_url, REPO_DIR], check=True)\n\n    # Configure git identity (Colab has no global config)\n    subprocess.run([\"git\", \"-C\", REPO_DIR, \"config\", \"user.name\", \"Colab Runner\"], check=True)\n    subprocess.run([\"git\", \"-C\", REPO_DIR, \"config\", \"user.email\", \"colab@chronotick.dev\"], check=True)\n\n    # Install tick2 package in editable mode\n    subprocess.run([\"pip\", \"install\", \"-q\", \"-e\", f\"{REPO_DIR}/tick2/\"], check=True)\n\n    # Ensure tick2 is importable (pip install via subprocess doesn't always\n    # update sys.path in the running kernel)\n    tick2_src = f\"{REPO_DIR}/tick2/src\"\n    if tick2_src not in sys.path:\n        sys.path.insert(0, tick2_src)\n\n    # Data: repo now includes the CSVs\n    DATA_DIR = f\"{REPO_DIR}/sensors/data\"\n    if not os.path.isdir(f\"{DATA_DIR}/24h_snapshot\"):\n        from google.colab import drive\n        drive.mount(\"/content/drive\")\n        DATA_DIR = \"/content/drive/MyDrive/chronotick2/data\"\n\n    # Output directory inside the repo (will be git-pushed)\n    RESULTS_DIR = f\"{REPO_DIR}/tick2/notebooks/output/02\"\nelse:\n    GITHUB_TOKEN = None\n    DATA_DIR = None\n    RESULTS_DIR = os.path.join(os.path.dirname(__file__) if \"__file__\" in dir() else \".\", \"output\", \"02\")\n\nprint(f\"Environment: {'Colab' if IN_COLAB else 'Local'}\")\nprint(f\"Data dir:    {DATA_DIR or '(default)'}\")\nprint(f\"Results dir: {RESULTS_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Model Definitions & Helpers ===\nimport importlib\nfrom pathlib import Path\n\nimport pandas as pd\n\n# Models ordered by dependency compatibility group.\n# Group A models share compatible deps and run first.\n# Before Group B/C, conflicting packages are uninstalled so pip can resolve\n# the new model's pinned versions cleanly.\n#\n# Dep conflicts (why we need the cleanup cycle):\n#   granite-tsfm pins torch<2.9           vs  Colab has torch==2.9\n#   chronos needs scikit-learn>=1.6       vs  toto-ts pins scikit-learn==1.5\n#   toto-ts pins torch==2.7              vs  uni2ts needs torch<2.5\nMODEL_CONFIGS = [\n    # --- Group A: compatible (chronos + granite + timesfm) ---\n    {\n        \"name\": \"chronos2-small\",\n        \"cleanup\": [],\n        \"install\": ['pip install -q \"chronos-forecasting[extras]>=2.2\"'],\n        \"verify\": \"chronos\",\n        \"group\": \"A\",\n    },\n    {\n        \"name\": \"chronos2-base\",\n        \"cleanup\": [],\n        \"install\": [],  # same package as chronos2-small\n        \"verify\": \"chronos\",\n        \"group\": \"A\",\n    },\n    {\n        # granite-tsfm pins torch<2.9 which conflicts with Colab's torch 2.9.\n        # Install with --no-deps to keep the CUDA torch, then add real deps.\n        \"name\": \"granite-ttm\",\n        \"cleanup\": [],\n        \"install\": [\n            'pip install -q \"granite-tsfm>=0.3.3\" --no-deps',\n            'pip install -q \"transformers>=4.56,<5\" datasets deprecated',\n        ],\n        \"verify\": \"tsfm_public\",\n        \"verify_deep\": \"tsfm_public.models.tinytimemixer:TinyTimeMixerForPrediction\",\n        \"group\": \"A\",\n    },\n    {\n        # TimesFM 2.5 is only on GitHub (PyPI has old 1.x API).\n        # The repo is missing __init__.py in timesfm_2p5/; we add it.\n        # Non-editable install to avoid namespace package issues.\n        \"name\": \"timesfm-2.5\",\n        \"cleanup\": [],\n        \"install\": [\n            \"git clone -q --depth 1 https://github.com/google-research/timesfm /content/timesfm 2>/dev/null || true\",\n            \"touch /content/timesfm/src/timesfm/timesfm_2p5/__init__.py\",\n            'pip install -q \"/content/timesfm[torch]\"',\n        ],\n        \"verify\": \"timesfm\",\n        \"verify_deep\": \"timesfm.timesfm_2p5.timesfm_2p5_torch:TimesFM_2p5_200M_torch\",\n        \"group\": \"A\",\n    },\n    # --- Group B: toto (exact-pinned deps) ---\n    # Uninstall granite + chronos first so pip can resolve toto's pins.\n    # toto-ts uses pkg_resources which was removed in setuptools>=82.\n    {\n        \"name\": \"toto\",\n        \"cleanup\": [\n            \"pip uninstall -y -q chronos-forecasting granite-tsfm 2>/dev/null; true\",\n        ],\n        \"install\": [\n            'pip install -q \"setuptools<81\"',\n            \"pip install -q toto-ts\",\n        ],\n        \"verify\": \"toto\",\n        \"group\": \"B\",\n    },\n    # --- Group C: moirai (uni2ts pins torch<2.5) ---\n    # Install with --no-deps to avoid torch downgrade, then add missing deps.\n    {\n        \"name\": \"moirai-1.1-small\",\n        \"cleanup\": [\n            \"pip uninstall -y -q toto-ts 2>/dev/null; true\",\n        ],\n        \"install\": [\n            \"pip install -q uni2ts --no-deps\",\n            'pip install -q \"einops>=0.7\" \"gluonts>=0.14\" jaxtyping hydra-core python-dotenv lightning safetensors huggingface_hub',\n        ],\n        \"verify\": \"uni2ts\",\n        \"verify_deep\": \"uni2ts.model.moirai:MoiraiModule\",\n        \"group\": \"C\",\n    },\n]\n\n\ndef _can_import(module_name: str) -> bool:\n    \"\"\"Check if a Python module is importable.\"\"\"\n    try:\n        importlib.import_module(module_name)\n        return True\n    except (ImportError, ModuleNotFoundError):\n        return False\n\n\ndef _deep_verify(spec: str) -> tuple[bool, str]:\n    \"\"\"Try to import module:class from a 'module.path:ClassName' spec.\n\n    Returns (success, error_message).\n    \"\"\"\n    mod_path, _, cls_name = spec.partition(\":\")\n    try:\n        mod = importlib.import_module(mod_path)\n        if cls_name and not hasattr(mod, cls_name):\n            return False, f\"{mod_path} imported but {cls_name} not found\"\n        return True, \"\"\n    except Exception as e:\n        return False, f\"{mod_path}: {e}\"\n\n\ndef install_model_deps(cfg: dict) -> bool:\n    \"\"\"Install dependencies for a model, cleaning up conflicts first.\n\n    Flow: check cache -> cleanup conflicting packages -> install -> verify.\n    Returns True if the model is ready to run.\n    \"\"\"\n    name, verify = cfg[\"name\"], cfg[\"verify\"]\n\n    # Already importable -- no install needed\n    if not cfg.get(\"cleanup\") and _can_import(verify):\n        # Also run deep verify if specified\n        deep = cfg.get(\"verify_deep\")\n        if deep:\n            ok, err = _deep_verify(deep)\n            if not ok:\n                print(f\"  [{verify}] top-level OK but deep verify failed: {err}\")\n                # Fall through to reinstall\n            else:\n                print(f\"  [{verify}] already available\")\n                return True\n        else:\n            print(f\"  [{verify}] already available\")\n            return True\n\n    if not IN_COLAB and not _can_import(verify):\n        print(f\"  [SKIP] {verify} not installed (install manually for local runs)\")\n        return False\n\n    # If no install commands, just verify (e.g., chronos2-base shares chronos pkg)\n    if not cfg[\"install\"] and not cfg[\"cleanup\"]:\n        return _can_import(verify)\n\n    # Cleanup: uninstall packages whose version pins conflict with this model\n    for cmd in cfg.get(\"cleanup\", []):\n        print(f\"  $ {cmd}\")\n        subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=120)\n\n    # After cleanup, check again (maybe we don't need to install)\n    importlib.invalidate_caches()\n    if _can_import(verify) and not cfg.get(\"cleanup\"):\n        print(f\"  [{verify}] already available\")\n        return True\n\n    # Install\n    for cmd in cfg[\"install\"]:\n        print(f\"  $ {cmd}\")\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=600)\n        if result.returncode != 0:\n            tail = result.stderr.strip().split(\"\\n\")[-5:]\n            print(\"  [FAIL]\", \"\\n        \".join(tail))\n            return False\n\n    importlib.invalidate_caches()\n\n    # Verify: top-level import\n    if not _can_import(verify):\n        print(f\"  [FAIL] {verify} not importable after install\")\n        return False\n\n    # Deep verify: check specific class/submodule\n    deep = cfg.get(\"verify_deep\")\n    if deep:\n        ok, err = _deep_verify(deep)\n        if not ok:\n            print(f\"  [FAIL] {err}\")\n            return False\n\n    print(f\"  [{verify}] ready\")\n    return True\n\n\ndef _model_csv_name(model_name: str, device: str) -> str:\n    \"\"\"Build device-tagged CSV filename: e.g. 'chronos2-small_cuda.csv'.\"\"\"\n    return f\"{model_name}_{device}.csv\"\n\n\ndef load_model_results(out_dir: Path, model_name: str, device: str) -> pd.DataFrame | None:\n    \"\"\"Load previously saved per-model results for this device, or None.\"\"\"\n    csv_path = out_dir / _model_csv_name(model_name, device)\n    if csv_path.exists():\n        return pd.read_csv(csv_path)\n    # Fallback: check for legacy untagged file (from earlier runs)\n    legacy = out_dir / f\"{model_name}.csv\"\n    if legacy.exists():\n        df = pd.read_csv(legacy)\n        # Only use if device matches\n        if \"device\" in df.columns and (df[\"device\"] == device).all():\n            return df\n    return None\n\n\ndef save_model_results(out_dir: Path, model_name: str, device: str, df: pd.DataFrame) -> None:\n    \"\"\"Save per-model results CSV tagged by device.\"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n    csv_path = out_dir / _model_csv_name(model_name, device)\n    df.to_csv(csv_path, index=False)\n    print(f\"  Saved: {csv_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === Imports, Config & Data ===\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\n\nfrom tick2.benchmark.runner import BenchmarkConfig, run_benchmark, results_to_dataframe\nfrom tick2.benchmark.reporting import format_summary, results_to_latex, save_results\nfrom tick2.data.preprocessing import TARGET_COL, get_feature_cols, load_all\nfrom tick2.models.registry import get_model, list_models\nfrom tick2.utils.gpu import clear_gpu_memory\n\nsns.set_theme(style=\"whitegrid\", font_scale=1.1)\n\n# --- Device selection ---\n# Set DEVICE_OVERRIDE to force a specific device:\n#   \"cpu\"  — run all models on CPU (for latency comparison)\n#   \"cuda\" — force GPU\n#   None   — auto-detect (GPU if available, else CPU)\nDEVICE_OVERRIDE = None\n\nif DEVICE_OVERRIDE:\n    device = DEVICE_OVERRIDE\nelse:\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif device == \"cuda\":\n    props = torch.cuda.get_device_properties(0)\n    vram = getattr(props, \"total_memory\", getattr(props, \"total_mem\", 0))\n    print(f\"GPU:  {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {vram / 1024**3:.1f} GB\")\nelse:\n    print(\"Running on CPU\")\nprint(f\"Device: {device}\")\n\n# --- Benchmark config ---\n# Uses defaults from BenchmarkConfig:\n#   context_lengths: [512, 1024, 1800, 3600]\n#   horizons: [60, 120, 300]\n#   n_samples: 25 per combo (averaged for robust metrics)\nconfig = BenchmarkConfig()\n\nprint(f\"Context lengths: {config.context_lengths}\")\nprint(f\"Horizons:        {config.horizons}\")\nprint(f\"Samples/combo:   {config.n_samples}\")\nn_combos = len(config.context_lengths) * len(config.horizons) * len(config.use_covariates)\nprint(f\"Configs/machine: {n_combos} ({n_combos} x 4 machines = {n_combos * 4} total/model)\")\n\n# --- Load sensor data ---\ndata_dir = Path(DATA_DIR) if DATA_DIR else None\ndatasets = load_all(data_dir=data_dir, snapshot=\"24h_snapshot\")\nfor name, (df, cats) in datasets.items():\n    print(f\"  {name:16s}: {len(df):6d} rows, {len(get_feature_cols(df)):3d} features\")\n\n# --- Results directory ---\nresults_dir = Path(RESULTS_DIR)\nresults_dir.mkdir(parents=True, exist_ok=True)\nprint(f\"\\nResults: {results_dir}\")\nprint(f\"Models:  {list_models()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run All Benchmarks\n\nFor each model: install deps, load, benchmark across all machines/configs,\nsave per-model CSV, unload. Cached results are loaded on resume.\n\n**Config:** 4 context lengths x 3 horizons x 2 covariate modes x 4 machines = 96 combos/model,\neach averaged over 25 random windows. On Colab T4, expect ~60-90 min per model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set FORCE_RERUN to re-benchmark models that already have cached CSVs.\n# Useful after fixing model wrappers. Set to a list of model names or True for all.\nFORCE_RERUN = False  # e.g., [\"granite-ttm\", \"timesfm-2.5\"] or True\n\ncompleted = {}  # model_name -> DataFrame\nskipped = []\n\nfor cfg in MODEL_CONFIGS:\n    model_name = cfg[\"name\"]\n    print(f\"\\n{'='*60}\")\n    print(f\" {model_name}  [Group {cfg['group']}]  device={device}\")\n    print(f\"{'='*60}\")\n\n    # Resume: load cached results if available (unless forced)\n    force = FORCE_RERUN is True or (\n        isinstance(FORCE_RERUN, list) and model_name in FORCE_RERUN\n    )\n    if not force:\n        cached = load_model_results(results_dir, model_name, device)\n        if cached is not None:\n            print(f\"  [CACHED] {len(cached)} rows from previous run ({device})\")\n            completed[model_name] = cached\n            continue\n\n    # Install dependencies\n    if not install_model_deps(cfg):\n        skipped.append(model_name)\n        continue\n\n    # Benchmark\n    model = None\n    try:\n        clear_gpu_memory()\n        model = get_model(model_name)\n        model.load(device=device)\n        print(f\"  Loaded on {device}, ~{model.memory_footprint_mb():.0f} MB\")\n\n        run_results = run_benchmark(model, datasets, config, progress=True)\n        model_df = results_to_dataframe(run_results)\n\n        save_model_results(results_dir, model_name, device, model_df)\n        completed[model_name] = model_df\n        print(f\"  Done: {len(run_results)} configs, mean MAE = {model_df['mae'].mean():.4f}\")\n\n    except Exception as e:\n        print(f\"  [FAIL] {e}\")\n        import traceback\n        traceback.print_exc()\n        skipped.append(model_name)\n\n    finally:\n        del model\n        clear_gpu_memory()\n\n# --- Summary ---\nprint(f\"\\n{'='*60}\")\nprint(f\"  Device:    {device}\")\nprint(f\"  Completed: {list(completed.keys())}\")\nif skipped:\n    print(f\"  Skipped:   {skipped}\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Merge results from current run with any other device runs already on disk.\n# This lets us compare GPU vs CPU side-by-side in the same notebook.\nall_csvs = sorted(results_dir.glob(\"*_cuda.csv\")) + sorted(results_dir.glob(\"*_cpu.csv\"))\n# Also pick up legacy untagged files\nall_csvs += sorted(results_dir.glob(\"*.csv\"))\n# Deduplicate by reading all and concatenating\nall_dfs = []\nseen_files = set()\nfor csv_path in all_csvs:\n    if csv_path.name in seen_files or csv_path.name.startswith(\"zero_shot\"):\n        continue\n    seen_files.add(csv_path.name)\n    all_dfs.append(pd.read_csv(csv_path))\n\nif all_dfs:\n    results_df = pd.concat(all_dfs, ignore_index=True)\n    # Drop exact duplicates (same model+machine+ctx+hz+cov+device)\n    results_df = results_df.drop_duplicates(\n        subset=[\"model\", \"machine\", \"context_length\", \"horizon\", \"with_covariates\", \"device\"],\n        keep=\"last\",\n    )\n    devices = results_df[\"device\"].unique()\n    print(f\"Results: {len(results_df)} rows across devices: {list(devices)}\")\n    print(format_summary(results_df))\n    display(results_df)\nelse:\n    results_df = pd.DataFrame()\n    print(\"No results collected. Check install/runtime failures above.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Visualizations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not results_df.empty:\n    df = results_df.copy()\n    uni_df = df[~df[\"with_covariates\"]]\n\n    # --- 1. MAE by Model and Machine (univariate) ---\n    fig, ax = plt.subplots(figsize=(12, 5))\n    if not uni_df.empty:\n        pivot = uni_df.pivot_table(values=\"mae\", index=\"model\", columns=\"machine\")\n        pivot.plot(kind=\"bar\", ax=ax)\n        ax.set_ylabel(\"MAE (ppm)\")\n        ax.set_title(\"Zero-Shot MAE by Model and Machine (Univariate)\")\n        ax.legend(title=\"Machine\")\n        plt.xticks(rotation=45)\n    plt.tight_layout()\n    fig.savefig(results_dir / \"mae_by_model_machine.png\", dpi=150, bbox_inches=\"tight\")\n    plt.show()\n\n    # --- 2. Covariate Effect ---\n    cov_models = df[df[\"model\"].isin(df[df[\"with_covariates\"]][\"model\"].unique())]\n    if not cov_models.empty and len(cov_models[\"with_covariates\"].unique()) > 1:\n        fig, ax = plt.subplots(figsize=(10, 5))\n        sns.barplot(data=cov_models, x=\"model\", y=\"mae\", hue=\"with_covariates\", ax=ax)\n        ax.set_ylabel(\"MAE (ppm)\")\n        ax.set_title(\"Covariate Effect: Univariate vs. Multivariate\")\n        ax.legend(title=\"With Covariates\")\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        fig.savefig(results_dir / \"covariate_effect.png\", dpi=150, bbox_inches=\"tight\")\n        plt.show()\n\n    # --- 3. Inference Latency ---\n    fig, ax = plt.subplots(figsize=(10, 5))\n    time_data = df.groupby(\"model\")[\"inference_ms\"].mean().sort_values()\n    time_data.plot(kind=\"barh\", ax=ax, color=\"steelblue\")\n    ax.set_xlabel(\"Mean Inference Time (ms)\")\n    ax.set_title(\"Inference Latency by Model\")\n    plt.tight_layout()\n    fig.savefig(results_dir / \"inference_latency.png\", dpi=150, bbox_inches=\"tight\")\n    plt.show()\n\n    # --- 4. Context Length Sensitivity ---\n    if len(uni_df[\"context_length\"].unique()) > 1:\n        fig, ax = plt.subplots(figsize=(10, 5))\n        sns.lineplot(\n            data=uni_df, x=\"context_length\", y=\"mae\",\n            hue=\"model\", style=\"model\", markers=True, ax=ax,\n        )\n        ax.set_xlabel(\"Context Length (timesteps)\")\n        ax.set_ylabel(\"MAE (ppm)\")\n        ax.set_title(\"MAE vs. Context Length\")\n        plt.tight_layout()\n        fig.savefig(results_dir / \"context_sensitivity.png\", dpi=150, bbox_inches=\"tight\")\n        plt.show()\n\n    print(f\"Saved figures to: {results_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Export Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not results_df.empty:\n    csv_path, latex_path = save_results(results_df, results_dir, prefix=\"zero_shot\")\n    print(f\"CSV:   {csv_path}\")\n    print(f\"LaTeX: {latex_path}\")\n    print(f\"\\n{results_to_latex(results_df)}\")\n\n    # Summary: device breakdown\n    if len(results_df[\"device\"].unique()) > 1:\n        print(\"\\n=== Per-Device Summary ===\")\n        for dev in sorted(results_df[\"device\"].unique()):\n            sub = results_df[results_df[\"device\"] == dev]\n            print(f\"\\n{dev}: {len(sub)} rows, {sub['model'].nunique()} models\")\n            print(sub.groupby(\"model\")[\"inference_ms\"].agg([\"mean\", \"max\"]).to_string())"
  },
  {
   "cell_type": "markdown",
   "source": "## Save & Push Results\n\nCommit all benchmark outputs (CSVs, figures, LaTeX) to the repo so they can\nbe accessed from another machine via `git pull`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if IN_COLAB:\n    os.chdir(REPO_DIR)\n\n    # Stage all benchmark outputs\n    subprocess.run([\"git\", \"add\", \"tick2/notebooks/output/02/\"], check=True)\n\n    # Check if there's anything to commit\n    status = subprocess.run(\n        [\"git\", \"status\", \"--porcelain\", \"tick2/notebooks/output/02/\"],\n        capture_output=True, text=True,\n    )\n    if status.stdout.strip():\n        subprocess.run(\n            [\"git\", \"commit\", \"-m\", \"results: notebook 02 zero-shot benchmark outputs\"],\n            check=True,\n        )\n        if GITHUB_TOKEN:\n            subprocess.run([\"git\", \"push\"], check=True)\n            print(\"Pushed notebook 02 outputs to GitHub.\")\n        else:\n            print(\"Committed locally but GITHUB_TOKEN not set — skipping push.\")\n            print(\"Set the secret in Colab sidebar > Secrets > GITHUB_TOKEN\")\n    else:\n        print(\"No new outputs to commit.\")\nelse:\n    print(f\"Local run. Outputs saved to: {results_dir}\")\n    print(\"Run 'git add tick2/notebooks/output/02/ && git commit && git push' to share.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}