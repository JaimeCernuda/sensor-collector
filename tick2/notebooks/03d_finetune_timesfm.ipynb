{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": "# ChronoTick 2: TimesFM 2.5 Fine-Tuning\n\nFine-tune Google TimesFM 2.5 (200M) on univariate clock drift data.\nFT is strictly univariate; sensor covariates are only used at inference\nvia XReg (ridge regression wrapper).\n\n## Experiments\n- **E1_uni_uni**: Univariate train + univariate inference (`lr=1e-4`)\n- **E2_uni_xreg**: Same checkpoint as E1 + XReg at inference (no retraining)\n- **E3_uni_xreg_lr5**: Lower LR training (`lr=5e-5`) + XReg inference\n\nAll share: `num_epochs=50`, `batch_size=64`, `context_length=128`.\n\nE1 and E2 produce identical checkpoints; only evaluation differs.\nThe `train_key` field deduplicates training.\n\n## Training Mode\nSet `TRAINING_MODE` to \"combined\" or \"per_machine\"."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Setup ===\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or os.path.exists(\"/content\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    REPO_DIR = \"/content/sensor-collector\"\n",
    "    REPO_URL = \"https://github.com/JaimeCernuda/sensor-collector.git\"\n",
    "    GITHUB_TOKEN = None\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "\n",
    "        GITHUB_TOKEN = userdata.get(\"GITHUB_TOKEN\")\n",
    "    except Exception:\n",
    "        print(\"WARNING: GITHUB_TOKEN not available\")\n",
    "    auth_url = (\n",
    "        f\"https://{GITHUB_TOKEN}@github.com/JaimeCernuda/sensor-collector.git\"\n",
    "        if GITHUB_TOKEN\n",
    "        else REPO_URL\n",
    "    )\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        subprocess.run(\n",
    "            [\"git\", \"-C\", REPO_DIR, \"remote\", \"set-url\", \"origin\", auth_url], check=True\n",
    "        )\n",
    "        subprocess.run([\"git\", \"-C\", REPO_DIR, \"fetch\", \"-q\", \"origin\"], check=True)\n",
    "        subprocess.run(\n",
    "            [\"git\", \"-C\", REPO_DIR, \"reset\", \"--hard\", \"origin/main\"], check=True\n",
    "        )\n",
    "    else:\n",
    "        subprocess.run([\"git\", \"clone\", \"-q\", auth_url, REPO_DIR], check=True)\n",
    "    subprocess.run(\n",
    "        [\"git\", \"-C\", REPO_DIR, \"config\", \"user.name\", \"Colab Runner\"], check=True\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"git\", \"-C\", REPO_DIR, \"config\", \"user.email\", \"colab@chronotick.dev\"],\n",
    "        check=True,\n",
    "    )\n",
    "    subprocess.run([\"pip\", \"install\", \"-q\", \"-e\", f\"{REPO_DIR}/tick2/\"], check=True)\n",
    "    tick2_src = f\"{REPO_DIR}/tick2/src\"\n",
    "    if tick2_src not in sys.path:\n",
    "        sys.path.insert(0, tick2_src)\n",
    "\n",
    "    # Always mount Drive â€” needed for checkpoint persistence (models too large for git)\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "\n",
    "    # Data: prefer repo copy, fall back to Drive\n",
    "    DATA_DIR = f\"{REPO_DIR}/sensors/data\"\n",
    "    if not os.path.isdir(f\"{DATA_DIR}/24h_snapshot\"):\n",
    "        DATA_DIR = \"/content/drive/MyDrive/chronotick2/data\"\n",
    "\n",
    "    RESULTS_DIR = f\"{REPO_DIR}/tick2/notebooks/output/03\"\n",
    "else:\n",
    "    GITHUB_TOKEN = None\n",
    "    DATA_DIR = None\n",
    "    RESULTS_DIR = os.path.join(\n",
    "        os.path.dirname(\"__file__\") if \"__file__\" in dir() else \".\", \"output\", \"03\"\n",
    "    )\n",
    "\n",
    "DEVICE_DIR_MAP = {\"cuda\": \"gpu\", \"cpu\": \"cpu\"}\n",
    "\n",
    "\n",
    "def checkpoint_push(label):\n",
    "    \"\"\"Git add, commit, and push notebook 03d results.\"\"\"\n",
    "    if not IN_COLAB:\n",
    "        return\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"git\", \"-C\", REPO_DIR, \"add\", \"tick2/notebooks/output/03/\"],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        status = subprocess.run(\n",
    "            [\n",
    "                \"git\",\n",
    "                \"-C\",\n",
    "                REPO_DIR,\n",
    "                \"status\",\n",
    "                \"--porcelain\",\n",
    "                \"tick2/notebooks/output/03/\",\n",
    "            ],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "        if not status.stdout.strip():\n",
    "            return\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"git\",\n",
    "                \"-C\",\n",
    "                REPO_DIR,\n",
    "                \"commit\",\n",
    "                \"-m\",\n",
    "                f\"results: notebook 03d timesfm {label} ({device_label})\",\n",
    "            ],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        if GITHUB_TOKEN:\n",
    "            subprocess.run(\n",
    "                [\"git\", \"-C\", REPO_DIR, \"fetch\", \"-q\", \"origin\"],\n",
    "                capture_output=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "            subprocess.run(\n",
    "                [\"git\", \"-C\", REPO_DIR, \"rebase\", \"origin/main\"],\n",
    "                capture_output=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "            subprocess.run(\n",
    "                [\"git\", \"-C\", REPO_DIR, \"push\"],\n",
    "                check=True,\n",
    "                capture_output=True,\n",
    "                timeout=60,\n",
    "            )\n",
    "            print(f\"  [CHECKPOINT] Pushed {label}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [CHECKPOINT WARNING] {e}\")\n",
    "\n",
    "\n",
    "print(f\"Environment: {'Colab' if IN_COLAB else 'Local'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Install TimesFM Dependencies ===\n",
    "if IN_COLAB:\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"git\",\n",
    "            \"clone\",\n",
    "            \"-q\",\n",
    "            \"--depth\",\n",
    "            \"1\",\n",
    "            \"https://github.com/google-research/timesfm\",\n",
    "            \"/content/timesfm\",\n",
    "        ],\n",
    "        capture_output=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"touch\", \"/content/timesfm/src/timesfm/timesfm_2p5/__init__.py\"],\n",
    "        check=True,\n",
    "    )\n",
    "    subprocess.run(\n",
    "        [\"pip\", \"install\", \"-q\", \"/content/timesfm[torch]\"],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "import timesfm\n",
    "\n",
    "assert hasattr(timesfm, \"TimesFM_2p5_200M_torch\"), \"TimesFM 2.5 not available\"\n",
    "print(\"timesfm ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports, Config & Training Mode ===\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from tick2.finetuning.base import FineTuneConfig\n",
    "from tick2.finetuning.data_prep import prepare_datasets\n",
    "from tick2.finetuning.evaluate import (\n",
    "    compare_ft_vs_zero_shot,\n",
    "    evaluate_finetuned,\n",
    "    load_zero_shot_baselines,\n",
    ")\n",
    "from tick2.finetuning.timesfm_ft import finetune_timesfm, load_finetuned_timesfm\n",
    "from tick2.utils.gpu import clear_gpu_memory\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# --- User Configuration ---\n",
    "TRAINING_MODE = \"combined\"  # \"combined\" or \"per_machine\"\n",
    "DEVICE_OVERRIDE = None  # None = auto-detect, \"cuda\", or \"cpu\"\n",
    "FORCE_RETRAIN = False  # Set True to retrain even if cached results exist\n",
    "\n",
    "# --- Derived settings ---\n",
    "device = DEVICE_OVERRIDE or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_label = DEVICE_DIR_MAP.get(device, device)\n",
    "config = FineTuneConfig(\n",
    "    context_length=1024,\n",
    "    prediction_length=96,\n",
    "    max_covariates=30,\n",
    "    seed=42,\n",
    ")\n",
    "print(f\"Device: {device}, Mode: {TRAINING_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load and Prepare Data ===\n",
    "data_dir = Path(DATA_DIR) if DATA_DIR else None\n",
    "prepared = prepare_datasets(config, data_dir=data_dir)\n",
    "for name, p in prepared.items():\n",
    "    print(\n",
    "        f\"  {name:16s}: train={len(p.split.train)}, \"\n",
    "        f\"val={len(p.split.val)}, test={len(p.split.test)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fine-Tune TimesFM 2.5 (E1, E2, E3) ===\n",
    "from tick2.finetuning.base import FineTuneResult\n",
    "from tick2.utils.colab import (\n",
    "    load_checkpoint_from_drive,\n",
    "    save_checkpoint_to_drive,\n",
    "    setup_training_log,\n",
    ")\n",
    "\n",
    "output_base = Path(RESULTS_DIR)\n",
    "ft_output_dir = output_base / \"timesfm_ft\" / TRAINING_MODE\n",
    "device_results_dir = output_base / device_label\n",
    "device_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_path = setup_training_log(ft_output_dir)\n",
    "print(f\"Training log: {log_path}\")\n",
    "\n",
    "# --- Experiment definitions ---\n",
    "# E1: Univariate train + univariate inference\n",
    "# E2: Same checkpoint as E1 + XReg at inference (no retraining)\n",
    "# E3: Lower LR training + XReg inference\n",
    "# train_key deduplicates training: E1 and E2 share the \"default\" key\n",
    "EXPERIMENTS = [\n",
    "    {\n",
    "        \"name\": \"E1_uni_uni\",\n",
    "        \"train_key\": \"default\",\n",
    "        \"with_xreg_eval\": False,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"num_epochs\": 50,\n",
    "        \"batch_size\": 64,\n",
    "        \"context_length\": 128,\n",
    "        \"horizon_length\": 32,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"E2_uni_xreg\",\n",
    "        \"train_key\": \"default\",\n",
    "        \"with_xreg_eval\": True,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"num_epochs\": 50,\n",
    "        \"batch_size\": 64,\n",
    "        \"context_length\": 128,\n",
    "        \"horizon_length\": 32,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"E3_uni_xreg_lr5\",\n",
    "        \"train_key\": \"lr5\",\n",
    "        \"with_xreg_eval\": True,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"num_epochs\": 50,\n",
    "        \"batch_size\": 64,\n",
    "        \"context_length\": 128,\n",
    "        \"horizon_length\": 32,\n",
    "    },\n",
    "]\n",
    "\n",
    "all_ft_results = []\n",
    "experiment_labels = {}\n",
    "trained_keys = {}  # train_key -> (ckpt_path, ft_results)\n",
    "\n",
    "for exp in EXPERIMENTS:\n",
    "    exp_name = exp[\"name\"]\n",
    "    train_key = exp[\"train_key\"]\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\n",
    "        f\"  {exp_name}  (train_key={train_key},\"\n",
    "        f\" xreg_eval={exp['with_xreg_eval']},\"\n",
    "        f\" lr={exp['learning_rate']})\"\n",
    "    )\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    exp_dir = ft_output_dir / exp_name\n",
    "    cached_eval = device_results_dir / f\"timesfm-2.5-ft-{exp_name}_{TRAINING_MODE}.csv\"\n",
    "\n",
    "    # If train_key already trained, reuse checkpoint\n",
    "    # (E2 reuses E1's)\n",
    "    if train_key in trained_keys:\n",
    "        ckpt_path, prev_results = trained_keys[train_key]\n",
    "        print(f\"  [REUSE] Checkpoint from train_key={train_key}: {ckpt_path}\")\n",
    "        stub = FineTuneResult(\n",
    "            model_name=f\"timesfm-2.5-ft-{exp_name}\",\n",
    "            machine=TRAINING_MODE,\n",
    "            checkpoint_path=str(ckpt_path),\n",
    "            train_loss=(prev_results[0].train_loss if prev_results else []),\n",
    "            val_loss=(prev_results[0].val_loss if prev_results else []),\n",
    "            best_epoch=(prev_results[0].best_epoch if prev_results else 0),\n",
    "            training_time_s=0.0,\n",
    "            config=exp,\n",
    "        )\n",
    "        all_ft_results.append(stub)\n",
    "        experiment_labels[exp_name] = [stub]\n",
    "        continue\n",
    "\n",
    "    # Check for cached eval or checkpoint\n",
    "    ckpt_local = exp_dir / \"combined\" / \"best_model.pt\"\n",
    "    ckpt_dir = exp_dir / \"combined\"\n",
    "\n",
    "    if not ckpt_local.exists() and not FORCE_RETRAIN:\n",
    "        drive_name = f\"timesfm_ft/{TRAINING_MODE}/{exp_name}\"\n",
    "        resumed = load_checkpoint_from_drive(\n",
    "            model_name=drive_name,\n",
    "            local_path=str(ckpt_dir),\n",
    "        )\n",
    "        if resumed:\n",
    "            print(f\"  [RESUMED] From Drive: {resumed}\")\n",
    "\n",
    "    if cached_eval.exists() and not FORCE_RETRAIN:\n",
    "        print(f\"  [CACHED] Eval exists: {cached_eval}\")\n",
    "        stub = FineTuneResult(\n",
    "            model_name=f\"timesfm-2.5-ft-{exp_name}\",\n",
    "            machine=TRAINING_MODE,\n",
    "            checkpoint_path=str(ckpt_dir),\n",
    "            config=exp,\n",
    "        )\n",
    "        all_ft_results.append(stub)\n",
    "        experiment_labels[exp_name] = [stub]\n",
    "        trained_keys[train_key] = (ckpt_dir, [stub])\n",
    "        continue\n",
    "\n",
    "    if ckpt_local.exists() and not FORCE_RETRAIN:\n",
    "        print(f\"  [CACHED] Checkpoint: {ckpt_local}\")\n",
    "        stub = FineTuneResult(\n",
    "            model_name=f\"timesfm-2.5-ft-{exp_name}\",\n",
    "            machine=TRAINING_MODE,\n",
    "            checkpoint_path=str(ckpt_dir),\n",
    "            config=exp,\n",
    "        )\n",
    "        all_ft_results.append(stub)\n",
    "        experiment_labels[exp_name] = [stub]\n",
    "        trained_keys[train_key] = (ckpt_dir, [stub])\n",
    "        continue\n",
    "\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    try:\n",
    "        ft_results = finetune_timesfm(\n",
    "            prepared=prepared,\n",
    "            config=config,\n",
    "            output_dir=str(exp_dir),\n",
    "            training_mode=TRAINING_MODE,\n",
    "            learning_rate=exp[\"learning_rate\"],\n",
    "            num_epochs=exp[\"num_epochs\"],\n",
    "            batch_size=exp[\"batch_size\"],\n",
    "            context_length=exp[\"context_length\"],\n",
    "            horizon_length=exp[\"horizon_length\"],\n",
    "        )\n",
    "\n",
    "        for r in ft_results:\n",
    "            r.model_name = f\"timesfm-2.5-ft-{exp_name}\"\n",
    "            print(f\"  {r.machine}: {r.training_time_s:.1f}s\")\n",
    "\n",
    "        all_ft_results.extend(ft_results)\n",
    "        experiment_labels[exp_name] = ft_results\n",
    "        trained_keys[train_key] = (ckpt_dir, ft_results)\n",
    "\n",
    "        save_checkpoint_to_drive(\n",
    "            local_path=ckpt_dir,\n",
    "            model_name=(f\"timesfm_ft/{TRAINING_MODE}/{exp_name}\"),\n",
    "        )\n",
    "        checkpoint_push(exp_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] {exp_name}: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        clear_gpu_memory()\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"  Completed: {list(experiment_labels.keys())}\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate Fine-Tuned Models ===\n",
    "from tick2.finetuning.data_prep import combine_training_data\n",
    "from tick2.models.timesfm import TimesFMWrapper\n",
    "\n",
    "# Compute shared feature intersection for XReg evaluation\n",
    "_, shared_features_all = combine_training_data(prepared)\n",
    "eval_features = shared_features_all[: config.max_covariates]\n",
    "print(\n",
    "    f\"Shared eval features: {len(eval_features)}\"\n",
    "    f\" (capped from {len(shared_features_all)})\"\n",
    ")\n",
    "\n",
    "eval_dfs = []\n",
    "\n",
    "for exp in EXPERIMENTS:\n",
    "    exp_name = exp[\"name\"]\n",
    "    with_xreg = exp[\"with_xreg_eval\"]\n",
    "    print(f\"\\n--- Evaluating {exp_name} (xreg={with_xreg}) ---\")\n",
    "\n",
    "    cached_eval = device_results_dir / f\"timesfm-2.5-ft-{exp_name}_{TRAINING_MODE}.csv\"\n",
    "    if cached_eval.exists() and not FORCE_RETRAIN:\n",
    "        print(f\"  [CACHED] {cached_eval}\")\n",
    "        eval_dfs.append(pd.read_csv(cached_eval))\n",
    "        continue\n",
    "\n",
    "    # Find checkpoint (may come from a shared train_key)\n",
    "    exp_dir = ft_output_dir / exp_name\n",
    "    ckpt_dir = exp_dir / \"combined\"\n",
    "    if not ckpt_dir.exists():\n",
    "        # Check if this experiment reuses another's checkpoint\n",
    "        results_for_exp = experiment_labels.get(exp_name, [])\n",
    "        if results_for_exp and results_for_exp[0].checkpoint_path:\n",
    "            ckpt_dir = Path(results_for_exp[0].checkpoint_path)\n",
    "    if not ckpt_dir.exists():\n",
    "        print(f\"  [SKIP] No checkpoint for {exp_name}\")\n",
    "        continue\n",
    "\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    try:\n",
    "        ft_model = load_finetuned_timesfm(str(ckpt_dir))\n",
    "\n",
    "        ft_wrapper = TimesFMWrapper(\n",
    "            model_name=f\"timesfm-2.5-ft-{exp_name}\",\n",
    "        )\n",
    "        ft_wrapper._model = ft_model\n",
    "        ft_wrapper._device = device\n",
    "\n",
    "        # E1 (univariate eval): no covariates\n",
    "        # E2/E3 (XReg eval): pass shared features\n",
    "        shared_cols = eval_features if with_xreg else None\n",
    "\n",
    "        results_for_exp = experiment_labels.get(exp_name, [])\n",
    "        ft_epochs = results_for_exp[0].best_epoch if results_for_exp else None\n",
    "        ft_time = results_for_exp[0].training_time_s if results_for_exp else None\n",
    "        ft_machines = results_for_exp[0].machine if results_for_exp else \"\"\n",
    "\n",
    "        eval_df = evaluate_finetuned(\n",
    "            model=ft_wrapper,\n",
    "            prepared=prepared,\n",
    "            config=config,\n",
    "            training_mode=f\"ft_{TRAINING_MODE}\",\n",
    "            ft_epochs=ft_epochs,\n",
    "            ft_time_s=ft_time,\n",
    "            ft_train_machines=ft_machines,\n",
    "            shared_feature_cols=shared_cols,\n",
    "        )\n",
    "\n",
    "        if not eval_df.empty:\n",
    "            eval_df[\"experiment\"] = exp_name\n",
    "            eval_df.to_csv(cached_eval, index=False)\n",
    "            eval_dfs.append(eval_df)\n",
    "            print(f\"  MAE: {eval_df['mae'].mean():.4f}\")\n",
    "            print(f\"  Saved: {cached_eval}\")\n",
    "        else:\n",
    "            print(f\"  [WARN] No eval results for {exp_name}\")\n",
    "\n",
    "        checkpoint_push(f\"eval-{exp_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  [FAIL] Eval {exp_name}: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        clear_gpu_memory()\n",
    "\n",
    "# Combine all evaluation results\n",
    "if eval_dfs:\n",
    "    ft_eval_df = pd.concat(eval_dfs, ignore_index=True)\n",
    "    print(f\"\\nTotal FT eval rows: {len(ft_eval_df)}\")\n",
    "    print(f\"Mean MAE:  {ft_eval_df['mae'].mean():.4f}\")\n",
    "    print(f\"Mean RMSE: {ft_eval_df['rmse'].mean():.4f}\")\n",
    "    if ft_eval_df[\"coverage\"].notna().any():\n",
    "        print(f\"Mean Coverage: {ft_eval_df['coverage'].mean():.1%}\")\n",
    "    display(ft_eval_df)\n",
    "else:\n",
    "    ft_eval_df = pd.DataFrame()\n",
    "    print(\"No evaluation results collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Zero-Shot Baselines ===\n",
    "zs_dir = output_base.parent / \"output\" / \"02\"\n",
    "zs_results = load_zero_shot_baselines(zs_dir, model_name=\"timesfm-2.5\")\n",
    "print(f\"Zero-shot: {len(zs_results)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Comparison: Fine-Tuned vs Zero-Shot ===\n",
    "if not ft_eval_df.empty and not zs_results.empty:\n",
    "    combined = compare_ft_vs_zero_shot(ft_eval_df, zs_results)\n",
    "\n",
    "    # --- Per-experiment improvement vs best ZS ---\n",
    "    best_zs = (\n",
    "        zs_results.groupby(\"machine\")[\"mae\"]\n",
    "        .agg([\"min\", \"idxmin\"])\n",
    "        .rename(columns={\"min\": \"best_zs_mae\"})\n",
    "    )\n",
    "    best_zs[\"best_zs_ctx\"] = zs_results.loc[best_zs[\"idxmin\"], \"context_length\"].values\n",
    "    best_zs = best_zs.drop(columns=[\"idxmin\"])\n",
    "\n",
    "    summary_rows = []\n",
    "    for machine in ft_eval_df[\"machine\"].unique():\n",
    "        if machine not in best_zs.index:\n",
    "            continue\n",
    "        bzs_mae = best_zs.loc[machine, \"best_zs_mae\"]\n",
    "        bzs_ctx = int(best_zs.loc[machine, \"best_zs_ctx\"])\n",
    "\n",
    "        for exp in EXPERIMENTS:\n",
    "            exp_name = exp[\"name\"]\n",
    "            with_xreg = exp[\"with_xreg_eval\"]\n",
    "\n",
    "            # Filter to matching covariate mode:\n",
    "            # E1 (univariate) -> with_covariates=False rows\n",
    "            # E2/E3 (XReg)    -> with_covariates=True rows\n",
    "            ft_mask = (\n",
    "                ft_eval_df[\"model\"].str.contains(exp_name, na=False)\n",
    "                & (ft_eval_df[\"machine\"] == machine)\n",
    "                & (ft_eval_df[\"with_covariates\"] == with_xreg)\n",
    "            )\n",
    "            if not ft_mask.any():\n",
    "                continue\n",
    "            ft_mae = ft_eval_df.loc[ft_mask, \"mae\"].mean()\n",
    "\n",
    "            if bzs_mae > 0:\n",
    "                imp = (bzs_mae - ft_mae) / bzs_mae * 100\n",
    "                summary_rows.append(\n",
    "                    {\n",
    "                        \"machine\": machine,\n",
    "                        \"experiment\": exp_name,\n",
    "                        \"with_xreg\": with_xreg,\n",
    "                        \"ft_mae\": ft_mae,\n",
    "                        \"best_zs_ctx\": bzs_ctx,\n",
    "                        \"best_zs_mae\": bzs_mae,\n",
    "                        \"vs_best_zs_pct\": imp,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if summary_rows:\n",
    "        summary_df = pd.DataFrame(summary_rows)\n",
    "        print(\"=== FT vs Best Zero-Shot ===\")\n",
    "        display(summary_df.round(4))\n",
    "\n",
    "        print(\"\\n=== Per-Experiment Summary ===\")\n",
    "        for exp in EXPERIMENTS:\n",
    "            exp_name = exp[\"name\"]\n",
    "            ed = summary_df[summary_df[\"experiment\"] == exp_name]\n",
    "            if not ed.empty:\n",
    "                ft_m = ed[\"ft_mae\"].mean()\n",
    "                zs_m = ed[\"best_zs_mae\"].mean()\n",
    "                imp_m = ed[\"vs_best_zs_pct\"].mean()\n",
    "                xreg_label = \"XReg\" if exp[\"with_xreg_eval\"] else \"uni\"\n",
    "                print(\n",
    "                    f\"  {exp_name} ({xreg_label}): FT={ft_m:.4f},\"\n",
    "                    f\" ZS={zs_m:.4f},\"\n",
    "                    f\" improvement={imp_m:+.1f}%\"\n",
    "                )\n",
    "    else:\n",
    "        print(\"Could not compute improvement.\")\n",
    "\n",
    "    print(f\"\\nCombined results: {len(combined)} rows\")\n",
    "    display(combined)\n",
    "elif ft_eval_df.empty:\n",
    "    combined = pd.DataFrame()\n",
    "    print(\"No FT results to compare.\")\n",
    "else:\n",
    "    combined = ft_eval_df.copy()\n",
    "    print(\"No zero-shot baselines to compare against.\")\n",
    "    display(ft_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualizations ===\n",
    "fig_dir = output_base / \"figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not ft_eval_df.empty:\n",
    "    # --- 1. MAE Comparison Bar Chart (multi-experiment) ---\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "    plot_rows = []\n",
    "\n",
    "    # Add ZS baseline\n",
    "    if not zs_results.empty:\n",
    "        for machine in zs_results[\"machine\"].unique():\n",
    "            m_zs = zs_results[zs_results[\"machine\"] == machine]\n",
    "            plot_rows.append(\n",
    "                {\n",
    "                    \"machine\": machine,\n",
    "                    \"variant\": \"Zero-Shot\",\n",
    "                    \"mae\": m_zs[\"mae\"].mean(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Add FT experiments (filter to representative covariate mode)\n",
    "    for exp in EXPERIMENTS:\n",
    "        exp_name = exp[\"name\"]\n",
    "        with_xreg = exp[\"with_xreg_eval\"]\n",
    "        exp_data = ft_eval_df[\n",
    "            ft_eval_df[\"model\"].str.contains(exp_name, na=False)\n",
    "            & (ft_eval_df[\"with_covariates\"] == with_xreg)\n",
    "        ]\n",
    "        for machine in exp_data[\"machine\"].unique():\n",
    "            m_ft = exp_data[exp_data[\"machine\"] == machine]\n",
    "            plot_rows.append(\n",
    "                {\n",
    "                    \"machine\": machine,\n",
    "                    \"variant\": exp_name,\n",
    "                    \"mae\": m_ft[\"mae\"].mean(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if plot_rows:\n",
    "        plot_df = pd.DataFrame(plot_rows)\n",
    "        sns.barplot(\n",
    "            data=plot_df,\n",
    "            x=\"machine\",\n",
    "            y=\"mae\",\n",
    "            hue=\"variant\",\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_ylabel(\"MAE (ppm)\")\n",
    "        ax.set_title(\"TimesFM 2.5: FT vs Zero-Shot MAE by Machine\")\n",
    "        ax.legend(\n",
    "            title=\"Variant\",\n",
    "            bbox_to_anchor=(1.05, 1),\n",
    "            loc=\"upper left\",\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(\n",
    "            fig_dir / \"timesfm_ft_vs_zs_mae.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)\n",
    "\n",
    "    # --- 2. Coverage Comparison (if available) ---\n",
    "    if ft_eval_df[\"coverage\"].notna().any():\n",
    "        cov_data = ft_eval_df[ft_eval_df[\"coverage\"].notna()]\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        sns.barplot(\n",
    "            data=cov_data,\n",
    "            x=\"machine\",\n",
    "            y=\"coverage\",\n",
    "            hue=\"model\",\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.axhline(\n",
    "            0.8,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.5,\n",
    "            label=\"80% target\",\n",
    "        )\n",
    "        ax.set_ylabel(\"Coverage\")\n",
    "        ax.set_title(\"Prediction Interval Coverage\")\n",
    "        ax.legend(\n",
    "            title=\"Model\",\n",
    "            bbox_to_anchor=(1.05, 1),\n",
    "            loc=\"upper left\",\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(\n",
    "            fig_dir / \"timesfm_ft_coverage.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"Saved figures to: {fig_dir}\")\n",
    "else:\n",
    "    print(\"No results to visualize.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Export Results ===\n",
    "from tick2.benchmark.reporting import results_to_latex, save_results\n",
    "\n",
    "if not ft_eval_df.empty:\n",
    "    # Save combined FT results\n",
    "    ft_csv = device_results_dir / f\"timesfm-2.5-ft-all_{TRAINING_MODE}.csv\"\n",
    "    ft_eval_df.to_csv(ft_csv, index=False)\n",
    "    print(f\"FT results CSV: {ft_csv}\")\n",
    "\n",
    "if not combined.empty:\n",
    "    csv_path, latex_path = save_results(\n",
    "        combined,\n",
    "        output_base,\n",
    "        prefix=f\"timesfm_ft_{TRAINING_MODE}\",\n",
    "    )\n",
    "    print(f\"Comparison CSV:   {csv_path}\")\n",
    "    print(f\"Comparison LaTeX: {latex_path}\")\n",
    "    latex = results_to_latex(\n",
    "        combined,\n",
    "        caption=f\"TimesFM 2.5 fine-tuning vs zero-shot ({TRAINING_MODE})\",\n",
    "        label=\"tab:timesfm-ft\",\n",
    "    )\n",
    "    print(f\"\\n{latex}\")\n",
    "else:\n",
    "    print(\"No results to export.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final Push ===\n",
    "if IN_COLAB:\n",
    "    os.chdir(REPO_DIR)\n",
    "\n",
    "    subprocess.run(\n",
    "        [\"git\", \"add\", \"tick2/notebooks/output/03/\"],\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    status = subprocess.run(\n",
    "        [\"git\", \"status\", \"--porcelain\", \"tick2/notebooks/output/03/\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "    )\n",
    "    if status.stdout.strip():\n",
    "        msg = f\"results: notebook 03d timesfm-ft figures and combined ({device_label})\"\n",
    "        subprocess.run(\n",
    "            [\"git\", \"commit\", \"-m\", msg],\n",
    "            check=True,\n",
    "        )\n",
    "        if GITHUB_TOKEN:\n",
    "            subprocess.run(\n",
    "                [\"git\", \"fetch\", \"-q\", \"origin\"],\n",
    "                capture_output=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "            subprocess.run(\n",
    "                [\"git\", \"rebase\", \"origin/main\"],\n",
    "                capture_output=True,\n",
    "                timeout=30,\n",
    "            )\n",
    "            subprocess.run([\"git\", \"push\"], check=True)\n",
    "            print(\"Pushed final outputs to GitHub.\")\n",
    "        else:\n",
    "            print(\"Committed locally (no token for push).\")\n",
    "    else:\n",
    "        print(\"No new outputs to commit.\")\n",
    "else:\n",
    "    print(f\"Local run. Outputs saved to: {output_base}\")\n",
    "    print(\n",
    "        \"Run 'git add tick2/notebooks/output/03/ && git commit && git push' to share.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}