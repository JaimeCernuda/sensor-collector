\Section{Temporal Coordination Challenges in Distributed Log Systems}

\Subsection{Time Synchronization Protocols}


The Network Time Protocol (NTP)~\cite{ntp} operates across billions of devices through a hierarchical architecture of time servers organized by stratum levels (starting at stratum 0), where clients estimate clock offset using a four-timestamp exchange. The calculation $o = ((t_2 - t_1) + (t_3 - t_4)) / 2$, where $t_1$ and $t_4$ represent local timestamps while $t_2$ and $t_3$ represent server timestamps. Network delay and measurement uncertainty follow from $d = (t_4 - t_1) - (t_3 - t_2)$ and $\sigma_{\text{measurement}} = \max(d/2, \sigma_{\text{server}})$ respectively. Modern implementations like \texttt{chrony}~\cite{ntp_performance_chrony_ntpd} achieve 1-10 millisecond accuracy on local networks through statistical filtering and hardware timestamping when available, degrading to 10-100 milliseconds across the internet. However, software timestamping introduces 1-10 milliseconds of jitter from operating system scheduling and network stack processing, while network asymmetry (where forward and reverse path delays differ due to routing or congestion) introduces systematic errors that NTP cannot detect or correct. These fundamental limitations stem from NTP's design philosophy: operation over general-purpose networks without infrastructure control, zero specialized hardware requirements, and prioritization of ubiquitous deployment over maximum accuracy.

The Precision Time Protocol (PTP)~\cite{ptp}, standardized as IEEE 1588, achieves sub-microsecond accuracy through hardware timestamping at the network interface physical layer, eliminating software-induced jitter. PTP-capable Network Cards (NICs) timestamp packets at the MAC layer before operating system involvement, while transparent clocks in network switches measure and compensate for forwarding delays. This infrastructure enables nanosecond-scale synchronization but requires substantial investment in specialized hardware: PTP-capable network interface cards, switches with transparent clock support, and GPS-disciplined grandmaster clocks with atomic oscillators per datacenter~\cite{ptp_hardware_costs_survey,meta_ptp_time_appliances}. Beyond hardware costs, PTP demands strict network topology control: the protocol assumes symmetric bidirectional paths where any asymmetry introduces undetectable timing errors, prohibiting common features including load balancing and dynamic routing. Quality of Service policies must strictly prioritize PTP traffic to prevent packet delay variation under congestion from degrading accuracy. These constraints fundamentally confine PTP to controlled data center environments, making it impractical for deployments lacking infrastructure control, edge computing spanning thousands of geographic locations, or IoT devices with power and cost constraints.

% Recent approaches including the White Rabbit Project~\cite{whiterabbit} (sub-nanosecond accuracy over specialized fiber), Huygens~\cite{huygens_nsdi2018_geng_svm_nanosec} (machine learning filtering of network jitter), Google's TrueTime~\cite{spanner_osdi2012_truetime_gps_atomic} (GPS and atomic clocks for bounded uncertainty), and Meta's Open Time Appliance~\cite{meta_ptp_time_appliances} (open-source PTP infrastructure) demonstrate ongoing research interest. However, a fundamental gap persists: distributed systems requiring better than millisecond precision must either deploy expensive PTP infrastructure or accept NTP's limitations, a three-order-of-magnitude performance gap with substantial cost differences.

\Subsection{Bounded Uncertainty Quantification}


Distributed systems increasingly require not just accurate timestamps but \textit{bounded uncertainty quantification} to enable coordination protocols without expensive distributed consensus. Google's Spanner~\cite{spanner_osdi2012_truetime_gps_atomic} exemplifies this through TrueTime, which leverages PTP to expose clock uncertainty explicitly via an API returning intervals $[\text{earliest}, \text{latest}]$ guaranteed to contain true time. By choosing commit timestamps greater than the latest bound and waiting until that timestamp is provably in the past at all nodes (a delay of approximately twice the uncertainty bound), Spanner achieves external consistency without traditional two-phase commit. The critical insight: bounded uncertainty enables strong guarantees even with moderate absolute accuracy. With 1-7 millisecond uncertainty bounds, Spanner achieves external consistency without requiring sub-microsecond clock precision: what matters is that all nodes agree on bounds rather than exact values.

Hybrid Logical Clocks~\cite{hlc_podc2014_kulkarni_causality} approach the problem differently, removing their dependency on PTP, by tracking causality through logical counters when physical clock uncertainty creates ambiguity. Each HLC timestamp consists of two components: (1) a physical time component $l$ that tracks the maximum wall-clock time the node has observed from itself or other nodes, and (2) a logical counter $c$ that increments when multiple events occur at the same physical time. 
% When a node receives a message with timestamp $(l_{\text{msg}}, c_{\text{msg}})$, it updates its own timestamp by taking the maximum of its local physical clock and $l_{\text{msg}}$, ensuring the timestamp advances monotonically. 
The key guarantee is that the physical component $l$ stays bounded: $|l - pt| \leq \epsilon$, where $pt$ is the actual physical time and $\epsilon$ is the clock synchronization error. This ensures HLC timestamps encode causal ordering through the counter while remaining anchored to physical time. However, HLC's correctness depends critically on maintaining bounded clock drift. If a node's system clock diverges beyond the synchronization bound $\epsilon$, due to NTP failures, hardware faults, or prolonged network partitions, the guarantee breaks down.
Systems including CockroachDB~\cite{cockroachdb_hlc_implementation} and YugabyteDB~\cite{yugabytedb_hlc_distributed} employ HLC to achieve causal consistency without specialized timing hardware, demonstrating that bounded uncertainty (rather than minimum absolute error) often suffices for strong distributed guarantees.

The common thread is that algorithms can tolerate substantial absolute timing error if uncertainty bounds enable correct decisions: formally, events at times $t_1 \pm \epsilon_1$ and $t_2 \pm \epsilon_2$ are correctly ordered when $t_1 + \epsilon_1 < t_2 - \epsilon_2$. Systems providing accurate timestamps without uncertainty bounds force conservative worst-case assumptions, while those quantifying uncertainty enable algorithms to balance wait times against required confidence levels.

\Subsection{Survey of existing models}

\label{sec:survey}

\input{ChronoTick/sections/Table/Table_merged}

Transformers have gained significant attention in time series forecasting due to their ability to capture complex sequential patterns, model multivariate correlations, and generate predictions across long horizons. Recent research has reimagined forecasting tasks by leveraging large language models (LLMs) such as GPT-3 and LLaMA-2, which can perform zero-shot forecasting by treating time series data points as tokens analogous to words in natural language sequences. Several works have explored using instruction-following LLMs directly on time series. For example, Time-LLM~\cite{TIME-LLM} repurposes LLMs by embedding time series patches with text data and prompting the frozen LLM for temporal modeling through cross-modal alignment, significantly improving its ability to capture complex temporal dynamics. AutoTimes~\cite{AutoTimes} further refines this approach by employing frozen LLMs that retain their general token transition capabilities while adding only a few trainable parameters to enable autoregressive next-token prediction. TEMPO~\cite{TEMPO} uses prompting techniques to integrate numerical and textual data, allowing users to provide natural language descriptions of time series characteristics (e.g., averages, seasonality) that are combined with numerical observations. These LLM-based approaches demonstrated that transformer architectures pretrained on language can capture temporal patterns, ultimately paving the way for purpose-built transformer models trained specifically on time series data. To our knowledge, TimeGPT-1~\cite{TimeGPT-1} is the first pre-trained foundation model demonstrating accurate zero-shot predictions across multiple domains without additional training.

\textbf{Attention mechanisms for time series.} 
Several works have focused on implementing specialized attention mechanisms for time series, including point-wise, patch-wise, and variate-wise representations. Crossformer~\cite{Crossformer} introduces a transformer-based architecture to capture both temporal and cross-dimensional dependencies in multivariate time series. PatchTST~\cite{PatchTST} proposes a channel-independent Patch Transformer that segments time series into patches to improve long-term forecasting precision. Chronos~\cite{Chronos} develops pre-trained transformer language models that convert time series data into discrete tokens using scaling and quantization methods rather than processing raw continuous values. By pretraining on diverse datasets, Chronos learns generic temporal representations and achieves strong zero-shot forecasting performance. Similarly, MOIRAI~\cite{MOIRAI} advances toward universal time series forecasting with a large masked-encoder transformer that addresses time series heterogeneity through multi-patch layers for cross-frequency pattern learning and employs a flexible attention mechanism for multivariate inputs.

\textbf{Decoder-only architectures.} 
Another line of research has focused on decoder-only models as streamlined alternatives to encoder-decoder transformers. Time-MoE~\cite{Time-MoE} introduces a universal decoder-only transformer architecture that uses sparse Mixture-of-Experts (MoE) structures for time series data. By activating only a subset of expert networks for each prediction, Time-MoE significantly improves forecasting accuracy while reducing computational load and maintaining high model capacity. Similarly, TimesFM~\cite{TimesFM}, developed by Google researchers, is a decoder-only foundation model pre-trained on 100 billion real-world time points. A key architectural difference is that TimesFM predicts the next patch as a function of all past input patches in an autoregressive manner, where the predicted output patch length exceeds the input patch length, enabling efficient long-horizon forecasting.

\textbf{Incorporating exogenous variables.} 
Recent approaches have moved beyond modeling endogenous patterns alone to incorporate external factors (exogenous variables) for more comprehensive predictions. TTMs~\cite{TTMs} is the first pre-trained lightweight model that explicitly captures cross-channel dependencies and external covariates via a multi-level architecture. By integrating external signals during fine-tuning, TTMs addresses the previously overlooked role of exogenous correlations and achieves significant performance gains. Similarly, TimeXer~\cite{TimeXer} introduces a transformer-based framework with a dual attention mechanism that fuses target and exogenous time series through patch-wise self-attention on the target series combined with variate-wise cross-attention to incorporate external covariate information. In parallel, TimeCAP~\cite{TimeCAP} presents a two-stage LLM-driven framework that transforms external knowledge and events into textual descriptions. It uses one LLM agent to generate a concise textual summary of the time-series context and a second LLM that consumes this enriched summary to forecast upcoming events.

\Subsection{Motivation: Time Series Foundation Models for Clock Modeling} 


\begin{figure*}[h!]
    \centering
    \subfigure[Short Window CPU]{
        \includegraphics[width=0.22\textwidth, trim=0 0 120 12, clip]{ChronoTick/sections/figures/MAE/unsy_cpu_short.png}
        \label{subfig:cpu_short}
    }%
    \subfigure[Long Window CPU]{
        \includegraphics[width=0.22\textwidth, trim=0 0 120 13, clip]{ChronoTick/sections/figures/MAE/unsy_cpu_long.png}
        \label{subfig:cpu_long}
    }%
    \subfigure[Short Window GPU]{
        \includegraphics[width=0.22\textwidth, trim=0 0 120 23, clip]{ChronoTick/sections/figures/MAE/unsy_gpu_short.png}
        \label{subfig:gpu_short}
    }%
    \subfigure[Long Window GPU]{
        \includegraphics[width=0.30\textwidth, trim=0 0 10 24, clip]{ChronoTick/sections/figures/MAE/unsy_gpu_long.png}
        \label{subfig:gpu_long}
    }
    \caption{Comparison of time series foundation model capabilities for predicting clock drift across different hardware platforms and prediction windows.}
    \label{fig:mae-comparison}
\end{figure*}

To evaluate the feasibility of using foundation models for predictive clock synchronization, we conducted a comprehensive benchmarking study across a selection of the above mentioned model architectures and hardware configurations. This section describes our data collection methodology, experimental platform, and comparative analysis of state-of-the-art time series foundation models.

\textbf{Data collection methodology.}
We collect two categories of telemetry data to capture both timing behavior and environmental factors that may influence clock drift. Data collection was performed on the Chameleon Cloud testbed~\cite{chamelon_citation} over a continuous 24-hour period to capture diurnal patterns and thermal variations in clock behavior. For timing sources, we gather: (1) the system clock, which provides a reference for wall-clock time maintained by the operating system; (2) a high-resolution monotonic clock that provides continuous, uninterrupted elapsed time measurements independent of system clock adjustments, critical for detecting fine-grained variations and jitter; (3) direct queries to an NTP server, from which we collect the reported time, round-trip time (RTT), offset, and delay; and (4) metrics from an existing synchronization daemon (Chrony in our experiments, configured in read-only mode) including NTP time, offset, skew, and drift. For hardware metrics, we collect: current power utilization through the Intelligent Platform Management Interface (IPMI), CPU temperature and frequency, and current node load. These environmental signals enable models to correlate clock behavior with system conditions.

\textbf{Experimental platform.}
Our evaluation platform is a single-node deployment designed to support GPU-accelerated computing workloads, particularly for machine learning inference and data-driven analysis. The system runs on a virtualized environment with limited CPU capacity, featuring 2 logical processors from an Intel Xeon architecture (model 85, AVX-512 capable). Each processor operates at a base frequency of 2.00 GHz with a shared 39 MB L3 cache. The platform includes an NVIDIA Tesla T4 GPU with 16 GB of GDDR6 memory (15.36 GiB usable), running driver version 550.54.15 and CUDA 12.4 on Ubuntu 22.04.

\textbf{Evaluation methodology.}
Figure~\ref{fig:mae-comparison} present the results of evaluating selected foundation models on our collected dataset. We group results into two horizon categories: short window (predicting 5s and 10s into the future) and long window (20s, 40s, and 60s into the future). For each model, we compute and average the median absolute error (MAE) and execution time across 25 randomly selected but consistent samples from the dataset, evaluating eight context window lengths ranging from 10s to 5 minutes. We repeat each experiment on both GPU and CPU to assess hardware-specific performance characteristics.

\textbf{Short-horizon forecasting results.}
In the short-horizon analysis across both devices, the Chronos family consistently achieves the lowest MAEs. Chronos-mini delivers the best combination of accuracy (MAE of $1.22\times10^{-5}$) and speed (0.0168\,s on GPU; 0.0705\,s on CPU). Moirai achieves comparable error ($2.72\times10^{-5}$) but suffers from high CPU latency (0.824\,s), limiting its practicality without GPU acceleration. Time-MoE offers a balanced trade-off with an MAE of $1.29\times10^{-5}$ and sub-second inference times (0.179\,s on GPU; 1.062\,s on CPU). TimesFM exhibits both higher error ($6.39\times10^{-4}$) and prohibitive CPU latency (9.03\,s). MOMENT shows high error ($4.97\times10^{-4}$) but delivers ultra-fast inference (0.033\,s on GPU; 0.0008\,s on CPU), making it suitable for applications prioritizing speed over precision. Finally, TTM lacks GPU support and records the highest MAE ($2.37\times10^{-3}$), indicating limited applicability for short-horizon forecasting.

\textbf{Long-horizon forecasting results.}
In the long-horizon analysis, the Chronos family again leads in accuracy: Chronos-Base achieves the lowest MAE of $1.55\times10^{-5}$, followed by Chronos-Small ($1.92\times10^{-5}$) and Chronos-Mini ($2.14\times10^{-5}$). Among these, Chronos-Mini offers the fastest inference (0.0151\,s on GPU; 0.0709\,s on CPU). Time-MoE maintains robust performance with an MAE of $4.25\times10^{-5}$ and moderate latency (0.2370\,s on GPU; 1.2825\,s on CPU). Moirai's error increases ($3.94\times10^{-4}$), and its CPU latency remains high (0.8341\,s), limiting CPU-bound applicability despite competitive GPU performance. TimesFM continues to produce higher errors ($5.95\times10^{-4}$) and CPU inference times (9.30\,s). MOMENT again exhibits high error ($5.44\times10^{-4}$) but delivers ultra-fast inference (0.0007\,s on CPU; 0.0339\,s on GPU). TTM remains unsupported on GPUs and maintains the highest MAE ($3.54\times10^{-3}$) on CPUs.
